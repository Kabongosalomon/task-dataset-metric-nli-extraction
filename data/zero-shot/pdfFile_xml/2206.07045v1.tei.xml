<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReCo: Retrieve and Co-segment for Zero-shot Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyungin</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Coop. Medianet Innovation Center Shanghai Jiao</orgName>
								<orgName type="institution">Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReCo: Retrieve and Co-segment for Zero-shot Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation has a broad range of applications, but its real-world impact has been significantly limited by the prohibitive annotation costs necessary to enable deployment. Segmentation methods that forgo supervision can side-step these costs, but exhibit the inconvenient requirement to provide labelled examples from the target distribution to assign concept names to predictions. An alternative line of work in language-image pre-training has recently demonstrated the potential to produce models that can both assign names across large vocabularies of concepts and enable zero-shot transfer for classification, but do not demonstrate commensurate segmentation abilities. In this work, we strive to achieve a synthesis of these two approaches that combines their strengths. We leverage the retrieval abilities of one such language-image pretrained model, CLIP, to dynamically curate training sets from unlabelled images for arbitrary collections of concept names, and leverage the robust correspondences offered by modern image representations to co-segment entities among the resulting collections. The synthetic segment collections are then employed to construct a segmentation model (without requiring pixel labels) whose knowledge of concepts is inherited from the scalable pre-training process of CLIP. We demonstrate that our approach, termed Retrieve and Co-segment (ReCo) performs favourably to unsupervised segmentation approaches while inheriting the convenience of nameable predictions and zero-shot transfer. We also demonstrate ReCo's ability to generate specialist segmenters for extremely rare objects 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The objective of semantic segmentation is to partition an image into coherent regions and to assign to each region a semantic label. This task has myriad applications across domains such as medical image analysis, autonomous driving, industrial process monitoring and wildlife tracking. However, there are several key challenges that have hindered the deployment of existing semantic segmentation approaches to date: (1) Cost: collecting manual pixel-level annotations is extraordinarily expensive (e.g. 90 minutes per image for high quality labels <ref type="bibr" target="#b13">[14]</ref>), limiting the use of fully-supervised approaches; (2) Flexibility: supervised approaches have typically been trained with limited lists of pre-defined categories and lack the ability to recognise rare or novel categories (such as those described by free-form text); (3) Complexity of deployment: unsupervised segmentation methods have dramatically reduced annotation costs, but still exhibit the inconvenience of requiring labelled examples to assign names to predictions; (4) Data access: many existing approaches (both supervised and unsupervised) are trained on the target data distribution, requiring both that this distribution is known at training time (limiting flexibility) and that this data is accessible, which may not be the case for legal/ethical reasons (e.g. medical image data).  <ref type="bibr" target="#b5">[6]</ref>, indicating promising results in the challenging zero-shot transfer setting. We also illustrate results with unsupervised adaptation to the target distribution (ReCo+), trading flexibility for improved segmentation quality.</p><p>There is a rich body of semantic segmentation literature that proposes solutions to subsets of these challenges, but to our knowledge no existing work addresses their full combination. To tackle all four challenges, we draw inspiration from two lines of recent research. The first line of research has shown that modern deep architectures (most notably vision transformers <ref type="bibr" target="#b16">[17]</ref>) develop the ability to infer the spatial extent of objects without pixel supervision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b53">54]</ref> and moreover can establish semantically consistent correspondences across images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>. The second line of research has demonstrated that large-scale visual-language pre-training <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b34">35]</ref> produces models that possess both a large vocabulary and remarkable zero-shot transfer potential-the ability to recognise concepts on target datasets without access to the target data distribution during training.</p><p>In this work, we target a synthesis of these two approaches that draws on their respective strengths for the semantic segmentation task. We first employ CLIP <ref type="bibr" target="#b71">[72]</ref> to curate training sets from unlabelled images for any desired list of concepts. We then exploit the robust semantic correspondences offered by modern vision backbones to co-segment concepts among the resulting curated collections. Finally, we use these co-segmented concepts to construct a segmentation model for the given concepts, without requiring training. This framework, which we term Retrieve and Co-segment (ReCo), performs favourably to existing unsupervised segmentation approaches while preserving the benefits of a wide vocabulary, named predictions and zero-shot transfer exemplified by CLIP.</p><p>Our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We propose the ReCo framework, enabling open vocabulary semantic segmentation, without pixel annotations or the need to provide labelled examples from the target domain thereby enabling zero-shot transfer; <ref type="bibr" target="#b1">(2)</ref> We compare our approach to prior work on standard semantic segmentation benchmarks (COCO-Stuff <ref type="bibr" target="#b5">[6]</ref>, Cityscapes <ref type="bibr" target="#b13">[14]</ref>, and KITTI-STEP <ref type="bibr" target="#b87">[88]</ref>), and further illustrate the ability of ReCo to segment rare concepts beyond these benchmarks; (3) For cases when the target image distribution is available, we demonstrate that a simple extension of our approach, ReCo+, can exploit this data access via unsupervised adaptation to bring further gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our work is connected to several themes in the literature, which we describe next.</p><p>Unsupervised semantic segmentation. There has been considerable recent progress towards unsupervised semantic segmentation with deep neural networks by leveraging ideas from self-supervised learning. Learning objectives based on maximising mutual information between views <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b59">60]</ref>, metric learning across proposals <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b82">83]</ref>, equivariance and invariance constraints <ref type="bibr" target="#b11">[12]</ref>, distillation of self-supervised feature correspondences <ref type="bibr" target="#b23">[24]</ref> and cross modal cues (vision and LiDAR) <ref type="bibr" target="#b85">[86]</ref> have all shown their potential for this task. One drawback of these approaches is a reliance on either nearest neighbour search on a held-out set with pixel level annotations or the Hungarian algorithm <ref type="bibr" target="#b44">[45]</ref> (optimally matching predictions against ground-truth semantic masks) to produce segments with names. By contrast, ReCo is independent of labelled examples during both training and inference, simplifying deployment.</p><p>Weakly-supervised semantic segmentation. To reduce annotation costs, a number of works have explored weaker cues such eye tracking <ref type="bibr" target="#b62">[63]</ref>, pointing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b10">11]</ref>, sparse pixel labels <ref type="bibr" target="#b78">[79]</ref>, scribbles <ref type="bibr" target="#b48">[49]</ref>, web-queried samples <ref type="bibr" target="#b35">[36]</ref>, boxes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b80">81]</ref>, extreme clicks <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b52">53]</ref>, image-level labels <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b69">70]</ref> and free-form text <ref type="bibr" target="#b90">[91]</ref>. However, such approaches still require the weak annotation to be attached to the data used to train the segmentation model-by contrast, ReCo can in principle train a segmenter from any unlabelled collection of images. ReCo also bears a conceptual similarity with webly-supervised approaches <ref type="bibr" target="#b18">[19]</ref> for semantic segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b76">77]</ref>. These methods employ an image search engine such as Google to provide training samples for concepts. However, their flexibility is limited (this approach cannot be applied to private or commercially sensitive data, for example) and they lack the ability to leverage the knowledge of the search engine itself during inference (we demonstrate that integrating the vision-language model into the inference procedure brings significant gains in performance).</p><p>Zero-shot semantic segmentation with pretrained language/vision-language embeddings. A diverse body of work has explored zero-shot semantic segmentation, broadly defined as the task of segmenting categories for which no labels were provided during training (often termed zerolabel semantic segmentation <ref type="bibr" target="#b89">[90]</ref>). The key idea underlying many of these works is to leverage relationships encoded in pretrained word embeddings (such as word2vec <ref type="bibr" target="#b54">[55]</ref> or GloVe <ref type="bibr" target="#b67">[68]</ref>) to enable generalisation to unseen categories <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b64">65]</ref>. More recent work has sought to leverage the vision-language embedding space learned by CLIP <ref type="bibr" target="#b71">[72]</ref> to improve dense prediction in various settings <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b91">92]</ref>. We adopt a variation of DenseCLIP <ref type="bibr" target="#b96">[97]</ref> as a component of our framework. Additionally, differently from the above, we pursue the formulation of zero-shot transfer popularised by CLIP <ref type="bibr" target="#b71">[72]</ref> which evaluates performance on unseen datasets rather than unseen categories. Consequently, unlike these works our model has no access to either labelled or unlabelled examples from the target data distribution (or pixel-level labels from the source distribution, as investigated by <ref type="bibr" target="#b91">[92]</ref>). We note one exception: in addition to their primary zero-shot evaluations, DenseCLIP <ref type="bibr" target="#b96">[97]</ref> also report an "annotation-free" evaluation without access to the target dataset-we compare our approach with theirs under an equivalent setting.</p><p>Large-vocabulary/rare concept segmenters. To scale up the number of concepts that can be segmented by a model several strategies based on captions <ref type="bibr" target="#b20">[21]</ref>, grounded text descriptions <ref type="bibr" target="#b38">[39]</ref> and annotation transfer <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> have been explored. In a differing direction, various losses and incremental learning techniques have been employed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b26">27]</ref> to better segment rare concepts. Unlike ReCo, however, each of the above approaches still requires costly pixel-level annotations.</p><p>Co-segmentation which aims to segment common regions among a collection of images, has been widely studied with classical computer vision approaches <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b84">85]</ref>. The topic has been revisited with deep learning using shared encoder networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b50">51]</ref>, iterative refinement <ref type="bibr" target="#b92">[93]</ref> and weak (class-label) supervision <ref type="bibr" target="#b27">[28]</ref>. While ReCo can in principle make use of any unsupervised co-segmentation algorithm, we find that a simple correlation strategy works well, and thus we adopt it for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce the ReCo framework (Sec. 3.1) which enables zero-shot transfer for semantic segmentation with arbitrary categories. We describe a language-based gating mechanism to enhance segmentation quality (Sec. 3.2), and a pseudo-labelling scheme, ReCo+, that adapts to a target distribution using predictions from ReCo (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieve and Co-segment (ReCo)</head><p>The inputs to ReCo are a collection of unlabelled images, and a list of text descriptions of concepts to be segmented. Through a combination of image retrieval and co-segmentation across the image collection, ReCo constructs a segmenter for the given concepts on the fly. During inference, this segmenter is applied without fine-tuning to images from a target distribution of interest, thus supporting the zero-shot transfer. The interaction of these three stages is illustrated in <ref type="figure">Fig. 2</ref> and discussed in more detail below.</p><p>Curating exemplars through image retrieval. The first hypothesis underpinning our approach is that it is possible to construct a sufficiently large and diverse unlabelled image collection that contains examples (or closely related examples) of any concept we wish to segment. We consider this hypothesis to be reasonable in light of the fact that recent research has produced a number of diverse datasets spanning billions of images <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b75">76]</ref>.</p><p>To make use of this hypothesis for segmentation, we employ a vision-language model to curate an archive of images for each concept of interest. In this work, we use CLIP <ref type="bibr" target="#b71">[72]</ref>, a model comprising  (2) Co-segmentation an image encoder and a text encoder that enables efficient image retrieval for free-form text queries. In more detail, for a given text query describing a concept c the CLIP text encoder ? T produces an embedding ? T (c) ? R e which can be compared against the embeddings produced by the image encoder ? I for each image x in the unlabelled collection U: {? I (x) ? R e : x ? U}. We then build an archive from the k images that form nearest neighbours for each concept c. While a more sophisticated strategy (for instance, adjusting the archive size according to concept difficulty or prevalence) is possible with ReCo, we found that this simple approach worked well, and so we adopt it here. Note that, thanks to mature approximate nearest neighbour search techniques <ref type="bibr" target="#b36">[37]</ref>, this archive construction process can be readily applied to a collection U containing billions of images.</p><formula xml:id="formula_0">F 1 ? F 1 Language-guided co-segmentation &amp; context elimination F 1 ? F k CLIP Image encoder Dense CLIP F 1 F 2 ? F k F 1 ? F 2 ? F 2 ? F 1 F 2 ? F 2 ? F 2 ? F k ? ? ? F k ? F 1 F k ? F 2 ? F k ? F k lionfish ? ? ? A photo</formula><p>Co-segmentation with seed pixels. The second hypothesis underpinning our approach is that modern vision-language models such as CLIP are capable of constructing archives of high purity (i.e. a high ratio of the archive images contain the concept of interest) provided that such exemplars exist in the underlying image collection U (our first hypothesis). Our second hypothesis, which we validate through experiments in Sec. 4, enables the use of co-segmentation to obtain concept segments.</p><p>Concretely, given the k images comprising the archive for category c constructed by CLIP, we aim to segment regions corresponding to concepts that re-occur across the images. Since we expect the archive to be of high purity, and since modern visual backbones provide consistent semantic correspondences across images <ref type="bibr" target="#b23">[24]</ref>, we adopt a strategy of first identifying a seed pixel in each image that we are confident belongs to the target category, c. Intuitively, good seed pixels are ones that have close neighbours (i.e. strong support) across each of the archive images, since we assume that: (i) the target concept is common to all images in the archive, (ii) our visual backbone will produce consistent features for pixels belonging to the same concept. We then use these seed pixels to construct a reference embedding that can be used to classify pixels belonging to c.</p><p>In detail, we first extract dense features for each image in the archive using a pre-trained image encoder ? I :</p><formula xml:id="formula_1">{F 1 , . . . , F k } = {? I (x 1 ), . . . , ? I (x k )}<label>(1)</label></formula><p>where F i ? R d?h?w denotes (spatially) dense features for the i th image with height h, width w and d channels. Each such feature is L2-normalised along its channel dimension. <ref type="bibr" target="#b1">2</ref> Note that any image encoder ? I can be employed here (it need not be CLIP).</p><p>We identify seed pixels in four steps: First, we construct an adjacency matrix A khw?khw among all features in the archive. Here, each of the k ? k submatrices, A ij ? R hw?hw , encodes pairwise similarities between features from image i and image j for i, j ? {1, ..., k}. Second, we aim to identify, for each pixel in the archive, the similarity of its nearest neighbour among each of the k images. To do so, we apply a max operator along the columns of each submatrix (reducing the hw colums of each submatrix to 1 and reducing the overall adjacency matrix dimensions to khw ? k). Third, we aim to identify the average support that each pixel has among the k archive images. For this, we apply a mean operator over the columns of each submatrix such that each row of the resulting khw ? 1 matrix encodes the mean maximum similarity across k images. Finally, we identify the seed pixel locations by applying an argmax operator to each of the k submatrices of size hw ? 1, yielding the spatial indices of the features in each image with highest average maximum similarity across the archive.</p><p>To construct a classifier for concept c, we simply average the embeddings of the k seed pixels from its archive and L2-normalise the resultant vector to produce the reference embedding f c ? R d .</p><p>Inference. To localise instances of the category in a new image x new , we first compute the dotproduct between f c and the L2-normalised dense features F new from the new image, and pass the result through a sigmoid:</p><formula xml:id="formula_2">P c new = ?(f c ? F new ) ? [0, 1] h?w (2) where P c</formula><p>new denotes an initial estimate of the probability map corresponding to category c. To refine this probability map, we draw inspiration from recent work <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b72">73]</ref> showing that dense visual CLIP features can be usefully correlated against a given CLIP text embedding. Concretely, we employ the DenseCLIP mechanism of Zhou et al. <ref type="bibr" target="#b96">[97]</ref> to highlight regions of the input that are salient for the target category c as follows. For image x new , we extract features V new ? R ev?h?w from the last self-attention layer values of the CLIP image encoder (here e v denotes the value feature dimension), project the features into the joint space R e with the CLIP image encoder's final linear projection and L2-normalise the result. We then compute the CLIP text embedding ? T (c) ? R e for the target concept c and L2-normalise it before producing a saliency map S c new ? [0, 1] h?w by applying the text embedding as a 1 ? 1 convolution to the visual features and applying a sigmoid activation function to the result. Note that our use of a sigmoid activation function differs from the softmax used by <ref type="bibr" target="#b96">[97]</ref>, since we process each concept independently.</p><p>Our final probability map for category c is produced by the Hadamard product of these estimates:</p><formula xml:id="formula_3">P c new = P c new S c new<label>(3)</label></formula><p>In case of multiple categories predictions, we concatenate all the category prediction maps and apply argmax to the category dimension. As a post-processing step, we also experiment with the effect of applying a CRF <ref type="bibr" target="#b43">[44]</ref> as a simple post-processing step (similarly to <ref type="bibr" target="#b23">[24]</ref>). Pseudocode for ReCo can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language-guided co-segmentation and context elimination</head><p>Even when an archive has high purity, co-segmenting the target concept c from the collection of images can be challenging due to the potential presence of distractor categoriesc that often co-occur with c. For example, cars often co-occur with roads and likewise aeroplanes with sky. To minimise the risk of the co-segmentation algorithm anchoring on unintended categories, we introduce two mechanisms into the co-segmentation procedure, namely, language-guided co-segmentation and context elimination.</p><p>Language-guided co-segmentation. Specifically, for each x i in the archive, we first compute the corresponding saliency map S c i ? [0, 1] h?w using DenseCLIP. We then vectorise this map and use it to filter the co-segmentation similarities by replacing each row of A j,i with the Hadamard product of itself with vec(S c i ) ? [0, 1] 1?hw (where vec(?) denotes vectorisation) for each submatrix j = {1, ..., k}. Intuitively, this serves to act as a gate that preserves similarities only for pixels identified by CLIP as salient for the target concept.</p><p>Context elimination. To reduce the influence of widely co-occurring distractors, we select a few common background categoriesc that frequently appear in images (e.g., tree, sky, road) and compute their reference embeddings fc as described in the seeding procedure of Sec. 3.1. We then use the resulting attention maps Pc i produced by these reference embeddings to suppress regions of common background context in the subsequent co-segmentation processes for different categories. This is done similarly as above, by vectorising and replacing each row of A j,i with the Hadamard product of itself with vec(1 ? Pc i ) ? [0, 1] 1?hw for each submatrix j = {1, ..., k}. If a target concept c is identical to one of the common background categoriesc, we replace f c with the corresponding fc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ReCo+: Fine-tuning ReCo with pseudo-labels</head><p>The ReCo framework outlined above requires no training and no access to the target distribution. However, it is possible to consider a simple extension to this idea when access to the target image distribution is available. Our proposed extension, ReCo+, simply trains a segmentation architecture (e.g., DeepLabv3+ <ref type="bibr" target="#b9">[10]</ref>) with the segmentation masks from ReCo as pseudo-masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implementation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to state-of-the-art methods for unsupervised semantic segmentation (Sec. 4.4). Finally, we showcase our model's ability to segment rare-category objects (Sec. 4.5) and discuss limitations (Sec. 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For our ablation study, we use the ImageNet1K <ref type="bibr" target="#b15">[16]</ref> validation set to curate archive for concepts of interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation performance in the zero-shot transfer setting, we use the PASCAL-Context <ref type="bibr" target="#b56">[57]</ref> validation set for evaluation, which has 5,104 images of 59 categories excluding the background class.</p><p>To compare with previous unsupervised segmentation methods, we use the ImageNet1K training set to construct the reference embedding for the concept we wish to segment. The dataset has 1K classes with 1.2M images. We evaluate on standard benchmarks including the Cityscapes <ref type="bibr" target="#b13">[14]</ref> validation split, which has 500 urban scene images of 27 categories, KITTI-STEP <ref type="bibr" target="#b87">[88]</ref> validation set, which is composed of 2,981 urban scene images of 19 categories, and COCO-Stuff <ref type="bibr" target="#b5">[6]</ref> validation split, which has 4,172 images of 171 low-level thing and stuff categories excluding background class. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, we use the 27 mid-level categories for evaluation. For unsupervised adaptation with ReCo+ (Sec. 3.1), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975 images, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset which has 9,000 images for each respective benchmark. We emphasize that no ground-truth labels are used for training.</p><p>Finally, to demonstrate our model's ability to segment rare concepts, we use the LAION-5B dataset <ref type="bibr" target="#b75">[76]</ref> with 5 billion images as a large collection of images that we expect to satisfy our first hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use the FireNet dataset <ref type="bibr" target="#b61">[62]</ref> which has 1,452 images spanning rare fire safety-related classes. For our experiment, we select the fire extinguisher class as an example of a concept that is important but rare in vision datasets (it is not contained in ImageNet1K <ref type="bibr" target="#b74">[75]</ref>, for example) and evaluate ReCo on 263 images containing at least one instance of the category. As a further proof of concept, we also demonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear in WordNet <ref type="bibr" target="#b55">[56]</ref>, or any labelled vision datasets that we are aware of).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Here, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics. Our implementation is based on the PyTorch library <ref type="bibr" target="#b65">[66]</ref> and will be made publicly available.  <ref type="figure">Figure 3</ref>: Ablation studies. Left: Image retrieval performance of different CLIP models on the ImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly strongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance on PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We observe a general trend towards improved performance with increasing archive size for all encoders.</p><p>random color jittering and Gaussian blurring. We use the Adam optimiser <ref type="bibr" target="#b42">[43]</ref> with an initial learning rate of 5 ? 10 ?4 and a weight decay of 2 ? 10 ?4 with the Poly learning rate schedule as in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b9">10]</ref>. Training consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single 24GB NVIDIA P40 GPU.</p><p>Inference. Whenever DenseCLIP is employed, we use the ResNet50x16 model (following <ref type="bibr" target="#b96">[97]</ref>) to construct a saliency map for each image. For the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the input images to 320?320 pixels as in <ref type="bibr" target="#b23">[24]</ref>. For the KITTI-STEP validation set, we use the original resolution of each image as in <ref type="bibr" target="#b40">[41]</ref>. For the FireNet benchmark, we resize the shorter side of images to 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap with probability of 0.5.</p><p>Evaluation metrics. Following the common practice <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>, we report pixel accuracy (Acc.) and mean intersection-over-union (mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>Ability of CLIP to curate archives. We begin by assessing the validity of our second hypothesisnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end, we evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set when constructing different archive sizes. In detail, for each archive size, k, we compute the precision of the top-k retrieved images based on whether the the ground-truth image-labels match the query text. As can be seen in <ref type="figure">Fig. 3</ref> (left), all CLIP models achieve solid retrieval performance, suggesting their potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs best, we employ this as our retrieval model in the remaining experiments.  <ref type="table">Table 1</ref>: Influence of ReCo components for zero-shot transfer on PASCAL-Context <ref type="bibr" target="#b56">[57]</ref>. We observe that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC), Context elimination (CE), and CRF <ref type="bibr" target="#b43">[44]</ref> post-processing each contribute to improved performance. All comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for archive curation. Influence of archive size and visual encoder used for co-segmentation. In <ref type="figure">Fig. 3 (right)</ref> we illustrate the effect of using different pre-trained architectures, e.g. MoCov2 <ref type="bibr" target="#b24">[25]</ref>, DINO <ref type="bibr" target="#b6">[7]</ref>, CLIP <ref type="bibr" target="#b71">[72]</ref>, DeiT-SIN <ref type="bibr" target="#b58">[59]</ref>, as the archive size (and thus the number of images used for co-segmentation) changes. The y-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context benchmark. We observe that using larger archives tends to improve performance (likely due to their reasonably high purity) albeit non-monotonically, and that features from DeiT-SIN  <ref type="table">Table 2</ref>: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle), and KITTI-STEP (right) validation sets. ?Models trained on Waymo Open <ref type="bibr" target="#b81">[82]</ref> and evaluated at the original resolution (reported from <ref type="bibr" target="#b85">[86]</ref>). The best score for each metric under each protocol is highlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under zero-shot transfer and unsupervised adaptation protocols, respectively. perform best. We therefore use DeiT-SIN with an archive size of k=50 for the remaining experiments otherwise stated.</p><p>Influence of ReCo framework components. We next assess the effect of employing DenseCLIP during inference, the language-guided co-segmentation and context elimination components of ReCo which seek to improve the quality of co-segmentation achieved across each archive to boost downstream segmentation performance. When applying context elimination, we select tree, sky, building, road, and person as common background concepts appearing in natural images to be suppressed. In Tab. 1, we show the effect of three of these strategies, together with the effect of applying a CRF <ref type="bibr" target="#b43">[44]</ref> as post-processing. We observe that integrating DenseCLIP into the inference procedure brings a significant gain in performance which we believe is driven by the notable robustness of CLIP features under zero-shot transfer <ref type="bibr" target="#b71">[72]</ref>. In addition, language-guided co-segmentation and context elimination further boost co-segmentation performance, while the CRF brings a small gain. We therefore use each of these strategies (including CRF post-processing) in the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to state-of-the-art unsupervised methods</head><p>We compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models on standard benchmarks, including COCO-Stuff <ref type="bibr" target="#b5">[6]</ref>, Cityscapes <ref type="bibr" target="#b13">[14]</ref> and KITTI-STEP <ref type="bibr" target="#b87">[88]</ref> under both zero-shot transfer and unsupervised adaptation (training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level categories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category is "outdoor objects", which may include many low-level categories beyond the target hierarchy). To avoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level categories and then merge the predictions into the mid-level categories. Additionally, we rephrase two category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes and KITTI-STEP based on the descriptions found in <ref type="bibr" target="#b13">[14]</ref>. A detailed discussion can be found in Appendix C.2.</p><p>As shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer. Under an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin on the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy compared to <ref type="bibr" target="#b23">[24]</ref> but a considerably higher mIoU. In <ref type="figure" target="#fig_0">Fig. 1</ref>, we visualise the sample predictions of our models on the COCO-Stuff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Segmenting rare concepts</head><p>By virtue of inheriting CLIP's diverse knowledge of nameable visual concepts, ReCo exhibits the ability to segment rare categories. We first demonstrate this ability for fire extinguisher objects, which have important fire-safety implications but seldom appear in popular semantic segmentation benchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as described in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9) <ref type="figure">Figure 4</ref>: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mechanism. We show selected samples from ReCo archives for each concept, together with their cosegmentations. In each cases, ReCo successfully identifies the regions associated with the concept.</p><p>metrics. In <ref type="figure">Fig. 4 (left)</ref> we visualise the co-segmentation produced by ReCo across sample images container fire extinguishers. As an additional demonstration <ref type="figure">(Fig. 4, right)</ref>, we also show cosegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases, we observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Limitations</head><p>Our work has several limitations: (1) There may be cases for which our first hypothesis does not hold-concepts so rare that they do not appear in billion-image scale datasets. In such cases, e.g. "a purple elephant with square orange feet wearing an inverted cowboy hat in front of the Doge's Palace", ReCo will struggle. (2) We make use of CLIP (a computationally heavy model) during inference. Future work could address this with pruning and hardware optimisation strategies. <ref type="formula" target="#formula_3">(3)</ref> We employ ImageNet as the unlabelled image collection for our primary investigations, which is known to exhibit an object-centric bias. While qualitative results suggest that ReCo can learn from extremely diverse data <ref type="bibr" target="#b75">[76]</ref>, a more comprehensive empirical evaluation would strengthen this result. (4) Models like CLIP are expensive to train and are consequently updated infrequently. If a new concept emerges (e.g. a new product) after CLIP was trained, ReCo will be unable to build an archive to enable co-segmentation for this concept. (5) Although our work is zero-shot in the sense that it is not trained on the target distribution, we still use of a subset of the target data distribution (primarily through ablations on PASCAL-Context) to guide the development of our method. As a result, our design likely contains choices that subtly favour performance on the evaluation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduced the ReCo framework for semantic segmentation zero-shot transfer. By drawing on the strengths of large-scale language-image pre-training and modern visual backbones, ReCo attains the ability to segment rare concepts and to directly assign names to concepts without labelled examples from the target distribution. In addition, experimental comparisons demonstrate that ReCo strongly outperforms existing zero-shot transfer approaches that forgo pixel supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>GS is supported by AI Factory, Inc. in Korea. GS would like to thank Yash Bhalgat, Tengda Han, Charig Yang, Guanqi Zhan, and Chuhan Zhang for proof-reading. SA would like to acknowledge the support of Z. Novak and N. Novak in enabling his contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In this supplementary material, we first discuss broader impact, the role of supervisory signals for ReCo and alternative approaches, and the datasets used in our work (Appendix A). Next we provide further details of experiments conducted in the main paper together with hyperparameters for training ReCo+ in Appendix B. We then report additional ablations investigating the influence of common category selection for context elimination and reducing ambiguity in category name in Appendix C. Finally, we provide additional qualitative results on COCO-Stuff in Sec. D to illustrate both successful and failure cases for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion of broader impact, supervision and data A.1 Broader Impact</head><p>Semantic segmentation is a dual use technology. It enables many applications with the potential for significant positive societal impact across domains in medical imaging, wildlife monitoring, improved fault detection in manufacturing processes etc. However, it is also vulnerable to abuse: it may enable unlawful surveillance or invasions of privacy, for example. By removing the requirement to collect pixel masks for concepts of interests, ReCo lowers the barrier to entry for any individual that wish to make practical use of segmentation, but makes no distinction on the ethical implications of the use case, positive or negative.</p><p>ReCo makes use of large-scale, unlabelled image collections. By their nature, such images are subject to minimal curation and sanitisation, and thus may contain not only biases across demographics, but also content that does not align with the ethical values of the user. Consequently, we emphasise that ReCo represents a research proof of concept that is not appropriate for real-world usage without extensive additional analysis of the specific deployment context in which it will be used and, in particular, safeguards to moderate the archive curation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Supervisory signals for ReCo and prior work</head><p>In the main paper, we compare to previous methods that are typically described as unsupervised. In practice, however, many methods (including ours) either implicitly or explicitly engage humans in the data curation process at some stage.</p><p>Supervision used by ReCo. (1) Similarly to prior work, our experiments make use of datasets constructed from photographs taken by humans for both training and evaluation. These photographs are naturally biased towards content that humans find interesting and are typically well-framed (with a concept of interest featuring prominently) or taken from a vantage point that offers a convenient scene overview (e.g. a roof-mounted camera on a vehicle driving on public roads). By training and evaluating on such data, our experimental results likely provide an optimistic assessment of performance when contrasted with other distributions (e.g. the video feed received by an autonomous mobile robot).</p><p>(2) For our comparisons to prior work, we use the ImageNet training subset without labels to curate archives. However, in practice, this dataset is not free from human involvement: it was curated by human workers who were asked to verify that each image contains a particular synset category (see <ref type="bibr" target="#b15">[16]</ref> for a discussion of the collection process). While annotators were encouraged to select images regardless of occlusions, the number of objects and clutter in the scene, this process nevertheless produced a relatively clean dataset with fairly object-centric images. (3) Several of our experiments make use of DeiT-SIN <ref type="bibr" target="#b58">[59]</ref>, which is trained on stylised ImageNet <ref type="bibr" target="#b19">[20]</ref> with labels. We don't believe that this supervision is critical, since in <ref type="figure">Figure 3</ref> of the main paper, we showed that ResNet50-MoCov2 <ref type="bibr" target="#b24">[25]</ref> which does not use labels achieves similar performance (less than 1 mIoU difference on PASCAL-Context <ref type="bibr" target="#b56">[57]</ref>). Moreover, we note that previous unsupervised methods to which we compare (e.g. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b11">12]</ref>) initialise their approach from supervised ImageNet training with the convention that unsupervised in this context denotes the fact that no pixel-level supervision is used. (4) By using CLIP <ref type="bibr" target="#b71">[72]</ref>, we also make use of a different kind of supervision, namely images paired with alt-text scraped from the web. Empirically, this data source has been shown to be extremely scalable and to enable generalisation to very large numbers of concepts <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b68">69]</ref>. Nevertheless, the creation of the original alt-text image descriptions (mostly) derives from a human source, and therefore provides a form of human supervision. In contrast to using ImageNet classification labels, this source of supervision is indispensable to ReCo.</p><p>We believe that the key factors to be considered when discussing the question of supervision are scalability and generalisation. We are typically not interested in unsupervised methods for their own sake, but rather because they offer the ability to cheaply scale up machine learning to larger training data sets that improve performance, and to build methods that go beyond the functionality afforded by labelled datasets (e.g. new classes, new tasks etc.). Subject to the caveats (e.g. human photographer bias) outlined above, we believe ReCo has the flexibility to scale up far beyond the experimental comparisons conducted in this work without requiring any changes to the underlying framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Discussion of consent in used datasets</head><p>In this work, we work primarily with widely used Computer Vision benchmarks: ImageNet <ref type="bibr" target="#b15">[16]</ref>, PASCAL-Context <ref type="bibr" target="#b56">[57]</ref>, Cityscapes <ref type="bibr" target="#b13">[14]</ref>, COCO-Stuff <ref type="bibr" target="#b5">[6]</ref>, KITTI-STEP <ref type="bibr" target="#b87">[88]</ref>. For these datasets, we do not conduct an independent investigation of consent beyond the considerations of the authors that released these datasets. For our final exploratory studies which make use of LAION-5B <ref type="bibr" target="#b75">[76]</ref>, we manually verified that no humans were present in the archives that were curated by ReCo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Discussion on whether data contains personally identifiable information or offensive content</head><p>We do not release any data as part of this work. By working with widely used Computer Vision benchmarks, we also restrict ourselves to imagery that is available in the public domain. We therefore believe that the risk that our work builds on harmful content or contributes to the leakage of personal information is low.</p><p>One exception to this is our use of the LAION-5B dataset for qualitative studies. We manually verified that no personally identifiable information or harmful content (as judged by the authors) was present in the archives curated by ReCo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Dataset licenses</head><p>Here we describe the terms/licenses of datasets used in our paper. For images in PASCAL-Context and COCO dataset, we comply with the Flickr Terms of Use and the Creative Commons Attribution 4.0 License for the COCO-Stuff annotations. For Cityscapes and ImageNet1K, we follow the terms stated on their official website 3 and the Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0) licence for KITTI-STEP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment details</head><p>Here we provide pseudocode for ReCo and details of experiments conducted in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Pseudocode for ReCo</head><p>In Alg. 1, we describe the pseudocode for the core of ReCo (to maintain readability, language-guided co-segmentation and context elimination are omitted since these follow a similar structure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Prompt engineering</head><p>To obtain the text embedding for a concept, we ensemble the textual features from 85 templates, e.g., "a photo of the {concept}" and "there is a {concept} in the scene" following <ref type="bibr" target="#b96">[97]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Details of ablation study to assess CLIP retrieval performance</head><p>We observe that two of the ImageNet1K class labels are not unique-they occur twice with different meanings (e.g., "crane" is used to represent both bird and machine), which makes retrieval inference and evaluation ambiguous. Therefore, we exclude those classes and use the remaining 996 categories for the experimental results reported in <ref type="figure">Fig. 3 (left)</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Hyperparameters for ReCo+ training</head><p>As described in Sec. 4.2 (in the main paper), we adopt DeepLabv3+ 4 <ref type="bibr" target="#b9">[10]</ref> with ResNet101 encoder <ref type="bibr" target="#b25">[26]</ref> for ReCo+ and train the network with standard data augmentations such as random scaling and horizontal flip following <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b78">79]</ref>. In detail, for geometric transformations, we use random scaling with a range of [0.5, 2.0], random crop with a crop size 320?320 pixels, and random horizontal flip with a probability of 0.5. For the photometric augmentations, we apply colour jittering <ref type="bibr" target="#b4">5</ref>   <ref type="table">Table 3</ref>: Effect of context category choices and reducing label ambiguity. Left: We find that suppressing 5 frequently appearing categories brings performance gain on PASCAL-Context <ref type="bibr" target="#b56">[57]</ref>.</p><p>Right: We observe that specifying the meaning of a class more concretely helps segmentation performance of ReCo on Cityscapes <ref type="bibr" target="#b13">[14]</ref>.</p><p>also employ Gaussian blurring with a kernel size of 10% of min(H, W ) where min(H, W ) returns the length of the shorter side of an image.</p><p>C Additional ablation studies C.1 Choices of context categories</p><p>As described in Sec. 3.2 (main paper), we propose to suppress the commonly appearing categories, e.g., sky, which co-occur with other classes, e.g., aeroplanes. To achieve this, we manually pick 5 frequently appearing classes in PASCAL-Context dataset <ref type="bibr" target="#b56">[57]</ref> and investigate the effect of different combinations of such categories. In Tab. 3 (left) we observe that suppressing the tree and sky categories yields a notable performance gain, while eliminating all five categories performs best. For this reason, we apply the context elimination strategy with these five categories in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Category name rephrasing to reduce ambiguity</head><p>We observe that it is important to specify a concept concretely to obtain retrieved images exhibiting similar visual appearance. For instance, in Cityscapes dataset, parking and vegetation can be rephrased to less ambiguous concepts parking lot and tree respectively based on their descriptions 6 in the paper accompanying the dataset <ref type="bibr" target="#b13">[14]</ref>. As can be seen in Tab. 3 (right), ReCo gains benefits in performance on Cityscapes by replacing the category names with less abstract concepts. This sensitivity is a consequence of our co-segmentation algorithm, which locates pixels that share similar visual features across multiple images. Thus we use the rephrased label names throughout the experiments in the paper.</p><p>In addition to the limitations listed in the main paper, this dependence on concrete/specific concept names can be considered a limitation of our approach (albeit one that is readily mitigated). However, we believe it is a reasonable requirement for methods that operate in the zero-shot transfer setting. Unlike fine-tuning methods that learn to associate abstract text descriptions to visual concepts by seeing examples from the target distribution, ReCo relies entirely on an adequate text description to disambiguate the concept. Since many computer vision datasets have been constructed with training and testing splits with the assumption that methods would make use of the training set, we believe it is probable that category names were not designed to be uniquely descriptive (hence the use of "parking" as a category in Cityscapes, which could be either a verb or a noun). Indeed, there may have been little perceived need to construct unambiguous category names when examples from the training set implicitly provide disambiguation of the concept. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional visualisations</head><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we show more visualisation samples on COCO-Stuff benchmark. Successful and failure cases are shown on the left and right, respectively. We note that ReCo tends to fail in predicting small objects, e.g., people in the bus, and so does ReCo+ which is trained on the ReCo's predictions as pseudo-labels. We conjecture that this is related to the stride of the image encoder used for ReCo, which is 16?16 for the case of DeiT-S/16-SIN <ref type="bibr" target="#b58">[59]</ref>. It could therefore potentially be improved by using an encoder with a smaller stride at the cost of increased computational burden.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We propose ReCo, a new framework for semantic segmentation zero-shot transfer without pixel supervision. The figure depicts ReCo segmentations for COCO-Stuff</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode for the core of ReCo (using PyTorch-like syntax) Input. a CLIP image encoder ? I , a CLIP text encoder ? T , an image encoder ? I , an image collection U, a concept c, the number of co-segmented images k Output. a reference image embedding ref_emb and a prediction of the concept c in a new image# retrieve images image_emb = l2_normalize(? I (U), dim=1) # NxC text_emb = l2_normalize(? T (c), dim=0) # C scores = mm(image_emb, text_emb) # N indices = argmax(scores)[:k] # k images = U[indices] # kx3xHxW # co-segment F = l2_normalize(? I (images), dim=1) # kxCxhxw F_flat = F.permute(1,0,2,3).view(C,k*h*w) # Cxkhw A = mm(F_flat.T, F_flat) # adjacency matrix, khwxkhw grid = zeros((k*h*w, k)) start_col = 0 # start column index for i in range(k): end_col = start_col + h*w # end column index grid[:,i] = max(A[:,start_col:start_col+end_col], dim=1) start_col = end_col avg_grid = mean(grid, dim=1) # khw seed_features = [] start_row = 0 # start row index for i in range(k): end_row = start_row + h*w # end row index index_1d = argmax(avg_grid[start_row:end_row]) start_row = end_row index_2d = [index_1d//w,index_1d%w] seed_features.append(F[i,:,index_2d[0],index_2d[1]]) seed_features = stack(seed_features, dim=0) # k?C ref_emb = l2_normalize(seed_features.mean(dim=0), dim=0) # C # inference F_new = l2_normalize(? I (new_image), dim=0) # Cxhxw prediction = sigmoid(mm(ref_emb, F_new)) # hxw mm:matrix multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Additional visualisations on COCO-Stuff. Left: Successful cases. Right: Typical failure cases. White pixels denote ignored regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>with 0.8, 0.8, 0.8, 0.2, and 0.8 for brightness, contrast, saturation, hue and probability parameters respectively. We</figDesc><table><row><cell>tree</cell><cell>sky</cell><cell>building</cell><cell>road</cell><cell>person</cell><cell>mIoU 5.7</cell><cell>parking ?parking lot</cell><cell>vegetation mIoU ?tree</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10.9 12.0 10.8 11.4 12.3</cell><cell></cell><cell>15.4 16.0 18.6 19.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Project page: https://www.robots.ox.ac.uk/~vgg/research/reco Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For clarity purposes, we assume that all images share the same spatial extent (and thus their dense features also do). However, the proposed co-segmentation approach can be applied to images with different resolutions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.cityscapes-dataset.com/license and https://www.image-net.org/download. php for Cityscapes and ImageNet1K respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the code for DeepLabv3+ from https://github.com/VainF/DeepLabV3Plus-Pytorch.<ref type="bibr" target="#b4">5</ref> We use ColorJitter function in torchvision package<ref type="bibr" target="#b65">[66]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">"Horizontal surfaces that are intended for parking and separated from the road, either via elevation or via a different texture/material, but not separated merely by markings." for parking and "Trees, hedges, and all kinds of vertically growing vegetation." for vegetation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cosegnet: Image cosegmentation using a conditional siamese convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avik</forename><surname>Hati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhasis</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajbabu</forename><surname>Velmurugan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">icoseg: Interactive cosegmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zero-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic aware attention based deep object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pointly-supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06404</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Picie: Unsupervised semantic segmentation using invariance and equivariance in clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Jang Hyun Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep feature factorization for concept discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Associating inter-image salient instances for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning object categories from google&apos;s image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12143</idno>
		<title level="m">Open-vocabulary image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Vision models are more robust and fair when pretrained on uncurated images without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Seessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08360</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-aware feature generation for zero-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangxuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by distilling feature correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Relieving long-tailed instance segmentation via pairwise class balance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhen</forename><surname>Yin-Yin He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02784</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Co-attention cnns for unsupervised object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uncertainty-aware learning for zero-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to segment every thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12698</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Webly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria V Ortiz</forename><surname>Bin Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Segovia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot semantic segmentation via variational mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised hierarchical semantic segmentation with multiview cosegmentation and clustering transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The Hungarian Method for the Assignment Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Language-driven semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranftl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Consistent structural relation learning for zero-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">ParseNet: Looking Wider to See Better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Crnet: Cross-reference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07839</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Half-integrality based algorithms for cosegmentation of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lopamudra</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Autoregressive unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with crossconsistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Panella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Melatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Boehm</surname></persName>
		</author>
		<ptr target="http://www.firenet.xyz" />
		<imprint>
			<biblScope unit="page" from="2022" to="2027" />
		</imprint>
	</monogr>
	<note type="report_type">Firenet dataset</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Training object class detectors from eye tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><forename type="middle">Df</forename><surname>Dim P Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A closer look at self-training for zero-label semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Pastore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cermelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Combined scaling for zero-shot transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10050</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Weakly supervised scene parsing with point-based distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Denseclip: Language-guided dense prediction with context-aware prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01518</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">LAION-5B: laion-5b: A new era of open large-scale multi-modal datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<ptr target="https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Bootstrapping the performance of webly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Unsupervised salient object detection with spectral cluster voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyungin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Samuel Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">All you need are a few pixels: semantic segmentation with pixelpick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyungin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<imprint>
			<publisher>ICCVW</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Localizing objects with self-supervised transformers and no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriane</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Roburin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14279</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Cosegmentation revisited: Models and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Drive&amp;segment: Unsupervised semantic segmentation of urban scenes via cross-modal distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Vobecky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hurych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriane</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11160</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Self-supervised transformers for unsupervised object discovery using normalized cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Vaufreydaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11539</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Step: Segmenting and tracking every pixel. In NeurIPS Track on Datasets and Benchmarks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Semantic projection network for zero-and few-label semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Groupvit: Semantic segmentation emerges from text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.11094</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14757</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Cyclesegnet: Object co-segmentation with cycle refinement and region correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guankai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Self-supervised visual representation learning from hierarchical grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Open vocabulary scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Denseclip: Extract free dense labels from clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01071</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
