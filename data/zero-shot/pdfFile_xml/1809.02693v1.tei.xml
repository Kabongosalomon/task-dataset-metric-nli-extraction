<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selective Refinement Network for High Performance Face Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
							<email>chicheng15@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@nlpr.ia.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zou</surname></persName>
							<email>xdzou@mail.ie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Electronics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selective Refinement Network for High Performance Face Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High performance face detection remains a very challenging problem, especially when there exists many tiny faces. This paper presents a novel single-shot face detector, named Selective Refinement Network (SRN), which introduces novel twostep classification and regression operations selectively into an anchor-based face detector to reduce false positives and improve location accuracy simultaneously. In particular, the SRN consists of two modules: the Selective Two-step Classification (STC) module and the Selective Two-step Regression (STR) module. The STC aims to filter out most simple negative anchors from low level detection layers to reduce the search space for the subsequent classifier, while the STR is designed to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the subsequent regressor. Moreover, we design a Receptive Field Enhancement (RFE) block to provide more diverse receptive field, which helps to better capture faces in some extreme poses. As a consequence, the proposed SRN detector achieves state-of-the-art performance on all the widely used face detection benchmarks, including AFW, PASCAL face, FDDB, and WIDER FACE datasets. Codes will be released to facilitate further studies on the face detection problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Face detection is a long-standing problem in computer vision with extensive applications including face alignment, face analysis, face recognition, etc. Starting from the pioneering work of Viola-Jones <ref type="bibr" target="#b23">(Viola and Jones 2004)</ref>, face detection has made great progress. The performances on several well-known datasets have been improved consistently, even tend to be saturated. To further improve the performance of face detection has become a challenging issue. In our opinion, there remains room for improvement in two aspects: (a) recall efficiency: number of false positives needs to be reduced at the high recall rates; (b) location accuracy: accuracy of the bounding box location needs to be improved. These two problems are elaborated as follows.</p><p>On the one hand, the average precision (AP) of current face detection algorithms is already very high, but the precision is not high enough at high recall rates, e.g., as shown * These authors contributed equally to this work. <ref type="bibr">?</ref>  The STR provides better initialization for the subsequent regressor, (d) which produces more accurate locations, i.e., as the IoU threshold increases, the AP gap gradually increases.</p><p>in <ref type="figure" target="#fig_0">Figure 1</ref>(b) of RetinaNet <ref type="bibr" target="#b13">(Lin et al. 2017b)</ref>, the precision is only about 50% (half of detections are false positives) when the recall rate is equal to 90%, which we define as the low recall efficiency. Reflected on the shape of the Precision-Recall curve, it has extended far enough to the right, but not steep enough. The reason is that existing algorithms pay more attention to pursuing high recall rate but ignore the problem of excessive false positives. Analyzing with anchor-based face detectors, they detect faces by classifying and regressing a series of preset anchors, which are generated by regularly tiling a collection of boxes with different scales and aspect ratios. To detect the tiny faces, e.g., less than 16 ? 16 pixels, it is necessary to tile plenty of small anchors over the image. This can improve the recall rate yet cause the the extreme class imbalance problem, which is the culprit leading to excessive false positives. To address this issue, researchers propose several solutions. R-CNN-like detectors <ref type="bibr" target="#b20">Ren et al. 2017</ref>) address the class imbalance by a two-stage cascade and sampling heuristics. As for single-shot detectors, RetinaNet proposes the focal loss to focus training on a sparse set of hard examples and down-weight the loss assigned to well-classified examples. RefineDet  addresses this issue using a preset threshold to filter out negative anchors. However, RetinaNet takes all the samples into account, which also leads to quite a few false positives. Although RefineDet filters out a large number of simple negative samples, it uses hard negative mining in both two steps, and does not make full use of negative samples. Thus, the recall efficiency of them both can be improved. On the other hand, the location accuracy in the face detection task is gradually attracting the attention of researchers. Although current evaluation criteria of most face detection datasets <ref type="bibr" target="#b9">(Jain and Learned-Miller 2010;</ref><ref type="bibr" target="#b28">Yang et al. 2016)</ref> do not focus on the location accuracy, the WIDER Face Challenge 1 adopts MS COCO <ref type="bibr" target="#b11">(Lin et al. 2014</ref>) evaluation criterion, which puts more emphasis on bounding box location accuracy. To visualize this issue, we use different IoU thresholds to evaluate our trained face detector based on RetinaNet on the WIDER FACE dataset. As shown in <ref type="figure" target="#fig_0">Figure  1(d)</ref>, as the IoU threshold increases, the AP drops dramatically, indicating that the accuracy of the bounding box location needs to be improved. To this end, <ref type="bibr" target="#b4">Gidaris et al. (Gidaris and Komodakis 2015)</ref> propose iterative regression during inference to improve the accuracy. Cascade R-CNN <ref type="bibr" target="#b2">(Cai and Vasconcelos 2018)</ref> addresses this issue by cascading R-CNN with different IoU thresholds. RefineDet ) applies two-step regression to single-shot detector. However, blindly adding multi-step regression to the specific task (i.e., face detection) is often counterproductive.</p><p>In this paper, we investigate the effects of two-step classification and regression on different levels of detection layers and propose a novel face detection framework, named Selective Refinement Network (SRN), which selectively applies two-step classification and regression to specific levels of detection layers. The network structure of SRN is shown in <ref type="figure">Figure 2</ref>, which consists of two key modules, named as the Selective Two-step Classification (STC) module and the Selective Two-step Regression (STR) module. Specifically, the STC is applied to filter out most simple negative samples (illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(a)) from the low levels of detection layers, which contains 88.9% samples. As shown in <ref type="bibr">Figure 1(b)</ref>, RetinaNet with STC improves the recall efficiency to a certain extent. On the other hand, the design of STR draws on the cascade idea to coarsely adjust the locations and sizes of anchors (illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(c)) from high levels of detection layers to provide better initialization for the subsequent regressor. In addition, we design a Receptive Field Enhancement (RFE) to provide more diverse receptive fields to better capture the extreme-pose faces. Extensive experiments have been conducted on AFW, PASCAL face, FDDB, and WIDER FACE benchmarks and we set a new state-of-the-art performance.</p><p>In summarization, we have made the following main contributions to the face detection studies: 1 http://wider-challenge.org ? We present a STC module to filter out most simple negative samples from low level layers to reduce the classification search space.</p><p>? We design a STR module to coarsely adjust the locations and sizes of anchors from high level layers to provide better initialization for the subsequent regressor.</p><p>? We introduce a RFE module to provide more diverse receptive fields for detecting extreme-pose faces.</p><p>? We achieve state-of-the-art results on AFW, PASCAL face, FDDB, and WIDER FACE datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Face detection has been a challenging research field since its emergence in the 1990s. Viola and Jones pioneer to use Haar features and AdaBoost to train a face detector with promising accuracy and efficiency <ref type="bibr" target="#b23">(Viola and Jones 2004)</ref>, which inspires several different approaches afterwards <ref type="bibr" target="#b10">(Liao, Jain, and Li 2016;</ref><ref type="bibr" target="#b1">Brubaker et al. 2008;</ref><ref type="bibr" target="#b18">Pham and Cham 2007)</ref>. Apart from those, another important job is the introduction of Deformable Part Model (DPM) <ref type="bibr" target="#b15">(Mathias et al. 2014;</ref><ref type="bibr" target="#b27">Yan et al. 2014a;</ref><ref type="bibr" target="#b32">Zhu and Ramanan 2012)</ref>.</p><p>Recently, face detection has been dominated by the CNNbased methods. <ref type="bibr">CascadeCNN (Li et al. 2015)</ref> improves detection accuracy by training a serious of interleaved CNN models and following work <ref type="bibr" target="#b19">(Qin et al. 2016)</ref> proposes to jointly train the cascaded CNNs to realize end-to-end optimization. MTCNN ) proposes a joint face detection and alignment method using multi-task cascaded CNNs. Faceness <ref type="bibr" target="#b27">(Yang et al. 2015)</ref> formulates face detection as scoring facial parts responses to detect faces under severe occlusion. UnitBox <ref type="bibr" target="#b28">(Yu et al. 2016)</ref> introduces an IoU loss for bounding box prediction. EMO <ref type="bibr" target="#b33">(Zhu et al. 2018)</ref> proposes an Expected Max Overlapping score to evaluate the quality of anchor matching. SAFD <ref type="bibr" target="#b6">(Hao et al. 2017</ref>) develops a scale proposal stage which automatically normalizes face sizes prior to detection. S 2 AP <ref type="bibr" target="#b22">(Song et al. 2018)</ref> pays attention to specific scales in image pyramid and valid locations in each scales layer. PCN <ref type="bibr" target="#b21">(Shi et al. 2018</ref>) proposes a cascade-style structure to rotate faces in a coarseto-fine manner. Recent work (Bai et al. 2018) designs a novel network to directly generate a clear super-resolution face from a blurry small one.</p><p>Additionally, face detection has inherited some achievements from generic object detectors, such as It consists of STC, STR, and RFB. STC uses the first-step classifier to filter out most simple negative anchors from low level detection layers to reduce the search space for the second-step classifier. STR applies the first-step regressor to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the second-step regressor. RFE provides more diverse receptive fields to better capture extreme-pose faces.</p><p>ture. FAN <ref type="bibr" target="#b26">(Wang, Yuan, and Yu 2017)</ref> proposes an anchorlevel attention into RetinaNet to detect the occluded faces. In this paper, inspired by the multi-step classification and regression in RefineDet ) and the focal loss in RetinaNet, we develop a state-of-the-art face detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Refinement Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Structure</head><p>The overall framework of SRN is shown in <ref type="figure">Figure 2</ref>, we describe each component as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone.</head><p>We adopt ResNet-50 <ref type="bibr" target="#b7">(He et al. 2016</ref>) with 6-level feature pyramid structure as the backbone network for SRN. The feature maps extracted from those four residual blocks are denoted as C2, C3, C4, and C5, respectively. C6 and C7 are just extracted by two simple down-sample 3 ? 3 convolution layers after C5. The lateral structure between the bottom-up and the top-down pathways is the same as <ref type="bibr" target="#b12">(Lin et al. 2017a</ref>). P2, P3, P4, and P5 are the feature maps extracted from lateral connections, corresponding to C2, C3, C4, and C5 that are respectively of the same spatial sizes, while P6 and P7 are just down-sampled by two 3 ? 3 convolution layers after P5.</p><p>Dedicated Modules. The STC module selects C2, C3, C4, P2, P3, and P4 to perform two-step classification, while the STR module selects C5, C6, C7, P5, P6, and P7 to conduct two-step regression. The RFE module is responsible for enriching the receptive field of features that are used to predict the classification and location of objects.</p><p>Anchor Design. At each pyramid level, we use two specific scales of anchors (i.e., 2S and 2 ? 2S, where S represents the total stride size of each pyramid level) and one aspect ratios (i.e., 1.25). In total, there are A = 2 anchors per level and they cover the scale range 8 ? 362 pixels across levels with respect to the network's input image.</p><p>Loss Function. We append a hybrid loss at the end of the deep architecture, which leverage the merits of the focal loss and the smooth L 1 loss to drive the model to focus on more hard training examples and learn better regression results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Two-Step Classification</head><p>Introduced in RefineDet , the two-step classification is a kind of cascade classification implemented through a two-step network architecture, in which the first step filters out most simple negative anchors using a preset negative threshold ? = 0.99 to reduce the search space for the subsequent step. For anchor-based face detectors, it is necessary to tile plenty of small anchors over the image to detect small faces, which causes the extreme class imbalance between the positive and negative samples. For example, in the SRN structure with the 1024 ? 1024 input resolution, if we tile 2 anchors at each anchor point, the total number of samples will reach 300k. Among them, the number of positive samples is only a few dozen or less. To reduce search space of classifier, it is essential to do two-step classification to reduce the false positives. However, it is unnecessary to perform two-step classification in all pyramid levels. Since the anchors tiled on the three higher levels (i.e., P5, P6, and P7) only account for 11.1% and the associated features are much more adequate. Therefore, the classification task is relatively easy in these three higher pyramid levels. It is thus dispensable to apply two-step classification on the three higher pyramid levels, and if applied, it will lead to an increase in computation cost. In contrast, the three lower pyramid levels (i.e., P2, P3, and P4) have the vast majority of samples (88.9%) and lack of adequate features. It is urgently needed for these low pyramid levels to do two-step classification in order to alleviate the class imbalance problem and reduce the search space for the subsequent classifier.</p><p>Therefore, our STC module selects C2, C3, C4, P2, P3, and P4 to perform two-step classification. As the statistical result shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the STC increases the positive/negative sample ratio by approximately 38 times, from around 1:15441 to 1:404. In addition, we use the focal loss in both two steps to make full use of samples. Unlike Re-fineDet , the SRN shares the same classification module in the two steps, since they have the same task to distinguish the face from the background. The experimental results of applying the two-step classification on each pyramid level are shown in <ref type="table" target="#tab_1">Table 2</ref>. Consistent with our analysis, the two-step classification on the three lower pyramid levels helps to improve performance, while on the three higher pyramid levels is ineffective.</p><p>The loss function for STC consists of two parts, i.e., the loss in the first step and the second step. For the first step, we calculate the focal loss for those samples selected to perform two-step classification. And for the second step, we just focus on those samples that remain after the first step filtering. With these definitions, we define the loss function as:</p><formula xml:id="formula_0">L STC ({p i }, {q i }) = 1 N s1 i?? L FL (p i , l * i ) + 1 N s2 i?? L FL (q i , l * i ),<label>(1)</label></formula><p>where i is the index of anchor in a mini-batch, p i and q i are the predicted confidence of the anchor i being a face in the first and second steps, l * i is the ground truth class label of anchor i, N s1 and N s2 are the numbers of positive anchors in the first and second steps, ? represents a collection of samples selected for two-step classification, and ? represents a sample set that remains after the first step filtering. The binary classification loss L FL is the sigmoid focal loss over two classes (face vs. background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Two-Step Regression</head><p>In the detection task, to make the location of bounding boxes more accurate has always been a challenging problem. Current one-stage methods rely on one-step regression based on various feature layers, which is inaccurate in some challenging scenarios, e.g., MS COCO-style evaluation standard. In recent years, using cascade structure <ref type="bibr" target="#b2">Cai and Vasconcelos 2018)</ref> to conduct multi-step regression is an effective method to improve the accuracy of the detection bounding boxes.</p><p>However, blindly adding multi-step regression to the specific task (i.e., face detection) is often counterproductive. Experimental results (see <ref type="table" target="#tab_3">Table 4</ref>) indicate that applying two-step regression in the three lower pyramid levels impairs the performance. The reasons behind this phenomenon are twofold: 1) the three lower pyramid levels are associated with plenty of small anchors to detect small faces. These small faces are characterized by very coarse feature representations, so it is difficult for these small anchors to perform two-step regression; 2) in the training phase, if we let the network pay too much attention to the difficult regression task on the low pyramid levels, it will cause larger regression loss and hinder the more important classification task.</p><p>Based on the above analyses, we selectively perform twostep regression on the three higher pyramid levels. The motivation behind this design is to sufficiently utilize the detailed features of large faces on the three higher pyramid levels to regress more accurate locations of bounding boxes and to make three lower pyramid levels pay more attention to the classification task. This divide-and-conquer strategy makes the whole framework more efficient.</p><p>The loss function of STR also consists of two parts, which is shown as below:</p><formula xml:id="formula_1">L STR ({x i }, {t i }) = i?? [l * i = 1]L r (x i , g * i ) + i?? [l * i = 1]L r (t i , g * i ),<label>(2)</label></formula><p>where g * i is the ground truth location and size of anchor i, x i is the refined coordinates of the anchor i in the first step, t i is the coordinates of the bounding box in the second step, ? represents a collection of samples selected for two-step regression, l * i and ? are the same as defined in STC. Similar to Faster R-CNN <ref type="bibr" target="#b20">(Ren et al. 2017)</ref>, we use the smooth L 1 loss as the regression loss L r . The Iverson bracket indicator function [l * i = 1] outputs 1 when the condition is true, i.e., l * i = 1 (the anchor is not the negative), and 0 otherwise. Hence [l * i = 1]L r indicates that the regression loss is ignored for negative anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Receptive Field Enhancement</head><p>At present, most detection networks utilize ResNet and VG-GNet as the basic feature extraction module, while both of them possess square receptive fields. The singleness of the receptive field affects the detection of objects with different aspect ratios. This issue seems unimportant in face detection task, because the aspect ratio of face annotations is about 1:1 in many datasets. Nevertheless, statistics shows that the WIDER FACE training set has a considerable part of faces that have an aspect ratio of more than 2 or less than 0.5. Consequently, there is mismatch between the receptive field of network and the aspect ratio of faces.</p><p>To address this issue, we propose a module named Receptive Field Enhancement (RFE) to diversify the receptive field of features before predicting classes and locations. In particular, RFE module replaces the middle two convolution layers in the class subnet and the box subnet of RetinaNet. The structure of RFE is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Our RFE module adopts a four-branch structure, which is inspired by the Inception block <ref type="bibr" target="#b22">(Szegedy et al. 2015)</ref>. To be specific, first, we use a 1?1 convolution layer to decrease the channel number to one quarter of the previous layer. Second, we use 1 ? k and k ? 1 (k = 3 and 5) convolution layer to provide rectangular receptive field. Through another 1 ? 1 convolution layer, the feature maps from four branches are concatenated together. Additionally, we apply a shortcut path to retain the original receptive field from previous layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference</head><p>Training Dataset. All the models are trained on the training set of the WIDER FACE dataset <ref type="bibr" target="#b28">(Yang et al. 2016)</ref>. It consists of 393, 703 annotated face bounding boxes in 32, 203 images with variations in pose, scale, facial expression, occlusion, and lighting condition. The dataset is split into the training (40%), validation (10%) and testing (50%) sets, and defines three levels of difficulty: Easy, Medium, Hard, based on the detection rate of EdgeBox (Zitnick and Doll?r 2014). Data Augmentation. To prevent over-fitting and construct a robust model, several data augmentation strategies are used to adapt to face variations, described as follows. 1) Applying some photometric distortions introduced in previous work <ref type="bibr" target="#b8">(Howard 2013)</ref> to the training images. 2) Expanding the images with a random factor in the interval <ref type="bibr">[1,</ref><ref type="bibr">2]</ref> by the zero-padding operation. 3) Cropping two square patches and randomly selecting one for training. One patch is with the size of the image's shorter side and the other one is with the size determined by multiplying a random number in the interval [0.5, 1.0] by the image's shorter side. 4) Flipping the selected patch randomly and resizing it to 1024 ? 1024 to get the final training sample. Anchor Matching.</p><p>During the training phase, anchors need to be divided into positive and negative samples. Specifically, anchors are assigned to ground-truth face boxes using an intersection-over-union (IoU) threshold of ? p ; and to background if their IoU is in [0, ? n ). If an anchor is unassigned, which may happen with overlap in [? n , ? p ), it is ignored during training. Empirically, we set ? n = 0.3 and ? p = 0.7 for the first step, and ? n = 0.4 and ? p = 0.5 for the second step.</p><p>Optimization. The loss function for SRN is just the sum of the STC loss and the STR loss, i.e., L = L STC + L STR . The backbone network is initialized by the pretrained ResNet-50 model <ref type="bibr" target="#b20">(Russakovsky et al. 2015)</ref> and all the parameters in the newly added convolution layers are initialized by the "xavier" method. We fine-tune the SRN model using SGD with 0.9 momentum, 0.0001 weight decay, and batch size 32. We set the learning rate to 10 ?2 for the first 100 epochs, and decay it to 10 ?3 and 10 ?4 for another 20 and 10 epochs, respectively. We implement SRN using the Py-Torch library <ref type="bibr" target="#b17">(Paszke et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference.</head><p>In the inference phase, the STC first filters the regularly tiled anchors on the selected pyramid levels with the negative confidence scores larger than the threshold ? = 0.99, and then STR adjusts the locations and sizes of selected anchors. After that, the second step takes over these refined anchors, and outputs top 2000 high confident detections. Finally, we apply the non-maximum suppression (NMS) with jaccard overlap of 0.5 to generate the top 750 high confident detections per image as the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We first analyze the proposed method in detail to verify the effectiveness of our contributions. Then we evaluate the final model on the common face detection benchmark datasets, including AFW (Zhu and Ramanan 2012), PAS-CAL Face <ref type="bibr" target="#b27">(Yan et al. 2014b</ref>), FDDB (Jain and Learned-Miller 2010), and WIDER FACE <ref type="bibr" target="#b28">(Yang et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Analysis</head><p>We conduct a set of ablation experiments on the WIDER FACE dataset to analyze our model in detail. For a fair comparison, we use the same parameter settings for all the experiments, except for specified changes to the components. All models are trained on the WIDER FACE training set and evaluated on the validation set.</p><p>Ablation Setting. To better understand SRN, we ablate each component one after another to examine how each proposed component affects the final performance. Firstly, we use the ordinary prediction head in <ref type="bibr" target="#b13">(Lin et al. 2017b</ref>) instead of the proposed RFE. Secondly, we ablate the STR or STC module to verity their effectiveness. The results of ablation experiments are listed in <ref type="table" target="#tab_0">Table 1</ref> and some promising conclusions can be drawn as follows.</p><p>Selective Two-step Classification. Experimental results of applying two-step classification to each pyramid level are shown in <ref type="table" target="#tab_1">Table 2</ref>, indicating that applying two-step classification to the low pyramid levels improves the performance, especially on tiny faces. Therefore, the STC module selectively applies the two-step classification on the low pyramid levels (i.e., P2, P3, and P4), since these levels are associated with lots of small anchors, which are the main source of false positives. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we find that after using the STC module, the AP scores of the detector are improved from 95.1%, 93.9% and 88.0% to 95.3%, 94.4% and  In order to verify whether the improvements benefit from reducing the false positives, we count the number of false positives under different recall rates. As listed in <ref type="table" target="#tab_2">Table 3</ref>, our STC effectively reduces the false positives across different recall rates, demonstrating the effectiveness of the STC module.  We only add the STR module to our baseline detector to verify its effectiveness. As shown in <ref type="table" target="#tab_0">Table 1</ref>, it produces much better results than the baseline, with 0.8%, 0.9% and 0.8% AP improvements on the Easy, Medium, and Hard subsets. Experimental results of applying two-step regression to each pyramid level (see <ref type="table" target="#tab_3">Table 4</ref>) confirm our previous analysis. Inspired by the detection evaluation metric of MS COCO, we use 4 IoU thresh-olds {0.5, 0.6, 0.7, 0.8} to compute the AP, so as to prove that the STR module can produce more accurate localization. As shown in <ref type="table" target="#tab_4">Table 5</ref>, the STR module produces consistently accurate detection results than the baseline method. The gap between the AP across all three subsets increases as the IoU threshold increases, which indicate that the STR module is important to produce more accurate detections. In addition, coupled with the STC module, the performance is further improved to 96.1%, 95.0% and 90.1% on the Easy, Medium and Hard subsets, respectively.  Receptive Field Enhancement. The RFE is used to diversify the receptive fields of detection layers in order to capture faces with extreme poses. Comparing the detection results between fourth and fifth columns in <ref type="table" target="#tab_0">Table 1</ref>, we notice that RFE consistently improves the AP scores in different subsets, i.e., 0.3%, 0.3%, and 0.1% APs on the Easy, Medium, and Hard categories. These improvements can be mainly attributed to the diverse receptive fields, which is useful to capture various pose faces for better detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on Benchmark</head><p>AFW Dataset. It consists of 205 images with 473 labeled faces. The images in the dataset contains cluttered backgrounds with large variations in both face viewpoint and appearance. We compare SRN against seven state-ofthe-art methods and three commercial face detectors (i.e., Face.com, Face++ and Picasa). As shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>, SRN outperforms these state-of-the-art methods with the top AP score (99.87%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL Face Dataset.</head><p>It has 1, 335 labeled faces in 851 images with large face appearance and pose variations. We present the precision-recall curves of the proposed SRN method and six state-of-the-art methods and three commercial face detectors (i.e., SkyBiometry, Face++ and Picasa) in <ref type="figure" target="#fig_3">Figure 4(b)</ref>. SRN achieves the state-of-the-art results by improving 4.99% AP score compared to the second best method STN <ref type="bibr" target="#b3">(Chen et al. 2016)</ref>. FDDB Dataset. It contains 5, 171 faces annotated in 2, 845 images with a wide range of difficulties, such as occlusions, difficult poses, and low image resolutions. We evaluate the proposed SRN detector on the FDDB dataset and compare it with several state-of-the-art methods. As shown in <ref type="figure" target="#fig_3">Figure  4</ref>(c), our SRN sets a new state-of-the-art performance, i.e., 98.8% true positive rate when the number of false positives is equal to 1000. These results indicate that SRN is robust to varying scales, large appearance changes, heavy occlusions, and severe blur degradations that are prevalent in detecting face in unconstrained real-life scenarios. WIDER FACE Dataset. We compare SRN with eighteen state-of-the-art face detection methods on both the validation and testing sets. To obtain the evaluation results on the testing set, we submit the detection results of SRN to the authors for evaluation. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, we find that SRN performs favourably against the state-of-the-art based on the average precision (AP) across the three subsets, especially on the Hard subset which contains a large amount of small faces. Specifically, it produces the best AP scores in all subsets of both validation and testing sets, i.e., 96.4% (Easy), 95.3% (Medium) and 90.2% (Hard) for validation set, and 95.9% (Easy), 94.9% (Medium) and 89.7% (Hard) for testing set, surpassing all approaches, which demonstrates the superiority of the proposed detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have presented SRN, a novel single shot face detector, which consists of two key modules, i.e., the STC and the STR. The STC uses the first-step classifier to filter out most simple negative anchors from low level detection layers to reduce the search space for the second-step classifier, so as to reduce false positives. And the STR applies the first-step regressor to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the second-step regressor, in order to improve the location accuracy of bounding boxes. Moreover, the RFE is introduced to provide diverse receptive fields to better capture faces in some extreme poses. Extensive experiments on the AFW, PASCAL face, FDDB and WIDER FACE datasets demonstrate that SRN achieves the state-of-the-art detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The effects of STC and STR on recall efficiency and location accuracy. (a) The STC and STR increase the positives/negatives ratio by about 38 and 3 times respectively, (b) which improve the precision by about 20% at high recall rates. (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Faster R-CNN<ref type="bibr" target="#b20">(Ren et al. 2017)</ref>, SSD<ref type="bibr" target="#b14">(Liu et al. 2016)</ref>, FPN<ref type="bibr" target="#b12">(Lin et al. 2017a</ref>) and RetinaNet<ref type="bibr" target="#b13">(Lin et al. 2017b)</ref>.Face R-CNN (Wang et al. 2017a) combines Faster R-CNN with hard negative mining and achieves promising results. Face R-FCN (Wang et al. 2017b) applies R-FCN in face detection and makes according improvements. The face detection model for finding tiny faces (Hu and Ramanan 2017) trains separate detectors for different scales. S 3 FD (Zhang et al. 2017) presents multiple strategies onto SSD to compensate for the matching problem of small faces. SSH (Najibi et al. 2017) models the context information by large filters on each prediction module. PyramidBox (Tang et al. 2018) utilizes contextual information with improved SSD network struc-Figure 2: Network structure of SRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Structure of RFE module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Evaluation on the common face detection datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Precision-recall curves on WIDER FACE validation and testing subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of various designs on the AP performance.</figDesc><table><row><cell>Component STC STR RFE Easy subset</cell><cell>95.1</cell><cell>! 95.3</cell><cell>SRN</cell><cell cols="3">! ! ! ! ! ! 95.9 96.1 96.4</cell></row><row><cell>Medium subset</cell><cell>93.9</cell><cell>94.4</cell><cell></cell><cell>94.8</cell><cell>95.0</cell><cell>95.3</cell></row><row><cell>Hard subset</cell><cell>88.0</cell><cell>89.4</cell><cell></cell><cell>88.8</cell><cell>90.1</cell><cell>90.2</cell></row><row><cell cols="7">89.4% on the Easy, Medium and Hard subsets, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="8">: AP performance of the two-step classification ap-</cell></row><row><cell cols="4">plied to each pyramid level.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STC</cell><cell>B</cell><cell>P2</cell><cell>P3</cell><cell>P4</cell><cell>P5</cell><cell>P6</cell><cell>P7</cell></row><row><cell>Easy</cell><cell cols="7">95.1 95.2 95.2 95.2 95.0 95.1 95.0</cell></row><row><cell cols="8">Medium 93.9 94.2 94.3 94.1 93.9 93.7 93.9</cell></row><row><cell>Hard</cell><cell cols="7">88.0 88.9 88.7 88.5 87.8 88.0 87.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of false positives at different recall rates.</figDesc><table><row><cell>Recall (%)</cell><cell>10 30 50 80</cell><cell>90</cell><cell>95</cell></row><row><cell># FP of RetinaNet</cell><cell cols="3">3 24 126 2801 27644 466534</cell></row><row><cell cols="4"># FP of SRN (STC only) 1 20 101 2124 13163 103586</cell></row><row><cell cols="2">Selective Two-step Regression.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>AP performance of the two-step regression applied to each pyramid level.</figDesc><table><row><cell>STR</cell><cell>B</cell><cell>P2</cell><cell>P3</cell><cell>P4</cell><cell>P5</cell><cell>P6</cell><cell>P7</cell></row><row><cell>Easy</cell><cell cols="7">95.1 94.8 94.3 94.8 95.4 95.7 95.6</cell></row><row><cell cols="8">Medium 93.9 93.4 93.7 93.9 94.2 94.4 94.6</cell></row><row><cell>Hard</cell><cell cols="7">88.0 87.5 87.7 87.0 88.2 88.2 88.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">: AP at different IoU thresholds on the WIDER</cell></row><row><cell>FACE Hard subset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IoU</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell></row><row><cell>RetinaNet</cell><cell>88.1</cell><cell>76.4</cell><cell>57.8</cell><cell>28.5</cell></row><row><cell>SRN (STR only)</cell><cell>88.8</cell><cell>83.4</cell><cell>66.5</cell><cell>38.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding tiny faces in the wild with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the design of cascades of boosted ensembles for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Brubaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised transformer network for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Gidaris and Komodakis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Fast R-CNN. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scale-aware face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Finding tiny faces</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Learned-Miller ; Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>University of Massachusetts, Amherst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>A convolutional neural network cascade for face detection</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jain</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li ;</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSH: single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Pytorch</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast training and selection of haar features using statistics in boosting-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint training of cascaded CNN for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Russakovsky et al. 2015. Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time rotation-invariant face detection with progressive calibration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond trade-off: Accelerate fcn-based face detector with higher accuracy</title>
		<idno>Tang et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR. Pyramidbox: A context-assisted single shot face detector. In ECCV</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>and Jones</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Face r-cnn</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Detecting faces using region-based fully convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Face attention network: An effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu ;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SPL</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">S 3 FD: Single shot scale-invariant face detector</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>and Ramanan 2012</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchors perspective</title>
	</analytic>
	<monogr>
		<title level="m">CVPR. [Zitnick and Doll?r</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
