<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECURRENT NEURAL NETWORK TRANSDUCER FOR AUDIO-VISUAL SPEECH RECOGNITION Index Terms-Audio-visual speech recognition, recur- rent neural network transducer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaki</forename><surname>Makino</surname></persName>
							<email>1tmakino@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
							<email>2hankliao@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Assael</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DeepMind</orgName>
								<address>
									<addrLine>6 Pancras Square, Kings Cross</addrLine>
									<postCode>N1C 4AG</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DeepMind</orgName>
								<address>
									<addrLine>6 Pancras Square, Kings Cross</addrLine>
									<postCode>N1C 4AG</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basilio</forename><surname>Garcia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otavio</forename><surname>Braga</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Siohan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">IEEE Service Center / 445 Hoes Lane</orgName>
								<address>
									<postBox>P.O. Box 1331 / Piscataway</postBox>
									<postCode>08855-1331</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RECURRENT NEURAL NETWORK TRANSDUCER FOR AUDIO-VISUAL SPEECH RECOGNITION Index Terms-Audio-visual speech recognition, recur- rent neural network transducer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents a large-scale audio-visual speech recognition system based on a recurrent neural network transducer (RNN-T) architecture. To support the development of such a system, we built a large audio-visual (A/V) dataset of segmented utterances extracted from YouTube public videos, leading to 31k hours of audio-visual training content. The performance of an audio-only, visual-only, and audio-visual system are compared on two large-vocabulary test sets: a set of utterance segments from public YouTube videos called YTDEV18 and the publicly available LRS3-TED set. To highlight the contribution of the visual modality, we also evaluated the performance of our system on the YTDEV18 set artificially corrupted with background noise and overlapping speech. To the best of our knowledge, our system significantly improves the state-of-the-art on the LRS3-TED set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>While the performance of automatic speech recognition (ASR) systems has significantly improved over the past several years, outstanding challenges remain for ubiquitous ASR. In particular, state-of-the-art recognizers can fail in noisy environments or in presence of overlapping speech. In some applications such as auto-captioning of videos, multiple modalities are available when transcribing speech. The visual signal can supply the spelling of obscure names and terms when shown on-screen, or provide contextual information related to the visual scene <ref type="bibr" target="#b0">[1]</ref>. Further, the motion of the lips constrains the possible phonemes and hence words that can be spoken. With the availability of sophisticated neural network architectures and large amounts of multimedia data, it is intriguing to explore how audio and visual modalities can be combined to yield improvements not otherwise available in a unimodal setting.</p><p>Our work is motivated by the recent success in automatic lip reading by <ref type="bibr" target="#b1">[2]</ref> and audio-visual speech recognition in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In the former, a state-of-the-art Vision-to-Phoneme model (V2P) is developed using a carefully and unprecedentedlysized visual corpus of speaking faces with their corresponding text transcripts extracted from YouTube public videos. A large VGG-inspired neural network was used for extracting visual features, with a connectionist temporal classification (CTC) based system <ref type="bibr" target="#b4">[5]</ref> to predict phonemes. In the latter, using a large audio-visual data set with captions derived mainly from British television, the authors were able to use transformer-based self-attention network blocks <ref type="bibr" target="#b5">[6]</ref> to model the audio and visual modalities with a sequence-to-sequence loss <ref type="bibr" target="#b6">[7]</ref>. With their approach called TM-seq2seq, they demonstrate that combining audio and visual features yields better performance than audio-only models on a large vocabulary recognition task of transcribing TED talk videos, released as a data set called LRS3-TED <ref type="bibr" target="#b7">[8]</ref>.</p><p>As for most machine learning tasks, the performance of an AV-ASR system is driven by the availability of high-quality training datasets. In this work, we first use the approach described in <ref type="bibr" target="#b8">[9]</ref> to mine a 150k hours, audio-only speech recognition corpus from YouTube with labels derived from user-uploaded captions. Next, we filter this dataset using face tracking technology similar to the work in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> to select segmented utterances with a matching on-screen speaking face, leading to a 31k hours audio-visual corpus. Similar to <ref type="bibr" target="#b9">[10]</ref>, we apply face landmark smoothing to stabilize the face thumbnails. But in contrast to previous AV-ASR work, we synchronize audio to video frames by adjusting the audio feature frame rate while maintaining a fixed short-term Fourier transform analysis window.</p><p>In terms of modeling, as observed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, a weakness of CTC modeling as used in V2P is that the network outputs are conditionally independent of each other and require an external language model, such as a N-gram Finite State Transducer to perform well. In contrast, using a sequence-tosequence loss allows the language model to be learned implicitly as part of the decoder. We advocate using the recurrent neural network transducer (RNN-T), first described in <ref type="bibr" target="#b11">[12]</ref>, as a more natural model for speech recognition. The RNN-T consists of two components, a transcription or encoder network and a recurrent prediction or decoder network. The encoder can be viewed as the acoustic or visual model, with in our case, inputs being mel-filterbank coefficients <ref type="bibr" target="#b12">[13]</ref> or visual embeddings derived using a VGG inspired network <ref type="bibr" target="#b13">[14]</ref> as 3-D convolutions in V2P. The decoder network models the output of the current symbol given the previous output symbol, in our case character-level outputs. The encoder and decoder outputs are fed into a joint network where the output is normalized to yield a softmax over characters. We believe that the RNN-T is a better model for speech since the output dependencies tend to be local and the use of LSTM layers for the encoder and decoder better models this locality. Compared to sequence-to-sequence models, the use of attention may be less desirable since the model spends capacity learning that the most important features for the output are the nearest input frames. In other words, for speech recognition the alignment of input features to output symbols is monotonic. The use of character level outputs, rather than phonemes, provides a simple way of achieving an open-vocabulary system.</p><p>We evaluate our approach on a general YouTube transcription task for audio, video, and audio-visual speech recognition. To illustrate the impact of the visual modality on ASR performance, we artificially corrupted our evaluation dataset by adding either babble noise at various signal-to-noise ratios (SNR), or a few seconds of overlapping speech at the beginning or end of each evaluation utterance. We also provide experimental results on the publicly available large-vocabulary benchmark LRS3-TED <ref type="bibr" target="#b7">[8]</ref> and report, to the best of our knowledge, state-of-the-art results on that set.</p><p>The paper is organized as follows. In Section 2 we describe the way we construct our training set and present our test sets. Section 3 focuses on the architecture of our system, starting with our process to generate synchronized audio and visual features, followed by our RNN-T approach that operates on either audio, visual, or AV features. Experiments are discussed in Section 4, Section 5 highlights the AI Principles followed in this work, and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATASETS</head><p>The impact of the visual modality on human speech perception has been documented as early as 1954 <ref type="bibr" target="#b14">[15]</ref>. In particular, the place of articulation can provide valuable information to help differentiate acoustically confusable sounds <ref type="bibr" target="#b15">[16]</ref>. As a result, and as highlighted in several reviews of AV-ASR technology <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, attempts were made in the early 90's to combine audio and visual modalities for ASR. Until a few years ago, the large majority of the publicly available AV datasets suitable for AV-ASR were limited in size and scope, consisting mostly of tens of hours of spoken digits or command-andcontrol word sequences with a limited number of speakers recorded in a laboratory environment under controlled lighting conditions and face orientation <ref type="bibr" target="#b17">[18]</ref>.</p><p>In 2016, researchers proposed a novel end-to-end trained model for visual speech recognition <ref type="bibr" target="#b18">[19]</ref> and approach to construct large AV datasets suitable for AV-ASR <ref type="bibr" target="#b19">[20]</ref>. Starting from TV shows and their corresponding closed-captions provided by the BBC, they applied computer vision techniques to select short audio segments with an on-screen speaking face matching the audio. In subsequent studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, they extended their approach to a larger amount of data, leading to the LRS2 &amp; LRS3 datasets consisting of videos from the BBC and TED-talks, respectively, for a total of 800 hours of transcribed AV content. More recently, applying this method to public videos from YouTube lead to an AV-ASR dataset consisting of 3.9k hours of videos <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this paper, we expand the approach of <ref type="bibr" target="#b1">[2]</ref> to construct a significantly larger dataset. As in <ref type="bibr" target="#b1">[2]</ref>, we build on the semisupervised approach described in <ref type="bibr" target="#b8">[9]</ref> to mine a large audio dataset from YouTube videos. When YouTube users and content creators upload a public video on YouTube, they have the option of uploading subtitles alongside their video. A forced alignment procedure is then used to synchronize the useruploaded transcripts with the audio to generate time-aligned captions. We take advantage of user-uploaded captions to automatically mine our AV dataset. First, we run ASR on the uploaded video and string-align the ASR transcripts with the user-uploaded transcripts. Audio segments where the ASR and user-uploaded transcripts agree are selected as training utterances, treating the user-uploaded transcripts as groundtruth reference. Because of the scale of YouTube, this leads to the construction of a large audio training set, in the order of 150k hours of data for American English. Next, we extract the video snippets corresponding to the selected utterances and run face tracking to locate all on-screen faces <ref type="bibr">[23,</ref><ref type="bibr" target="#b22">24]</ref>. A visual speech classifier is used to identify the speaking face, if any, that spans each audio segment, followed by an audiovisual synchrony detector to reject dubbed videos.</p><p>The result of that process is a collection of short utterances (from a few seconds to tens of seconds long) totaling 31k hours of data, where with high confidence, the audio matches the user-uploaded transcript, and the selected face video track matches the audio. This exceeds the amount of training data used for training the V2P model of <ref type="bibr" target="#b1">[2]</ref> by a factor of 5, and the TM-seq2seq model of <ref type="bibr" target="#b6">[7]</ref> by a factor of 10. The size of our training set is mitigated by the fact that we do not augment the training data by perturbing the audio or image data. We believe the main reason we obtain much more data than in the V2P work is due to accepting a greater range of face pan and tilt angles, i.e. more than +/-30 degrees. Unlike the LRW/LRS2/LRS3 datasets that are restricted to professionally generated video content, our dataset spans a much greater variety of content of speaking faces in the wild. We use this dataset to train our unimodal audio and video systems, as well as our AV ASR system, and extract 70 hours of data as development set to tune our models.</p><p>Unlike our training set that is automatically constructed from videos with user-uploaded captions, we built our evaluation set from a collection of manually transcribed YouTube videos. Starting from a set of 1000 hours of transcribed videos, we applied the same face tracking process as in training to select a collection of utterances with a matching onscreen speaking face. Out of that process, we retained a set of 20k segmented utterances, with their corresponding manual reference transcripts and face tracks, called YTDEV18 and totaling 25 hours of data. We checked the video IDs to confirm that the videos used in the YTDEV18 and LRS3-TED evaluation sets are not included in the training data.</p><p>To study the impact of the visual modality on recognition performance, we artificially corrupt the YTDEV18 utterances in two ways. First, we add babble noise randomly selected from the NoiseX <ref type="bibr" target="#b23">[25]</ref> dataset at 0dB, 10dB, and 20dB SNR to each utterance. Second, we add overlapping speech from a competing speaker at the beginning/end of each utterance at an equal energy level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Synchronized Audio-Visual Frames</head><p>A challenge in audio-visual speech recognition is dealing with the difference in data rates between audio and visual features. In this work we start with audio sampled and mixed down to a 16kHz, single-channel signal using ffmpeg. A 25ms Hanning window with 10 ms shift is used to compute a short-time Fourier transform for a spectral representation of the audio. A mel-spaced bank of 80 triangular filters is applied followed by a log function to yield mel filterbank coefficients <ref type="bibr" target="#b12">[13]</ref>. These are stacked 2 frames to the left and 2 to the right to yield a 400 dimensional feature vector, while only keeping one in three frames similar to <ref type="bibr" target="#b24">[26]</ref>, for a final frame rate of 33 1 /3 fps.</p><p>Because our data is heterogeneous in terms of video standards and frame rates, we follow an approach similar to <ref type="bibr" target="#b1">[2]</ref> and downsample high frame rates down to a maximum of 30 fps, retaining only videos between 23 fps to 30 fps, so that 23.98/24 fps (cinemas), 25 fps (PAL/SECAM), 29.97 fps (NTSC) and 30 fps videos are kept. FaceNet <ref type="bibr" target="#b22">[24]</ref> is applied to detect and track faces. Face landmarks are then extracted from these tracks and smoothed using a temporal Gaussian kernel. A difference from the V2P data processing pipeline is that we do not filter out segments with extreme face pan or tilt angles to train a more robust model, however we do enforce a minimum eye distance of 80 pixels. Crops around the mouth are then extracted as 128x128 RGB pixel thumbnails.</p><p>Variable video frame rates typically do not match the audio frame rate leading to unsynchronized audio and visual features as illustrated in <ref type="figure">Figure 1</ref>. In <ref type="bibr" target="#b2">[3]</ref>, the video is downsampled to 25 fps and the audio feature stacked and sampled to match this frame rate to yield synchronized frame rates. We believe that the down-sampling of the video stream may degrade the visual features and instead propose to operate on the variable video frame rate and extract audio features at a matching rate. Since the original audio has a high sampling rate, the 25 ms STFT window can be advanced at a rate proportional the video frame rate, here 1 /3 of the video frame length, as shown in <ref type="figure">Figure 2</ref>. This provides an alternate means of getting synchronized audio-visual features without changing the visual features. In Section 4.3, we illustrate the difference in performance between extracting features at the proposed variable frame rate vs. fixed frame rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RNN-T for Audio-Visual Speech Recognition</head><p>Audio features are derived as discussed in the previous sec- volutional filters as the features used in V2P <ref type="bibr" target="#b9">[10]</ref>. The number of filters increases from 64 in the first block, to 128, 256, 512 and 512 in the final layer. After each convolutional layer, before pooling, group norm <ref type="bibr" target="#b25">[27]</ref> is applied with 32 groups, over the spatial dimensions, but not the temporal, which gives a small improvement. The output of the video model yields an embedding vector of 512 coefficients for every input frame. The video model has 11.7M parameters. A detailed description of the video model can be found in <ref type="bibr" target="#b1">[2]</ref>. To create a visual transcription model, the 512 dimensional output is fed into the encoder by turning on the 'video switch' in <ref type="figure">Figure 3</ref>.</p><p>For an A/V model, the audio and visual features are enabled and concatenated to form a 912-dimensional feature vector that is fed into the encoder as shown in <ref type="figure">Figure 3</ref>. A summary of the parameters, allocated to the various parts of the network for the full A/V model, is shown in <ref type="table">Table 1</ref> minus any temporary variables used only in training, e.g. moment estimates. For simplicity, the shape information listed is for the main kernel of the layer only and for example does not list the gamma and beta terms from layer norm, however the parameter count includes them. Layer normalization <ref type="bibr" target="#b26">[28]</ref> is used in all the LSTM layers to make training more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head><p>To train the models in this work, we use TensorFlow to implement our neural network models. An Adam optimizer <ref type="bibr" target="#b27">[29]</ref> is used for training. Unless otherwise noted, we used a learning rate schedule warming up linearly to 2 ? 10 ?3 in 20k steps, staying constant for 50k steps, before exponentially decaying. We train our models on tensor processing units (TPUs) with 128 cores and a batch size of 16 sent to each core for an effective batch size per step of 2048 utterances. For all results, we select the model checkpoint that produces the best result on a held-out validation shard of the training data. We present results first on a selection of YouTube utterances where we  <ref type="table">Table 1</ref>. Audio-visual RNN-T model architecture.</p><p>found they had visual tracks that covered most of the speech in the segment and were well synchronized. This set of utterances is collected from over 1000 videos totaling over 20k utterances and called YTDEV18. We then present results on the publicly available LRS3-TED test set which contains about 10k words and 1436 utterances. The results are measured in word error rates (WER) of the output of frame-synchronized beam decoder <ref type="bibr" target="#b28">[30]</ref>, where at most 4 highest scoring candidates are retained at every step during decoding. CI denotes the half width of the 95% confidence intervals based on <ref type="bibr" target="#b29">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline Results</head><p>Training an audio-visual model can be difficult since the speech signal is far more informative than the visual. However, our results in <ref type="table">Table 2</ref> show that an A/V RNN-T model can be trained from scratch on A/V data without pre-training any components. On the 31k training set, an audio-only sytem can be trained that yields a 21.5% word error rate (WER) on YTDEV18. In comparison, the video-only system gives a 48.5% error rate. In combination, an A/V system trained on both modalities, leads to a better system at 20.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eval Set</head><p>Train &amp; Eval Mode WER (%) ? 95% CI Audio 21.5 ? 0.5 YTDEV18 Visual 48.5 ? 0.6 Audio+Visual 20.5 ? 0.5 <ref type="table">Table 2</ref>. Results on YTDEV18 where the training modalities match the test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training with Drop-out</head><p>Our best results so far have been with an A/V model trained on both audio and visual data. However it is interesting to see how such system behaves when one of these modes are missing. We evaluated our ASR system that trained from both audio and visual modalities with unimodal test data. As shown  <ref type="table">Table 3</ref>. Results with A+V models trained with different drop-out rates and tested on audio-only (A), visual-only (V) and audio-visual (A+V) modes. All values are WER (%) ? 95% CI.</p><p>in the first row of results in <ref type="table">Table 3</ref>, when the A/V trained model is evaluated on a audio-only version of the test set, we found that the performance degrades from 20.5% to 24.0% demonstrating that the model has learned to depend on both modes for recognition. More surprising, is that when evaluated on a visual-only version of the test set, the results become extremely poor at 98.8%. In real-world data, both modalities may not always be present so it may be prudent to improve results on unimodal data for robustness.</p><p>To boost results with missing modalities, we simulate the effect during training by randomly dropping out a modality at the sentence level as in <ref type="bibr" target="#b30">[32]</ref>. As shown in <ref type="table">Table 3</ref>, the first row of results indicates when no drop-out is applied. In the second row, there is a 30% chance the audio is dropped during training. This makes the model learn to use the video signal almost as well as the visual-only system: 50.3% compared to 48.5% and the overall A/V system is better at 19.8%. The cost is the results on only audio-only data are much worse, now at 46%. To improve the audio-only results, we also tried training with a 30% chance of audio being dropped, 10% chance of video being dropped, or neither. For this model with audio and video drop-out, we use a different learning rate schedule (4 ? 10 ?3 and kept constant until 100k steps) to compensate for slower learning. This did not have an appreciable affect on the audio-only and A/V results but surprisingly did harm the visual-only results by 10% relative. Thus, for the rest of paper we report results with 30% audio dropout, but no visual drop-out. Overall, these results suggest that it was easier for the network to learn from audio modality than from visual modality, unless it is forced by drop-out during training. They also indicate that the current strategy of modality fusion through the concatenation of features is not robust to these different input conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Variable Versus Fixed Frame Rate</head><p>We compared the performance with variable frame rate to those of the fixed frame rates. For fixed frame rates, we chose 33 1 /3 frames-per-second, to match a conventional 10ms?3 audio frame system; 30 fps since about half of the videos are 30 or 29.97 fps; and 24 fps to avoid frame duplication. <ref type="table">Table 4</ref> shows the results with different frame rate conditions. The difference between these frame rate conditions are not significant, as with a different learning rate schedule we  <ref type="table">Table 4</ref>. Results on YTDEV18 with different frame rate conditions. All values are WER (%) ? 95% CI.</p><p>observed that fixed 30fps model achieved slightly better accuracy than variable frame rate model. We can conclude, using a variable frame rate is as effective as using fixed frame rates with an RNN-T model. <ref type="table" target="#tab_5">Table 5</ref> presents results on noisy datasets. In all cases, the models are tested with the same set of modalities as used in training. We find that there is about a 5% relative improvement from having an audio-visual model over an audio-only on the YTDEV18 test set without added noise. We can see that the advantage of the multimodal model is greater on the noisier versions of the test set. The visual-only model performs the same regardless of added noise, which in the case of the severe 0dB babble noise conditions, is better than either using audio-only or audio-visual systems. This is not entirely unexpected because there is no added noise in our audio training data. We can artificially add overlapping speech to the audio training data by randomly selecting an utterance from the training set as a form of multi-style training <ref type="bibr" target="#b31">[33]</ref>. We do   <ref type="table">Table 6</ref>. Results on noisy versions of YTDEV18 for audio (A), visual (V) and audio-visual (A+V) trained models where overlapping speech is added to the training audio 10% of the time. All values are WER(%) ? 95% CI.. so with a 10% chance at a level between 0-20dB randomly selected over a uniform distribution. In <ref type="table">Table 6</ref>, we report results with models trained in such a manner. The audio-only model shows improvement from the overlap training across all the noise condidtions. The audio-visual model shows even greater relative improvements, especially on the 0db babble and overlapping speech tests. We believe that providing the visual signal allows for the A/V model to better ignore competing speech, similar to speaker seperation/speech enhancement with visual features <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b33">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Impact of Visual Features on Noisy Speech</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance on LRS3-TED Dataset</head><p>To provide a reference on how our models work on a publicly available data set, we report results on the LRS3-TED dataset, using our models trained only on the YouTube 31khr data set, in <ref type="table" target="#tab_6">Table 7</ref> along with some previously published results. The CTC-V2P system in <ref type="bibr" target="#b1">[2]</ref> was shown to yield better transcription results than professional human lipreaders on YouTube video and despite not training on the available training data had a word error rate of 55.1% on this test set. Another comparison is the TM-seq2seq model from <ref type="bibr" target="#b2">[3]</ref>. TM-seq2seq was shown to better than CTC and demonstrated how combining audio and video could yield improved results on a large-vocabulary task. Our system, using far more data and a RNN-T model makes significant improvement over both of these systems, achieving a 33.6% word error rate training for video-only training and testing, and 4.5% for a combined A/V system. The improvement in using both audio and visual modalities over audio holds for this test set as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AUDIO-VISUAL ASR &amp; AI PRINCIPLES</head><p>The development of Audio-Visual ASR technology, and more specifically lip-reading, raises issues related to privacy, especially in light of the performance of our system on the LRS3-TED dataset when using the visual modality only. It should be noted that this level of performance relates to the nature of the LRS3-TED task which involves professionally produced content under good lighting conditions (no shadows), high video frame-rate (? 24 fps), and a cooperative speaker facing the camera. This is illustrated by the significantly higher visual-only WER on the YTDEV18 set (48.5%), which reflects "in-the-wild" conditions, compared to the LRS3-TED set (33.6%), which corresponds to studio-quality recordings. We also evaluated the performance of our video-only system as a function of the image quality by labeling utterances from the YTDEV18 set as "high quality" when the face had a minimum eye distance of 80 pixels, a bounding box diagonal of 300 pixels, a maximum absolute pan angle of 30 degrees and a tilt angle less than 10 degrees. Everything else was labeled as "low quality", with 70% of the utterances falling in that category. The visual-only WER on the low quality utterances was 57.0%, to be compared with 37.1% on the high quality utterances. We would expect an even greater degradation in performance on video streams originating from low resolution and low frame rate devices such as CCTV cameras.</p><p>We are aware of the risk-benefit trade-off for use of this technology and our work abides by Google AI Principles <ref type="bibr" target="#b34">[36]</ref>. We are hoping that this work, by improving the robustness of speech recognition systems, will increase the reach of ASR technology to a larger population of users, as well as the development of assistive technology. One area of interest is enabling people with impaired speech, such as for example people suffering from Lou Gehrig's disease, to continue operating speech-enabled devices and equipment by relying more on the visual modality.</p><p>Last, it should be noted that the data and models developed in this work are restricted to a small group of researchers working on this project and are handled in compliance with the European Union General Data Protection Regulation <ref type="bibr" target="#b35">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we presented an RNN-T based speech recognition system operating on audio-visual content. We adopted a fully automated approach to mine a large audio-visual training set out of YouTube public videos, relying on advances in ASR and computer vision to select utterances with an onscreen speaking face, its corresponding audio signal and its matching user-uploaded captions. We used a V2P frontend to extract visual features from thumbnails located on the speaker mouth region and enforced extracting audio features at the same frame rate as the visual features. This enabled concatenating the audio and visual features at the input of the RNN-T encoder, leading to a similar modeling architecture for our audio, visual, and AV system. We illustrated that our AV system slightly improves over an audio-only system when trained on the same amount of training data, but leads to significant performance improvement in presence of babble noise or overlapping speech. We also described how the use of such a large training set leads to state-of-the-art performance on the publicly available LRS3-TED set. Future work will focus on comparing the performance of our system with other alternative approaches such as attention-based and transformed-based models and in exploring the use of modality-imbalanced training sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>tion 3. 1 .Fig. 3 .</head><label>13</label><figDesc>In the audio-only model, the 400 dimensional stack of filterbank coefficients is fed into the encoder part of the RNN-T model by turning on only the 'audio switch' in Figure 3. In our experiments we use a 5-layer stack of bidirectional LSTMs in the encoder part of the model with 512 nodes used in each direction for a total of 1024 nodes in each layer. The decoder is comprised of 2 layers of unidirectional LSTMs each project down to 640 nodes. The joint part of the RNN-T model combines the encoder and decoder in a 640 dimensional space. The graphemic output space is 75 characters. The video model is composed of 5 blocks of 3?3?3 con-RNN-T model architecture. Isolating modalities is made possible with the video and audio switches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>?0.5 98.8 ?0.1 20.5 ?0.5 YTDEV18 30% -46.0 ?0.9 50.3 ?0.7 19.8 ?0.5 30% 10% 45.6 ?0.9 55.3 ?0.7 20.0 ?0.5</figDesc><table><row><cell></cell><cell cols="2">Drop-out Chance</cell><cell></cell><cell>Eval Mode</cell><cell></cell></row><row><cell>Eval Set</cell><cell>A</cell><cell>V</cell><cell>A</cell><cell>V</cell><cell>A+V</cell></row><row><cell></cell><cell>-</cell><cell cols="2">-24.0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>?0.6 48.6 ?0.6 48.5 ?0.6 50.1 ?0.6 A+V A+V 19.8 ?0.5 20.4 ?0.5 20.2 ?0.5 20.1 ?0.5 A A 21.5 ?0.5 21.6 ?0.5 21.2 ?0.5 21.4 ?0.5</figDesc><table><row><cell cols="2">Modes used</cell><cell></cell><cell cols="2">Frame rate (fps)</cell></row><row><cell cols="2">Train Eval</cell><cell>variable</cell><cell>33 1 /3</cell><cell>30</cell><cell>24</cell></row><row><cell>V</cell><cell>V</cell><cell>48.5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>?0.5 57.4 ?0.6 overlapping speech 40.6 ?0.6 37.4 ?0.6</figDesc><table><row><cell>Test set</cell><cell>Added noise</cell><cell>A</cell><cell>A+V</cell><cell>V</cell></row><row><cell></cell><cell>-</cell><cell cols="2">21.5 ?0.5 20.5 ?0.5</cell><cell></cell></row><row><cell></cell><cell>babble, 20dB</cell><cell cols="2">22.5 ?0.5 21.2 ?0.5</cell><cell></cell></row><row><cell cols="2">YTDEV18 babble, 10dB</cell><cell cols="3">28.1 ?0.5 24.8 ?0.5 48.5 ?0.6</cell></row><row><cell></cell><cell>babble, 0dB</cell><cell>64.5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results on noisy versions of YTDEV18 for audio (A), visual (V) and audio-visual (A+V) models trained on uncorrupted training data. All values are WER(%) ? 95% CI. ?0.5 42.3 ?0.6 overlapping speech 31.5 ?0.6 25.6 ?0.6</figDesc><table><row><cell>Test set</cell><cell>Added noise</cell><cell>A</cell><cell>A+V</cell><cell>V</cell></row><row><cell></cell><cell>-</cell><cell cols="2">21.0 ?0.5 20.0 ?0.5</cell><cell></cell></row><row><cell></cell><cell>babble, 20dB</cell><cell cols="2">21.9 ?0.5 20.5 ?0.5</cell><cell></cell></row><row><cell cols="2">YTDEV18 babble, 10dB</cell><cell cols="3">26.6 ?0.5 23.1 ?0.5 48.5 ?0.6</cell></row><row><cell></cell><cell>babble, 0dB</cell><cell>57.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Results of the RNN-T model, CTC-V2P [2] (trained on ?30 ? face rotations) and TM-seq2seq [3] on LRS3-TED.</figDesc><table><row><cell>Training Data</cell><cell>Model</cell><cell cols="2">Mode WER(%)</cell></row><row><cell>YT, 4khrs</cell><cell>CTC-V2P</cell><cell>V</cell><cell>55.1</cell></row><row><cell cols="2">BBC+TED, 1.4khrs TM-seq2seq</cell><cell>V</cell><cell>58.9</cell></row><row><cell cols="2">BBC+TED, 1.4khrs TM-seq2seq</cell><cell>A</cell><cell>8.3</cell></row><row><cell cols="3">BBC+TED, 1.4khrs TM-seq2seq A+V</cell><cell>7.2</cell></row><row><cell>YT, 31khrs</cell><cell>RNN-T</cell><cell>V</cell><cell>33.6</cell></row><row><cell>YT, 31khrs</cell><cell>RNN-T</cell><cell>A</cell><cell>4.8</cell></row><row><cell>YT, 31khrs</cell><cell>RNN-T</cell><cell>A+V</cell><cell>4.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual features for context-aware speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large-scale visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsav</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrayne</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Mulville</surname></persName>
		</author>
		<idno>abs/1807.05162</idno>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Ben Coppin, Ben Laurie</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Andrew Senior, and Nando de Freitas</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audiovisual speech recognition with a hybrid CTC/attention architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">LRS3-TED: A large-scale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs:1809.00496</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large-scale visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsav</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrayne</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Mulville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Coppin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Laurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep lip reading: A comparison of models and an online application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning Workshop on Representation Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllable word recognition in continuously spoken sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual contribution to speech intelligibility in noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Sumby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustical Society America</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech recognition and sensory integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dom</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Scientist</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerasimos</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chalapathy</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for multimodal automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Czyzewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bozena</forename><surname>Kostek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bratoszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Kotus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Szykulski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LipNet: End-to-end sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shillingford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Technology Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Shimon Whiteson, and Nando de Freitas</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lip reading in profile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steeneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1993-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Context dependent phone models for LSTM RNN acoustic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A comparison of sequence-to-sequence models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient computation of confidence intervals forword error rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Miguel</forename><surname>Vilar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust audio-visual speech recognition using bimodal DF-SMN with multi-condition training and dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multistyle training for robust isolated-word speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Artificial Intelligence at Google: Our Principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://ai.google/principles/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Regulation (EU) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/EC (General Data Protection Regulation)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>European Union</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Law</surname></persName>
		</author>
		<ptr target="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
