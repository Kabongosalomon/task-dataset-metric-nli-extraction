<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOSD-Net: Joint Semantic Object Segmentation and Depth Estimation from Monocular images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-19">19 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Baidu Autonomous Driving Technology Department (ADT)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<postCode>M5B 2K3</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Baidu Autonomous Driving Technology Department (ADT)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SOSD-Net: Joint Semantic Object Segmentation and Depth Estimation from Monocular images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-19">19 Jan 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>semantic objectness</term>
					<term>depth estimation</term>
					<term>semantic estimation</term>
					<term>object segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation and semantic segmentation play essential roles in scene understanding. The state-of-the-art methods employ multi-task learning to simultaneously learn models for these two tasks at the pixel-wise level. They usually focus on sharing the common features or stitching feature maps from the corresponding branches. However, these methods lack in-depth consideration on the correlation of the geometric cues and the scene parsing. In this paper, we first introduce the concept of semantic objectness to exploit the geometric relationship of these two tasks through an analysis of the imaging process, then propose a Semantic Object Segmentation and Depth Estimation Network (SOSD-Net) based on the objectness assumption. To the best of our knowledge, SOSD-Net is the first network that exploits the geometry constraint for simultaneous monocular depth estimation and semantic segmentation. In addition, considering the mutual implicit relationship between these two tasks, we exploit the iterative idea from the expectation-maximization algorithm to train the proposed network more effectively. Extensive experimental results on the Cityscapes and NYU v2 dataset are presented to demonstrate the superior performance of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth estimation and semantic segmentation, as two major components in scene understanding, have received a lot of attention in the computer vision community. In recent years, with the successful applications of deep convolutional neural networks, the performance of depth estimation and semantic segmentation has been greatly improved <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, owing to the superior representation ability of the deep features over the classical handcrafted features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Monocular depth estimation is an essential approach in understanding the 3D geometry of a scene <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13</ref>]. Depth estimation is usually formulated as a regression problem that assigns each pixel a continuous depth value. However, this task exists inherent ambiguity with some scene priors, as analyzed in He et al. <ref type="bibr" target="#b11">[12]</ref>. The scene priors refer to the elements that can remedy the ambiguity of the monocular depth estimation, such as the physical size of the objects in the scene, and the focal length information of the camera, etc. To improve the accuracy of monocular depth estimation, these ambiguous elements need to be properly integrated into the network during the training and inferring process of the network. With the multi-scale fusion and hierarchical representation of deep networks, the precision of semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref> has been greatly improved. Nevertheless, most segmentation models have limitations in certain scenarios, like segmenting slender objects such as poles. If we can obtain an accurate depth map, there generally exists a depth margin between the poles and the surrounding background or objects. Thus, the depth information can greatly help to improve the segmentation performance, especially in challenging situations.</p><p>In order to explore the correlation of the depth information and semantic segmentation, jointly train a network to simultaneously learn the two tasks become an attractive direction in scene understanding. One popular approach is to combining multi-task neural activations via employing the network architecture interaction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. However, the geometric constraint is not explicitly explored in the fusion process. To obtain an optimal descent direction of the common weights, another approach <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> is designing the jointoptimization objective functions by adaptively selecting loss weight of each task during the training phase. This approach is designed only to pursue a better sharing feature representation, without considering the geometric relationship of the two tasks.</p><p>In this paper, we propose to explore the geometric relationship between monocular semantic segmentation and depth estimation, and design a novel neural network (SOSD-Net) to embed the semantic objectness, making it possible to simultaneously learn the geometric cues and scene parsing, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The proposed network is designed strictly according to the geometric constraints to boost up the performance of the two tasks by integrating the information of the objectness.</p><p>Specifically, when inferring the monocular depth information, the semantic objectness will fuse the features from the semantic segmentation, and vice versa. In addition, the supervised learning of the two tasks is essentially a parameter estimation problem of a Gaussian mixture model. Inspired by the idea of Expectation Maximization (EM) algorithm, we propose an effective learning strategy to alternative optimize the weights of the scene parsing and the geometric cues during the training phase. The proposed method is extensively evaluated on the CityScapes <ref type="bibr" target="#b20">[21]</ref> and NYU v2 datasets <ref type="bibr" target="#b21">[22]</ref>, and the experimental validation shows that the SOSD-Net outperforms the state-of-the-art multi-task approaches in the one-stage training phase, demonstrating the effectiveness of our proposed algorithm.</p><p>In summary, the key contributions of this paper include:</p><p>1. We propose a Semantic Objectness Segmentation and Depth Estimation Network (SOSD-Net) to enhance the learning ability of joint monocular depth estimation and semantic segmentation. 2. An effective learning strategy is proposed to alternatively update the specific weights of SOSD-Net, which significantly improves the performance of the two tasks. 3. We achieve competing results over the state-of-the-art one-stage models on two popular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review the related work in the following three problems: semantic segmentation, depth estimation, and multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>With the powerful representational and inferring ability, many models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> based on the deep convolutional neural networks have achieved significant improvement on several segmentation datasets, especially compared with the classical hand-crafted methods. Long et al. <ref type="bibr" target="#b0">[1]</ref> made a breakthrough by successfully converting the classification network to a pixel-wise segmentation network, replacing the fully connected layers with convolutional layers. Inspired by this idea, the recent semantic segmentation networks can be broadly classified into three categories. The first group <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> designs convolutional encoder-decoder network structures to gradually capture the high semantic information and recovering the spatial information. The second group of methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14]</ref> is to exploit multi-scale information to grasp better global and contextual information. The last group <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref> is to explore the conditional Markov Random Field to optimize the segmentation result. In addition, Kre?o et al. <ref type="bibr" target="#b37">[38]</ref> propose a novel scale selection layer to extract convolutional features at the scale of the reconstructed depth to improve the performance of the semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Depth Estimation</head><p>Learning depth from a single image has been extensively studied in the literature. To tackle this task, classic methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> usually make strong geometric assumptions about the scene structure, and employ the Markov Random Field (MRF) to infer the depth by leveraging the hand-crafted features. Non-parametric algorithm <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7]</ref> is another type of classical methods, which employ global scene features to search for candidate images that are close to the input image from a training database in the feature space. Other methods are based on the advanced deep learning models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. Eigen et al. <ref type="bibr" target="#b10">[11]</ref> addressed this issue by fusing the depths from the global network and refined network, which was extended to use a multi-scale convolutional network in a deeper neural network <ref type="bibr" target="#b3">[4]</ref>. Recently, the unsupervised learning methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> achieved significant progress. By exploiting the epipolar geometry constraints, <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> taken the inferred monocular depth as an intermediate result in computing the reconstruction loss. Due to the inherent ambiguity of the monocular depth estimation, He et al. <ref type="bibr" target="#b11">[12]</ref> proposed a novel deep neural network to remedy the ambiguity caused by the focal length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-task Learning</head><p>Multi-task learning aims to improve the performance of various computer vision problems. According to the design of network structure and loss function, the methods of multi-task learning are mainly divided into two categories. One of the methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> is to let the network automatically learn the connection relationship among tasks, where the loss function is a weighted sum of all branches. Another method <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> is to search the optimal descent direction of the gradient by adaptively selecting the weighting factors during the training process. Xu et al. <ref type="bibr" target="#b53">[54]</ref> proposed a PAD-Net to utilize auxiliary tasks to facilitate optimizing the semantic segmentation and depth estimation. R. Zamir et al. <ref type="bibr" target="#b54">[55]</ref> proposes a fully computational approach to model the structure of space of visual tasks, building the relationship between the depth estimation and the semantic segmentation from normals. However, these methods only directly learn the two tasks without explicitly exploring the geometric constraints between the monocular depth estimation and semantic segmentation. In this work, we propose an SOSD-Net to achieve a deep geometric relationship between monocular depth estimation and semantic segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe the proposed SOSD-Net for monocular depth estimation and semantic segmentation. We first introduce the geometry constraint to embed the deep relation of the monocular depth and semantic information, then elaborate the network architecture of the SOSD-Net. Finally, we present the details of the proposed learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geometry Constraint</head><p>Without loss of generality, we assume the space object is linear, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. According to the perspective projection model <ref type="bibr" target="#b55">[56]</ref>, the image of the planar space object S under (f, O) is I, which can be formulated by the following equation.</p><formula xml:id="formula_0">d ? ? u 1 v 1 1 ? ? = ? ? f x 0 u x 0 f y u y 0 0 1 ? ? ? ? X 1 Y 1 d ? ?<label>(1)</label></formula><p>where (X 1 , Y 1 ) is the coordinate of a space point, (u, v) is the coordinates of the space point on the image, (u x , u y ) is the coordinate of the principal point, and (f x , f y ) corresponds to the camera's focal length. Through the above projection equation, it is notable that the monocular depth estimation is an ill-posed problem, which makes it difficult to accurately recover the true depth. However, if we only consider the object-level depth and assume that the depth of the inner region of the object is approximately consistent, the geometric relationship can be reduced to the following 2D-3D size information of an object.</p><formula xml:id="formula_1">?u = f x ?X d , ?v = f y ?Y d (2) where ?u = u 1 ?u 2 , ?v = v 1 ?v 2 , ?X = X 1 ?X 2 , ?Y = Y 1 ?Y 2 .</formula><p>Furthermore, we can extend the above 2D-3D size information to 2D-3D area information as below. The geometric relationship of the equation <ref type="formula">(3)</ref> is called semantic objectness in this paper, which embeds the correlation of the semantic and the corresponding depth. In general, after semantic segmentation, we can obtain the 2D area information ?u?v of an object by implementing a simple post-process operation. In addition, the area information ?X?Y of an object under a specific perspective is unique. Thus, we can establish a close relationship between the object-level semantic and the corresponding depth.</p><formula xml:id="formula_2">d 2 = f x f y ?X?Y ?u?v (3)</formula><p>In practice, the depth of the inner region of most objects is not consistent. However, if only taking a local area of the object into consideration, then the assumption of consistent depth is satisfied. Current deep neural networks can express very complex functions due to their non-linearity and a large number of parameters. Therefore, we use this powerful tool to express the local implicit relationship, and introduce a novel deep convolutional network (SOSD-Net) to embed the semantic objectness relation between the monocular depth estimation and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SOSD-Net Architecture</head><p>The overall SOSD-Net architecture is depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. It consists of four components as described below: a CNN backbone to extract a contextual feature, a Decoder for three feature maps (Common Representation, Semantic Feature, Depth Feature), a semantic-to-depth unit to learn monocular depth, and a depth-to-semantic unit to learn semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Backbone</head><p>The backbone takes an input image and generates an intermediate feature map to be processed by each subtask. Similar to DeepLabV3+ <ref type="bibr" target="#b13">[14]</ref>, the backbone of the proposed SOSD-Net consists of xception-65 <ref type="bibr" target="#b56">[57]</ref> and three parallel components, i.e., an atrous spatial pyramid pooling (ASPP), a cross-channel  learner, and a full-image extractor, as shown in <ref type="figure">Figure 4</ref>. The ASPP and the pure 1 ? 1 convolution are applied to effectively fuse complex contextual information, guided by the global information from the full-image extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Decoder</head><p>Based on the global feature map from the Backbone, the role of the decoder is mainly to extract fine-grained feature maps for Semantic Feature, Common Representation, and Depth Feature, respectively. In order to remedy the structure loss caused by the stride convolution, the decoder fuses the Refined fp (green block) from the Backbone in <ref type="figure" target="#fig_2">Figure 3</ref>. Based on the Refined fp, we first take one convolution layer to extract information for each task, respectively. Then combining the upsampling global feature maps, the decoder employs one convolutional layer and two convolutional layers to generate the Semantic Feature and Depth Feature, respectively. The detailed parameters of the decoder are shown in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Semantic-to-Depth</head><p>Having obtained the common features, a classic decoder for monocular depth estimation employs the skip-connection and upsampling modules to obtain the high-resolution depth maps. The weights of the network are updated by minimizing the depth loss function. In order to embed the semantic objectness information for the monocular depth estimation, as described in equation <ref type="formula">(3)</ref>, we propose a semantic-to-depth module to effectively fuse the deep 2D-3D area information. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, the deep 3D area information is extracted from the common feature maps, defined as the deep area information of an object under a certain perspective. In order to maintain the detailed structure of the subtasks, we set the stride of the convolution to 1. In the semantic-to-depth unit of the SOSD-Net, we first utilize two convolution layers with 2 and 1 channels to generate a heatmap, which is referred to as 3D latent shared representation of an object, related to ?X?Y , as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. In addition, we take another two convolution layers with 2 and 1 channels to obtain another heatmap from the semantic segmentation, which is the 2D latent shared representation of an object, related to (?u?v) ?1 . Since the public datasets are of fixed-focal-length, we use batch normalization to automatically embed the focal length information into the two sub-branches. Next, we employ the deep 2D-3D area feature to infer the depth cue by leveraging pixel-wise multiplication and square root operations. After combining the information from the depth features and the Pixel Sqrt, this module conducts a convolution layer with 1 channel to infer depth maps. At last, we employ an upsampling operation (bilinear interpolation) on the previous depth to obtain the full-resolution depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Depth-to-Semantic</head><p>Similar to the semantic-to-depth, the semantic branch also embeds the deep 2D-3D area feature by integrating the features from previous components. However, in terms of implementation details, the depth-to-semantic is different from the semantic-to-depth, as shown in <ref type="figure" target="#fig_6">Figure 7</ref>. For example, we first apply two convolution layers with 64 and 32 channels to generate a latent variable from the pure depth branch, which is related to the d ?2 . As for the 3D latent shared representation of an object, we realize it by leveraging another two convolution layers with 64 and 32 channels on the common representation. After fusing the information from the two sub-branches by pixel-wise multiplication, we employ a 1 ? 1 convolution with 32 channels to parse the semantic cue. Having concatenated the feature maps from the semantic feature and the semantic cue, this module adds one convolution layer to infer the semantic segmentation. To obtain the full-resolution segmentation, we take the same upsampling operation to increase the resolution of the semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning and Loss Function</head><p>In essence, the weight learning of deep networks is equivalent to a problem of maximum likelihood estimation. In the field of classical machine learning, simultaneously learning the depth and semantic segmentation from a single image can be regarded as a Gaussian Mixture Model (GMM), which can be effectively solved by the EM algorithm <ref type="bibr" target="#b57">[58]</ref>. In the process of parameter optimization, EM can simplify the complex estimation problem. It first optimizes some parameters (? 1 ) by fixing other parameters (? 2 ) in the parameter space, and then optimize the other parameters ? 2 by fixing the parameters ? 1 until achieving the optimal parameters. Inspired by the strategy of EM, we propose an effective training method to alternatively learn the weights of the SOSD-Net by taking the deep 2D-3D area information as hidden variables, which first learns the weight of the depth branch by fixing the weight of the semantic branch, and then learns the weight of the semantic branch by fixing the weight of the depth branch until the convergence of the proposed model.</p><p>Let Y = ?(I, x sem ; ?) denote the fused outputs of the depth branch given an image I and its semantic feature x sem , where ? = (? dep , ? 3d , ? 2d , ? com ) corresponds to the parameters involved in the depth features, ?X?Y , (?x?y) ?1 and backbone network, respectively, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. As for the learning if p = 0 then ? learning depth <ref type="bibr">4:</ref> for t = 1 to 3 do ? (? dep , ? 3d , ? 2d ) 5:</p><formula xml:id="formula_3">?(t) = ?(t) ? ? ? ?(t) ?(I, x sem ; ?) 6:</formula><p>end for 7:</p><formula xml:id="formula_4">? com = ? com ? ? 3 t=1 ? t ? ? com ?(I, x sem ; ?) 8: p ? 1 9:</formula><p>else ? learning semantic 10:</p><formula xml:id="formula_5">for t = 1 to 3 do ? (? sem , ? 3d , ? d ?2 )</formula><p>11:</p><formula xml:id="formula_6">?(t) = ?(t) ? ? ? ?(t) ?(I, x dep ; ?) 12:</formula><p>end for 13:</p><formula xml:id="formula_7">? com = ? com ? ? 3 t=1 ? t ? ? com ?(I, x dep ; ?) 14: p ? 0 15:</formula><p>end if 16: end for process of the monocular depth, we first update the weights of the semantic-todepth, and then merge the backward loss from each branch to learn the weights of the common backbone. For example, for the green branch ((?x?y) ?1 ) in <ref type="figure" target="#fig_5">Figure 6</ref>, the update process of the weights can be formulated by the following equation.</p><formula xml:id="formula_8">? 2d (t) = ? 2d (t) ? ? ? ? 2d (t) ?(I, x sem ; ?)<label>(4)</label></formula><p>where ? is the learning rate, and ? ? 2d (t) ?(I, x sem ; ?) is the gradient of ? with respect to ? 2d . We use the same strategy of weights update for the purple branch, and the orange branch, respectively. Having updated the weights of the three branches, we merge the backward loss and learn the weights of the backbone as the following equation.</p><formula xml:id="formula_9">? com = ? com ? ? 3 t=1 ? t ? ? com ?(I, x sem ; ?)<label>(5)</label></formula><p>where ? t is the weighting factor of the t-subnet in terms of gradient-based backpropagation. We set ? t to 1 in this paper. Similarly, when learning the semantic segmentation, we use the same strategy to lean the weights of the proposed model, taking the monocular depth information and the deep 3D area information as hidden variables. The detailed learning strategy is shown in Algorithm 1. Finally, the parameters of the proposed network are learned by the gradient-based backpropagation method, whose goal is minimizing the loss function defined on the prediction and the ground truth.</p><p>Semantic segmentation. The cross-entropy loss is employed to learn the pixel-wise class probabilities, which is obtained by averaging the loss over the pixels with semantic labels in each mini-batch during the training phase.</p><formula xml:id="formula_10">L semantic = ? 1 N N i=1 c * i log(c i )<label>(6)</label></formula><p>where c i = e zi / c e zi,c is the class prediction at pixel i given the output z of the final feature maps, c * i is the corresponding ground truth, and N is the number of pixels.</p><p>Depth estimation. L 1 loss is employed to learn the pixel-wise depth, which minimizes the absolute Euclidean distance between the depth prediction and the corresponding ground truth.</p><formula xml:id="formula_11">L depth = 1 N N i=1 |y i ? y * i |<label>(7)</label></formula><p>where y i is the depth prediction of the i-th pixel, y * i is the corresponding ground truth, and N is the number of valid pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Analysis</head><p>To demonstrate the effectiveness of the proposed SOSD-Net for simultaneous learning the monocular depth and semantic segmentation, we carry out comprehensive experiments on two publicly available datasets: CityScapes <ref type="bibr" target="#b20">[21]</ref> and NYU v2 <ref type="bibr" target="#b21">[22]</ref>. In the following subsections, we report the details of our implementation and the evaluation results. Some ablations studies based on CityScapes are discussed to give a more detailed analysis of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets and Data Augmentation. The CityScapes dataset <ref type="bibr" target="#b20">[21]</ref> is a large dataset for road scene understanding. It comprises stereo imagery from automotive-grade stereo cameras with 22cm baseline, labeled with instance and semantic segmentation of 20 classes. Inverse depth images are provided, labeled with the SGM method <ref type="bibr" target="#b58">[59]</ref>. The dataset was collected over 50 different cities spanning several months, which consists of training, validation, and test sets containing 2,975, 500, and 1,525 images, respectively.</p><p>Following the suggestion in Ozan et al. <ref type="bibr" target="#b19">[20]</ref>, the input images and the corresponding depth maps are resized to 256 ? 512. The training data are augmented on the fly during the training phase. The RGB and depth images are scaled with a randomly selected ratio from {0.5, 0.75, 1, 1.25, 1.5, 1.75}. In addition, the RGB-D images are also transformed using color transformations and flip with a chance of 0.5. Please note that the proposed method has the potential to support training on full resolution input by the use of online data preparation. For example, before feeding the data to the model, we can randomly crop the input image to small patches, which will consume the same memory of GPU with the strategy of resizing input samples. However, the state-of-the-art approaches adopted the samples with a fixed resolution to train their models. For the fairness of the comparison, we employ the same resolution to train the proposed model for evaluation.</p><p>The NYU v2 <ref type="bibr" target="#b21">[22]</ref> consists of 464 scenes (480 ? 640), captured using Microsoft Kinect. Following the official split, the training dataset is composed of 249 scenes with 795 pair-wise images, and the testing dataset includes 215 scenes with 654 pair-wise images. The input images and the corresponding depths are augmented on the fly during the training phase, which is scaled with a randomly selected ratio from {1, 1.2, 1.5}, transformed using color transformations, and flipped with a chance of 0.5.</p><p>Evaluation Metrics. For quantitative evaluation of the depth estimation on the NYU v2 dataset, we report errors obtained with the following widely adopted error metrics. To evaluate the performance of the semantic segmentation on the NYU v2 dataset, we use mean Intersection over Union (mIoU), mean accuracy, and pixel accuracy as metrics.</p><p>? Average relative error:</p><formula xml:id="formula_12">rel = 1 N yi?|N | |yi?y * i | y * i ? Root mean squared error: rms = 1 N yi?|N | |y i ? y * i | 2</formula><p>? Average log 10 error:</p><formula xml:id="formula_13">log 10 = 1 N yi?|N | |log 10 (y i ) ? log 10 (y * i )|</formula><p>? Accuracy with threshold t: percentage (%) of y i subject to max(</p><formula xml:id="formula_14">y * i yi , yi y * i ) = ? &lt; t(t ? [1.25, 1.25 2 , 1.25 3 ])</formula><p>where y i is the estimated depth, y * i denotes the corresponding ground truth, and N is the total number of valid pixels in all images of the validation set.</p><p>For the CityScapes dataset, we use mean absolute error and mIoU to evaluate the depth estimation and semantic segmentation, respectively. Implementation Details. We implement the proposed model using both PaddlePaddle <ref type="bibr" target="#b59">[60]</ref> and TensorFlow frameworks, and train the network on the NVIDIA Tesla P40 with 24GB memory. The results in this paper are from the TensorFlow implementation. The objective function is optimized using Adam method <ref type="bibr" target="#b60">[61]</ref>. During the initialization stage, the weight layers in the first part of the architecture are initialized using the corresponding pre-trained model (Xception) on the ILSVRC <ref type="bibr" target="#b61">[62]</ref> dataset for image classification. The weights of the specific task are assigned by sampling a Gaussian with zero mean and 0.01 variance, and the learning rate is set at 0.0001. We set the batch size of the two datasets to 16. Finally, our model is trained with 60 epochs for the NYU Depth v2 dataset, and 40 epochs for the CityScapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We conduct various ablation studies to analyze the performance of our approach. The baseline model (MTL) is a classical multi-task model with a backbone extracting common features, with two task-specific paths to infer the depth and the semantic, respectively, and the corresponding optimization objective is  a linear combination of each branch loss. According to the description in section 3, the improved versions of the baseline include: (i) SOSD-Net (SOSD-Net: adding semantic objectness to the baseline model), (ii) ESOSD-Net (SOSD-Net with the EM learning strategy). The comparative experimental results are shown in <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table">Table 3</ref>. Note that the baseline model, the improved versions, and the single-task model share the same advanced backbone (Xception), which extracts the features for the subnets to infer specific-task information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">SOSD-Net</head><p>For the CityScapes dataset, it can be observed that SOSD-Net obtains better performance than the MTL model, e.g., SOSD-Net improves the Segmentation mIoU by 1.6% (from 65.6% to 67.2%), and reduces the Disparity error by 0.06 (from 2.64 to 2.58), as reported in <ref type="table" target="#tab_0">Table 1</ref>. SOSD-Net also outperforms the Semantic only method, improving the mIoU of semantic segmentation by a margin of 5.2, while shows comparable performance with Depth only approach.  <ref type="table">Table 3</ref>: Quantitative improvement when learning monocular depth with our proposed model. Experiments are conducted on the NYU dataset (480 ? 640). Results are shown from the test set. We observe a significant performance improvement when training SOSD-Net with EMstyle strategy, over both single-task models and MTL.</p><p>In addition, comparing with independent models and MTL model, SOSD-Net still maintains comparable inference time and the number of parameters.</p><p>Meanwhile, we also evaluated SOSD-Net model on the NYU v2 dataset. As reported in <ref type="table" target="#tab_1">Table 2</ref>, SOSD-Net outperforms MTL (0.433 vs. 0.417, 0.625 vs. 0.610, 0.722 vs. 0.710) and Semantic only model (0.433 vs. 0.385, 0.625 vs. 0.591, 0.722 vs. 0.687) on all metrics of semantic segmentation. As for depth estimation evaluation, SOSD-Net also obtains obvious performance gains on all metrics, as shown in <ref type="table">Table 3</ref>. These ablation studies demonstrate that using the semantic objectness is able to improve the performance of monocular depth estimation and semantic segmentation.</p><p>To further investigate the effect of SOSD-Net on the two tasks, we visualize the feature maps leaned from the semantic-to-depth unit, as shown in <ref type="figure" target="#fig_8">Figure 8</ref>. The final depth is learned by fusing the information from the pure depth branch and the semantic-to-depth branch, respectively. Compared with the pure depth branch (third row, second column), the final depth (first row, second column) has a more detailed structure over the entire area of the pedestrians, which benefits from the semantic-to-depth branch (third row, first column). The visualization further verifies the outstanding performance of the semantic-to-depth branch, so it is consistent with the geometric constraint, as described in equation <ref type="bibr" target="#b2">(3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Segmentation  <ref type="table">Table 4</ref>: Performance of the multi-task algorithms in semantic segmentation and depth estimation on the CityScapes dataset (sub-sampled to a resolution of 256 ? 512). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">EM Learning Strategy</head><p>We verify the effectiveness of the EM learning strategy in boosting the performance of the monocular depth estimation and semantic segmentation. For the CityScapes dataset, it can be observed that the ESOSD-Net clearly outperforms MTL and SOSD-Net in the two tasks, as reported in <ref type="table" target="#tab_0">Table 1</ref>. For example, compared with the SOSD-Net, ESOSD-Net improves the Segmentation mIoU by 1.0% (from 67.2% to 68.2%) and reduces the Disparity error by 0.17 (from 2.58 to 2.41). Note that in terms of inference time and the number of parameters, ESOSD-Net is the same as SOSD-Net. In addition, ESOSD-Net also outperforms the single-task models, improving the mIoU of semantic segmentation by a margin of 6.2, and reducing the Disparity error by 0.06 (from 2.47 to 2.41).</p><p>In addition, we also evaluated ESOSD-Net on the NYU v2 dataset. As reported in <ref type="table" target="#tab_1">Table 2</ref>, ESOSD-Net obviously outperforms Semantic only model, MTL, and SOSD-Net on all metrics of semantic segmentation. For example, compared with SOSD-Net, ESOSD-Net improves the Mean IoU by 1.7% (from 0.433 to 0.450), the Mean Accuracy by 2.2% (from 0.625 to 0.647), and Pixel Accuracy by 1.1% (from 0.722 o 0.733).</p><p>As for the depth estimation evaluation, ESOSD-Net also leads to a large improvement on all metrics, as shown in <ref type="table">Table 3</ref>. For example, compared with SOSD-Net, ESOSD-Net reduces the rel, rms, and log 10 by 0.4%, 0.013 and 0.002, and simultaneously improving the ? 1 , ? 2 , and ? 3 by 0.8%, 0.5%, and 0.1%, respectively. In addition, ESOSD-Net also outperforms the Depth only method, reducing the rel, rms, and log 10 by 2.2%, 0.123, and 0.016, and simultaneously improving the ? 1 , ? 2 , and ? 3 by a margin of 9.2%, 2.7%, and 0.8%, respectively. The results of the ESOSD-Net clearly outperforms single-task, MTL, and SOSD-Net, further demonstrating the effectiveness of the proposed EM learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Benchmark Performance</head><p>In the first series of experiments, we focus on the CityScapes dataset <ref type="bibr" target="#b20">[21]</ref>. The proposed model is evaluated and compared with the state-of-the-art methods including Kendall et al. <ref type="bibr" target="#b17">[18]</ref>, GradNorm <ref type="bibr" target="#b18">[19]</ref> and Ozan <ref type="bibr" target="#b19">[20]</ref>, as reported in <ref type="table">Table 4</ref>. It can be seen that our ESOSD-Net improves the accuracy of semantic segmentation by a margin of 2% ? 4%, compared with previous methods in all settings. For inverse depth estimation, our ESOSD-Net outperforms the previous methods with 0.1 ? 0.2 points gap on the mean absolute error.</p><p>Meanwhile, we also evaluated the proposed model on the NYU Depth v2 dataset. The comparison with the state-of-the-art algorithms are shown in <ref type="table" target="#tab_4">Table 5 and Table 6</ref>, respectively. As observed from <ref type="table" target="#tab_4">Table 5</ref>, compared with Deng et al. <ref type="bibr" target="#b62">[63]</ref> , FCN <ref type="bibr" target="#b0">[1]</ref> , Eigen and Fergus <ref type="bibr" target="#b3">[4]</ref>, and Context <ref type="bibr" target="#b35">[36]</ref> , the proposed ESOSD-Net achieves a remarkable improvement. When comparing the RefineNet <ref type="bibr" target="#b25">[26]</ref>, our proposed method shows outstanding performance on the Mean Accuracy and achieves competing performance on the mean IoU and pixel accuracy. As observed in <ref type="table" target="#tab_4">Table 5</ref>, our proposed method is also competitive with the two-stage approaches, e.g., ESOSD-Net is comparable to the two-stage approach PAD-Net <ref type="bibr" target="#b53">[54]</ref> (0.450 vs. 0.502, 0.647 vs. 0.623, 0.733 vs. 0.752), and outperforms Gupta et al. <ref type="bibr" target="#b63">[64]</ref> and Arsalan et al. <ref type="bibr" target="#b52">[53]</ref> in all metrics. These results further demonstrate the effectiveness of ESOSD-Net. <ref type="table" target="#tab_6">Table 6</ref> shows the evaluation result of the depth estimation. With the same number of samples (795) and a one-stage training strategy, the proposed ESOSD-Net model outperforms the state-of-the-art methods. For example, compared with the E. and F. <ref type="bibr" target="#b3">[4]</ref>, ESOSD-Net improves the rel by 1.3% (from 0.158 to 0.145), the ? 1 by 3.6 % (from 76.9% to 80.5%), the ? 2 by 1.2 % (from 95.0% to 96.2%), and the ? 3 by 0.4 % (from 98.8% to 99.2%), respectively. Meanwhile, ESOSD-Net reports the rms of 0.514, an improvement of 0.127 over 0.641 achieved by the E. and F. <ref type="bibr" target="#b3">[4]</ref>. The experiments demonstrate the superior performance of the ESOSD-Net in depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mean IoU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Accuracy</head><p>Pixel Accuracy Two-stage: Gupta et al. <ref type="bibr" target="#b63">[64]</ref> 0.286 -0.603 Arsalan et al. <ref type="bibr" target="#b52">[53]</ref> 0.392 0.523 0.686 PAD-Net <ref type="bibr" target="#b53">[54]</ref> 0.502 0.623 0.752 One-stage: Deng et al. <ref type="bibr" target="#b62">[63]</ref> -0.315 0.638 FCN <ref type="bibr" target="#b0">[1]</ref> 0.292 0.422 0.600 Eigen and Fergus <ref type="bibr" target="#b3">[4]</ref> 0.341 0.451 0.656 Context <ref type="bibr" target="#b35">[36]</ref> 0.406 0.536 0.700 RefineNet <ref type="bibr" target="#b25">[26]</ref> 0.465 0.589 0.736 ESOSD-Net 0.450 0.647 0.733 When comparing with other one-stage approaches using a large number of training samples, ESOSD-Net also achieves competing performance. As reported in <ref type="table" target="#tab_6">Table 6</ref>, ESOSD-Net outperforms He et al. <ref type="bibr" target="#b11">[12]</ref> on all metrics, and achieves comparable performance with Lai <ref type="bibr" target="#b43">[44]</ref> and DORN <ref type="bibr" target="#b4">[5]</ref> (rms, ? 1 , ? 2 , ? 3 ), while shows slightly weakness on rel and log 10 . This is mainly because that Lai <ref type="bibr" target="#b43">[44]</ref> and DORN <ref type="bibr" target="#b4">[5]</ref> utilized a large number of samples: 96k and 120k, respectively, which are 120? and 150? than 795 samples used in our model.</p><p>At last, we can observe that the depth performance of ESOSD-Net is also competitive with the two-stage approaches, e.g., ESOSD-Net outperforms the two-stage approach Joint HCRF <ref type="bibr" target="#b14">[15]</ref> and Jafari et al. <ref type="bibr">[</ref>  terms of the rms (0.514 vs. 0.582), ? 2 (0.962 vs. 0.954) and ? 3 (0.992 vs. 0.987), while shows slightly weakness on the rel (0.145 vs. 0.120), log 10 (0.062 vs. 0.055), and ? 1 (0.805 vs. 0.817). Nevertheless, it should be mentioned that PAD-Net <ref type="bibr" target="#b53">[54]</ref> is trained by auxiliary tasks with two-stage strategy, benefiting from the additional supervised information.</p><p>In summary, the proposed method achieves better performance than the state-of-the-art methods using a one-stage training strategy, under the same number of training samples. Furthermore, compared with the two-stage methods, the proposed ESOSD-Net also achieves competing performance. The experiments strongly demonstrate the effectiveness and superior performance of the ESOSD-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>We select several samples from the CityScapes dataset and the NYU Depth v2 dataset for visualization, as shown in <ref type="figure" target="#fig_9">Figure 9</ref>. It is obvious that even there exist holes in the depth ground truth of some vehicles, ESOSD-Net can not only predict accurate depth maps with good smoothness, it can also parse the same visual semantic effect as the ground truth. In addition, as shown in <ref type="figure" target="#fig_0">Figure 10</ref>, the proposed model exhibits very close qualitative visualization effects in comparison with the corresponding ground truth on the NYU v2 dataset, which demonstrates the effectiveness of the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a geometric constraint to reveal the semantic objectness relationship between the monocular depth estimation and semantic segmentation. Through this constraint, we can employ the semantic information of the scene to alleviate the ambiguity in monocular depth estimation, and simultaneously boost the accuracy of the semantic segmentation. In order to explore this constraint, the paper proposes a novel network structure (SOSD-Net) to effectively embed semantic objectness information from the geometry cues and scene parsing. We have also proposed an EM-style learning strategy to effectively train the SOSD-Net. Through extensive experimental evaluations and comparisons on the CityScapes dataset and NYU v2 dataset, the proposed ESOSD-Net achieves outstanding performance over state-of-the-art multi-task methods using the one-stage training strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Joint optimization of monocular depth, semantic, and semantic objectness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The projection process of a planar object. Where O is the optical center, I is the image of the planar object S, d is the depth of the object, and f is the focal length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Our proposed SOSD-Net architecture leverages a shared-encoder backbone and a Decoder for semantic feature, common representation and depth feature, followed by depth-tosemantic and semantic-to-depth modules to learn semantic segmentation and depth estimation from a single image, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>The detailed structure of the Backbone. The detailed structure of the Decoder. The Refined fp (green block) is generated from the Backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The structure of the semantic-to-depth module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>The structure of the depth-to-semantic module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 EM Learning Strategy 1 :</head><label>1</label><figDesc>Parameters Initialization, set p ? 0 2: for i = 1 to N do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of the feature maps from the semantic-to-depth unit. The first row shows the input image and monocular depth estimation, the second row shows the feature map of the 2D latent shared representation and 3D latent shared representation, related to (?u?v) ?1 and ?X?Y , the third row shows the depth intensity from the semantic-to-depth and pure depth branch, and the last row shows the ground truth of the semantic segmentation and depth estimation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative examples of monocular depth estimation and 19-class scene parsing results on the CityScapes dataset (256 ? 512). The second and the fourth rows corresponding to the predictions of the depth estimation and semantic segmentation. The third and the last rows corresponding to the ground truth of the depth estimation and semantic segmentation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative examples of monocular depth estimation and 40-classes scene parsing results on the NYU Depth v2 dataset (480 ? 640). The second and the fourth rows corresponding to the predictions of the depth estimation and semantic segmentation. The third and the last rows corresponding to the ground truth of the depth estimation and semantic segmentation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative improvement when learning semantic segmentation and depth with the proposed SOSE-Net and EM-style learning strategy. Experiments were conducted on the CityScapes dataset (sub-sampled to a resolution of 256?512). Results are shown from the validation set. It is clear that the inference speed and the number of parameters are comparable,</figDesc><table><row><cell>Method</cell><cell>Segmentation mIoU [%]</cell><cell cols="2">Disparity error [px]</cell><cell cols="2">Inference speed (ms)</cell><cell>Number of parameters (M)</cell></row><row><cell>Semantic only</cell><cell>62.0</cell><cell>-</cell><cell></cell><cell cols="2">139.1</cell><cell>23.4</cell></row><row><cell>Depth only</cell><cell>-</cell><cell cols="2">2.47</cell><cell cols="2">140.7</cell><cell>23.4</cell></row><row><cell>MTL</cell><cell>65.6</cell><cell cols="2">2.64</cell><cell cols="2">142.2</cell><cell>23.6</cell></row><row><cell>SOSD-Net</cell><cell>67.2</cell><cell cols="2">2.58</cell><cell cols="2">159.0</cell><cell>24.0</cell></row><row><cell>ESOSD-Net</cell><cell>68.2</cell><cell cols="2">2.41</cell><cell cols="2">159.0</cell><cell>24.0</cell></row><row><cell cols="2">Method</cell><cell>Mean IoU</cell><cell cols="2">Mean Accuracy</cell><cell>Pixel Accuracy</cell></row><row><cell cols="3">Semanitc only 0.385</cell><cell cols="2">0.591</cell><cell>0.687</cell></row><row><cell cols="2">MTL</cell><cell>0.417</cell><cell cols="2">0.610</cell><cell>0.710</cell></row><row><cell cols="2">SOSD-Net</cell><cell>0.433</cell><cell cols="2">0.625</cell><cell>0.722</cell></row><row><cell cols="2">ESOSD-Net</cell><cell>0.450</cell><cell cols="2">0.647</cell><cell>0.733</cell></row></table><note>we observe an improvement of performance when training with SOSD-Net, over both single- task models and MTL. Additionally, we observe a larger improvement when training on the two-tasks using the EM-style strategy (ESOSD-Net). The result shows that SOSD-Net can automatically build a better relation to embedding the scene parsing and depth estimation, and the EM-style can learn the two-tasks more effectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative improvement when learning semantic segmentation with our proposed model. Experiments are conducted on the NYU dataset (480 ? 640). Results are shown from the test set. It is observed that SOSD-Net with EM-style achieves better performance over both single-task models and MTL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparison with state-of-the-art methods on the scene parsing task on the NYU Depth v2 dataset (480 ? 640).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Quantitative comparison with state-of-the-art methods on the depth estimation task on the NYU Depth v2 dataset (480 ? 640).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the Shenzhen Fundamental Research Fund (Subject Arrangement) under Grant JCYJ20170412170602564, and the National Natural Science Foundation of China under 61822603, Grant U1813218, Grant U1713214, Grant 61672306, Grant 61572271. This work was jointly supported by Baidu Inc, Tsinghua University, and the University of Ryerson. The authors would like to thank Baidu for providing the computing resources. Thanks go to Ruijie Hou, Lixia Shen and Guangyao Yang for discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast depth extraction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1729881416663370</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep feature augmentation for occluded image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">107737</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mdfn: Multi-scale deep feature learning network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">107149</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network, ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="78" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Triplet adversarial domain adaptation for pixel-level classification of vhr remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3558" to="3573" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning depth from single images with deep neural network embedding focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4676" to="4689" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic map building based on object detection for indoor navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="525" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Indoor segmentation and support inference from rgbd images</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<title level="m">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4151" to="4160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>Conditional random fields as recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Van Den Hengel, I. Reid, Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional scale invariance for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?au?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3-d depth reconstruction from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning-based, automatic 2d-to-3d image and video conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3485" to="3496" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5354" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spindle-net: Cnns for monocular depth inference with dilation kernel method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2504" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Depth-map completion for large indoor scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">107112</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep unsupervised binary descriptor learning through locality consistency and self distinctiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint semantic segmentation and depth estimation with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pad-net: multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
	<note>Taskonomy: Disentangling task transfer learning</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Guide to three dimensional structure and motion factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<ptr target="https://github.com/PaddlePaddle/Paddle" />
		<title level="m">Paddlepaddle: Parallel distributed deep learning</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Semantic segmentation of rgbd images with mutex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1733" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analyzing modular cnn architectures for joint depth prediction and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4620" to="4627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
