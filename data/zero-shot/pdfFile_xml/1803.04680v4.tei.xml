<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Frame Quality Enhancement for Compressed Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yang</surname></persName>
							<email>yangren@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Xu</surname></persName>
							<email>maixu@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zulin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Li</surname></persName>
							<email>tianyili@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Frame Quality Enhancement for Compressed Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, ignoring the similarity between consecutive frames. In this paper, we investigate that heavy quality fluctuation exists across compressed video frames, and thus low quality frames can be enhanced using the neighboring high quality frames, seen as Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as a first attempt in this direction. In our approach, we firstly develop a Support Vector Machine (SVM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are as the input. The MF-CNN compensates motion between the non-PQF and PQFs through the Motion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement subnet (QE-subnet) reduces compression artifacts of the non-PQF with the help of its nearest PQFs. Finally, the experiments validate the effectiveness and generality of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code of our MFQE approach is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the past decades, video has become significantly popular over the Internet. According to the Cisco Data Traffic Forecast <ref type="bibr" target="#b6">[7]</ref>, video generates 60% of Internet traffic in 2016, and this figure is predicted to reach 78% by 2020. When transmitting video over the bandwidth-limited Internet, video compression has to be applied to significantly save the coding bit-rate <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26]</ref>. However, the compressed video inevitably suffers from compression artifacts <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>, which may severely degrade the Quality of Experience (QoE). Therefore, it is necessary to study on quality enhancement for compressed video. * Mai Xu is the corresponding author of this paper.  Recently, there has been increasing interest in enhancing the visual quality of compressed image/video <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42]</ref>. For example, Dong et al. <ref type="bibr" target="#b8">[9]</ref> designed a four-layer Convolutional Neural Network (CNN) <ref type="bibr" target="#b21">[22]</ref>, named AR-CNN, which considerably improves the quality of JPEG images. Later, Yang et al. designed a Decoder-side Scalable CNN (DS-CNN) for video quality enhancement <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref>. However, when processing a single frame, all existing quality enhancement approaches do not take any advantage of information in the neighbouring frames, and thus their performance is largely limited. As <ref type="figure" target="#fig_0">Figure 1</ref> shows, the quality of compressed video dramatically fluctuates across frames. Therefore, it is possible to use the high quality frames (i.e., Peak Quality Frames, called PQFs 1 ) to enhance the quality of their neighboring low quality frames (non-PQFs). This can be seen as Multi-Frame Quality Enhancement (MFQE), similar to multi-frame super-resolution <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>This paper proposes an MFQE approach for compressed video. Specifically, we first investigate that there exists large quality fluctuation across frames, for video sequences compressed by almost all coding standards. Thus, it is necessary to find PQFs that can be used to enhance the quality of their adjacent non-PQFs. To this end, we train a Sup-port Vector Machine (SVM) as a no-reference method to detect PQFs. Then, a novel Multi-Frame CNN (MF-CNN) architecture is proposed for quality enhancement, in which both the current frame and its adjacent PQFs are as the inputs. Our MF-CNN includes two components, i.e., Motion Compensation subnet (MC-subnet) and Quality Enhancement subnet (QE-subnet). The MC-subnet is developed to compensate the motion between the current non-PQF and its adjacent PQFs. The QE-subnet, with a spatio-temporal architecture, is designed to extract and merge the features of the current non-PQF and the compensated PQFs. Finally, the quality of the current non-PQF can be enhanced by QEsubnet that takes advantage of the high quality content in the adjacent PQFs. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the current non-PQF (frame 96) and the nearest PQFs (frames 93 and 97) are fed in to the MF-CNN of our MFQE approach. As a result, the low quality content (basketball) of the non-PQF (frame 96) can be enhanced upon the same content but with high quality in the neighboring PQFs (frames 93 and 97). Moreover, <ref type="figure" target="#fig_0">Figure 1</ref> shows that our MFQE approach also mitigates the quality fluctuation, because of the considerable quality improvement of non-PQFs.</p><p>The main contributions of this paper are: (1) We analyze the frame-level quality fluctuation of video sequences compressed by various video coding standards. <ref type="bibr" target="#b1">(2)</ref> We propose a novel CNN-based MFQE approach, which can reduce the compression artifacts of non-PQFs by making use of the neighboring PQFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Quality enhancement. Recently, extensive works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4]</ref> have focused on enhancing the visual quality of compressed image. Specifically, Foi et al. <ref type="bibr" target="#b9">[10]</ref> applied pointwise Shape-Adaptive DCT (SA-DCT) to reduce the blocking and ringing effects caused by JPEG compression. Later, Jancsary et al. <ref type="bibr" target="#b17">[18]</ref> proposed reducing JPEG image blocking effects by adopting Regression Tree Fields (RTF). Moreover, sparse coding was utilized to remove the JPEG artifacts, such as <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b5">[6]</ref>. Recently, deep learning has also been successfully applied to improve the visual quality of compressed image. Particularly, Dong et al. <ref type="bibr" target="#b8">[9]</ref> proposed a four-layer AR-CNN to reduce the JPEG artifacts of images. Afterwards, D 3 <ref type="bibr" target="#b37">[38]</ref> and Deep Dual-domain Convolutional Network (DDCN) <ref type="bibr" target="#b11">[12]</ref> were proposed as advanced deep networks for the quality enhancement of JPEG image, utilizing the prior knowledge of JPEG compression. Later, DnCNN was proposed in <ref type="bibr" target="#b45">[46]</ref> for several tasks of image restoration, including quality enhancement. Most recently, Li et al. <ref type="bibr" target="#b23">[24]</ref> proposed a 20layer CNN, achieving the state-of-the-art quality enhancement performance for compressed image.</p><p>For the quality enhancement of compressed video, the Variable-filter-size Residue-learning CNN (VRCNN) <ref type="bibr" target="#b7">[8]</ref> was proposed to replace the inloop filters for HEVC intra-coding. However, the CNN in <ref type="bibr" target="#b7">[8]</ref> was designed as a component of the video encoder, so that it is not practical for already compressed video. Most recently, a Deep CNNbased Auto Decoder (DCAD), which contains 10 CNN layers, was proposed in <ref type="bibr" target="#b36">[37]</ref> to reduce the distortion of compressed video. Moreover, Yang et al. <ref type="bibr" target="#b41">[42]</ref> proposed the DS-CNN approach for video quality enhancement. In <ref type="bibr" target="#b41">[42]</ref>, DS-CNN-I and DS-CNN-B, as two subnetworks of DS-CNN, are used to reduce the artifacts of intra-and intercoding, respectively. More importantly, the video encoder does not need to be modified when applying the DCAD <ref type="bibr" target="#b36">[37]</ref> and DS-CNN <ref type="bibr" target="#b41">[42]</ref> approaches. Nevertheless, all above approaches can be seen as single-frame quality enhancement approaches, as they do not use any advantageous information available in the neighboring frames. Consequently, the video quality enhancement performance is severely limited.</p><p>Multi-frame super-resolution.  <ref type="bibr" target="#b19">[20]</ref>, in which the neighboring frames are warped according to the estimated motion, and both the current and warped neighboring frames are fed into a super-resolution CNN to enlarge the resolution of the current frame. Later, Li et al. <ref type="bibr" target="#b22">[23]</ref> proposed replacing VSRnet by a deeper network with residual learning strategy. Besides, other deep learning approaches of video super-resolution were proposed in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>The aforementioned multi-frame super-resolution approaches are motivated by the fact that different observations of the same objects or scenes are probably available across frames of a video. As a result, the neighboring frames may contain the content missed when downsampling the current frame. Similarly, for compressed video, the low quality frames can be enhanced by taking advantage of their adjacent higher quality frames, because heavy quality fluctuation exists across compressed frames. Consequently, the quality of compressed video may be effectively improved by leveraging the multi-frame information. To the best of our knowledge, our MFQE approach proposed in this paper is the first attempt in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Quality fluctuation of compressed video</head><p>In this section, we analyze the quality fluctuation of compressed video alongside the frames. First, we establish a database including 70 uncompressed video sequences, selected from the datasets of Xiph.org <ref type="bibr" target="#b39">[40]</ref> and JCT-VC <ref type="bibr" target="#b0">[1]</ref>.  database including 70 uncompressed video sequences, selected from the datasets of Xiph.org <ref type="bibr" target="#b39">[40]</ref> and JCT-VC <ref type="bibr" target="#b0">[1]</ref>. We compress these sequences using various video coding standards, including MPEG-1 <ref type="bibr" target="#b10">[11]</ref>, MPEG-2 <ref type="bibr" target="#b29">[30]</ref>, MPEG-4 <ref type="bibr" target="#b30">[31]</ref>, H.264/AVC <ref type="bibr" target="#b38">[39]</ref> and HEVC <ref type="bibr" target="#b32">[33]</ref>. The quality of each compressed frame is evaluated in terms of Peak Signal-to-Noise Ratio (PSNR). <ref type="figure" target="#fig_1">Figure 2</ref> shows the PSNR curves of 8 video sequences, which are compressed by different standards. It can be seen that the compression quality obviously fluctuates along with the frames. We further measure the STandard Deviation (STD) of frame-level quality for each compressed video. As shown in <ref type="table" target="#tab_3">Table 1</ref>, the STD values of all five standards are above 1.00 dB, which are averaged over the 70 compressed sequences. The maximal STD among the 70 sequences reaches 3.97 dB, 4.00 dB, 3.84 dB, 5.67 dB and 3.34 dB for MPEG-1, MPEG-2, MPEG-4, H.264 and HEVC, respectively. This reflects the remarkable fluctuation of frame-level quality after video compression. Moreover, <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of the frame-level PSNR and subjective quality for one sequence, compressed by the latest HEVC standard. It can be observed from Figure 3 that there exists frequently alternate PQFs and Valley Quality Frames (VQFs). Here, PQF is defined as the frame whose quality is higher than its previous and subsequent frames. In contrast, VQF indicates the frame with lower quality than its previous and subsequent frames. As shown in this figure, the PSNR of non-PQFs (frames 58-60), especially the VQF (frame 60), is obviously lower than that of the nearest PQFs (frames 57 and 61). Moreover, non-PQFs (frames 58-60) also have much lower subjective quality than the nearest PQFs (frames 57 and 61), e.g., in the region of number "87". Additionally, the content of frames 57-61 is very similar. Hence, the visual quality of non-PQFs can be improved by using the content in the nearest PQFs.</p><p>To further analyze the peaks and valleys of frame-level quality, we measure the Peak-Valley Difference (PVD) and Peak Separation (PS) for the PSNR curves of each compressed video sequence. As seen in <ref type="figure" target="#fig_3">Figure 3</ref>-(a), PVD is denoted as the PSNR difference between the PQF and its n- earest VQF, and PS indicates the number of frames between two PQFs. The averaged PVD and PS values of the 70 compressed video sequences are shown in <ref type="table" target="#tab_3">Table 1</ref> for each video coding standard. It can be seen that the averaged PVD values are higher than 1.00 dB in most cases, and the latest HEVC standard has the highest value of 1.50 dB. This verifies the large quality difference between PQFs and VQFs. Additionally, the PS values are approximately or less than 5 frames for each coding standard. In particular, the PS values are less than 3 frames for the H.264 and HEVC standards. Such a short distance between two PQFs indicates that the content of frames between the adjacent PQFs may be highly similar. Therefore, the PQFs probably contain some useful content which is distorted in their neighboring non-PQFs. Motivated by this, our MFQE approach is proposed to enhance the quality of non-PQFs through the advantageous information of the nearest PQFs. <ref type="figure">Figure 4</ref> shows the framework of our MFQE approach. In the MFQE approach, we first detect the PQFs that are used for quality enhancement of other non-PQFs. In practical application, the raw sequences are not available in video quality enhancement, and thus the PQFs and non-PQFs cannot be distinguished through comparison with the raw sequences. Therefore, we develop a no-reference PQF detector in our MFQE approach, which is detailed in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The proposed MF-CNN approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Framework</head><p>The quality of detected PQFs can be enhanced by DS-CNN <ref type="bibr" target="#b41">[42]</ref>, which is a single-frame approach for video quality enhancement. It is because the adjacent frames of a PQF are with lower quality and cannot benefit the quality enhancement of this PQF. Here, we modify the DS-CNN via replacing the Rectified Linear Units (ReLU) by Parametric ReLU (PReLU) to avoid zero gradients <ref type="bibr" target="#b13">[14]</ref>, and we also We compress these sequences using various video coding standards, including MPEG-1 <ref type="bibr" target="#b10">[11]</ref>, MPEG-2 <ref type="bibr" target="#b29">[30]</ref>, MPEG-4 <ref type="bibr" target="#b30">[31]</ref>, H.264/AVC <ref type="bibr" target="#b38">[39]</ref> and HEVC <ref type="bibr" target="#b32">[33]</ref>. The quality of each compressed frame is evaluated in terms of Peak Signal-to-Noise Ratio (PSNR). <ref type="figure" target="#fig_1">Figure 2</ref> shows the PSNR curves of 8 video sequences, which are compressed by different standards. It can be seen that the compression quality obviously fluctuates along with the frames. We further measure the STandard Deviation (STD) of frame-level quality for each compressed video. As shown in <ref type="table" target="#tab_3">Table 1</ref>, the STD values of all five standards are above 1.00 dB, which are averaged over the 70 compressed sequences. The maximal STD among the 70 sequences reaches 3.97 dB, 4.00 dB, 3.84 dB, 5.67 dB and 3.34 dB for MPEG-1, MPEG-2, MPEG-4, H.264 and HEVC, respectively. This reflects the remarkable fluctuation of framelevel quality after video compression. Moreover, <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of the frame-level PSNR and subjective quality for one sequence, compressed by the latest HEVC standard. It can be observed from Figure 3 that there exists frequently alternate PQFs and Valley Quality Frames (VQFs). Here, PQF is defined as the frame whose quality is higher than its previous and subsequent frames. In contrast, VQF indicates the frame with lower quality than its previous and subsequent frames. As shown in this figure, the PSNR of non-PQFs (frames 58-60), especially the VQF (frame 60), is obviously lower than that of the nearest PQFs (frames 57 and 61). Moreover, non-PQFs (frames 58-60) also have much lower subjective quality than the nearest PQFs (frames 57 and 61), e.g., in the region of number "87". Additionally, the content of frames 57-61 is very similar. Hence, the visual quality of non-PQFs can be improved by using the content in the nearest PQFs.</p><p>To further analyze the peaks and valleys of frame-level quality, we measure the Peak-Valley Difference (PVD) and Peak Separation (PS) for the PSNR curves of each compressed video sequence. As seen in <ref type="figure" target="#fig_3">Figure 3</ref>-(a), PVD is denoted as the PSNR difference between the PQF and its nearest VQF, and PS indicates the number of frames between two PQFs. The averaged PVD and PS values of the 70 compressed video sequences are shown in <ref type="table" target="#tab_3">Table 1</ref> for each video coding standard. It can be seen that the averaged PVD values are higher than 1.00 dB in most cases, and the latest HEVC standard has the highest value of 1.50 dB. This verifies the large quality difference between PQFs and VQFs. Additionally, the PS values are approximately or less than 5 frames for each coding standard. In particular, the PS values are less than 3 frames for the H.264 and HEVC standards. Such a short distance between two PQFs indicates that the content of frames between the adjacent PQFs may be highly similar. Therefore, the PQFs probably contain some useful content which is distorted in their neighboring non-PQFs. Motivated by this, our MFQE approach is proposed to enhance the quality of non-PQFs through the advantageous information of the nearest PQFs.</p><p>4. The proposed MF-CNN approach 4.1. Framework <ref type="figure">Figure 4</ref> shows the framework of our MFQE approach. In the MFQE approach, we first detect the PQFs that are used for quality enhancement of other non-PQFs. In practical application, the raw sequences are not available in video quality enhancement, and thus the PQFs and non-PQFs cannot be distinguished through comparison with the raw sequences. Therefore, we develop a no-reference PQF detector in our MFQE approach, which is detailed in Section 4.2.</p><p>The quality of detected PQFs can be enhanced by DS-CNN <ref type="bibr" target="#b41">[42]</ref>, which is a single-frame approach for video quality enhancement. It is because the adjacent frames of a PQF are with lower quality and cannot benefit the quality enhancement of this PQF. Here, we modify the DS-CNN via replacing the Rectified Linear Units (ReLU) by Parametric ReLU (PReLU) to avoid zero gradients <ref type="bibr" target="#b13">[14]</ref>, and we also apply residual learning <ref type="bibr" target="#b12">[13]</ref> to improve the quality enhance-  ment performance.</p><p>For non-PQFs, the MF-CNN is proposed to enhance the quality that takes advantage of the nearest PQFs (i.e., both previous and subsequent PQFs). The MF-CNN architecture is composed of the MC-subnet and the QE-subnet. The MC-subnet is developed to compensate the temporal motion across the neighboring frames. To be specific, the MCsubnet firstly predicts the temporal motion between the current non-PQF and its nearest PQFs. Then, the two nearest PQFs are warped with the spatial transformer according to the estimated motion. As such, the temporal motion between the non-PQF and PQFs can be compensated. The MC-subnet is to be introduced in Section 4.3.</p><p>Finally, the QE-subnet, which has a spatio-temporal architecture, is proposed for quality enhancement, as introduced in Section 4.4. In the QE-subnet, both the current non-PQF and the compensated PQFs are as the inputs, and then the quality of the current non-PQF can be enhanced under the help of the adjacent compensated PQFs. Note that, in the proposed MF-CNN, the MC-subnet and QE-subnet are trained jointly in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SVM-based PQF detector</head><p>In our MFQE approach, an SVM classifier is trained to achieve no-reference PQF detection. Recall that PQF is the frame with higher quality than the adjacent frames. Thus both the features of the current and four neighboring frames are used to detect PQFs. In our approach, the PQF detector follows the no-reference quality assessment method <ref type="bibr" target="#b28">[29]</ref> to extract 36 spatial features from the current frame, each of which is one-dimensional. Beyond, such kinds of spatial features are also extracted from two previous frames and two incoming frames. Consequently, 180 one-dimensional features are obtained to predict whether a frame is a PQF or non-PQF, based on the SVM classifier.</p><p>In our SVM classifier, l n ? {0, 1} denotes the output class label indicating whether the n-th frame is a PQF (positive sample with l n = 1) or non-PQF (negative sample with l n = 0). We use the LIBSVM library <ref type="bibr" target="#b4">[5]</ref> to train the SVM classifier, in which the probability of l n = 1 can be obtained for each frame and denoted as p n . In our SVM classifier, the Radial Basis Function (RBF) is used as the kernel.</p><p>Finally, {l n , p n } N n=1 can be obtained from the SVM classifier, in which N is the total number of frames in the video sequence. In our PQF detector, we further refine the results of the SVM classifier according to the prior knowledge of PQF. Specifically, the following two strategies are developed to refine the labels {l n } N n=1 of the PQF detector. (1) According to the definition of PQF, it is impossible that the PQFs consecutively appear. Hence, if the following case exists</p><formula xml:id="formula_0">{l n+i } j i=0 = 1 and l n?1 = l n+j+1 = 0, j ? 1,<label>(1)</label></formula><p>we set</p><formula xml:id="formula_1">l n+i = 0, where i = arg max 0?k?j (p n+k )<label>(2)</label></formula><p>in our PQF detector.</p><p>(2) According to the analysis of Section 3, PQFs frequently appear within a limited separation. For example, the average value of PS is 2.66 frames for HEVC compressed sequences. Here, we assume that D is the maximal separation between two PQFs. Given this assumption, if the results of {l n } N n=1 yields more than D consecutive zeros (non-PQFs):</p><formula xml:id="formula_2">{l n+i } d i=0 = 0 and l n?1 = l n+d+1 = 1, d &gt; D,<label>(3)</label></formula><p>one of frames need to be selected as PQF, and thus we set</p><formula xml:id="formula_3">l n+i = 1, where i = arg max 0&lt;k&lt;d (p n+k ).<label>(4)</label></formula><p>After refining {l n } N n=1 as discussed above, our PQF detector can locate PQFs and non-PQFs in the compressed video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MC-subnet</head><p>After PQFs are detected, the quality of non-PQFs can be enhanced by taking advantage of the neighboring PQFs. However, the temporal motion exists between non-PQFs and PQFs. Hence, we develop the MC-subnet to compensate the temporal motion across frames. In the following,    <ref type="figure" target="#fig_4">Figure 5</ref>, the STMC method adopts the convolutional layers to estimate the ?4 and ?2 down-scaling Motion Vector (MV) maps, denoted as M ?4 and M ?2 . In M ?4 and M ?2 , the downscaling is achieved by adopting some convolutional layers with the stride of 2. For details of these convolutional layers, refer to <ref type="bibr" target="#b2">[3]</ref>.</p><p>The down-scaling motion estimation is effective to handle large scale motion. However, because of down-scaling, the accuracy of MV estimation is reduced. Therefore, in addition to STMC, we further develop some additional convolutional layers for pixel-wise motion estimation in our MCsubnet, which does not contain any down-scaling process. The convolutional layers of pixel-wise motion estimation are described in <ref type="table" target="#tab_7">Table 2</ref>. As <ref type="figure" target="#fig_4">Figure 5</ref> shows, the output of STMC includes the ?2 down-scaling MV map M ?2 and the corresponding compensated PQF F ?2 p . They are concatenated with the original PQF and non-PQF, as the input to the convolutional layers of the pixel-wise motion estimation. Then, the pixel-wise MV map can be generated, which is denoted as M. Note that the MV map M contains two channels, i.e., horizonal MV map M x and vertical MV map M y . Here, x and y are the horizonal and vertical index of each pixel. Given M x and M y , the PQF is warped to compensate the temporal motion. Let the compressed PQF and non-PQF be F p and F np , respectively. The compensated PQF F p can be expressed as</p><formula xml:id="formula_4">F p (x, y) = I{F p (x + M x (x, y), y + M y (x, y))},<label>(5)</label></formula><p>where I{?} means the bilinear interpolation. The reason for the interpolation is that M x (x, y) and M y (x, y) may be non-integer values. Training strategy. Since it is hard to obtain the ground truth of MV, the parameters of the convolutional layers for motion estimation cannot be trained directly. The superresolution work <ref type="bibr" target="#b2">[3]</ref> trains the parameters by minimizing the MSE between the compensated adjacent frame and the current frame. However, in our MC-subnet, both the input F p and F np are compressed frames with quality distortion. Hence, when minimizing the MSE between F p and the F np , the MC-subnet learns to estimate the distorted MV, resulting in inaccurate motion estimation. Therefore, the MC-subnet is trained under the supervision of the raw frames. That is, we warp the raw frame of the PQF (denoted as F R p ) using the MV map output from the convolutional layers of motion estimation, and minimize the MSE between the compensated raw PQF (denoted as F R p ) and the raw non-PQF (denoted as F R np ). The loss function of the MC-subnet can be written by</p><formula xml:id="formula_5">L MC (? mc ) = ||F R p (? mc ) ? F R np || 2 2 ,<label>(6)</label></formula><p>where ? mc represents the trainable parameters of our MCsubnet. Note that the raw frames F R p and F R np are not required when compensating motion in the test.   <ref type="figure">Figure 6</ref>. Architecture of our QE-subnet. </p><formula xml:id="formula_6">? ??? ? ? C o n v 1 C o n v 2 C o n v 3 C o n v 4 C o n v 5 C o n v 6 C o n v 7 C o n v 8 C o n v 9 C o m p l e x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">QE-subnet</head><p>Given the compensated PQFs, the quality of non-PQFs can be enhanced through the QE-subnet, which is designed with spatio-temporal architecture. Specifically, together with the current processed non-PQF F np , the compensated previous and subsequent PQFs (denoted by F p1 and F p2 ) are input to the QE-subnet. This way, both the spatial and temporal features of these three frames are explored and merged. Consequently, the advantageous information in the adjacent PQFs can be used to enhance the quality of the non-PQF. It differs from the CNN-based image/singleframe quality enhancement approaches, which only handle the spatial information within one frame.</p><p>Architecture. The architecture of the QE-subnet is shown in <ref type="figure">Figure 6</ref>, and the details of the convolutional layers are presented in <ref type="table" target="#tab_9">Table 3</ref>. In the QE-subnet, the convolutional layers Conv 1, 2 and 3 are applied to extract the spatial features of input frames F p1 , F np and F p2 , respectively. Then, in order to use the high quality information of F p1 , Conv 4 is adopted to merge the features of F np and F p1 . That is, the outputs of Conv 1 and 2 are concatenated and then convolved by Conv 4. Similarly, Conv 5 is used to merge the features of F np and F p2 . Conv 6/7 is designed to extract more complex features from Conv 4/5. Consequently, the extracted features of Conv 6 and Conv 7 are non-linearly mapped to another space through Conv 8. Finally, the reconstructed residual, denoted as R np (? qe ), is achieved in Conv 9, and the non-PQF is enhanced by adding R np (? qe ) to the input non-PQF F np . Here, ? qe is defined as the trainable parameters of QE-subnet.</p><p>Training strategy. The MC-subnet and QE-subnet of our MF-CNN are trained jointly in an end-to-end manner. Assume that F R p1 and F R p2 are defined as the raw frames of the previous and incoming PQFs, respectively. The loss function of our MF-CNN can be formulated as</p><formula xml:id="formula_7">L MF (? mc , ? qe ) = a ? 2 i=1 ||F R pi (? mc ) ? F R np || 2 2 LMC: loss of MC-subnet +b ? F np + R np (? qe ) ? F R np 2 2 LQE: loss of QE-subnet .<label>(7)</label></formula><p>As <ref type="formula" target="#formula_7">(7)</ref> indicates, the loss function of the MF-CNN is the weighted sum of L MC and L QE , which are the loss functions of MC-subnet and QE-subnet, respectively. Because F p1 and F p2 generated by the MC-subnet are the basis of the following QE-subnet, we set a b at the beginning of training. After the convergence of L MC is observed, we set a b to minimize the MSE between F np + R np and F R np . As a result, the quality of non-PQF F np can be enhanced by using the high quality information of its nearest PQFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Settings</head><p>The experimental results are presented to validate the effectiveness of our MFQE approach. In our experiments, all 70 video sequences of our database introduced in Section 3 are randomly divided into the training set (60 sequences) and the test set (10 sequences). The training and test sequences are compressed by the latest HEVC standard, setting the Quantization Parameter (QP) to 42 and 37. We train two models of our MFQE approach for the sequences compressed at QP = 37 and 42, respectively. In the SVM-based PQF detector, the parameter D 2 is set to 6 in (3). It is because the maximal separation between two nearest PQFs is 6 frames in all training sequences compressed by HEVC. When training the MF-CNN, the raw and compressed sequences are segmented into 64 ? 64 patches as the training samples. The batch size is set to 64. We adopt the Adam algorithm <ref type="bibr" target="#b20">[21]</ref> with initial learning rate as 10 ?4 to minimize the loss function of <ref type="bibr" target="#b6">(7)</ref>. In the training stage, we initially set a = 1 and b = 0.01 of (7) to train the MC-subnet. After the MC-subnet converges, these hyperparameters are set as a = 0.01 and b = 1 to train the QE-subnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance of the PQF detector</head><p>Because PQF detection is the first stage of the proposed MFQE approach, the performance of our PQF detector is evaluated in terms of precision, recall and F 1 -score, as shown in <ref type="table" target="#tab_10">Table 4</ref>. We can see from <ref type="table" target="#tab_10">Table 4</ref> that at QP = 37, the average precision and recall of our SVM-based PQF detector are 90.68% and 92.11% , respectively. In addition, the F 1 -score, which is defined as the harmonic average of the precision and the recall, is 91.09% on average. Similar   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance of our MFQE approach</head><p>In this section, we evaluate the quality enhancement performance of our MFQE approach in terms of ?PSNR, which measures the PSNR difference between the enhanced and the original compressed sequence. Our performance is compared with AR-CNN <ref type="bibr" target="#b8">[9]</ref>, DnCNN <ref type="bibr" target="#b45">[46]</ref>, Li et al. Quality enhancement on non-PQFs. Our MFQE approach mainly focuses on enhancing the quality of non- <ref type="bibr" target="#b2">3</ref> Note that AR-CNN <ref type="bibr" target="#b8">[9]</ref>, DnCNN <ref type="bibr" target="#b45">[46]</ref> and Li et al. <ref type="bibr" target="#b23">[24]</ref> are re-trained on HEVC compressed samples for fair comparison.  PQFs using the multi-frame information. Therefore, we first assess the quality enhancement of non-PQFs. <ref type="figure" target="#fig_6">Figure 8</ref> shows the ?PSNR results averaged over PQFs, non-PQFs and VQFs of all 10 test sequences, compressed at QP = 37. As shown in this figure, our MFQE approach has a considerably larger PSNR improvement for non-PQFs, compared to that for PQFs. Furthermore, an even higher ?PSNR can be achieved for VQFs in our approach. In contrast, for compared approaches, the PSNR improvement of non-PQFs is similar to or even less than that of PQFs. Specifically, for non-PQFs, our MFQE approach doubles ?PSNR of DS-CNN <ref type="bibr" target="#b41">[42]</ref>, which performs best among all of the compared approaches. This validates the effectiveness of our MFQE approach in enhancing the quality of non-PQFs.</p><p>Overall quality enhancement. <ref type="table" target="#tab_11">Table 5</ref> presents the ?PSNR results averaged over all frames, for each test sequence. As this table shows, our MFQE approach outperforms all five compared approaches for all test sequences. To be specific, at QP = 37, the highest ?PSNR of our MFQE approach reaches 0.7716 dB. The averaged ?PSNR of our MFQE approach is 0.5102 dB, which is 87.78% higher than that of of Li et al. <ref type="bibr" target="#b23">[24]</ref> (0.2717 dB), and 57.86% higher than that of DS-CNN (0.3233 dB). Besides, more ?PSNR gain can be obtained in our MFQE approach, when compared with AR-CNN <ref type="bibr" target="#b8">[9]</ref>, DnCNN <ref type="bibr" target="#b45">[46]</ref> and DCAD <ref type="bibr" target="#b36">[37]</ref>. At QP = 42, our MFQE approach (?PSNR = 0.4610 dB) also doubles the PSNR improvement of the second best approach DS-CNN (?PSNR = 0.2189 dB). Thus, our MFQE approach is effective in overall quality enhancement. This is mainly due to the large improvement in non-PQFs, which are the majority of compressed video frames.</p><p>Quality fluctuation. Apart from the artifacts, the quality fluctuation of compressed video may also lead to degradation of QoE <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16]</ref>. Fortunately, as discussed above, our MFQE approach is able to mitigate the quality fluctuation, because of the higher PSNR improvement of non-PQFs. We evaluate the fluctuation of video quality in terms of the STD and PVD of the PSNR curve, as introduced in Section 3. <ref type="figure">Figure 7</ref> shows the STD and PVD values averaged over all test sequences, for the HEVC baseline and the quality enhancement approaches. As shown in this fig-AR-CNN <ref type="bibr" target="#b8">[9]</ref> DCAD <ref type="bibr" target="#b36">[37]</ref> Li et al. <ref type="bibr" target="#b23">[24]</ref> DS-CNN <ref type="bibr" target="#b41">[42]</ref> Our MFQE Raw</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PeopleOnStreet BasketballPass</head><p>DnCNN <ref type="bibr" target="#b45">[46]</ref> Vidyo1 <ref type="figure" target="#fig_0">Figure 10</ref>. Subjective quality performance on Vidyo1 at QP = 37, BasketballPass at QP = 37 and PeopleOnStreet at QP = 42.</p><p>ure, our MFQE approach succeeds in reducing the STD and PVD after enhancing the quality of the compressed sequences. By contrast, the five compared approaches enlarge the STD and PVD values over the HEVC baseline. Thus, our MFQE approach is able to mitigate the quality fluctuation and achieve better QoE, compared with other approaches. <ref type="figure">Figure 9</ref> further shows the PSNR curves of the HEVC baseline and our MFQE approach for two test sequences. It can be seen that the PSNR fluctuation of our MFQE approach is obviously less than the HEVC baseline. To summarize, our MFQE approach is effective to mitigate the quality fluctuation of compressed video, meanwhile enhancing video quality.</p><p>Subjective quality performance. <ref type="figure" target="#fig_0">Figure 10</ref> shows the subjective quality performance on the sequences Vidyo1 at QP = 37, BasketballPass at QP = 37 and PeopleOnStreet at QP = 42. One may observe from <ref type="figure" target="#fig_0">Figure 10</ref> that our MFQE approach reduces the compression artifacts more effectively than the five compared approaches. Specifically, the severely distorted content, e.g., the mouth in Vidyo1, the ball in BasketballPass and the shadow in PeopleOnStreet, can be finely restored in our MFQE approach upon the same content from the neighboring high quality frames. In contrast, such distortion can hardly be restored in the compared approaches, which only use the single low quality frame.</p><p>Effectiveness of utilizing PQFs. Finally, we validate the effectiveness of utilizing PQFs by re-training our MF-CNN to enhance non-PQFs using adjacent frames instead of PQFs. In our experiments, using adjacent frames instead of PQFs only has 0.3896 dB and 0.3128 dB ?PSNR at QP=37 and 42, respectively. By contrast, as aforementioned, utilizing PQFs achieves ?PSNR = 0.5102 dB and 0.4610 dB at QP = 37 and 42. Moreover, it has been discussed before that non-PQFs, which are enhanced taking advantage of PQFs, have much larger ?PSNR than PQFs. These validate the effectiveness of utilizing PQFs in our MFQE approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Transfer to H.264 standard</head><p>We further verify the generalization capability of our MFQE approach by transferring to H.264 compressed sequences. The training and test sequences of our database are compressed by H.264 at QP = 37. Then, the training sequences are used to fine-tune the MF-CNN model. Then, the quality of the test H.264 sequences is improved by our MFQE approach with the fine-tuned MF-CNN model. We find that the average PSNR of test sequences can be increased by 0.4540 dB. This result is comparable to that of HEVC (0.5102 dB). Therefore, the generalization capability of our MFQE approach can be verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a CNN-based MFQE approach to reduce compression artifacts of video. Differing from the conventional single frame quality enhancement, our MFQE approach improves the quality of one frame by using high quality content of its nearest PQFs. To this end, we developed an SVM-based PQF detector to classify PQFs and non-PQFs in compressed video. Then, we proposed a novel CNN framework, called MF-CNN, to enhance the quality of each non-PQF. Specifically, the MC-subnet of our MF-CNN compensates motion between PQFs and non-PQFs. Subsequently, the QE-subnet enhances the quality of each non-PQF by inputting the current non-PQF and the nearest compensated PQFs. Finally, experimental results showed that our MFQE approach significantly improves the quality of non-PQFs, far better than other state-of-the-art quality enhancement approaches. Consequently, the overall quality enhancement is considerably higher than other approaches, with less quality fluctuation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example for quality fluctuation (top) and quality enhancement performance (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>PSNR curves of compressed video for various compression standards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>PSNR curves of compressed video for various compression standards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Example of the frame-level PSNR and subjective quality for the HEVC compressed video sequence Football.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Architecture of our MC-subnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>?PSNR (dB) on PQFs, non-PQFs and VQFs of the test sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc><ref type="bibr" target="#b23">[24]</ref> <ref type="bibr" target="#b2">3</ref> , DCAD<ref type="bibr" target="#b36">[37]</ref> and DS-CNN<ref type="bibr" target="#b41">[42]</ref>. Among them, AR-CNN, DnCNN and Li et al. are the latest quality enhancement approaches for compressed image. DCAD and DS-CNN are the state-of-the-art video quality enhancement approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>42 Figure 9 .</head><label>429</label><figDesc>3653 dB, PVD = 0.9441 dB STD = 0.4201 dB, PVD = 1.1265 dB BarScene at QP = PSNR curves of HEVC and our MFQE approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>To our best knowledge, there exists no MFQE work for compressed video. The closest area is multi-frame video super-resolution. In the early years, Brandi et al. [2] and Song et al. [32] proposed to enlarge video resolution by taking advantage of high resolution key-frames. Recently, many multi-frame super-resolution approaches have employed deep neural networks. For example, Huang et al. [17] developed a Bidirectional Recurrent Convolutional Network (BRCN), which improves the super-resolution performance over traditional single-frame approaches. In 2016, Kappeler et al. proposed</figDesc><table /><note>a Video Super-Resolution network (VSRnet)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Averaged STD, PVD and PS values among our database.</figDesc><table><row><cell></cell><cell>MPEG-1</cell><cell>MPEG-2</cell><cell>MPEG-4</cell><cell>H.264</cell><cell>HEVC</cell></row><row><cell>STD (dB)</cell><cell>1.8336</cell><cell>1.8347</cell><cell>1.7823</cell><cell>1.6370</cell><cell>1.0566</cell></row><row><cell>PVD (dB)</cell><cell>1.1072</cell><cell>1.1041</cell><cell>1.0420</cell><cell>0.5275</cell><cell>1.5089</cell></row><row><cell>PS (frame)</cell><cell>5.3438</cell><cell>5.3630</cell><cell>5.3819</cell><cell>1.6365</cell><cell>2.6600</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Averaged STD, PVD and PS values among our database.</figDesc><table><row><cell></cell><cell>MPEG-1</cell><cell>MPEG-2</cell><cell>MPEG-4</cell><cell>H.264</cell><cell>HEVC</cell></row><row><cell>STD (dB)</cell><cell>1.8336</cell><cell>1.8347</cell><cell>1.7823</cell><cell>1.6370</cell><cell>1.0566</cell></row><row><cell>PVD (dB)</cell><cell>1.1072</cell><cell>1.1041</cell><cell>1.0420</cell><cell>0.5275</cell><cell>1.5089</cell></row><row><cell>PS (frame)</cell><cell>5.3438</cell><cell>5.3630</cell><cell>5.3819</cell><cell>1.6365</cell><cell>2.6600</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Convolutional layers for pixel-wise motion estimation. Caballero et al. proposed the Spatial Transformer Motion Compensation (STMC) method for multi-frame super-resolution. As shown in</figDesc><table><row><cell>Layers</cell><cell>Conv 1</cell><cell>Conv 2</cell><cell>Conv 3</cell><cell>Conv 4</cell><cell>Conv 5</cell></row><row><cell>Filter size</cell><cell>3 ? 3</cell><cell>3 ? 3</cell><cell>3 ? 3</cell><cell>3 ? 3</cell><cell>3 ? 3</cell></row><row><cell>Filter number</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>2</cell></row><row><cell>Stride</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Function</cell><cell>PReLU</cell><cell>PReLU</cell><cell>PReLU</cell><cell>PReLU</cell><cell>Tanh</cell></row><row><cell cols="6">the architecture and training strategy of our MC-subnet are</cell></row><row><cell cols="2">introduced in detail.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Architecture. In [3],</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 .</head><label>3</label><figDesc>Convolutional layers of our QE-subnet.</figDesc><table><row><cell>Layers</cell><cell>Conv 1/2/3</cell><cell>Conv 4/5</cell><cell>Conv 6/7</cell><cell>Conv 8</cell><cell>Conv 9</cell></row><row><cell>Filter size</cell><cell>9 ? 9</cell><cell>7 ? 7</cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>5 ? 5</cell></row><row><cell>Filter number</cell><cell>128</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>1</cell></row><row><cell>Stride</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Function</cell><cell>PReLU</cell><cell>PReLU</cell><cell>PReLU</cell><cell>PReLU</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>Performance of the PQF detector on the test sequences.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell>QP</cell><cell cols="2">Precision</cell><cell cols="2">Recall</cell><cell>F 1 -score</cell></row><row><cell></cell><cell cols="3">Our SVM-based</cell><cell>37</cell><cell cols="2">90.68%</cell><cell cols="2">92.11%</cell><cell>91.09%</cell></row><row><cell></cell><cell cols="3">PQF detector</cell><cell>42</cell><cell cols="2">93.98%</cell><cell cols="2">90.86%</cell><cell>92.23%</cell></row><row><cell></cell><cell cols="2">AR-CNN [9]</cell><cell cols="3">DnCNN [46]</cell><cell></cell><cell cols="2">Li et al. [24]</cell></row><row><cell></cell><cell cols="2">DCAD [37]</cell><cell cols="3">DS-CNN [42]</cell><cell></cell><cell cols="2">Our MFQE</cell><cell>HEVC baseline</cell></row><row><cell></cell><cell>dB</cell><cell></cell><cell>dB</cell><cell></cell><cell></cell><cell>dB</cell><cell></cell><cell>dB</cell></row><row><cell>1.2</cell><cell></cell><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell>1.8</cell><cell></cell><cell>1.8</cell></row><row><cell>1.1</cell><cell></cell><cell></cell><cell>1.1</cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell>1.4</cell></row><row><cell cols="3">1 STD at QP = 37</cell><cell cols="3">1 STD at QP = 42</cell><cell cols="3">1 PVD at QP = 37</cell><cell>1 PVD at QP = 42</cell></row><row><cell cols="9">Figure 7. Averaged STD and PVD values of the test sequences.</cell></row><row><cell></cell><cell></cell><cell cols="3">AR-CNN [9]</cell><cell cols="3">DnCNN [46]</cell><cell>Li et al. [24]</cell></row><row><cell></cell><cell></cell><cell cols="2">DCAD [37]</cell><cell></cell><cell cols="3">DS-CNN [42]</cell><cell>Our MFQE</cell></row><row><cell>0.8</cell><cell>?PSNR (dB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">?PSNR (dB)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell></row><row><cell>0</cell><cell>PQF</cell><cell cols="2">Non-PQF</cell><cell>VQF</cell><cell></cell><cell>0</cell><cell>PQF</cell><cell>Non-PQF</cell><cell>VQF</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) QP = 37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) QP = 42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc>Overall ?PSNR (dB) of the test sequences. BasketballPass 9: RaceHorses 10: MaD results can be found for all test sequences compressed at QP = 42, in which the average precision, recall and F 1 -score are 93.98%, 90.86% and 92.23%, respectively. Thus, the effectiveness of our SVM-based PQF detector is validated.</figDesc><table><row><cell></cell><cell></cell><cell>AR-CNN</cell><cell>DnCNN</cell><cell>Li et al.</cell><cell>DCAD</cell><cell>DS-CNN</cell><cell>MFQE</cell></row><row><cell>QP</cell><cell>Seq.</cell><cell>[9]</cell><cell>[46]</cell><cell>[24]</cell><cell>[37]</cell><cell>[42]</cell><cell>(our)</cell></row><row><cell></cell><cell>1</cell><cell>0.1287</cell><cell>0.1955</cell><cell>0.2523</cell><cell>0.1354</cell><cell>0.4762</cell><cell>0.7716</cell></row><row><cell></cell><cell>2</cell><cell>0.0718</cell><cell>0.1888</cell><cell>0.2857</cell><cell>0.0376</cell><cell>0.4228</cell><cell>0.6042</cell></row><row><cell></cell><cell>3</cell><cell>0.1095</cell><cell>0.1328</cell><cell>0.1872</cell><cell>0.1112</cell><cell>0.2394</cell><cell>0.4715</cell></row><row><cell></cell><cell>4</cell><cell>0.1304</cell><cell>0.2084</cell><cell>0.2170</cell><cell>0.0796</cell><cell>0.3173</cell><cell>0.4381</cell></row><row><cell></cell><cell>5</cell><cell>0.1900</cell><cell>0.2936</cell><cell>0.3645</cell><cell>0.2334</cell><cell>0.3252</cell><cell>0.5496</cell></row><row><cell>37</cell><cell>6</cell><cell>0.1522</cell><cell>0.1944</cell><cell>0.2630</cell><cell>0.1619</cell><cell>0.3728</cell><cell>0.5980</cell></row><row><cell></cell><cell>7</cell><cell>0.1445</cell><cell>0.2224</cell><cell>0.2570</cell><cell>0.1775</cell><cell>0.2777</cell><cell>0.3898</cell></row><row><cell></cell><cell>8</cell><cell>0.1305</cell><cell>0.2424</cell><cell>0.2939</cell><cell>0.1940</cell><cell>0.2790</cell><cell>0.4838</cell></row><row><cell></cell><cell>9</cell><cell>0.1573</cell><cell>0.2588</cell><cell>0.3034</cell><cell>0.2224</cell><cell>0.2720</cell><cell>0.3935</cell></row><row><cell></cell><cell>10</cell><cell>0.1490</cell><cell>0.2509</cell><cell>0.2926</cell><cell>0.2026</cell><cell>0.2498</cell><cell>0.4019</cell></row><row><cell></cell><cell>Ave.</cell><cell>0.1364</cell><cell>0.2188</cell><cell>0.2717</cell><cell>0.1556</cell><cell>0.3232</cell><cell>0.5102</cell></row><row><cell>42</cell><cell cols="7">Ave. 1: PeopleOnStreet 2: TunnelFlag 3: Kimono 4: BarScene 5: Vidyo1 0.1627 0.2073 0.1924 0.1282 0.2189 0.4610</cell></row><row><cell></cell><cell cols="3">6: Vidyo3 7: Vidyo4 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PQFs are defined as the frames whose quality is higher than their previous and subsequent frames.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">D should be changed according to coding standard and configurations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the NSFC projects under Grants 61573037 and 61202139, and Fok Ying-Tong education foundation under Grant 151061.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Common test conditions and software reference configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11, 5th meeting</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super resolution of video using key frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>the IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1608" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CAS-CNN: A deep convolutional neural network for image compression artifact suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="752" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno>27:1-27:27</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing artifacts in JPEG decompression via a learned dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cisco visual networking index: Global mobile data traffic forecast update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cisco</forename><surname>Systems</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A convolutional neural network approach for post-processing in hevc intra coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia Modeling (MMM)</title>
		<meeting>the International Conference on Multimedia Modeling (MMM)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The MPEG video compression algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J L</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-delay rate control for DCT video coding via ?-domain source modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="928" to="940" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive quantizationparameter clip scheme for smooth quality in H.264/AVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1911" to="1919" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="112" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image deblocking via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="663" to="677" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video super-resolution via motion compensation and deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient deep convolutional neural networks model for compressed image deblocking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting>the IEEE International Conference on Multimedia and Expo (ICME)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel method on optimal bit allocation at LCU level for rate control in HEVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting>the IEEE International Conference on Multimedia and Expo (ICME)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimal bit allocation for CTU level rate control in HEVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2409" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blocking artifacts suppression in block-coded images using overcomplete wavelet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="450" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end learning of video super-resolution with motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Conference on Pattern Recognition (GCPR)</title>
		<meeting>the German Conference on Pattern Recognition (GCPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="203" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Digital video coding standards and their role in video communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="907" to="924" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The MPEG-4 video standard verification model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video super-resolution algorithm using bi-directional overlapped block motion compensation and on-the-fly dictionary training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="274" to="285" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (HEVC) standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Psnr control for GOP-level constant quality in H.264 video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C D</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Signal Processing and Information Technology</title>
		<meeting>the IEEE International Symposium on Signal Processing and Information Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="612" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detection of blocking artifacts in compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1106" to="1108" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive non-local means filter for image deblocking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="530" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel deep learningbased method of improving coding efficiency from the decoder-end for HEVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Compression Conference (DCC)</title>
		<meeting>the Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">D3: Deep dual-domain based fast restoration of JPEG-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2764" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Overview of the H. 264/AVC video coding standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="560" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Xiph.org video test media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Org</surname></persName>
		</author>
		<ptr target="https://media.xiph.org/video/derf/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Subjective-qualityoptimized complexity control for HEVC decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Decoder-side HEVC quality enhancement with scalable convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="817" to="822" />
		</imprint>
	</monogr>
	<note>Multimedia and Expo (ICME</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Saliencyguided complexity control for HEVC decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Enhancing quality for HEVC compressed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Characterizing perceptual artifacts in compressed video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vision and Electronic Imaging XIX. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
