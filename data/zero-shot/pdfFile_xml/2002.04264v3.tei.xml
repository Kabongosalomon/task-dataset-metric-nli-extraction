<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
						</author>
						<title level="a" type="main">The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fine-grained image classification</term>
					<term>deep learning</term>
					<term>loss function</term>
					<term>mutual channel</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The key to solving fine-grained image categorization is finding discriminate and local regions that correspond to subtle visual traits. Great strides have been made, with complex networks designed specifically to learn part-level discriminate feature representations. In this paper, we show that it is possible to cultivate subtle details without the need for overly complicated network designs or training mechanisms -a single loss is all it takes. The main trick lies with how we delve into individual feature channels early on, as opposed to the convention of starting from a consolidated feature map. The proposed loss function, termed as mutual-channel loss (MC-Loss), consists of two channel-specific components: a discriminality component and a diversity component. The discriminality component forces all feature channels belonging to the same class to be discriminative, through a novel channel-wise attention mechanism. The diversity component additionally constraints channels so that they become mutually exclusive across the spatial dimension. The end result is therefore a set of feature channels, each of which reflects different locally discriminative regions for a specific class. The MC-Loss can be trained end-to-end, without the need for any boundingbox/part annotations, and yields highly discriminative regions during inference. Experimental results show our MC-Loss when implemented on top of common base networks can achieve stateof-the-art performance on all four fine-grained categorization datasets (CUB-Birds, FGVC-Aircraft, Flowers-102, and Stanford Cars). Ablative studies further demonstrate the superiority of the MC-Loss when compared with other recently proposed generalpurpose losses for visual classification, on two different base networks. Code available at https://github.com/dongliangchang/ Mutual-Channel-Loss</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Fine-grained image classification refers to the problem of differentiating sub-categories of a common visual category (e.g., bird species, car models) <ref type="bibr" target="#b22">[23]</ref>. The task is much harder when compared to conventional category-level classification <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, since visual differences between subordinate classes are often subtle and deeply embedded within local discriminative parts. As a result, it has become common knowledge that developing effective methods to extract information from the localized regions that capture subtle differences is the D. Chang, Y. Ding, J. Xie, Z. <ref type="bibr">Ma</ref>  where we learn part localized discriminate features directly on channels, without explicit part detection vs. conventional fine-grained classification methods that work on feature maps and with explicit network designs for part detection. We can observe those feature channels after the MC-Loss become class-aligned and each focus on different discriminative regions that roughly correspond to object parts. key for solving fine-grained image classification <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>Early works largely relied on manual part annotations, and followed a supervised learning paradigm <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Albeit with decent results reported, it had quickly become apparent that such supervised approaches are not scalable. This is because expert human annotations can be cumbersome to obtain and are often error-prone <ref type="bibr" target="#b36">[37]</ref>. More recent research has therefore concentrated on realizing parts in an unsupervised fashion <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b48">[49]</ref>. These approaches have been shown to yield performances on par or even exceeding those that relied on manual annotations, owing to their ability of mining discriminative parts that are otherwise missing or inaccurate in human labelled data. Again, the main focus is placed on how best to locate discriminative object parts. Increasingly more complicated networks have been proposed to perform part learning, mainly to compensate for the lack of annotation data. Two main components can be typically identified amongst these approaches: (i) a network component to explicitly perform part detection, and (ii) a way to ensure that features learned are maximally discriminative. Most recent work on fine-grained classification <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref> has shown state-of-the-art performance by simultaneously exploring these two components, cultivating their complementary properties.</p><p>In this paper, we follow the same motivation as above <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> to address the unique challenges of fine-grained classification. We importantly differ in not attempting to introduce any explicit network components for discriminate part discovery. Instead, we ask if it is even possible to simultaneously achieve both discriminative features for learning and part localization, with just a single loss. This design choice has a few salient advantages over prior art: (i) it does not introduce any extra network parameters, making the network easier to train, and (ii) it can in principle be applied to any existing or future network architecture. The key insight lies with how we delve into feature channels early on, as opposed to learning fine-grained part-level features on feature maps directly.</p><p>More specifically, we assume a fixed number of feature channels to represent each class. It follows that instead of applying constraints on the final feature maps, we impose a loss directly on the channels, so that all the feature channels belonging to the same class are (i) discriminative, i.e., they each contribute to discriminating the class from others, and (ii) mutually exclusive, i.e., each channel can attend to different local regions/parts. The end result is therefore, a set of feature channels that are class-aligned, each being discriminative on mutually distinct local parts. <ref type="figure" target="#fig_0">Figure 1</ref> offers a visualization. To the best of our knowledge, this is the first time that a single loss is proposed for fine-grained classification that does not require any specific network designs for partial localization.</p><p>Our loss is termed mutual-channel loss, MC-Loss in short. It has two components that work synergistically for finegrained feature learning. Firstly, a discriminality component is introduced to enforce all feature channels corresponding to a class to be discriminative on their own, before being fused. A novel channel attention mechanism is introduced, whereby during training a fixed percentage of channels is randomly masked out, forcing the remaining channels to become discriminative for a given class. We then apply crosschannel max pooling <ref type="bibr" target="#b13">[14]</ref> to fuse the feature channels and produce the final feature map which is now class-aligned and optimally discriminative.</p><p>Although every feature channel is now discriminative against a class, there is still no guarantee that most discriminative parts will be localized. This leads us to introduce the second component of our loss function, the diversity component. This component is specifically designed so that each channel within a group will attend to mutually distinct local parts. We achieve this goal by asking for maximum spatial de-correlation across channels belonging to the same class. This can be conveniently implemented by (again) applying cross-channel max pooling, then asking for maximum spatial summation. Ultimately, this is done to ensure as many discriminative parts are attended to as possible, therefore helping with fine-grained feature learning. Note that the diversity component would not work without its discriminative counterpart, since otherwise not all channels would be discriminative making localization much harder.</p><p>Extensive experiments are carried out on four commonly used fine-grained categorization datasets, CUB-200-2011 <ref type="bibr" target="#b37">[38]</ref>, FGVC-Aircraft <ref type="bibr" target="#b28">[29]</ref>, Flowers-102 <ref type="bibr" target="#b30">[31]</ref>, and Stanford Cars <ref type="bibr" target="#b16">[17]</ref>. The results show that our method can outperform the current the state-of-the-arts by a significant margin. Ablative studies are further conducted to draw insights towards each of the proposed loss components, and hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review previous works regarding both fine-grained image classification and relevant loss functions for similar purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fined-Grained Image Classification</head><p>Some of the earlier works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b44">[45]</ref> take advantages of bounding-box/part annotations, as an additional information for both training and testing. However, expert annotations are hard to source and can be prone to human error, and thus it hinders practical deployment in the wild scenarios. To address this issue, some other works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b47">[48]</ref> use annotations only during training and utilize a part-detection module during testing.</p><p>Recently, some frameworks employ a more general architecture that can localize discriminative parts within an image without any extra supervision from part annotations, and thus it makes the fine-grained image classification more feasible in real-world scenarios. Wang et al. <ref type="bibr" target="#b41">[42]</ref> claimed that improving mid-level convolutional feature representation can bring significant advantages for part-based fine-grained classification. This is accomplished by introducing a bank of discriminative filters in the classical convolutional neural networks (CNNs) architecture and it can be trained in an end-to-end fashion. Authors in <ref type="bibr" target="#b9">[10]</ref> presented a new procedure, called pairwise confusion (PC), in order to improve the generalization for finegrained image classification task by encouraging confusion in the output activations and forcing the model to focus on local discriminative features of the objects rather than the backgrounds. Meanwhile, Yang et al. <ref type="bibr" target="#b45">[46]</ref> proposed a novel multi-agent cooperative learning scheme which learns to identify the discriminative regions in the image in a selfsupervised way.</p><p>Despite all these improvements, part-based methods have difficulties in modelling the specific features of an image because of the complicated relationship that exists between the different distinct parts. In order to handle this complex interaction, some approaches encode higher-order statistics of convolutional features and extract compact holistic representations. Lin et al. <ref type="bibr" target="#b22">[23]</ref> added a bilinear pooling behind the dual CNNs to obtain discriminative feature representation of the whole image. As an extension of bilinear pooling, Cui et al. <ref type="bibr" target="#b7">[8]</ref> proposed a deep kernel pooling method that captures the highorder, non-linear feature interactions via compact and explicit feature mapping. A higher-order integration of hierarchical convolutional features has been introduced into an end-toend framework to derive rich representation of the local parts at different scales for fine-grained image classification <ref type="bibr" target="#b3">[4]</ref>. The very recent work by Zheng et al. <ref type="bibr" target="#b49">[50]</ref> is perhaps the closest to our work since they also operated at channel-level. They designed a multi-attention convolutional neural network (MA-CNN) to jointly learn discriminative parts and finegrained feature representation on each channel, which then got aggregated later on to construct the final fine-grained features.</p><p>Without exception, all previous approaches incur network changes to achieve part localization and/or discriminative feature learning. This is distinctively different to our approach of achieving the same via a single loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss Functions in CNNs</head><p>A recent trend has been noticed in the computer vision community towards designing task-specific loss functions to reinforce the CNNs with strong discriminative information. Intuitively, the extracted features are most discriminative when their intra-class compactness and inter-class separability are simultaneously maximized, i.e. the Fisher criterion. Wen et al. <ref type="bibr" target="#b43">[44]</ref> proposed the center loss to obtain the highly discriminative features for robust recognition by minimizing the interclass distance of deep features. Liu et al. <ref type="bibr" target="#b23">[24]</ref> introduced the A-softmax loss to learn angularly discriminative features for image classification on a deep hypersphere embedding manifold. Wang et al. <ref type="bibr" target="#b40">[41]</ref> embraced the idea of the Fisher criterion and proposed the large margin cosine loss (LMCL) to learn highly discriminative deep features for image recognition.</p><p>In addition, there are some works that focus on the effective use of training data. Lin et al. <ref type="bibr" target="#b21">[22]</ref> proposed the focal loss, a modified cross-entropy (CE) loss, in order to emphasize learning on hard samples and down-weight well-classified samples. Wan et al. <ref type="bibr" target="#b38">[39]</ref> proposed the large-margin Gaussian mixture (L-GM) loss by assuming a Gaussian mixture distribution for the deep features on the training set, which boosts a more effective discrimination of out-of-domain inputs.</p><p>Although all the aforementioned loss functions can obtain discriminative features to an extent, they do not explicitly encourage the network to focus on the localized discriminative regions. In contrast, our proposed MC-Loss function enforces the network to discover multiple discriminative regions, which also alleviates the need of complicated network designs unlike <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, and thus it makes our framework easy-toimplement and easy-to-interpret.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE MUTUAL-CHANNEL LOSS (MC-LOSS)</head><p>In this section, we propose the mutual-channel loss (MC-Loss) function to effectively navigate the model focusing on different discriminative regions without any fine-grained bounding-box/part annotations.</p><p>The network combined with the proposed MC-Loss in the training step is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Given an input image, it first extracts the feature maps by feeding the image into a base network; e.g., VGG16 <ref type="bibr" target="#b34">[35]</ref> or ResNet18 <ref type="bibr" target="#b14">[15]</ref>. Let the extracted feature maps be denoted as F ? R N ?W ?H , with height H, width W , and number of channels N . In the proposed MC-Loss, we need to set the value of N equals to c??, where c and ? indicate the number of classes in a dataset and the number of feature channels used to represent each class, respectively. Note that ? is a scalar hyper-parameter and empirically larger than 2. The n th vectored feature channel of F is represented as F n ? R WH , n = 1, 2, ? ? ? , N . Please note that we reshape each channel matrix of F of dimension W ? H to a vector of size W times H, i.e. WH. The grouped feature channels corresponding to i th class is indicated by F i ? R ??WH , i = 0, 1, ? ? ? , c ? 1. Mathematically, it can be represented as</p><formula xml:id="formula_0">F i = {F i??+1 , F i??+2 , ? ? ? , F i??+? } .<label>(1)</label></formula><p>Subsequently, F = {F 0 , F 1 , ? ? ? , F c?1 } enters into two streams of the network with two different sub-losses tailored for two distinct goals. In <ref type="figure" target="#fig_1">Figure 2</ref>, the cross-entropy stream considers F as the input to a fully connected (FC) layers with traditional CE loss L CE . Here, the cross-entropy loss encourages the network to extract informative features which mainly focus on the global discriminative regions. On the other side, the MC-Loss stream supervises the network to spotlight different local discriminative regions. The MC-Loss is then added to the CE loss with the weight of ? in the training step. Thus, the total loss function of the whole network can be defined as</p><formula xml:id="formula_1">Loss(F) = L CE (F) + ? ? L M C (F).<label>(2)</label></formula><p>Furthermore, the MC-Loss is a weighted summation of one discriminality component L dis and another diversity component L div . We define the MC-Loss as</p><formula xml:id="formula_2">L M C (F) = L dis (F) ? ? ? L div (F).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Discriminality Component</head><p>In our framework, each class is represented by a certain number of grouped feature channels. The discriminality component enforces the feature channels to be class-aligned and each feature channel corresponding to a particular class should be discriminative enough. The discriminality component L dis can be represented as</p><formula xml:id="formula_3">L dis (F) = LCE y, e g(F 0 ) , e g(F 1 ) , ? ? ? , e g(F c?1 ) T c?1 i=0 e g(F i ) Softmax ,<label>(4)</label></formula><p>where g(?) is defined as</p><formula xml:id="formula_4">g(Fi) = 1 WH WH k=1 GAP max j=1,2,??? ,? CCMP [Mi ? F i,j,k ] CWA ,<label>(5)</label></formula><p>where the GAP, the CCMP, and the CWA are short notations for global average pooling, cross-channel max pooling, and channel-wise attention, respectively. L CE (?, ?) is the crossentropy loss between the ground-truth class label y and the output of GAP.</p><formula xml:id="formula_5">M i = diag(Mask i ), where Mask i ? R ? is a 0-1 mask with randomly ? 2 zero(s). The ? 2</formula><p>ones and operation diag(?) put a vector on the principle diagonal of a diagonal matrix. The left block in <ref type="figure" target="#fig_2">Figure 3</ref>(a) shows the flow diagram of the discriminality component. CWA: While in case of traditional CNNs, trained with the classical CE loss objective, a certain subset of feature channels contain discriminative information, we here propose channelwise attention operation to enforce the network to equally capture discriminative information in all ? channels corresponding to a particular class. Unlike other channel-wiseattention design <ref type="bibr" target="#b5">[6]</ref> that intends to assign higher priority to the discriminative channels using soft-attention values, we assign random binary weights to the channels and stochastically select a few feature channels from every feature group F i during each iteration, thus explicitly encouraging every feature channel to contain sufficient discriminative information. This process could be visualized as a random channel-dropping operation. Please note that the CWA is used only during training and that the whole MC-Loss branch is not present at the time of inference. Therefore, the classification layer receives the same input feature distributions during both training and inference. CCMP: Cross-channel max pooling <ref type="bibr" target="#b13">[14]</ref> is used to compute the maximum response of each element across each feature channel in F i corresponding to a particular class, and thus it results into a one dimensional vector of size WH concurring to a particular class. Note that the cross-channel average pooling (CCAP) is an alternative of the CCMP, which only substitutes the max pooling operation by the average pooling. However, the CCAP tends to average each element across the group which may suppress the peaks of feature channels, i.e., attentions of local regions. On the contrary, the CCMP can preserve these attentions, and is found to be beneficial for fine-grained classification. GAP: Global average pooling <ref type="bibr" target="#b20">[21]</ref> is used to compute the average response of each feature channel, resulting in a c-dimensional vector where each element corresponds to one individual class. Finally, we use the CE loss function L CE to compute the dissimilarity between the ground-truth labels and the predicted probabilities given by the softmax function behind the GAP operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Diversity Component</head><p>The diversity component is an approximated distance measurement for feature channels to calculate the total similarity of all the channels. It is cheaper in computation with a constant complexity than other commonly used measurements like Euclidean distance and Kullback-Leibler divergence with a quadratic complexity. The diversity component illustrated along the right block of <ref type="figure" target="#fig_2">Figure 3</ref>(a) drives the feature channels in a group F i to become different from each other via training. In other words, different feature channels of a class should focus on different regions of the image, rather than all the channels focusing on the most discriminative region. Thus, it reduces the redundant information by diversifying the feature channels from every group and helps to discover different discriminative regions with respect to every class in an image. This operation can be interpreted as a cross-channel decorrelation in order to capture details from different salient regions of an image. After the softmax, we impose supervision directly at the convolutional filters by introducing a CCMP followed by a spatial-dimension summation to measure the degree of intersection. The diversity specific loss component L div can be defined as</p><formula xml:id="formula_6">L div (F) = 1 c c?1 i=0 h(F i ),<label>(6)</label></formula><p>where h(?) is defined as</p><formula xml:id="formula_7">h(F i ) = WH k=1 max j=1,2,??? ,? CCMP e F i,j,k WH k =1 e F i,j,k Softmax .<label>(7)</label></formula><p>The function softmax is a normalization on spatial dimensions and the CCMP here plays the same role as it does in the discriminality component.</p><p>The upper-bound of L div is equal to ? in the case of ? extremely different feature maps which means that they focus on different local regions, while the lower-bound is 1 facing ? same feature maps in F i which need to be optimized clearly shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In <ref type="figure" target="#fig_3">Figure 4</ref>, assuming that each feature channel is one-hot normalized by softmax, h(?) would response to the upper-bound 3 (? = 3) if each feature channel has the one in distinct locations, i.e., focusing on different local regions. Conversely, if obtaining identical feature channels, h(?) would response to the lower-bound 1. Ideally, we intend to maximize the L div term and thus it justifies the minus sign in Equation <ref type="bibr" target="#b7">8</ref>. A point is to be noted that the diversity component cannot work alone for classification, it acts as a regularizer on the top of discriminality loss to implicitly discover different discriminative regions in an image. Intuitively, we will discuss   the availability of the diversity component in visualization results in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSIONS</head><p>In this section, we evaluate the performance of our proposed method on the fine-grained image classification task. Firstly, the datasets and the implementation details are introduced in Section IV-A and IV-B, respectively. Subsequently, the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The foremost important thing to be noted is that the number of channels in the output feature maps extracted from a pretrained VGG16 (ResNet50), is fixed at 512 (2048). Say for example, if want to fix ? = 3 uniformly for every class, this would require 600, 300, 588, and 306 feature channels for CUB-200-2011 (with 200 classes), FGVC-Aircraft (with 150 classes), Stanford Cars (with 196 classes), and Flowers-102 datasets (with 102 classes), respectively. This is not feasible with the pre-trained VGG16 (ResNet50) since the number of feature channel is fixed at 512 (2048). On the other side, we intend to explore the pre-trained rich discriminative features of the VGG16 (ResNet50) that is learned on large ImageNet dataset and we fine-tune the pre-trained models with our proposed loss function in Equation 12. Therefore, we assign ? non-uniformly in order to serve the purpose of using pretrained VGG16 (ResNet50). Say for example, when we finetune the VGG16 pre-trained on the ImageNet classification dataset, we assign 2 feature channels for each of the first 88 classes and the rest 112 classes are modelled with 3 feature channels in case of CUB-200-2011 dataset. Please refer to <ref type="table" target="#tab_1">Table II</ref> for details.</p><p>In order to compare our proposed loss function with other state-of-the-art methods (see <ref type="table" target="#tab_1">Table III</ref> and IV), we resize every image to a size of 448 ? 448 (following others), then extract features using the VGG16 (ResNet50), and the B-CNN <ref type="bibr" target="#b22">[23]</ref> based on a VGG16 model pre-trained on the ImageNet classification dataset. We use Stochastic Gradient Descent optimizer and batch normalization as the regularizer. The learning rate of the pre-trained feature extraction layers are kept as 1 ? 10 ?4 , while the learning rate of fully connected layers is initially set at 0.01 and multiplied by 0.1 at 150 th and 225 th epoch, successively. We train our model for 300 epochs and weight decay value is kept as 5 ? 10 ?4 . Furthermore, we set the hyper-parameters of the MC-Loss as ? =0.005 and ?=10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with State-of-the-Art Methods</head><p>Irrespective of the backbone networks, the proposed MC-Loss achieves a consistent improvement over the other state-of-the-art methods. Especially, the proposed MC-Loss achieves the best accuracy of 97.70% on Flowers-102 dataset. Detailed results are listed in <ref type="table" target="#tab_1">Table III. From Table IV</ref>, it can  The component settings of the referred methods are also listed in <ref type="table" target="#tab_1">Table IV</ref>. In particular, A, B, C, D, and E denote stochastically initialized classification layers (c-layers), pretrained VGG16 with c-layers removed, pre-trained VGG19 with c-layers removed, pre-trained ResNet50 with c-layers removed, and pre-trained DenseNet161 with c-layers removed, respectively. While most of the methods modify their base architectures, the MC-Loss performs best on most datasets without any structural modification or adding extra parameters. The only optimization procedure, Paired Confusion (PC), is based on the bilinear CNN (B-CNN) which is same as ours. The MC-Loss achieves remarkable improvement compared with PC on all four datasets. When the backbone of the MC-Loss is the pre-trained VGG16, the MC-Loss does not perform better on CUB-200-2011 dataset, one reason that is due to the lack of feature channels. As mentioned in <ref type="table" target="#tab_1">Table II</ref>, with 512 feature channels, 88 classes on CUB-200-2011 dataset have only two feature channels. Since the birds on CUB-200-2011 dataset have rich discriminative regions, it is difficult to obtain robust descriptions with insufficient number of feature channels. Hence, the performance is worse than some referred methods. For Stanford Cars dataset, although 112 classes have two feature channels, the MC-Loss can still perform well due to the fact that the cars have less discriminative regions than the birds and the discriminative ability of the MC-Loss can compensate for the lack of feature channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>For ablation study, we train the backbone architecture (like the VGG16 or the ResNet18) from scratch using the loss function mentioned in <ref type="bibr">Equation 12</ref>, and we define number of output channels based on the requirement of assigning ? uniformly for every class, which is not possible with the pretrained VGG16 because of its fixed channel outputs. We resize every image to 224 ? 224. The learning rate of the complete network is initially set at 0.1 and multiplied by 0.1 at 150 th and 225 th epoch, while other settings are same with the earlier one. In addition, we set the hyper-parameters of the MC- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MC-loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MC-Loss minus iv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MC-Loss minus CWA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original image</head><p>Feature map 1 Feature map 2 Feature map 3 Merged feature map Loss as ? =1.5 and ?=10. Although the pre-trained VGG16 provides much better results, we have done this ablation study in order to justify the choice/potential of different hyperparameters (like ?) and different individual component of our loss function, where the backbone architecture is trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of ?:</head><p>In order to judge the influence of ? on the accuracy, we vary ? from 1 to 6 uniformly (for every class). Alongside, we also keep the ? assignment setup as detailed in <ref type="table" target="#tab_1">Table II</ref> and term this as MC-Loss (512). From <ref type="table">Table V</ref>, it can be interpreted that the MC-Loss (?=1) performs the worst and it signifies that only one discriminative region learned for each class is not enough for fine-grained image classification. The MC-Loss (?=3) achieves 3.71% higher accuracy on CUB-200-2011 dataset compared to MC-Loss (512) and thus it demonstrates that only two feature channels assigned to each class (recall from Table II that there are 88 classes contain only two feature channels for each of them) is not sufficient to capture the discriminative information in bird's images. Increasing the ? value beyond 3 decreases the performance along with the additional burden of the computational cost.</p><p>We speculate a drop in performance because when ? is large, the number of channels employed would exceed the number of useful "parts", therefore, introducing redundant channels that are counter-effective. We also verified this through a few visualizations in <ref type="figure" target="#fig_4">Figure 5</ref>. The first column represents the original image. The second to seventh columns shows visualizations of the localization regions obtained from 6 feature channels (? = 6), respectively. The last column represents the visualizations of the merged localization regions of 6 aforementioned feature channels. Red boxes: redundant channels; green boxes: channels that exhibit localized regions. Overall, it is to be noted that the number of feature channels has remarkable influence on the classification performance and accuracy is optimum when ? equals to 3. Therefore, had we been able to train a new VGG16 model on ImageNet dataset with sufficient number of output channels, such that ? can be set to 3 uniformly for every class in the fine-grained dataset, it is expected that the MC-Loss optimized on the top that pretrained model could have performed better than other methods for fine-grained classification.</p><p>Comparison with other loss-functions:  <ref type="figure" target="#fig_5">Figure 6</ref> demonstrates the accuracy curves of the MC-Loss and the other commonly used loss functions on the CUB-200-2011 dataset. From <ref type="figure" target="#fig_5">Figure 6</ref>, the MC-Loss exhibits improved optimisation characteristics and produces consistent gains in performance which are sustained throughout the training process. Visualization: In order to illustrate the advantages of the proposed MC-Loss intuitively, we applied the Grad-CAM <ref type="bibr" target="#b32">[33]</ref> to implement the visualization for the feature channels. The first row of <ref type="figure" target="#fig_6">Figure 7</ref> shows the most discriminative regions proposed by the VGG16 model while trained using the complete MC-Loss. It can be observed that the three feature channels that corresponds to a specified bird class focus on different discriminative regions, e.g. head, feet, wings, and body. Meanwhile, the second row of <ref type="figure" target="#fig_6">Figure 7</ref> shows the most discriminative regions while using only discriminality component alone in the MC-loss. We can observe that if we do not use the diversity component in the MC-Loss, the three feature channels learned by the VGG16 model tend to be similar to each other. This indicates that the learned feature channels cannot focus on different discriminative regions in absence of the diversity component, which reduces its ability in finegrained image classifications. The last row of <ref type="figure" target="#fig_6">Figure 7</ref> shows an example of the most discriminative regions predicted by the VGG16 model optimized by the MC-loss without channelwise attention operation. It can be clearly interpreted that if the channel-wise attention module is removed, only one of the three feature channels represents the correct discriminative region. The other two feature channels, although are different from each other, do not necessarily learn any discriminative information.</p><p>The quantitative comparisons about the aforementioned phenomenon are listed in <ref type="table" target="#tab_1">Table VII</ref>. We can observe that, if we use only the discriminality component of the MC-Loss, the classification accuracies drop by 1.46%, 1.62%, 1.30%, and 1.63% on CUB-200-2011, FGVC-Aircraft, Stanford Cars, and Flowers-102 datasets, respectively. Furthermore, if we remove the channel-wise attention in the discriminality component in the MC-Loss, the accuracies will be decreased by 2.60%, 0.90%, 1.51%, and 2.47%, respectively. Alternatively, in contrast to Equation 6 one could only consider the channel group that belongs to the ground-truth class. In particular, Equation 6 could be replaced as L div v2 (F) = h(F i ). In <ref type="table" target="#tab_1">Table VII</ref>, we report the performance of this design as MC- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class #1</head><p>Class #2 Class #1 &amp; Class #2 <ref type="figure">Fig. 8</ref>. Explanation of the soft channel label used in our MC-Loss with two categories as an example. The solid line boxes show four learned soft channel labels, and apq represents a number greater than 0 and less than 1, where p indicates the category, q indicates the the total number of classes. In the first box, the learned soft channel labels make the channels contained in the feature group of each category aggregate with their neighborhoods, which obeys the original settings. In the second box, the corresponding feature group of each category are scattered in the feature channels. In the third box, the second channel is shared by both categories. In the last box, the third channel is not assigned to any specific categories. Please refer to Section IV-E1 for details.</p><p>Loss-V2, while the rest of the things remain unchanged. We can see that the classification accuracy drops significantly. The reason is that while using our proposed diversity loss component all the channel groups influence each other during training, but this L div v2 only considers the diversity of a channel group belonging to the ground truth class during training. In other words, if we only consider one channel group during training, the other groups of channels might lose the diversity. Intuitively, Equation 6 is being able to cultivate cross-group/class information, which essentially helps the final classification. These results are consistent with the analysis about the visualizations in <ref type="figure" target="#fig_6">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. MC-Loss with Soft Channel Label</head><p>In order to make the proposed loss function more flexible and adaptive to the structures of the networks, especially to adapt the number of channels for network-extracted features, we propose the soft channel label in this section, which are used to assign channels to specific categories. Unlike the original settings, channels belonging to a particular category are no longer required to be aggregated into groups with their neighborhoods. In addition, the number of channels assigned to each category is uneven in this case, which facilitates the learning of more discriminative and diverse patterns in the difficult categories.</p><p>The learnable soft channel labels determine the channel groups for each category. In order to enable the ability of the model to learn soft channel labels, we use the SE-block <ref type="bibr" target="#b15">[16]</ref> which can learn the channel attention weight for each channel. During the training process, the soft channel labels are constrained so that those belonging to the same category have high similarity, while those belonging to different classes have low similarity. It should be emphasized that the optimization objective of the model in <ref type="bibr" target="#b11">(12)</ref> is unchanged, except that the grouping of features has been modified. In the following content, we will introduce how the soft channel labels are learned in detail and the penalties are imposed on them. 1) Soft Channel Label: As is well known, the SE-block can learn the channel attention weight for each channel. Therefore, we use the SE-block to learn the soft channel labels for each category. The weights belonging to one sample can be defined by</p><formula xml:id="formula_8">wi,j = t (i,j) 1 , t (i,j) 2 , ..., t (i,j) k?1 , t (i,j) k T ,<label>(8)</label></formula><p>where t i,j k ? (0, 1) indicates the importance of the specific channel, k = 1, 2, ..., N (N is the total number of channels) indicates the channel index, i indicates the category, i = 1, 2, ..., C (C is the total number of classes), and j indicates the sample index belonging to the i th category, j = 1, 2, ? ? ? , J i (J i is the total number of samples belonging to the i th category).</p><p>In addition, during the training process, each batch contains different categories of data, and the sample number in the batch of each category is inconsistent. The soft channel labels of the samples belonging to the same category are expressed as Wi = [wi,1, wi,2, ..., wi,J i ?1, wi,J i ] T ,</p><p>where the dimension of the W i is J i ? N . It can be found in Section III that the CCMP is an effective way to measure the difference between feature channels. However, the CCMP can be only used on channels belonging to one mere sample, rather than different samples, which is not suitable in this case. Therefore, we use a similar way to measure the similarity between the soft channel labels belonging to the same class, and propose a loss component L intra defined as</p><formula xml:id="formula_10">Lintra = 1 C C i=1 N k=1 max j=1,??? ,J i (Wi) .<label>(10)</label></formula><p>Intuitively, the soft channel labels belonging to different classes should be different when we apply them to assign the feature channels. Therefore, we also use the CCMP to measure the similarity between soft channel labels belonging to the different classes, and propose a loss component L inter defined as</p><formula xml:id="formula_11">Linter = ? N k=1 max i=1,??? ,C max j=1,??? ,J i (Wi) .<label>(11)</label></formula><p>Thus, the total loss function of the whole network can be defined as:</p><p>Loss(F) = LCE(F) + ? ? LMC (F) + Lintra + Linter. (12) <ref type="figure">Figure 8</ref> shows an illustration to illustrate the key idea of how the learned soft channel labels assign the feature channels to all classes and obtain a set of feature channels that are classaligned.</p><p>2) Experiments: We evaluate the proposed MC-Loss on the three aforementioned fine-grained image classification data. Except that channel assignment is no longer set manually but can be learned by the model itself, other settings remain the same. Furthermore, we set the hyper-parameters of the MC-Loss as ? =0.5 and ?=10.</p><p>Table VIII lists the comparison between the proposed MC-Loss and the MC-Loss with the soft channel labels. Using the pre-trained ResNet50 model as the feature extractor, the proposed MC-Loss with the soft channel label achieves the better accuracies of 87.8%, 92.9%, and 94.1% on CUB-200-2011, FGVC-Aircraft, and Stanford Cars datasets, respectively. In summary, the proposed soft channel labels make the MC-Loss more flexible and more adaptive to the structure of the network, especially the number of channels for networkextracted features, and have no change on the distribution of features, more easy to fine-tune.</p><p>In order to illustrate the advantages of the soft channel label, we show the channels assigned for each category based on the learned soft channel labels in <ref type="table" target="#tab_1">Table IX</ref>. It can be observed that the soft channel label-supervised channels assigned to different categories according to the soft channel labels are significantly different, and some channels are shared. For example, for the 5 th category, the most important channel is the 1933 th channel and the second most important one is the 1257 th channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we show that it is possible to learn discriminate localized part features for fine-grained classification, with a single loss. The proposed mutual-channel loss (MC-Loss) can effectively drive the feature channels to be more discriminative and focusing on various regions, without the need of fine-grained bounding-box/part annotations. We show that our loss can applied to different network architectures, and does not introduce any extra parameters in doing so. Experiments on all four fine-grained classification datasets confirm the superiority of the MC-Loss. In the future, we will investigate means of automatically searching for ?, without necessarily introducing considerably more network parameters. We will also look into applying the MC-Loss to other tasks that rely on local and discriminative regions, and extending it to work across different modalities ( e.g., for fine-grained sketch-based image retrieval).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Mutual-channel loss (MC-Loss)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of a typical fine-grained classification network where MC-Loss is used. The MC-Loss function considers the output feature channels of the last convolutional layer as the input and gathers together with the cross-entropy (CE) loss function using a hyper-parameter ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Overview of the MC-Loss. The MC-Loss consists of (i) a discriminality component (left) that makes F to be class-aligned and discriminative, and (ii) a diversity component (right) that supervises the feature channels to focus on different local regions. (b) Comparison of feature maps before (left) and after (right) applying MC-Loss, where feature channels become class aligned, and each attending to different discriminate parts. Please refer to Section III for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>A graphical explanation of the diversity component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of the localized regions returned from Grad-CAM<ref type="bibr" target="#b32">[33]</ref> based on a VGG16 model (trained from scratch) optimized by the MC-Loss. The higher energy region denotes the more discriminative part in the image. be observed that our MC-Loss achieves best accuracies of 92.90%, 94.40% on FGVC-Aircraft and the Stanford Cars, respectively. Moreover, it obtains a competitive result on CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The accuracies of the MC-loss and the other commonly used loss functions on the CUB-200-2011 dataset using the VGG16 as backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Channel visualizations (? = 3). The first column represents the original image. The second to fourth columns show visualizations of the localization regions obtained from 3 feature channels (? = 3), respectively. The last column represents the visualizations of the merged localization regions of 3 aforementioned feature channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, M. Wu, J. Guo are with the Pattern Recognition and Intelligent System Laboratory, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China.</figDesc><table /><note>X. Li is with School of Computer and Communication, Lanzhou University of Technology, Lanzhou 730050, China. AK Bhunia and Y.-Z. Song are with the Centre for Vision, Speech and Signal Processing, University of Surrey, London, United Kingdom.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF DATASETS.</figDesc><table><row><cell>Datasets</cell><cell cols="3">#Category #Training #Testing</cell></row><row><cell>CUB-200-2011</cell><cell>200</cell><cell>5994</cell><cell>5794</cell></row><row><cell>FGVC-Aircraft</cell><cell>100</cell><cell>6667</cell><cell>3333</cell></row><row><cell>Stanford Cars</cell><cell>196</cell><cell>8144</cell><cell>8041</cell></row><row><cell>Flowers-102</cell><cell>102</cell><cell>2040</cell><cell>6149</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ?</head><label>II</label><figDesc>VALUE ASSIGNMENT WHILE USING THE PRE-TRAINED VGG16/RESNET50 WITH 512/2048 FEATURE CHANNELS.</figDesc><table><row><cell>Datasets</cell><cell cols="2">2/10 feature channels 3/11 feature channels</cell></row><row><cell>CUB-200-2011</cell><cell>88/152</cell><cell>112/48</cell></row><row><cell>Stanford Cars</cell><cell>76/108</cell><cell>120/88</cell></row><row><cell>Datasets</cell><cell cols="2">5/20 feature channels 6/21 feature channels</cell></row><row><cell>FGVC-Aircraft</cell><cell>88/52</cell><cell>12/48</cell></row><row><cell>Flowers-102</cell><cell>100/94</cell><cell>2/8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III EXPERIMENTAL</head><label>III</label><figDesc>RESULTS (%) ON FLOWERS-102 DATASET USING THE PRE-TRAINED VGG16 AND RESNET50.</figDesc><table><row><cell>Method</cell><cell>Base Model</cell><cell>Flowers-102</cell></row><row><cell>Det.+seg. (CVPR13 [1])</cell><cell>SVM</cell><cell>80.7</cell></row><row><cell>Overfeat (CVPR14 workshop [34])</cell><cell>Overfeat</cell><cell>86.8</cell></row><row><cell>B-CNN (ICCV15 [23])</cell><cell>VGG16</cell><cell>92.5</cell></row><row><cell>Selective joint FT (CVPR17 [13])</cell><cell>ResNet152</cell><cell>95.8</cell></row><row><cell>PC (ECCV18 [10] )</cell><cell>B-CNN</cell><cell>93.7</cell></row><row><cell>PC (ECCV18 [10] )</cell><cell>DenseNet161</cell><cell>91.2</cell></row><row><cell>MC-Loss</cell><cell>VGG16</cell><cell>96.1</cell></row><row><cell>MC-Loss</cell><cell>ResNet50</cell><cell>96.8</cell></row><row><cell>MC-Loss</cell><cell>B-CNN</cell><cell>97.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EXPERIMENTAL</head><label>IV</label><figDesc>RESULTS (%) ON CUB-200-2011, FGVC-AIRCRAFT, AND STANFORD CARS DATASETS, RESPECTIVELY WITH PRE-TRAINED VGG16 AND RESNET50. THE BEST AND SECOND BEST RESULTS ARE RESPECTIVELY MARKED IN BOLD AND ITALIC FONTS. INFLUENCE OF FEATURE CHANNEL NUMBER ON FOUR FINE-GRAINED IMAGE CLASSIFICATION DATASETS (TRAINED FROM SCRATCH). ?=i MEANS EACH CATEGORY HAS i FEATURE CHANNELS.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Base Model</cell><cell cols="2">CUB-200-2011 FGVC-Aircraft</cell><cell>Stanford Cars</cell><cell>Model Component</cell></row><row><cell>FT VGGNet (CVPR17 [42])</cell><cell></cell><cell>VGG19</cell><cell>77.8</cell><cell>84.8</cell><cell>84.9</cell><cell>C+A</cell></row><row><cell>FT ResNet (CVPR18 [42])</cell><cell></cell><cell>ResNet50</cell><cell>84.1</cell><cell>88.5</cell><cell>91.7</cell><cell>D+A</cell></row><row><cell>B-CNN (ICCV15 [23])</cell><cell></cell><cell>VGG16</cell><cell>84.1</cell><cell>84.1</cell><cell>91.3</cell><cell>2B +A</cell></row><row><cell>KA (ICCV17 [4])</cell><cell></cell><cell>VGG16</cell><cell>85.3</cell><cell>88.3</cell><cell>91.7</cell><cell>B+A+Conv.(1,1)</cell></row><row><cell>KP (CVPR17 [8])</cell><cell></cell><cell>VGG16</cell><cell>86.2</cell><cell>86.9</cell><cell>92.4</cell><cell>B+ kernel pooling+A</cell></row><row><cell>MA-CNN (ICCV17 [50])</cell><cell></cell><cell>VGG19</cell><cell>86.5</cell><cell>89.9</cell><cell>92.8</cell><cell>C + 3A + channel grouping layers</cell></row><row><cell>PC (ECCV18 [10])</cell><cell></cell><cell>B-CNN</cell><cell>85.6</cell><cell>85.8</cell><cell>92.5</cell><cell>2B+A</cell></row><row><cell>PC (ECCV18 [10])</cell><cell></cell><cell>DenseNet161</cell><cell>86.9</cell><cell>89.2</cell><cell>92.9</cell><cell>2E + 2A</cell></row><row><cell>DFL-CNN (CVPR18 [42])</cell><cell></cell><cell>VGG16</cell><cell>86.7</cell><cell>92.0</cell><cell>93.8</cell><cell>B+2A+Conv.(1,1)</cell></row><row><cell>DFL-CNN (CVPR18 [42])</cell><cell></cell><cell>ResNet50</cell><cell>87.4</cell><cell>91.7</cell><cell>93.1</cell><cell>D+2A+Conv.(1,1)</cell></row><row><cell>NTS-Net (ECCV18 [46])</cell><cell></cell><cell>ResNet50</cell><cell>87.5</cell><cell>91.4</cell><cell>93.9</cell><cell>D+3A+6Conv.(3,3)</cell></row><row><cell>WPS-CPM (CVPR19 [12])</cell><cell cols="2">GoogLeNet + ResNet50</cell><cell>90.4</cell><cell>-</cell><cell>-</cell><cell>GoogleNet + D+A</cell></row><row><cell>TASN (CVPR19 [51])</cell><cell></cell><cell>ResNet50</cell><cell>87.9</cell><cell>-</cell><cell>93.8</cell><cell>D+A</cell></row><row><cell>MC-Loss</cell><cell></cell><cell>VGG16</cell><cell>78.7</cell><cell>91.0</cell><cell>92.8</cell><cell>B+A</cell></row><row><cell>MC-Loss</cell><cell></cell><cell>ResNet50</cell><cell>87.3</cell><cell>92.6</cell><cell>93.7</cell><cell>D+A</cell></row><row><cell>MC-Loss</cell><cell></cell><cell>B-CNN</cell><cell>86.4</cell><cell>92.9</cell><cell>94.4</cell><cell>2B+A</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="5">Base Model CUB-200-2011 FGVC-Aircraft Stanford Cars Flowers-102</cell></row><row><cell cols="2">MC-Loss (?=1)</cell><cell>VGG16</cell><cell>58.80</cell><cell>82.08</cell><cell>84.88</cell><cell>69.99</cell></row><row><cell cols="2">MC-Loss (?=2)</cell><cell>VGG16</cell><cell>62.11</cell><cell>88.66</cell><cell>90.61</cell><cell>81.98</cell></row><row><cell cols="2">MC-Loss (?=3)</cell><cell>VGG16</cell><cell>65.98</cell><cell>89.20</cell><cell>90.85</cell><cell>83.23</cell></row><row><cell cols="2">MC-Loss (?=5)</cell><cell>VGG16</cell><cell>64.39</cell><cell>89.01</cell><cell>90.80</cell><cell>82.84</cell></row><row><cell cols="2">MC-Loss (?=6)</cell><cell>VGG16</cell><cell>63.08</cell><cell>88.22</cell><cell>89.82</cell><cell>81.26</cell></row><row><cell cols="2">MC-Loss (512)</cell><cell>VGG16</cell><cell>62.27</cell><cell>88.46</cell><cell>90.78</cell><cell>82.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI COMPARISONS</head><label>VI</label><figDesc>OF CLASSIFICATION ACCURACIES (%) WITH DIFFERENT LOSS FUNCTIONS USING THE VGG16 AND THE RESNET18 AS BACKBONE ARCHITECTURE (TRAINED FROM SCRATCH). THE BEST AND THE SECOND BEST RESULTS ARE RESPECTIVELY MARKED IN BOLD AND ITALIC FONTS. ResNet18 28.53 / 45.70 82.90 / 79.90 76.59 / 79.12 40.90 / 65.75 Center Loss [44] VGG16 / ResNet18 51.38 / 50.26 88.26 / 83.86 89.27 / 81.84 62.53 / 69.51 A-softmax Loss [24] VGG16 / ResNet18 60.79 / 49.67 88.15 / 82.42 88.71 / 82.15 62.34 / 50.56</figDesc><table><row><cell>Method</cell><cell></cell><cell>Base Model</cell><cell cols="3">CUB-200-2011 FGVC-Aircraft Stanford Cars</cell><cell>Flowers-102</cell></row><row><cell cols="3">CE Loss VGG16 / Focal Loss [22] VGG16 / ResNet18</cell><cell>31.12 / 47.67</cell><cell>80.85 / 80.47</cell><cell cols="2">77.02 / 79.75 48.19 / 66.87</cell></row><row><cell>COCO Loss [25]</cell><cell cols="2">VGG16 / ResNet18</cell><cell>48.31 / 46.01</cell><cell>86.41 / 80.02</cell><cell cols="2">67.27 / 72.38 63.31 / 66.76</cell></row><row><cell>LGM Loss [39]</cell><cell cols="2">VGG16 / ResNet18</cell><cell>28.14 / 44.91</cell><cell>87.49 / 80.98</cell><cell cols="2">71.27 / 74.37 57.78 / 66.84</cell></row><row><cell>LMCL Loss [41]</cell><cell cols="2">VGG16 / ResNet18</cell><cell>41.11 / 46.01</cell><cell>86.17 / 78.52</cell><cell cols="2">49.57 / 71.17 66.43 / 67.72</cell></row><row><cell>MC-Loss</cell><cell cols="2">VGG16 / ResNet18</cell><cell>65.98 / 59.41</cell><cell>89.2 / 85.57</cell><cell>90.85 / 87.47</cell><cell>83.23 / 79.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE VII</cell><cell></cell><cell></cell></row><row><cell cols="7">ABLATION STUDY OF THE MC-LOSS (TRAINED FROM SCRATCH) ON FOUR FINE-GRAINED IMAGE CLASSIFICATION DATASETS.</cell></row><row><cell>Method</cell><cell></cell><cell cols="5">Base Model CUB-200-2011 FGVC-Aircraft Stanford Cars Flowers-102</cell></row><row><cell>MC-Loss</cell><cell></cell><cell>VGG16</cell><cell>65.98</cell><cell>89.20</cell><cell>90.85</cell><cell>83.23</cell></row><row><cell>MC-Loss-V2</cell><cell></cell><cell>VGG16</cell><cell>65.20</cell><cell>88.65</cell><cell>90.53</cell><cell>82.75</cell></row><row><cell cols="2">MC-Loss minus L div</cell><cell>VGG16</cell><cell>64.52</cell><cell>87.58</cell><cell>89.55</cell><cell>81.60</cell></row><row><cell cols="2">MC-Loss minus L dis</cell><cell>VGG16</cell><cell>26.94</cell><cell>79.75</cell><cell>69.13</cell><cell>38.53</cell></row><row><cell cols="2">MC-Loss minus CWA</cell><cell>VGG16</cell><cell>63.36</cell><cell>88.30</cell><cell>89.34</cell><cell>80.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VIshows the comparison between the proposed MC-Loss and other commonly used loss functions, on the four widely used finegrained image classification datasets. Results on the left and right hands of the slashes in the table are for the VGG16 and the ResNet18 respectively. Using the VGG16 model as the feature extractor, the proposed MC-Loss achieves the best accuracies of 65.98%, 89.20%, 90.85%, and 83.</figDesc><table><row><cell>23% on</cell></row><row><cell>CUB-200-2011, FGVC-Aircraft, Stanford Cars, and Flowers-</cell></row><row><cell>102 datasets, respectively. While using ResNet18 model as</cell></row><row><cell>the feature extractor, the proposed MC-Loss still obtains the</cell></row><row><cell>best performance on four fine-grained image classification</cell></row><row><cell>datasets. In summary, the proposed MC-Loss outperforms all</cell></row><row><cell>the compared methods on all the four fine-grained image</cell></row><row><cell>classification datasets for both the VGG16 and the ResNet18</cell></row><row><cell>base networks. Meanwhile,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII INFLUENCE</head><label>VIII</label><figDesc>OF THE SOFT CHANNEL LABELS ON THREE FINE-GRAINED IMAGE CLASSIFICATION DATASETS USING THE PRE-TRAINED RESNET50. TABLE IX EXAMPLES OF THE SOFT CHANNEL LABELS LEARNED BY THE RESNET50 (THE CHANNEL NUMBERIS FIXED AT 2048), SHOWING THE 10 MOST IMPORTANT CHANNELS FOR A PARTICULAR CATEGORY, THE IMPORTANCE OF WHICH GRADUALLY DECREASES FROM LEFT TO RIGHT. EXAMPLES OF THE FIRST FIVE CATEGORIES OF THE CUB-200-2011 DATASET ARE PROVIDED.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell cols="8">Base Model CUB-200-2011 FGVC-Aircraft Stanford Cars</cell></row><row><cell cols="2">MC-Loss</cell><cell></cell><cell cols="2">ResNet50</cell><cell></cell><cell>87.3</cell><cell></cell><cell>92.6</cell><cell></cell><cell>93.7</cell></row><row><cell cols="3">MC-Loss with soft channel label</cell><cell cols="2">ResNet50</cell><cell></cell><cell>87.8</cell><cell></cell><cell>92.9</cell><cell></cell><cell>94.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Top 10 important channels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>1</cell><cell>175</cell><cell cols="2">1119 1844</cell><cell>628</cell><cell>833</cell><cell>815</cell><cell>819</cell><cell cols="3">1938 1837 1135</cell></row><row><cell>2</cell><cell>175</cell><cell cols="2">1119 1577</cell><cell>628</cell><cell cols="3">1676 1837 1844</cell><cell>819</cell><cell cols="2">1135 1838</cell></row><row><cell>3</cell><cell>775</cell><cell cols="2">1188 1914</cell><cell>668</cell><cell>465</cell><cell cols="2">1765 1066</cell><cell>591</cell><cell>1053</cell><cell>670</cell></row><row><cell>4</cell><cell>257</cell><cell>1776</cell><cell>760</cell><cell>235</cell><cell>1739</cell><cell>753</cell><cell>1212</cell><cell>269</cell><cell cols="2">1215 1740</cell></row><row><cell>5</cell><cell>1933</cell><cell>257</cell><cell>746</cell><cell cols="4">1066 1217 1219 1063</cell><cell>273</cell><cell cols="2">1740 1739</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">classification accuracy comparisons with other state-of-theart methods are then provided in Section IV-C. In order to illustrate the advantages of different loss-components and design choices, a comprehensive ablation study is provided in Section IV-D.A. DatasetsWe evaluate the proposed MC-Loss on four widely used fine-grained image classification datasets, namely Caltech-UCSD-Birds (CUB-200-2011)<ref type="bibr" target="#b37">[38]</ref>, FGVC-Aircraft<ref type="bibr" target="#b28">[29]</ref>, Stanford Cars<ref type="bibr" target="#b16">[17]</ref>, and Flowers-102<ref type="bibr" target="#b30">[31]</ref>. The detailed summary of the datasets are provided inTable I. In order to keep consistency with other datasets, where datasets are divided into training and test set only, we consider both training and validation sets for training in case of Flowers-102 dataset. Moreover, we only use the category labels in our experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient object detection and segmentation for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An iterative co-saliency framework for rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised semantic-preserving adversarial hashing for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4032" to="4044" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ECCV</title>
		<meeting>eeding of IEEE ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV Workshops</title>
		<meeting>eeding of IEEE ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast mode decision based on grayscale similarity and inter-view correlation for depth map coding in 3d-hevc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="706" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semiheterogeneous three-way joint embedding network for sketch-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual cross-entropy loss for small-sample fine-grained vehicle classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4204" to="4212" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-grained vehicle classification with channel max pooling modified cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3224" to="3233" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational bayesian learning for dirichlet process mixture of inverted dirichlet distributions in non-gaussian image feature modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="463" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Insights into multiple/single lower bound approximation for extended variational inference in non-gaussian structured data modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new approach to track multiple vehicles with the combination of robust detection and two classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="186" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object-part attention model for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR Workshops</title>
		<meeting>eeding of IEEE CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parameter free large margin nearest neighbor for distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A web-based system for collaborative annotation of large image and video collections: an evaluation and user study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Volkmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Natsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 13th Annual ACM International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking feature distribution for loss functions in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial finegrained composition learning for unseen attribute-object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3741" to="3749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ECCV</title>
		<meeting>eeding of IEEE ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical part matching for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ECCV</title>
		<meeting>eeding of IEEE ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Finegrained age estimation in the wild with attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ECCV</title>
		<meeting>eeding of IEEE ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE ICCV</title>
		<meeting>eeding of IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceeding of IEEE CVPR</title>
		<meeting>eeding of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
