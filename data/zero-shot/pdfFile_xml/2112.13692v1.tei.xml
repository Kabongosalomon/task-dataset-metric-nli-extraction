<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting Convolutional networks with attention-based aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Augmenting Convolutional networks with attention-based aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attentionbased aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 1 Existing patch-based architectures such as MLP designs [15,56,57] or convMixer [2] yield poor accuracy/complexity trade-offs. 2 https://github.com/facebookresearch/deit MLP and other patch-based approaches. Architectures based on patches [39] have been proposed beyond transformers, in particular, based on Multi-Layer Perceptron (MLP) layers such as MLP-Mixer [56] and ResMLP [57]. Most related to our work, the ablation study of ResMLP [57] shows the potential of patch-wise convolution over MLPs in terms of performance. In line of the ConViT model [13], CoatNet [12] is a patch-based architecture with convolutional blocks followed by transformers blocks.Concurrently, replacing self-attention layers with convolution layers has been explored in ConvMixer [2].</p><p>Explainability of the classification decision. There are many strategies to explain the classification decision of a network <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b75">76]</ref>, and most notably by highlighting the most influential regions that led to a decision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b78">79]</ref>. The family of CAM methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b78">79]</ref> shows that the gradients from a network decision contain information about</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision transformers <ref type="bibr" target="#b17">[18]</ref> (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called "class token". This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads.</p><p>In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>ViT-S "ResNet-50" S60 S60 ? <ref type="figure">Figure 1</ref>. We augment convolutional neural networks with a learned attention-based aggregation layer. We visualize the attention maps for classification for diverse models. We first extract attention maps from a regular ViT-S <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref> with Dino-style <ref type="bibr" target="#b7">[8]</ref> vizualizations. Then we consider convnets in which we replace the average pooling by our learned attentionbased aggregation layer. Unlike ViT, this layer directly provides the contribution of the patches in the weighted pooling. This is shown for a "ResNet-50 <ref type="bibr" target="#b23">[24]</ref>", and with our new simple patch-based model (PatchConvNet-S60) that we introduce to increase the attention map resolution. We can specialize this attention per class, as shown with S60 ?.</p><p>attention <ref type="bibr" target="#b4">[5]</ref>. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches. Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in <ref type="figure">Figures 1</ref> and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision.</p><p>In <ref type="figure">Figure 1</ref>, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. <ref type="bibr" target="#b7">[8]</ref>. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a "ResNet-50" augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling.</p><p>For this purpose we introduce a simple patch-based convolutional architecture 1 that keeps the input resolution constant throughout the network. This design departs from the historical pyramidal architectures of LeNet <ref type="bibr" target="#b36">[37]</ref>, AlexNet <ref type="bibr" target="#b35">[36]</ref> or ResNet <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see <ref type="figure">Figure 3</ref> for an overview of this network.</p><p>In summary, we make the following contributions:</p><p>? We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling;</p><p>? We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions;</p><p>? We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter.</p><p>We share the architecture definition and pretrained models 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Attention-based architectures for vision. Early works have introduced attention into convnets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b66">67]</ref>, but it is only recently that a fully attention-based architecture, the vision transformer <ref type="bibr" target="#b17">[18]</ref> (ViT), has become competitive with convnets on ImageNet <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref>. The particularity of this model is that it processes images as a set of non-overlapping patches, without any convolutional or downsampling layers. Nevertheless, several works have recently proposed to re-introduce convolutions and downsam-  <ref type="figure">Figure 2</ref>. We provide three images for which the attention-based aggregation stage is specialized so as to provide one attention map per class. We display the attention for the top-3 classes w.r.t. the model prediction.</p><p>pling into this architecture. For example, some architectures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b70">71]</ref> leverage convolutional layers in the first layers of the vision transformer architecture, while others, such as Swin <ref type="bibr" target="#b40">[41]</ref>, LeViT <ref type="bibr" target="#b21">[22]</ref>, or PiT <ref type="bibr" target="#b26">[27]</ref> exploit a pyramid structure to gradually reduce the spatial resolution of the features. These pyramid-based methods are more compatible with prior detection frameworks, and aim at improving the computational efficiency (FLOPs). As a downside, these pyramidal approaches dramatically reduce the resolution of the last layers, and hence the quality of their attention maps, making their predictions harder to interpret. Another shortcoming is their relatively high memory usage <ref type="bibr" target="#b49">[50]</ref>.</p><p>object locations that can be projected back to the image. These methods act as general external probes that project the network activity back into the image space, even though Oquab et al. <ref type="bibr" target="#b43">[44]</ref> have shown evidence that convnet features contain rough information about the localization of objects. Unlike these external approaches, the self-attention layers of vision transformers offer a direct access to the location of the information used to make classification decisions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. Our built-in class attention mechanism shares the same spirit of interpretable by design computer vision models <ref type="bibr" target="#b48">[49]</ref>. However, unlike our mechanism, selfattention layers do not distinguish between classes on the same image without additional steps <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attention-based pooling with PatchConvNet</head><p>The learned aggregation layer is best associated with a high-resolution feature map. Therefore, while it can be combined with any convolutional architecture like a regular ResNet-50, our suggestion is to combine it with an architecture that maintains the resolution all across the layers. Some works exist, however they offer an underwhelming trade-offs <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. To remedy to that problem, we introduce PatchConvNet. This design, which illustrated in <ref type="figure">Figure 3</ref>, is intended to concentrate most of the compute and parameters in the columnar trunk. The architecture family is parametrized by the embedding dimension d, and the number of repeated blocks in the trunk N . Below, we describe the architecture and its training in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture design</head><p>The convolutional stem is a light-weight pre-processing of the image pixels whose role is to segment and map an image into a set of vectors. In ViT, this exactly corresponds to the patch extraction step <ref type="bibr" target="#b17">[18]</ref>. Therefore, we refer to the vectors resulting from this pre-processing as patches. Recent papers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> have shown that it is best to adopt a convolutional pre-processing, in particular for stability reasons <ref type="bibr" target="#b69">[70]</ref>. In our case, we borrow the convolutional stem from LeVit <ref type="bibr" target="#b21">[22]</ref>: a small ConvNet that is applied to the image of size W ? H ? 3 and produces a vector map of W/16 ? H/16 ? d. It can be viewed as a set of k nonoverlapping d-dimensional patches. In our experimental results, except if mentioned otherwise, we use a convolutional stem consisting of four 3 ? 3 convolutions with a stride of 2 ? 2, followed by a GELU non-linearity <ref type="bibr" target="#b25">[26]</ref>. We illustrate the convolutional stem in <ref type="figure">Figure 3</ref>.</p><p>The column, or trunk, is the part of the model which accounts for most of the layers, parameters, and compute. It consists of N stacked residual convolutional blocks as depicted in <ref type="figure">Figure 3</ref>. The block starts with a normalization, followed by a 1 ? 1 convolution, then a 3 ? 3 convolution for spatial processing, a squeeze-and-excitation layer <ref type="bibr" target="#b29">[30]</ref> for mixing channel-wise features, and finally a 1 ? 1 convolution right before the residual connection. Note that we can interpret the 1 ? 1 convolutions as linear layers. A GELU non-linearity follows the first two convolutions. The output of this block has the same shape as its input: the same number of tokens of the same dimension d.</p><p>Using BatchNorm <ref type="bibr" target="#b31">[32]</ref> often yields better results than LayerNorm <ref type="bibr" target="#b2">[3]</ref>, provided the batches are large enough. As shown in Section 4, we also observe this for our model family. However, BatchNorm is less practical when training large models or when using large image resolutions because of its dependency on batch size. In that setup, using BatchNorm requires an additional synchronization step across multiple machines. This synchronization increases the amount of node-to-node communication required per step, and in turn, training time. In other situations, like for detection and segmentation, the images are large, limiting the batch size and possibly impacting performance. Because of all those reasons, unless stated otherwise, we adopt LayerNorm.</p><p>Attention-based pooling. At the output of the trunk, the pre-processed vectors are aggregated using a cross-attention layer inspired by transformers. We illustrate this aggregation mechanism in <ref type="figure">Figure 3</ref>. A query class token attends to the projected patches and aggregates them as a weighted summation. The weights depend on the similarity of projected patches with a trainable vector (CLS) akin to a class token. The resulting d-dimensional vector is subsequently added to the CLS vector and processed by a feed-forward network (FFN). As opposed to the class-attention decoder by Touvron et al. <ref type="bibr" target="#b58">[59]</ref> we use a single block and a single head. This drastic simplification has the benefit of avoiding the dilution of attention across multiple channels. Consequently, the communication between the class token and the pre-processed patches occurs in a single softmax, directly reflecting how the pooling operator weights each patch.</p><p>We can easily specialize the attention maps per class by replacing the CLS vector with a k ? d matrix, where each of the k columns is associated with one of the classes. This specialization allows us to visualize an attention map for each class, as shown in <ref type="figure">Figure 2</ref>. The impact of the additional parameters and resulting FLOPS is minimal for larger models in the family. However, this design increases peak memory usage and makes the optimization of the network more complicated. We typically do that in a fine-tuning stage with a lower learning rate and smaller batch size to circumvent these issues. By default, we use the more convenient single class token.   <ref type="figure">Figure 3</ref>. Detail of the full model, with the convolutional stem on the left, the convolutional main block in the middle, and here toppled with multi-class attention-based pooling on the right.  Image size S60 trained @224 S60 FT@384 S60 FT@size S60 Imnet21k FT@size <ref type="figure">Figure 6</ref>. Accuracy at different resolutions for the S60 model. We analyze models trained at size 224 or fine-tuned (FT) @384, and compare them to models fine-tuned at the target inference size to show the tolerance to test-time resolution changes. The best model are pre-trained on ImageNet21k at 224 or 320 and fine-tuned at test-time resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discussion: analysis &amp; properties</head><p>Below we discuss several properties of our convolutional trunk augmented with the proposed attention-based aggregation stage.</p><p>1. Simple parametrization. Our main models are fully defined by width and depth. See <ref type="figure">Figure 4</ref> for results obtained with these models at two different resolutions (224 and 384). Following the same convention as in previous work on vision transformers and vision MLPs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>, we refer by S the models with an vector size of d = 384 per patch, by B when d = 768, and by L for d = 1024. We use the S60 model for most of our ablations and comparisons since it has a similar number of parameters and FLOPs as a ResNet-50.</p><p>2. Visualization. Our model allows to easily visualize the network activity. Saliency maps are directly extracted from our network without any post-processing.</p><p>3. Constant resolution across the trunk. The patch-based processing leads to a single processing resolution in the trunk. Therefore the activation size is constant across the whole network. The memory usage is (almost) constant at inference time, up to the pre-and post-processing stage, which are comparatively less demanding. Compared to traditional ConvNets, the network has a coarser processing in the early stages, but a finer resolution towards the output of the trunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Linear scaling with image size. This is a key difference with Vision Transformers. Pyramidal transformers such as LeVit, SwinTransformer or MViT partly solve the problem by breaking the quadratic component by rapidly down-scaling the image. However, they don't avoid the memory peaks happening with very large images. As a consequence of that constant memory usage and linear scaling, our model smoothly scales to larger resolutions, as shown in <ref type="figure">Figure 5</ref> where we report the Peak Memory usage as a function of the image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Easy change of resolution. We do not require any positional encoding, as the relative patch positions are handled by the convolutions. In that respect our approach is more flexible than most approaches that needs to be fine-tuned or trained from scratch for each possible tar-get resolution. In <ref type="figure">Figure 6</ref> we show that the properties of our models are quite stable under relatively significant resolution changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">No max-pooling.</head><p>There is no max-pooling or other non-reversible operator in our architecture. Formally the function implemented by the trunk is bijective until the aggregation stage. We do not exploit this property in this paper, but it may be useful in contexts like image generation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training recipes</head><p>Like many other works (see Liu et al. <ref type="bibr" target="#b39">[40]</ref>, <ref type="table" target="#tab_8">Table I</ref>), our training algorithm inherits from the DeiT <ref type="bibr" target="#b58">[59]</ref> procedure for training transformers. We adopt the Lamb optimizer <ref type="bibr" target="#b72">[73]</ref> (a variant of AdamW <ref type="bibr" target="#b41">[42]</ref>) with a half-cosine learning schedule and label smoothing <ref type="bibr" target="#b53">[54]</ref>. For data augmentation, we include the RandAugment <ref type="bibr" target="#b10">[11]</ref> variant by Wightman et al. <ref type="bibr" target="#b65">[66]</ref>, Mixup <ref type="bibr" target="#b76">[77]</ref> (? = 0.8) and CutMix <ref type="bibr" target="#b73">[74]</ref> (? = 1.0). Notably, we include Stochastic Depth <ref type="bibr" target="#b30">[31]</ref> that is very effective for deep transformers <ref type="bibr" target="#b58">[59]</ref>, and for which we observe the same effect with our deep PatchConvNet. We adopt a uniform drop rate for all layers, and we cross-validate this parameter on ImageNet1k for each model (scores in Table B.3). We also adopt LayerScale <ref type="bibr" target="#b58">[59]</ref>. For the deepest models, the drop-rate hyper-parameter (often called "droppath") can be set as high as 0.5, meaning that we can potentially drop half of the trunk. A desirable byproduct of this augmentation is that it accelerates the training. Note that we do not use gradient clipping, Polyak averaging, or erasing to keep our procedure simple enough.</p><p>We now detail some context-dependent adjustments, based on datasets (ImageNet1k or ImageNet21k), and training (from scratch or fine-tuned). Note that, apart our sensivity study, we use the same Seed 0 for all our experiments <ref type="bibr" target="#b65">[66]</ref> to prevent picking a "lucky seed" <ref type="bibr" target="#b44">[45]</ref> that would not be representative of the model performance.</p><p>Training on ImageNet1k. We train during 400 epochs with a batch size of 2048 and a learning rate fixed at 3.10 ?3 for all models. Based on early experiments, we fixed the weight decay to 0.01 for S models and 0.05 for wider models, but practically we observed that the stochastic depth parameter had a preponderant influence and the most important to adjust, similar to prior observations with ViT et al. <ref type="bibr" target="#b58">[59]</ref>. We use repeated augmentation <ref type="bibr" target="#b5">[6]</ref> only when training with this dataset.</p><p>Fine-tuning at higher resolutions. We fine-tune our models at higher resolutions in order to correct the train-test resolution discrepancy <ref type="bibr" target="#b60">[61]</ref>, and to analyze the behavior of our models at higher resolutions. This can save a significant amount of resources because models operating at larger resolutions are very demanding to train. For fine-tuning, we use a smaller batch size of 1024 in order to compensate for the larger memory requirements. We fix the learning rate to 10 ?5 , the weight decay to 0.01, and fine-tune during 10 epochs for all our models.</p><p>Training on ImageNet21k. We train during 90 epochs as in prior works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41]</ref>. We trained with a batch size of 2048 with a learning rate of 3.10 ?3 and weight decay of 0.01, or when possible with a batch size of 4096 to accelerate the training. In that case we adjust the learning rate to 4.10 ?3 .</p><p>Fine-tuning from ImageNet21k to ImageNet1k is a more involved modification of the network than just finetuning across resolutions because one needs to re-learn the classifiers. In that case, we adopt a longer fine-tuning schedule of 100 epochs along with a batch size of 1024 and an initial learning rate of 5.10 ?4 with a half-cosine schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main experimental results</head><p>This section presents our main experimental results in Image classification, detection and segmentation. We also include an ablation study. We refer the reader to the supplemental material for some additional hyper-parameter studies. Our code depend on the PyTorch <ref type="bibr" target="#b0">[1]</ref> and timm libraries <ref type="bibr" target="#b64">[65]</ref>. We will share model weights along with a PyTorch implementation of our main models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification results</head><p>We first compare our model with competing approaches on the validation set of ImageNet1k (Imnet-val / Top-1) and ImageNet-v2 in <ref type="table" target="#tab_2">Table 1</ref>. We report the compute requirement as reflected by FLOPs, the Peak memory usage, the number of parameters, and a throughput at inference time measured for a constant batch-size of 256 images.</p><p>We compare with various models, including classic models like ResNet-50 revisited with modern training recipes such as the one recently proposed by Wightman et al. <ref type="bibr" target="#b65">[66]</ref>. Note however that different models may have received a different optimization effort, therefore the results on a single criterion are mostly indicative. That being pointed out, we believe that the PatchConvNet results show that a simple columnar architecture is a viable choice compared to other attention-based approaches that are more difficult to optimize or scale.</p><p>Higher-resolution. There is a fine interplay between model size and resolution when it comes to the specific optimization of FLOPs and accuracy. We refer to the findings of Bello et al. <ref type="bibr" target="#b3">[4]</ref> who discussed some of these relationships,  All models are trained on ImageNet1k only without distillation nor selfsupervised pre-training. We report Top-1 accuracy on the validation set of ImageNet1k and ImageNet-V2 with different measure of complexity: throughput, FLOPs, number of parameters and peak memory usage. The throughput and peak memory are measured on a single V100-32GB GPU with batch size fixed to 256 and mixed precision. For ResNet <ref type="bibr" target="#b23">[24]</ref> and Reg-Net <ref type="bibr" target="#b45">[46]</ref> we report the improved results from Wightman et al. <ref type="bibr" target="#b65">[66]</ref>. Note that different models may have received a different optimization effort. ?R indicates that the model is fine-tuned at the resolution R. for instance the fact that small networks are better associated with smaller resolution. In our work we have not optimized for the Pareto curve specifically. Since this trade-off is only one out of multiple criteria depending on the context, we prefer to report most of our results at the 224 and 384 resolutions. <ref type="table" target="#tab_2">Table 1</ref> shows that our model significantly benefit from larger resolution images. See also <ref type="figure">Figures 5  and 6</ref> where we analyze PatchConvNet as a function of the image size. <ref type="table" target="#tab_4">Table 2</ref> we analyze PatchConvNet pre-trained on ImageNet21k with different fine-tuning resolution. All network are pre-trained on ImageNet21k during 90 epochs at resolution 224 ? 224, finetune on ImageNet1k at resolution 384 ? 384 and then fine-tune at bigger resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Segmentation results and detection</head><p>Semantic segmentation We evaluate our models with semantic segmentation experiments on the ADE20k dataset <ref type="bibr" target="#b79">[80]</ref>. This dataset consist in 20k training and 5k validation images with labels over 150 categories. For the training, we adopt the same schedule as in Swin <ref type="bibr" target="#b40">[41]</ref>: 160k iterations with UpperNet <ref type="bibr" target="#b67">[68]</ref>. At test time we evaluate with a single scale similarly to XciT <ref type="bibr" target="#b18">[19]</ref> and multi-scale as in Swin <ref type="bibr" target="#b40">[41]</ref>. As our approach is not pyramidal we only use the final output of our network in UpperNet. Unlike concurrent approaches we only use the output of our network at different levels in UpperNet which simplifies the approach. Our results are reported in <ref type="table" target="#tab_8">Table 3</ref>. We can observe that our approach although simpler is at the same level as the state-of-the-art Swin architecture <ref type="bibr" target="#b40">[41]</ref> and outperforms XCiT <ref type="bibr" target="#b18">[19]</ref> in terms of FLOPs-mIoU tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection &amp; instance segmentation</head><p>We have evaluated our models on detection and instance segmentation tasks on COCO <ref type="bibr" target="#b37">[38]</ref>. We adopt the Mask R-CNN <ref type="bibr" target="#b22">[23]</ref> setup with the commonly used ?3 schedule. Similar to segmenta- <ref type="table" target="#tab_8">Table 3</ref>. ADE20k semantic segmentation performance using UperNet <ref type="bibr" target="#b68">[69]</ref> (in comparable settings). All models are pre-trained on ImageNet1k except models with ? symbol that are pre-trained on ImageNet21k.  tion experiments, as our approach is not pyramidal, we only use the final output of our network in Mask R-CNN <ref type="bibr" target="#b22">[23]</ref>. Our results are in <ref type="table" target="#tab_6">Table 4</ref>. We can observe that our simple approach is on par with state of the art architecture like Swin <ref type="bibr" target="#b40">[41]</ref> and XCiT <ref type="bibr" target="#b18">[19]</ref> in terms of FLOPs-AP tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>All our ablation have been carried out with "Seed 0", i.e., we report only one result without handpicking. For this reason one must keep in mind that there is a bit of noise in the performance measurements: On ImageNet1k-val, we have measured with the seeds 1 to 10 a standard deviation of ?0.11% in top-1 accuracy for a S60 model, which concurs with measurements done on ResNet-50 trained with modern training procedures <ref type="bibr" target="#b65">[66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic depth.</head><p>Our main parameter is the stochastic depth, whose effect is analyzed in <ref type="figure">Fig. 8</ref>. This regulariza- tion slows down the training, yet with long enough schedules, higher values of the drop-path hyperparameter lead to better performance at convergence. We train with the values reported in <ref type="table" target="#tab_8">Table B</ref>.3. When fine-tuning at higher resolutions or from ImageNet21k, we reduce this drop-path by 0.1. See also Appendix A for a preliminary ablation on the learning rate and weight decay, which showed that the performance is relatively stable with respect to these parameters. Fixing this hyper-parameter couple is possibly suboptimal but makes it convenient and more resource-efficient to adjust a single hyper-parameter per model. Therefore, we have adopted this choice in all our experiments.</p><p>Architectural ablation. In <ref type="table" target="#tab_8">Table 5</ref>, we have conducted various ablations of our architecture with the S60 model. We compare the impact of class attention vs. averagepooling. Average-pooling is the most common aggregation strategy in ConvNet while class attention is only used with transformers <ref type="bibr" target="#b58">[59]</ref>. We compare also convolutional stem vs. linear projection for the patch extraction in the image, LayerNorm vs. BatchNorm and Multi-heads class attention as used in CaiT <ref type="bibr" target="#b58">[59]</ref> vs. single-head class attention. Our single-head design reduces the memory consumption and simplifies attention map visualization.</p><p>Attention-based pooling with ConvNets. Interestingly, our learned aggregation stage increases the performance of a very competitive ResNet model. When adopting the recent training recipe from Wightman et al. <ref type="bibr" target="#b65">[66]</ref>, we obtain 80.1% top-1 accuracy on Imagenet1k by adding a learned pooling to a ResNet50. This is an improvement of +0.3% <ref type="table" target="#tab_8">Table 5</ref>. Ablation of our model: we modify each time a single architectural characteristic in our PatchConvNet model S60, and measure how it affects the classification performance on ImageNet1k. Batch-normalization improves the performance a bit. The convolutional stem is key for best performance, and the class-attention brings a slight improvement in addition to enabling attention-based visualisation properties. We point out that we have not optimized the training recipes further (either without or with class-attention). This result is reported for a single run (Seed 0) in both cases.</p><p>Patch pre-processing. In the vanilla patch-based approaches as vision transformers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref> and MLP-style models <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, the images patches are embedded by one linear layer. Recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b70">71]</ref> show that replacing this linear patch pre-processing by a few convolutional layers allows to have a more stable architecture <ref type="bibr" target="#b70">[71]</ref> with better performance. So, in our work we choose to use a convolutional stem instead of pure linear projection. We provide in <ref type="table" target="#tab_8">Table 5</ref> an ablation of this component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced a full patch-based ConvNet with no pyramidal structure. We used an attention-based pooling on top of the trunk, akin to the attention mechanism in transformers, which offers visualization properties. Our model is only parametrized by its width and depth, and its training does not require a heavy hyper-parameter search. We demonstrated its interest on several computer vision tasks: classification, segmentation, detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations:</head><p>There is no perfect metric for measuring the overall performance of a given neural network architecture <ref type="bibr" target="#b13">[14]</ref>. We have provided 4 different metrics but there are probably some aspects that are not considered. Deep and wide models have the same behaviour with respect to FLOPs but the wider models have the advantage to be associated with a lower latency <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b74">75]</ref>. We have mostly experimented with depth rather than width because deep models consume less memory at inference time, which makes them an appealing choice when dealing with higher resolution images <ref type="bibr" target="#b3">[4]</ref>, as is the case in segmentation and detection.</p><p>Broader impact: Large scale deep learning models are effective for many different computer vision applications, but the way they reach their decision is still not yet fully understood. When deploying such machine learning-based systems, there would be a benefit to be able to illustrate their choices in critical applications. We hope that our model, by its simplicity, and by its built-in internal visualization mechanism, may foster this direction of interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Hyper-parameter study: exploration phase</p><p>In this appendix we discuss the grid searches that we have done for the material presented in this paper. During our exploration phase, we have modified only a few variables hyper-parameters to avoid some potential overfitting, which usually results from the exploration of a large hyperparameter space: we have solely changed the learning rate (LR), the weight decay (WD) and the drop-path parameter involved in stochastic depth (SD). For the same reason we have selected a relatively coarse grid search. We have fixed the batch size to 2048, and changed the hyper-parameters by setting them from the following values:</p><p>? LR ? { 0.001, 0.0015, 0.002, 0.003, 0.004, 0.005 } ;</p><p>? WD ? { 0.001, 0.01, 0.03, 0.05, 0.1, 0.15, 0.2 } ;</p><p>? SD ? { 0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4 ,0.5 }.</p><p>Note, we have not exhaustively spanned the product space of these values with a grid search: after a few tests on a few models (mostly: S36 &amp; S60), we concluded that we could set LR = 3.10 ?3 . We had the same conclusion for setting WD = 0.01, yet for larger models trained on Imagenet-val, we preemptively increased the regularization to WD=0.05 for larger models (d = 384) in case the lack of regularization would have affected the convergence (which we noticed with very small values of WD for small models, see our ablation in <ref type="table" target="#tab_8">Table B</ref>.1). However, the difference does not seem statistically significant from the value WD = 0.01 in the few experiments that we have done subsequently. While our choice are likely not optimal for all models, in our opinion the benefit of taking a single tuple (LR,WD) for models of all depth vastly overcome the risk of overfitting/over-estimating the performance. The other hyper-parameters are inherited from typical values in the literature <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b65">66]</ref> without any optimization from us, and therefore could potentially be improved.</p><p>Regarding the last hyper-parameter SD, as observed by Touvron et al. <ref type="bibr" target="#b58">[59]</ref> for vision transformers, we noticed that validating this hyper-parameter properly is key to performance. Since this validation is carried out on Imagenet, in the main paper we have reported results on Imagenet-V2 to ensure an independent test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablations</head><p>Hyper-parameters. Some regularization is needed for convergence and the learning rate should be kept below a threshold (0.005). The LR and SD hyper-parameters are the more influential on the performance. those of BatchNorm. We can see that BatchNorm increases the PatchConvNet top-1 accuracy. This difference tends to be lower for the deeper models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transfer Learning experiments</head><p>We evaluate our architecture on 6 transfer learning tasks. The datasets used are summarized Table D.1. For fine-tuning we used the procedure used in CaiT <ref type="bibr" target="#b58">[59]</ref> and DeiT <ref type="bibr" target="#b57">[58]</ref>. Our results are summarized Table D.2. We can observe that our architecture achieves competitive performance on transfer learning tasks. All models are trained on ImageNet1k only without distillation nor selfsupervised pre-training. We report Top-1 accuracy on the validation set of ImageNet1k and ImageNet-V2 with different measure of complexity: throughput, FLOPs, number of parameters and peak memory usage. The throughput and peak memory are measured on a single V100-32GB GPU with batch size fixed to 256 and mixed precision. For ResNet <ref type="bibr" target="#b23">[24]</ref> and Reg-Net <ref type="bibr" target="#b45">[46]</ref> we report the improved results from Wightman et al. <ref type="bibr" target="#b65">[66]</ref>. Note that different models may have received a different optimization effort. ?R indicates that the model is fine-tuned at the resolution R.   <ref type="bibr" target="#b27">[28]</ref> 265,240 3,003 1,010 Flowers-102 <ref type="bibr" target="#b42">[43]</ref> 2,040 6,149 102 Stanford Cars <ref type="bibr" target="#b33">[34]</ref> 8,144 8,041 196 CIFAR-100 <ref type="bibr" target="#b34">[35]</ref> 50,000 10,000 100 CIFAR-10 <ref type="bibr" target="#b34">[35]</ref> 50,000 10,000 10 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>16x16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Analysis of the accuracy as a function of width (S: d = 384, B: d = 768) and depth N . Depending on the performance criterion (importance of latency, resolution, FLOPs), one could prefer either deeper models or wider models. See Bello et al. [4] for a study on the relationship between model size, resolution and compute. Peak memory for varying resolution and different models. Some models like Swin require a full training at the target resolution. Our model scales linearly as a function of the image surface, like other ConvNets. This is in contrast to most attention-based models, which abide by a quadratic complexity for images large enough.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Trade-offs for ImageNet-1k top 1 accuracy vs. FLOPs requirement and peak memory requirements (for a batch of 256 images). Patchbased architectures are comparatively inferior w.r.t. the accuracy-FLOP trade-off than hierarchical ones, but offer better operating points in terms of the accuracy-memory compromise at inference time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 Figure 8 .</head><label>58</label><figDesc>Effect of stochastic depth on the performance for varying training duration for a PatchConvNet-B120 model trained @ resolution 224. The corresponding hyper-parameter (drop-path) is selected among 0.3, 0.4 or 0.5 in that case, which means that we randomly drop up to half of the layers. Smaller values of the drop-rate converge more rapidly but saturate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>Modification to the architecture Top-1 acc. none 82.1 class-attention ? average pooling 81.9 conv-stem ? linear projection 80.0 layer-normalization ? batch-normalization 82.4 single-head attention ? multi-head attention 81.9 a single class-token ? one class-token per class 81.1 to the corresponding 300-epoch baseline based on average pooling. The class attention only slightly increases the number of FLOPs of the models: 4.6B vs 4.1B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Classification</figDesc><table /><note>with Imagenet1k training. We compare archi- tectures with based on convolutional networks, Transformers and feed- forward networks with comparable FLOPs and number of parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>ImageNet21k pre-training: Comparison of PatchConvNet finetuned at different resolutions on ImageNet1k. We report peak memory (MB) and throughput (im/s) on one GPU V100 with batch size 256 and mixed precision. Larger resolution provides classification improvement with the same model, but significantly increase the resource requirements.[italic refers to a few results obtained with a longer training].</figDesc><table><row><cell cols="5">Model GFLOPs Peak Mem throughput Res</cell><cell>Imnet-val Acc</cell></row><row><cell>S60</cell><cell>4.0</cell><cell>1322</cell><cell>1129</cell><cell>224</cell><cell>82.9 [83.5]</cell></row><row><cell>S60</cell><cell>6.6</cell><cell>2091</cell><cell>692</cell><cell>288</cell><cell>84.0 [84.4]</cell></row><row><cell>S60</cell><cell>11.8</cell><cell>3604</cell><cell>388</cell><cell>384</cell><cell>84.6 [84.9]</cell></row><row><cell>S60</cell><cell>20.9</cell><cell>6296</cell><cell>216</cell><cell>512</cell><cell>85.0 [85.4]</cell></row><row><cell>B60</cell><cell>15.8</cell><cell>2794</cell><cell>547</cell><cell>224</cell><cell>85.0 [85.4]</cell></row><row><cell>B60</cell><cell>26.1</cell><cell>4235</cell><cell>328</cell><cell>288</cell><cell>85.7</cell></row><row><cell>B60</cell><cell>46.5</cell><cell>7067</cell><cell>185</cell><cell>384</cell><cell>86.1 [86.5]</cell></row><row><cell>L60</cell><cell>28.1</cell><cell>3913</cell><cell>394</cell><cell>224</cell><cell>85.6</cell></row><row><cell>L60</cell><cell>46.4</cell><cell>5801</cell><cell>237</cell><cell>288</cell><cell>86.1</cell></row><row><cell>L60</cell><cell>82.5</cell><cell>9506</cell><cell>132</cell><cell>384</cell><cell>86.4</cell></row><row><cell>B120</cell><cell>29.8</cell><cell>3313</cell><cell>280</cell><cell>224</cell><cell>86.0</cell></row><row><cell>B120</cell><cell>49.3</cell><cell>4752</cell><cell>169</cell><cell>288</cell><cell>86.6</cell></row><row><cell>B120</cell><cell>87.7</cell><cell>7587</cell><cell>96</cell><cell>384</cell><cell>86.9</cell></row><row><cell>L120</cell><cell>53.0</cell><cell>4805</cell><cell>204</cell><cell>224</cell><cell>86.1</cell></row><row><cell>L120</cell><cell>87.5</cell><cell>6693</cell><cell>123</cell><cell>288</cell><cell>86.6</cell></row><row><cell>L120</cell><cell>155.5</cell><cell>10409</cell><cell>68</cell><cell>384</cell><cell>87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>COCO object detection and instance segmentation performance on the mini-val set. All backbones are pre-trained on ImageNet1k, use Mask R-CNN model<ref type="bibr" target="#b22">[23]</ref> and are trained with the same 3? schedule.</figDesc><table><row><cell>Backbone</cell><cell cols="3">#params GFLOPs AP b AP b 50 AP b 75</cell><cell>AP m AP m 50 AP m 75</cell></row><row><cell>ResNet50 [24]</cell><cell>44.2M</cell><cell>180</cell><cell>41.0 61.7 44.9</cell><cell>37.1 58.4 40.1</cell></row><row><cell>ResNet101 [24]</cell><cell>63.2M</cell><cell>260</cell><cell>42.8 63.2 47.1</cell><cell>38.5 60.1 41.3</cell></row><row><cell cols="2">ResNeXt101-64 [72] 101.9M</cell><cell>424</cell><cell>44.4 64.9 48.8</cell><cell>39.7 61.9 42.6</cell></row><row><cell>PVT-Small [63]</cell><cell>44.1M</cell><cell></cell><cell>43.0 65.3 46.9</cell><cell>39.9 62.5 42.8</cell></row><row><cell>PVT-Medium [63]</cell><cell>63.9M</cell><cell></cell><cell>44.2 66.0 48.2</cell><cell>40.5 63.1 43.5</cell></row><row><cell>XCiT-S12/16</cell><cell>44.4M</cell><cell>295</cell><cell>45.3 67.0 49.5</cell><cell>40.8 64.0 43.8</cell></row><row><cell>XCiT-S24/16 [19]</cell><cell>65.8M</cell><cell>385</cell><cell>46.5 68.0 50.9</cell><cell>41.8 65.2 45.0</cell></row><row><cell>ViL-Small [78]</cell><cell>45.0M</cell><cell>218</cell><cell>43.4 64.9 47.0</cell><cell>39.6 62.1 42.4</cell></row><row><cell>ViL-Medium [78]</cell><cell>60.1M</cell><cell>294</cell><cell>44.6 66.3 48.5</cell><cell>40.7 63.8 43.7</cell></row><row><cell>ViL-Base [78]</cell><cell>76.1M</cell><cell>365</cell><cell>45.7 67.2 49.9</cell><cell>41.3 64.4 44.5</cell></row><row><cell>Swin-T [41]</cell><cell>47.8M</cell><cell>267</cell><cell>46.0 68.1 50.3</cell><cell>41.6 65.1 44.9</cell></row><row><cell>Ours-S60</cell><cell>44.9M</cell><cell>264</cell><cell>46.4 67.8 50.8</cell><cell>41.3 64.8 44.2</cell></row><row><cell>Ours-S120</cell><cell>67.4M</cell><cell>339</cell><cell>47.0 69.0 51.4</cell><cell>41.9 65.6 44.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>TableB.1 and B.2 provide the accuracy obtained when varying our hyper-parameters with the S60 model, with our baseline as LR = 3.10 ?3 , WD = 0.01 and SD = 0.15. Weight Decay (WD). Rows highlight in red with "fail@E" indicates that the training has failed at Epoch E. The model reaches a reasonable performance over a wide set of values. For instance the intervals LR ? [0.2, 0.3] or WD ? [0.01, 0.02] lead to similar values. The optimization is stable with reasonable performance for hyper-parameters covering large intervals (LR ? [0.1, 0.5] or WD ? [0.005, 0.05]).</figDesc><table><row><cell cols="2">Model LR</cell><cell>WD</cell><cell>SD</cell><cell>Imagenet-val</cell></row><row><cell></cell><cell></cell><cell cols="3">ablation: learning rate</cell></row><row><cell>S60</cell><cell cols="2">0.0005 0.01</cell><cell>0.15</cell><cell>77.00</cell></row><row><cell>S60</cell><cell cols="2">0.0010 0.01</cell><cell>0.15</cell><cell>80.70</cell></row><row><cell>S60</cell><cell cols="2">0.0015 0.01</cell><cell>0.15</cell><cell>81.58</cell></row><row><cell>S60</cell><cell cols="2">0.0020 0.01</cell><cell>0.15</cell><cell>81.92</cell></row><row><cell>S60</cell><cell cols="2">0.0030 0.01</cell><cell>0.15</cell><cell>82.10</cell></row><row><cell>S60</cell><cell cols="2">0.0040 0.01</cell><cell>0.15</cell><cell>81.59</cell></row><row><cell>S60</cell><cell cols="2">0.0050 0.01</cell><cell>0.15</cell><cell>80.31</cell></row><row><cell>S60</cell><cell cols="2">0.0070 0.01</cell><cell>0.15</cell><cell>failed@34</cell></row><row><cell></cell><cell></cell><cell cols="3">ablation: weight decay</cell></row><row><cell>S60</cell><cell cols="3">0.0030 0.001 0.15</cell><cell>failed@92</cell></row><row><cell>S60</cell><cell cols="3">0.0030 0.002 0.15</cell><cell>failed@105</cell></row><row><cell>S60</cell><cell cols="3">0.0030 0.005 0.15</cell><cell>81.66</cell></row><row><cell>S60</cell><cell cols="3">0.0030 0.010 0.15</cell><cell>82.10</cell></row><row><cell>S60</cell><cell cols="3">0.0030 0.020 0.15</cell><cell>82.03</cell></row><row><cell>S60</cell><cell cols="3">0.0030 0.050 0.15</cell><cell>81.59</cell></row><row><cell>S60</cell><cell cols="3">0.0030 0.100 0.15</cell><cell>81.33</cell></row><row><cell cols="5">Table B.1. Sensitivity to our hyper-parameters for the S60 model: Learn-</cell></row><row><cell>ing Rate (LR),</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table B</head><label>B</label><figDesc>.2 analyses their interaction, which shows that they can be set relatively independently.LayerNorm vs BatchNorm. LayerNorm is the most used normalisation in transformers while BatchNorm is the most used normalisation with ConvNets. For simplicity we have used LayerNorm as it does not require (batch) statistics synchronisation during training, which tends to significantly slow the training, especially on an infrastructure with relatively high synchronisation costs.InTable B.3 we compare the effects of LayerNorm with I 80.01 80.56 80.77 0.05 80.62 81.56 81.60 81.82 0.1 80.75 81.78 82.00 81.90 0.15 80.70 81.58 81.92 82.10 0.2 80.43 81.44 81.70 81.90 Table B.2. Analysis of Learning rate vs stochastic depth hyper-parameters (S60, WD=0.01).</figDesc><table><row><cell></cell><cell></cell><cell>learning rate</cell><cell></cell></row><row><cell>SD</cell><cell cols="3">0.001 0.0015 0.002 0.003</cell></row><row><cell cols="4">0 79.51 Table B.3. Comparison of PatchConvNet with Layer-Normalization and</cell></row><row><cell cols="4">Batch-Normalization: Performance on Imagenet-1k-val after pre-training</cell></row><row><cell cols="4">on Imagenet-1k-train only. The drop-path parameter value is obtained by</cell></row><row><cell cols="4">cross-validation on Imagenet1k for each model. Batch-Normalization usu-</cell></row><row><cell cols="4">ally provides a slight improvement in classification, but but with large mod-</cell></row><row><cell cols="4">els the need to synchronization can significantly slow down the training</cell></row><row><cell cols="4">(in some cases like training a B120 model on AWS, it almost doubled the</cell></row><row><cell cols="4">training time). Therefore we do not use it in the main paper.</cell></row><row><cell></cell><cell></cell><cell cols="2">Imagenet-val Top-1 acc.</cell></row><row><cell cols="4">Model drop-path LayerNorm BatchNorm</cell></row><row><cell>S20</cell><cell>0.0</cell><cell>78.7</cell><cell>78.8</cell></row><row><cell>S36</cell><cell>0.05</cell><cell>80.7</cell><cell>81.2</cell></row><row><cell>S60</cell><cell>0.15</cell><cell>82.1</cell><cell>82.4</cell></row><row><cell>S120</cell><cell>0.2</cell><cell>83.2</cell><cell>83.4</cell></row><row><cell>B36</cell><cell>0.2</cell><cell>82.8</cell><cell>83.5</cell></row><row><cell>B60</cell><cell>0.3</cell><cell>83.5</cell><cell>83.9</cell></row><row><cell>B120</cell><cell>0.4</cell><cell>84.1</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table C .</head><label>C</label><figDesc>1. Comparison of architectures on classification. We compare different architectures based on convolutional networks, Transformers and feedforward networks with comparable FLOPs and number of parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table D.1. Datasets used for our transfer learning tasks.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train size Test size #classes</cell></row><row><cell>iNaturalist 2018 [29]</cell><cell>437,513</cell><cell>24,426</cell><cell>8,142</cell></row><row><cell>iNaturalist 2019</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table D .</head><label>D</label><figDesc>2. Results in transfer learning.</figDesc><table><row><cell>Model</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>Flowers</cell><cell>Cars</cell><cell>iNat-18</cell><cell>iNat-19</cell><cell>FLOPs</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<idno>2021-08-01. 5</idno>
		<ptr target="https://pytorch.org/vision/stable/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patches are all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to The Tenth International Conference on Learning Representations, 2022. under review</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Jonathon Shlens, and Barret Zoph. Revisiting ResNets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Attention augmented convolutional networks. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Highperformance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prantik</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<title level="m">Transformer interpretability beyond attention visualization. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical automated data augmentation with a reduced search space</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The efficiency misnomer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12894</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">RepMLP: Re-parameterizing convolutions into fullyconnected layers for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01883</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09681</idno>
		<title level="m">Cross-covariance image transformers</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Non-deep networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01136</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16302</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Belongie. The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2018 dataset</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>CIFAR</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Are we ready for a new paradigm shift? a survey on visual deep mlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linmi</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Jianping Fan, and Zhiqiang He. A survey of visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongchao</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">torch.manual seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08203</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>Neurips</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Non-discriminative data or weak model? on the relative importance of data and model resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baccash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hui</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03019</idno>
		<title level="m">Global self-attention networks for image recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">MLP-Mixer: An all-MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">ResMLP: feedforward networks for image classification with data-efficient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>I, II, III</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, 2021. 3, 5</title>
		<meeting><address><addrLine>7, I, II, III</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Grafit: Learning fine-grained image representations with coarse labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy. Neurips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Score-cam: Score-weighted visual explanations for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>6, 7, I, II, III</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?ter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<title level="m">Piotr Doll?r, and Ross Girshick. Early convolutions help transformers see better</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14881</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cutmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04899</idno>
		<title level="m">Regularization strategy to train strong classifiers with localizable features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15358</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Scene parsing through ade20k dataset. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
