<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
							<email>ghaeinim@eecs.oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<email>sadid.hasan@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Datla</surname></persName>
							<email>vivek.datla@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Liu</surname></persName>
							<email>joey.liu@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Lee</surname></persName>
							<email>kathy.lee1@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
							<email>ashequl.qadir@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ling</surname></persName>
							<email>yuan.ling@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
							<email>aaditya.prakash@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
							<email>xfern@eecs.oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Farri</surname></persName>
							<email>dimeji.farri@philips.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel deep learning architecture to address the natural language inference (NLI) task. Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis. Instead, we propose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to efficiently model the relationship between a premise and a hypothesis during encoding and inference. We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions. Finally, we demonstrate how the results can be improved further with an additional preprocessing step. Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the Stanford NLI dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Inference (NLI; a.k.a. Recognizing Textual Entailment, or RTE) is an important and challenging task for natural language understanding <ref type="bibr">(MacCartney and Manning, 2008)</ref>. The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothesis. <ref type="table" target="#tab_0">Table 1</ref> shows few example relationships from the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr">(Bowman et al., 2015)</ref>.</p><p>Recently, NLI has received a lot of attention from the researchers, especially due to the availability of large annotated datasets like SNLI (Bow- * This work was conducted as part of an internship program at Philips Research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P a</head><p>A senior is waiting at the Relationship window of a restaurant that serves sandwiches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H b</head><p>A person waits to be Entailment served his food. A man is looking to order Neutral a grilled cheese sandwich. A man is waiting in line Contradiction for the bus. a P, Premise. b H, Hypothesis.  <ref type="bibr">et al., 2015)</ref>. Various deep learning models have been proposed that achieve successful results for this task <ref type="bibr">(Gong et al., 2017;</ref><ref type="bibr" target="#b9">Wang et al., 2017;</ref><ref type="bibr">Chen et al., 2017;</ref><ref type="bibr" target="#b11">Yu and Munkhdalai, 2017a;</ref><ref type="bibr" target="#b1">Parikh et al., 2016;</ref><ref type="bibr" target="#b13">Zhao et al., 2016;</ref><ref type="bibr" target="#b5">Sha et al., 2016)</ref>. Most of these existing NLI models use attention mechanism to jointly interpret and align the premise and hypothesis. Such models use simple reading mechanisms to encode the premise and hypothesis independently. However, such a complex task require explicit modeling of dependency relationships between the premise and the hypothesis during the encoding and inference processes to prevent the network from the loss of relevant, contextual information. In this paper, we refer to such strategies as dependent reading.</p><p>There are some alternative reading mechanisms available in the literature <ref type="bibr" target="#b5">(Sha et al., 2016;</ref><ref type="bibr" target="#b4">Rockt?schel et al., 2015)</ref> that consider dependency aspects of the premise-hypothesis relationships. However, these mechanisms have two major limitations:</p><p>? So far, they have only explored dependency aspects during the encoding stage, while ignoring its benefit during inference.</p><p>? Such models only consider encoding a hypothesis depending on the premise, disre-arXiv:1802.05577v2 [cs.CL] 11 Apr 2018 garding the dependency aspects in the opposite direction.</p><p>We propose a dependent reading bidirectional LSTM (DR-BiLSTM) model to address these limitations. Given a premise u and a hypothesis v, our model first encodes them considering dependency on each other <ref type="bibr">(u|v and v|u)</ref>. Next, the model employs a soft attention mechanism to extract relevant information from these encodings. The augmented sentence representations are then passed to the inference stage, which uses a similar dependent reading strategy in both directions, i.e. u ? v and v ? u. Finally, a decision is made through a multi-layer perceptron (MLP) based on the aggregated information.</p><p>Our experiments on the SNLI dataset show that DR-BiLSTM achieves the best single model and ensemble model performance obtaining improvements of a considerable margin of 0.4% and 0.3% over the previous state-of-the-art single and ensemble models, respectively. Furthermore, we demonstrate the importance of a simple preprocessing step performed on the SNLI dataset. Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state-of-the-art ensemble model and improves our ensemble model to outperform the state-of-the-art ensemble model by a remarkable margin of 0.7%. Finally, we perform an extensive analysis to clarify the strengths and weaknesses of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early studies use small datasets while leveraging lexical and syntactic features for NLI <ref type="bibr">(Mac-Cartney and Manning, 2008)</ref>. The recent availability of large-scale annotated datasets <ref type="bibr">(Bowman et al., 2015;</ref><ref type="bibr">Williams et al., 2017)</ref> has enabled researchers to develop various deep learning-based architectures for NLI. <ref type="bibr" target="#b1">Parikh et al. (2016)</ref> propose an attention-based model <ref type="bibr">(Bahdanau et al., 2014</ref>) that decomposes the NLI task into sub-problems to solve them in parallel. They further show the benefit of adding intra-sentence attention to input representations. <ref type="bibr">Chen et al. (2017)</ref> explore sequential inference models based on chain LSTMs with attentional input encoding and demonstrate the effectiveness of syntactic information. We also use similar attention mechanisms. However, our model is distinct from these models as they do not benefit from dependent reading strategies. <ref type="bibr" target="#b4">Rockt?schel et al. (2015)</ref> use a word-by-word neural attention mechanism while <ref type="bibr" target="#b5">Sha et al. (2016)</ref> propose re-read LSTM units by considering the dependency of a hypothesis on the information of its premise (v|u) to achieve promising results. However, these models suffer from weak inferencing methods by disregarding the dependency aspects from the opposite direction (u|v). Intuitively, when a human judges a premise-hypothesis relationship, s/he might consider back-and-forth reading of both sentences before coming to a conclusion. Therefore, it is essential to encode the premise-hypothesis dependency relations from both directions to optimize the understanding of their relationship. <ref type="bibr" target="#b9">Wang et al. (2017)</ref> propose a bilateral multiperspective matching (BiMPM) model, which resembles the concept of matching a premise and hypothesis from both directions. Their matching strategy is essentially similar to our attention mechanism that utilizes relevant information from the other sentence for each word sequence. They use similar methods as Chen et al. (2017) for encoding and inference, without any dependent reading mechanism.</p><p>Although NLI is well studied in the literature, the potential of dependent reading and interaction between a premise and hypothesis is not rigorously explored. In this paper, we address this gap by proposing a novel deep learning model (DR-BiLSTM). Experimental results demonstrate the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Our proposed model (DR-BiLSTM) is composed of the following major components: input encoding, attention, inference, and classification. Let u = [u 1 , ? ? ? , u n ] and v = [v 1 , ? ? ? , v m ] be the given premise with length n and hypothesis with length m respectively, where u i , v j ? R r is an word embedding of r-dimensional vector. The task is to predict a label y that indicates the logical relationship between premise u and hypothesis v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Encoding</head><p>RNNs are the natural solution for variable length sequence modeling, consequently, we utilize a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) for encoding the given sentences. For ease of presentation, we only describe how we encode u depending on v. The same procedure is utilized for the reverse direction (v|u).</p><p>To dependently encode u, we first process v using the BiLSTM. Then we read u through the BiL-STM that is initialized with previous reading final states (memory cell and hidden state). Here we represent a word (e.g. u i ) and its context depending on the other sentence (e.g. v). Equations 1 and 2 formally represent this component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v, s</head><formula xml:id="formula_0">v = BiLSTM(v, 0) u, ? = BiLSTM(u, s v ) (1) u, s u = BiLSTM(u, 0) v, ? = BiLSTM(v, s u )<label>(2)</label></formula><p>where {? ? R n?2d ,? ? R n?2d , s u } and {v ? R m?2d ,v ? R m?2d , s v } are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of u and v respectively. Note that, "?" in these equations means that we do not care about the associated variable and its value. BiLSTM inputs are the word embedding sequences and initial state vectors.? andv are passed to the next layer as the output of the input encoding component. The proposed encoding mechanism yields a richer representation for both premise and hypothesis by taking the history of each other into account. Using a max or average pooling over the independent and dependent readings does not further improve our model. This was expected since dependent reading produces more promising and relevant encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention</head><p>We employ a soft alignment method to associate the relevant sub-components between the given premise and hypothesis. In deep learning models, such purpose is often achieved with a soft attention mechanism. Here we compute the unnormalized attention weights as the similarity of hidden states of the premise and hypothesis with Equation 3 (energy function).</p><formula xml:id="formula_1">e ij =? iv T j , i ? [1, n], j ? [1, m]<label>(3)</label></formula><p>where? i andv j are the dependent reading hidden representations of u and v respectively which are computed earlier in Equations 1 and 2. Next, for each word in either premise or hypothesis, the relevant semantics in the other sentence is extracted and composed according to e ij . Equations 4 and 5 provide formal and specific details of this procedure.</p><formula xml:id="formula_2">u i = m j=1 exp(e ij ) m k=1 exp(e ik )v j , i ? [1, n] (4) v j = n i=1 exp(e ij ) n k=1 exp(e kj )? i , j ? [1, m]<label>(5)</label></formula><p>where? i represents the extracted relevant information ofv by attending to? i while? j represents the extracted relevant information of? by attending tov j .</p><p>To further enrich the collected attentional information, a trivial next step would be to pass the concatenation of the tuples (? i ,? i ) or (v j ,? j ) which provides a linear relationship between them. However, the model would suffer from the absence of similarity and closeness measures. Therefore, we calculate the difference and element-wise product for the tuples (? i ,? i ) and (v j ,? j ) that represent the similarity and closeness information respectively <ref type="bibr">(Chen et al., 2017;</ref><ref type="bibr">Kumar et al., 2016)</ref>.</p><p>The difference and element-wise product are then concatenated with the computed vectors, (? i ,? i ) or (v j ,? j ), respectively. Finally, a feedforward neural layer with ReLU activation function projects the concatenated vectors from 8ddimensional vector space into a d-dimensional vector space (Equations 6 and 7). This helps the model to capture deeper dependencies between the sentences besides lowering the complexity of vector representations.</p><formula xml:id="formula_3">a i = [? i ,? i ,? i ?? i ,? i ? i ] p i = ReLU(W p a i + b p ) (6) b j = [v j ,? j ,v j ?? j ,v j ? j ] q j = ReLU(W p b j + b p )<label>(7)</label></formula><p>Here stands for element-wise product while W p ? R 8d?d and b p ? R d are the trainable weights and biases of the projector layer respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>During this phase, we use another BiLSTM to aggregate the two sequences of computed matching vectors, p and q from the attention stage (Section 3.2). This aggregation is performed in a sequential manner to avoid losing effect of latent variables that might rely on the sequence of matching vectors.</p><p>Instead of aggregating the sequences of matching vectors individually, we propose a similar dependent reading approach for the inference stage. We employ a BiLSTM reading process (Equations 8 and 9) similar to the input encoding step discussed in Section 3.1. But rather than passing just the dependent reading information to the next step, we feed both independent reading (p andq) and dependent reading (p andq) to a max pooling layer, which selects maximum values from each sequence of independent and dependent readings (p i andp i ) as shown in Equations 10 and 11. The main intuition behind this architecture is to maximize the inferencing ability of the model by considering both independent and dependent readings.q , s q = BiLSTM(q, 0)</p><formula xml:id="formula_4">p, ? = BiLSTM(p, s q ) (8) p, s p = BiLSTM(p, 0) q, ? = BiLSTM(q, s p ) (9) p = MaxPooling(p,p) (10) q = MaxPooling(q,q)<label>(11)</label></formula><p>Here {p ? R n?2d ,p ? R n?2d , s p } and {q ? R m?2d ,q ? R m?2d , s q } are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of p and q respectively. BiLSTM inputs are the word embedding sequences and initial state vectors. Finally, we convertp ? R n?2d andq ? R m?2d to fixed-length vectors with pooling, U ? R 4d and V ? R 4d . As shown in Equations 12 and 13, we employ both max and average pooling and describe the overall inference relationship with concatenation of their outputs.</p><formula xml:id="formula_5">U = [MaxPooling(p), AvgPooling(p)]<label>(12)</label></formula><formula xml:id="formula_6">V = [MaxPooling(q), AvgPooling(q)]<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification</head><p>Here, we feed the concatenation of U and V ([U, V ]) into a multilayer perceptron (MLP) classifier that includes a hidden layer with tanh activation and softmax output layer. The model is trained in an end-to-end manner.</p><formula xml:id="formula_7">Output = MLP([U, V ])<label>(14)</label></formula><p>4 Experiments and Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The Stanford Natural Language Inference (SNLI) dataset contains 570K human annotated sentence pairs. The premises are drawn from the Flickr30k <ref type="bibr" target="#b3">(Plummer et al., 2015)</ref> corpus, and then the hypotheses are manually composed for each relationship class (entailment, neutral, contradiction, and -). The "-" class indicates that there is no consensus decision among the annotators, consequently, we remove them during the training and evaluation following the literature. We use the same data split as provided in <ref type="bibr">Bowman et al. (2015)</ref> to report comparable results with other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We use pre-trained 300-D Glove 840B vectors <ref type="bibr" target="#b2">(Pennington et al., 2014)</ref> to initialize our word embedding vectors. All hidden states of BiLSTMs during input encoding and inference have 450 dimensions (r = 300 and d = 450). The weights are learned by minimizing the log-loss on the training data via the Adam optimizer (Kingma and Ba, 2014). The initial learning rate is 0.0004. To avoid overfitting, we use dropout <ref type="bibr" target="#b6">(Srivastava et al., 2014)</ref> with the rate of 0.4 for regularization, which is applied to all feedforward connections. During training, the word embeddings are updated to learn effective representations for the NLI task. We use a fairly small batch size of 32 to provide more exploration power to the model. Our observation indicates that using larger batch sizes hurts the performance of our model. For n number of models, the best performance on the development set is used as the criteria to determine the final ensemble. The best performance on development set (89.22%) is observed using 6 models and is henceforth considered as our final DR-BiLSTM (Ensemble) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ensemble Strategy</head><p>Ensemble methods use multiple models to obtain better predictive performance. Previous works typically utilize trivial ensemble strategies by either using majority votes or averaging the probability distributions over the same model with different initialization seeds <ref type="bibr" target="#b9">(Wang et al., 2017;</ref><ref type="bibr">Gong et al., 2017)</ref>. By contrast, we use weighted averaging of the probability distributions where the weight of each model is learned through its performance on the SNLI development set. Furthermore, the differences between our models in the ensemble originate from: 1) variations in the number of dependent readings (i.e. 1 and 3 rounds of dependent reading), 2) projection layer activation (tanh and ReLU in Equations 6 and 7), and 3) different initialization seeds.</p><p>The main intuition behind this design is that the effectiveness of a model may depend on the complexity of a premise-hypothesis instance. For a simple instance, a simple model could perform better than a complex one, while a complex instance may need further consideration toward disambiguation. Consequently, using models with different rounds of dependent readings in the encoding stage should be beneficial. <ref type="figure">Figure 2</ref> demonstrates the observed performance of our ensemble method with different number of models. The performance of the models are reported based on the best obtained accuracy on the development set. We also study the effectiveness of other ensemble strategies e.g. majority voting, and averaging the probability distributions. But, our ensemble strategy performs the best among them (see Section A in the Appendix for additional details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Preprocessing</head><p>We perform a trivial preprocessing step on SNLI to recover some out-of-vocabulary words found in the development set and test set. Note that our vocabulary contains all words that are seen in the training set, so there is no out-of-vocabulary word in it. The SNLI dataset is not immune to human errors, specifically, misspelled words. We noticed that misspelling is the main reason for some of the observed out-of-vocabulary words. Consequently, we simply fix the unseen misspelled words using Microsoft spell-checker (other approaches like edit distance can also be used). Moreover, while dealing with an unseen word during evaluation, we try to: 1) replace it with its lower case, or 2) split the word when it contains a "-" (e.g. "marsh-like") or starts with "un" (e.g. "unloading"). If we still could not find the word in our vocabulary, we consider it as an unknown word. In the next subsection, we demonstrate the importance and impact of such trivial preprocessing (see Section B in the Appendix for additional details). <ref type="table">Table 2</ref> shows the accuracy of the models on training and test sets of SNLI. The first row represents a baseline classifier presented by Bowman et al. <ref type="formula" target="#formula_0">(2015)</ref> that utilizes handcrafted features. All other listed models are deep-learning based. The gap between the traditional model and deep learning models demonstrates the effectiveness of deep learning methods for this task. We also report the estimated human performance on the SNLI dataset, which is the average accuracy of five annotators in comparison to the gold labels <ref type="bibr">(Gong et al., 2017)</ref>. It is noteworthy that recent deep learning models surpass the human performance in the NLI task. <ref type="table">Table 2</ref>, previous deep learning models (rows 2-19) can be divided into three categories: 1) sentence encoding based models (rows 2-7), 2) single inter-sentence attention-based models (rows 8-16), and 3) ensemble inter-sentence attention-based models <ref type="bibr">(rows 17-19)</ref>. We can see that inter-sentence attention-based models perform better than sentence encoding based models, which supports our intuition. Natural language inference requires a deep interaction between the premise and hypothesis. Inter-sentence attention-based approaches can provide such interaction while sentence encoding based models fail to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>To further enhance the modeling of interaction between the premise and hypothesis for efficient disambiguation of their relationship, we introduce the dependent reading strategy in our proposed DR-BiLSTM model. The results demonstrate the effectiveness of our model. DR-BiLSTM (Single) achieves 88.5% accuracy on the test set which is noticeably the best reported result among the existing single models for this task. Note that the difference between DR-BiLSTM and Chen et al. <ref type="formula" target="#formula_0">(2017)</ref>   <ref type="bibr">et al., 2015)</ref> 83.9% 80.6% <ref type="bibr" target="#b7">(Vendrov et al., 2015)</ref> 98.8% 81.4% <ref type="bibr">(Mou et al., 2016)</ref> 83.3% 82.1% <ref type="bibr">(Bowman et al., 2016)</ref> 89.2% 83.2% <ref type="bibr">(Liu et al., 2016b)</ref> 84.5% 84.2% <ref type="bibr" target="#b11">(Yu and Munkhdalai, 2017a)</ref> 86.2% 84.6% <ref type="bibr" target="#b4">(Rockt?schel et al., 2015)</ref> 85.3% 83.5% <ref type="bibr" target="#b8">(Wang and Jiang, 2016)</ref> 92.0% 86.1% <ref type="bibr">(Liu et al., 2016a)</ref> 88.5% 86.3% <ref type="bibr" target="#b1">(Parikh et al., 2016)</ref> 90.5% 86.8% <ref type="bibr" target="#b12">(Yu and Munkhdalai, 2017b)</ref> 88.5% 87.3% <ref type="bibr" target="#b5">(Sha et al., 2016)</ref> 90.7% 87.5% <ref type="bibr" target="#b9">(Wang et al., 2017)</ref>   <ref type="table">Table 2</ref>: Accuracies of the models on the training set and test set of SNLI. DR-BiLSTM (Ensemble) achieves the accuracy of 89.3%, the best result observed on SNLI, while DR-BiLSTM (Single) obtains the accuracy of 88.5%, which considerably outperforms the previous non-ensemble models. Also, utilizing a trivial preprocessing step yields to further improvements of 0.4% and 0.3% for single and ensemble DR-BiLSTM models respectively. &lt; 0.001 over the Chi-square test 1 .</p><p>To further improve the performance of NLI systems, researchers have built ensemble models. Previously, ensemble systems obtained the best performance on SNLI with a huge margin. <ref type="table">Table 2</ref> shows that our proposed single model achieves competitive results compared to these reported ensemble models. Our ensemble model considerably outperforms the current state-of-the-art by obtaining 89.3% accuracy.</p><p>Up until this point, we discussed the performance of our models where we have not considered preprocessing for recovering the out-ofvocabulary words. In <ref type="table">Table 2</ref>, "DR-BiLSTM (Single) + Process", and "DR-BiLSTM (Ensem.) + Process" represent the performance of our models on the preprocessed dataset. We can see that our preprocessing mechanism leads to further improvements of 0.4% and 0.3% on the SNLI test set for our single and ensemble models respectively. In fact, our single model ("DR-BiLSTM (Single) + Process") obtains the state-of-the-art performance over both reported single and ensemble models by performing a simple preprocessing step. Furthermore, "DR-BiLSTM (Ensem.) + Process" outperforms the existing state-of-the-art remarkably (0.7% improvement). For more comparison and analyses, we use "DR-BiLSTM (Single)" and "DR-BiLSTM (Ensemble)" as our single and ensemble models in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation and Configuration Study</head><p>We conducted an ablation study on our model to examine the importance and effect of each major component. Then, we study the impact of BiL-STM dimensionality on the performance of the development set and training set of SNLI. We investigate all settings on the development set of the SNLI dataset.   <ref type="table" target="#tab_5">Table 3</ref> shows the ablation study results on the development set of SNLI along with the statistical significance test results in comparison to the proposed model, DR-BiLSTM. We can see that all modifications lead to a new model and their differences are statistically significant with a p-value of &lt; 0.001 over Chi square test. <ref type="table" target="#tab_5">Table 3</ref> shows that removing any part from our model hurts the development set accuracy which indicates the effectiveness of these components.</p><p>Among all components, three of them have noticeable influences: max pooling, difference in the attention stage, and dependent reading.</p><p>Most importantly, the last four study cases in <ref type="table" target="#tab_5">Table 3</ref> (rows 8-11) verify the main intuitions behind our proposed model. They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement, specifically in the encoding stage. We are convinced that the importance of dependent reading in the encoding stage originates from its ability to focus on more important and relevant aspects of the sentences due to its prior knowledge of the other sentence during the encoding procedure.  <ref type="figure" target="#fig_2">Figure 3</ref> shows the behavior of the proposed model accuracy on the training set and development set of SNLI. Since the models are selected based on the best observed development set accuracy during the training procedure, the training accuracy curve (red, top) is not strictly increasing. <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates that we achieve the best performance with 450-dimensional BiLSTMs. In other words, using BiLSTMs with lower dimensionality causes the model to suffer from the lack of space for capturing proper information and dependencies. On the other hand, using higher dimensionality leads to overfitting which hurts the performance on the development set. Hence, we use 450-dimensional BiLSTM in our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis</head><p>We first investigate the performance of our models categorically. Then, we show a visualization of the energy function in the attention stage <ref type="table" target="#tab_5">(Equation 3)</ref> for an instance from the SNLI test set.</p><p>To qualitatively evaluate the performance of our models, we design a set of annotation tags that can be extracted automatically. This design is inspired by the reported annotation tags in <ref type="bibr">Williams et al. (2017)</ref>. The specifications of our annotation tags are as follows:</p><p>? High Overlap: premise and hypothesis sentences share more than 70% tokens.</p><p>? Regular Overlap: sentences share between 30% and 70% tokens.</p><p>? Low Overlap: sentences share less than 30% tokens.</p><p>? Long Sentence: either sentence is longer than 20 tokens.</p><p>? Regular Sentence: premise or hypothesis length is between 5 and 20 tokens.</p><p>? Short Sentence: either sentence is shorter than 5 tokens.</p><p>? Negation: negation is present in a sentence.</p><p>? Quantifier: either of the sentences contains one of the following quantifiers: much, enough, more, most, less, least, no, none, some, any, many, few, several, almost, nearly.</p><p>? Belief: either of the sentences contains one of the following belief verbs: know, believe, understand, doubt, think, suppose, recognize, forget, remember, imagine, mean, agree, disagree, deny, promise.  <ref type="table" target="#tab_6">Table 4</ref> can be divided into four major categories: 1) gold label data, 2) word overlap, 3) sentence length, and 4) occurrence of special words. We can see that DR-BiLSTM (Ensemble) performs the best in all categories which matches our expectation. Moreover, DR-BiLSTM (Single) performs noticeably better than ESIM in most of the categories except "Entailment", "High Overlap", and "Long Sentence", for which our model is not far behind (gaps of 0.2%, 0.5%, and 0.9%, respectively). It is noteworthy that DR-BiLSTM   (Single) performs better than ESIM in more frequent categories. Specifically, the performance of our model in "Neutral", "Negation", and "Quantifier" categories (improvements of 1.4%, 3.5%, and 1.9%, respectively) indicates the superiority of our model in understanding and disambiguating complex samples. Our investigations indicate that ESIM generates somewhat uniform attention for most of the word pairs while our model could effectively attend to specific parts of the given sentences and provide more meaningful attention. In other words, the dependent reading strategy enables our model to achieve meaningful representations, which leads to better attention to obtain further gains on such categories like Negation and Quantifier sentences (see Section C in the Appendix for additional details).</p><p>Finally, we show a visualization of the normalized attention weights (energy function, Equation 3) of our model in <ref type="figure" target="#fig_3">Figure 4</ref>. We show a sentence pair, where the premise is "Male in a blue jacket decides to lay the grass.", and the hypothesis is "The guy in yellow is rolling on the grass.", and its logical relationship is contradiction. <ref type="figure" target="#fig_3">Figure 4</ref> indicates the model's ability in attending to critical pairs of words like &lt;Male, guy&gt;, &lt;decides, rolling&gt;, and &lt;lay, rolling&gt;. Finally, high attention between {decides, lay} and {rolling}, and {Male} and {guy} leads the model to correctly classify the sentence pair as contradiction (for more samples with attention visualizations, see Section D of the Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel natural language inference model (DR-BiLSTM) that benefits from a dependent reading strategy and achieves the state-of-theart results on the SNLI dataset. We also introduce a sophisticated ensemble strategy and illustrate its effectiveness through experimentation. Moreover, we demonstrate the importance of a simple preprocessing step on the performance of our proposed models. Evaluation results show that the preprocessing step allows our DR-BiLSTM (single) model to outperform all previous single and ensemble methods. Similar superior performance is also observed for our DR-BiLSTM (ensemble) model. We show that our ensemble model outperforms the existing state-of-the-art by a considerable margin of 0.7%. Finally, we perform an extensive analysis to demonstrate the strength and weakness of the proposed model, which would pave the way for further improvements in this domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ensemble Strategy Study</head><p>We use the following configurations in our ensemble model study:</p><p>? DR-BiLSTM (with different initialization seeds): here, we consider 6 DR-BiLSTMs with different initialization seeds.</p><p>? tanh-Projection: same configuration as DR-BiLSTM, but we use tanh instead of ReLU as the activation function in Equations 6 and 7 in the paper:</p><formula xml:id="formula_8">p i = tanh(W p a i + b p ) (15) q j = tanh(W p b j + b p )<label>(16)</label></formula><p>? DR-BiLSTM (with 1 round of dependent reading): same configuration as DR-BiLSTM, but we do not use dependent reading during the inference process. In other words, we usep =p andq =q instead of Equations 10 and 11 in the paper respectively.</p><p>? DR-BiLSTM (with 3 rounds of dependent reading): same configuration as the above, but we use 3 rounds of dependent reading. Formally, we replace Equations 1 and 2 in the paper with the following equations respectively:</p><formula xml:id="formula_9">?, s v = BiLSTM(v, 0) ?, s vu = BiLSTM(u, s v ) ?, s vuv = BiLSTM(v, s vu ) u, ? = BiLSTM(u, s vuv ) (17) ?, s u = BiLSTM(u, 0) ?, s uv = BiLSTM(v, s u ) ?, s uvu = BiLSTM(u, s uv ) v, ? = BiLSTM(v, s uvu )<label>(18)</label></formula><p>Our final ensemble model, DR-BiLSTM (Ensemble) is the combination of the following 6 models: tanh-Projection, DR-BiLSTM (with 1 round of dependent reading), DR-BiLSTM (with 3 rounds of dependent reading), and 3 DR-BiLSTMs with different initialization seeds.</p><p>We also experiment with majority voting and averaging the probability distribution strategies for ensemble models using the same set of models as our weighted averaging ensemble method (as described above). <ref type="figure">Figure 5</ref> shows the behavior of the majority voting strategy with different number of models. Interestingly, the best development accuracy is also observed using 6 individual models including tanh-Projection, DR-BiLSTM (with 1 round of dependent reading), DR-BiLSTM (with 3 rounds of dependent reading), and 3 DR-BiLSTMs with varying initialization seeds that are different from our DR-BiLSTM (Ensemble) model.</p><p>We should note that our weighted averaging ensemble strategy performs better than the majority voting method in both development set and test set of SNLI, which indicates the effectiveness of our approach. Furthermore, our method could show more consistent behavior for training and test sets when we increased the number of models (see <ref type="figure">Figure</ref>  our observations, averaging the probability distributions fails to improve the development set accuracy using two and three models, so we did not study it further. <ref type="table" target="#tab_11">Table 5</ref> shows some erroneous sentences from the SNLI test set along with their corrected equivalents (after preprocessing). Furthermore, we show the energy function (Equation 3 in the paper) visualizations of 6 examples from the aforementioned data samples in <ref type="figure" target="#fig_5">Figures 6, 7, 8, 9</ref>, 10, and 11. Each figure presents the visualization of an original erroneous sample along its corrected version. These figures clearly illustrate that fixing the erroneous words leads to producing correct attentions over the sentences. This can be observed by comparing the attention for the erroneous words and corrected words, e.g. "daschunds" and "dachshunds" in the premise of <ref type="figure" target="#fig_5">Figures 6 and 7</ref>. Note that we add two dummy notations (i.e. FOL , and EOL ) to all sentences which indicate their beginning and end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Preprocessing Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Category Study</head><p>Here we investigate the normalized attention weights of DR-BiLSTM and ESIM for four samples that belong to Negation and/or Quantifier categories <ref type="figure" target="#fig_1">(Figures 12 -15</ref>). Each figure illustrates the normalized energy function of DR-BiLSTM (left diagram) and ESIM (right diagram) respec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Sentence Corrected Sentence Froends ride in an open top vehicle together.</head><p>Friends ride in an open top vehicle together. A middle easten store.</p><p>A middle eastern store. A woman is looking at a phtographer A woman is looking at a photographer The mother and daughter are fighitn.</p><p>The mother and daughter are fighting. Two kiled men hold bagpipes Two killed men hold bagpipes A woman escapes a from a hostile enviroment A woman escapes a from a hostile environment Two daschunds play with a red ball Two dachshunds play with a red ball A black dog is running through a marsh-like area.</p><p>A black dog is running through a marsh like area. a singer wearing a jacker performs on stage a singer wearing a jacket performs on stage There is a sculture There is a sculpture Taking a neverending break Taking a never ending break The woman has sounds emanting from her mouth.</p><p>The woman has sounds emanating from her mouth. the lady is shpping the lady is shopping A Bugatti and a Lambourgini compete in a road race. A Bugatti and a Lamborghini compete in a road race.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Attention Study</head><p>In this section, we show visualizations of 18 samples of normalized attention weights (energy function, see Equation 3 in the paper). Each column in <ref type="figure" target="#fig_1">Figures 16, 17, and 18</ref>, represents three data samples that share the same premise but differ in hypothesis. Also, each row is allocated to a specific logical relationship (Top: Entailment, Middle: Neutral, and Bottom: Contradiction). DR-BiLSTM classifies all data samples reported in these figures correctly.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1 demonstrates a high-level view of our proposed NLI framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A high-level view of DR-BiLSTM. The data (premise u and hypothesis v, depicted with cyan and red tensors respectively) flows from bottom to top. Relevant tensors are shown with the same color and elements with the same colors share parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Impact of BiLSTM dimensionality in the proposed model on the training set (red, top) and development set (blue, bottom) accuracies of the SNLI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Normalized attention weights for a sample from the SNLI test set. Darker color illustrates higher attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>sample (dachshunds in premise).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the energy function for one erroneous sample (a) and the fixed sample (b). The gold label is Entailment. Our model returns Contradiction for the erroneous sample, but correctly classifies the fixed sample. tively. Provided figures indicate that ESIM assigns somewhat similar attention to most of the pairs while DR-BiLSTM focuses on specific parts of the given premise and hypothesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>sample (dachshunds in premise).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the energy function for one erroneous sample (a) and the fixed sample (b). The gold label is Neutral. Our model returns Contradiction for the erroneous sample, but correctly classifies the fixed sample. sample (Friends in hypothesis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of the energy function for one erroneous sample (a) and the fixed sample (b). The gold label is Neutral. Our model returns Entailment for the erroneous sample, but correctly classifies the fixed sample. Erroneous sample (easten in hypothesis). Fixed sample (eastern in hypothesis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of the energy function for one erroneous sample (a) and the fixed sample (b). The gold label is Entailment. Our model returns Contradiction for the erroneous sample, but correctly classifies the fixed sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>sample (jacket in hypothesis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of the energy function for one erroneous sample (a) and the fixed sample (b). The gold label is Entailment. Our model returns Neutral for the erroneous sample, but correctly classifies the fixed sample. Fixed sample (sculpture in hypothesis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :Figure 13 :Figure 14 :</head><label>111314</label><figDesc>Visualization of the energy function for one erroneous sample (a) and the fixed sample (b). The gold label is Entailment. Our model returns Neutral for the erroneous sample, but correctly classifies the fixed sample. Normalized attention of ESIM Figure 12: Visualization of the normalized attention weights of DR-BiLSTM (a) and ESIM (b) models for one sample from the SNLI test set. This sample belongs to the Negation category. The gold label is Contradiction. Our model returns Contradiction while ESIM returns Entailment. Visualization of the normalized attention weights of DR-BiLSTM (a) and ESIM (b) models for one sample from the SNLI test set. This sample belongs to the Negation category. The gold label is Contradiction. Our model returns Contradiction while ESIM returns Entailment. Visualization of the normalized attention weights of DR-BiLSTM (a) and ESIM (b) models for one sample from the SNLI test set. This sample belongs to both Negation and Quantifier categories. The gold label is Neutral. Our model returns Neutral while ESIM returns Contradiction. Normalized attention of ESIM Figure 15: Visualization of the normalized attention weights of DR-BiLSTM (a) and ESIM (b) models for one sample from the SNLI test set. This sample belongs to both Negation and Quantifier categories. The gold label is Entailment. Our model returns Entailment while ESIM returns Contradiction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Normalized attention weights for 6 data samples from the test set of SNLI dataset. (a,c,e) and (b,d,f) represent the normalized attention weights for Entailment, Neutral, and Contradiction logical relationships of two premises (Instance 1 and 2) respectively. Darker color illustrates higher attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Normalized attention weights for 6 data samples from the test set of SNLI dataset. (a,c,e) and (b,d,f) represent the normalized attention weights for Entailment, Neutral, and Contradiction logical relationships of two premises (Instance 3 and 4) respectively. Darker color illustrates higher attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Normalized attention weights for 6 data samples from the test set of SNLI dataset. (a,c,e) and (b,d,f) represent the normalized attention weights for Entailment, Neutral, and Contradiction logical relationships of two premises (Instance 5 and 6) respectively. Darker color illustrates higher attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples from the SNLI dataset.</figDesc><table><row><cell>man</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>is statistically significant with a p-value of</figDesc><table><row><cell>Model</cell><cell>Accuracy Train Test</cell></row><row><cell>(Bowman et al., 2015) (Feature)</cell><cell>99.7% 78.2%</cell></row><row><cell>(Bowman</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>elem. prd, element-wise product. dep. infer, dependent reading inference.</figDesc><table><row><cell>Model</cell><cell cols="2">Dev Acc a p-value</cell></row><row><cell>DR-BiLSTM</cell><cell>88.69%</cell><cell>-</cell></row><row><cell>DR-BiLSTM -hidden MLP</cell><cell>88.45%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -average pooling</cell><cell>88.50%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -max pooling</cell><cell>88.39%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -elem. prd b</cell><cell>88.51%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -difference</cell><cell>88.24%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -diff c &amp; elem. prd</cell><cell>87.96%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -inference pooling</cell><cell>88.46%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -dep. infer d</cell><cell>88.43%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -dep. enc e</cell><cell>88.26%</cell><cell>&lt;0.001</cell></row><row><cell>DR-BiLSTM -dep. enc &amp; infer</cell><cell>88.20%</cell><cell>&lt;0.001</cell></row></table><note>a Dev Acc, Development Accuracy.bc diff, difference.de dep. enc, dependent reading encoding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results. Performance of different configurations of the proposed model on the development set of SNLI along with their pvalues in comparison to DR-BiLSTM (Single).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>shows the frequency of aforementioned annotation tags in the SNLI test set along with the performance (accuracy) of ESIM (Chen et al., 2017), DR-BiLSTM (Single), and DR-BiLSTM (Ensemble).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Categorical performance analyses (accu-</cell></row><row><cell>racy) of ESIM (Chen et al., 2017), DR-BiLSTM</cell></row><row><cell>(DR(S)) and Ensemble DR-BiLSTM (DR(E)) on</cell></row><row><cell>the SNLI test set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>/doi.org/10.1162/neco.1997.9.8.1735. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Samuel R. Bowman, Jon Gauthier, Abhinav Ras-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">togi, Raghav Gupta, Christopher D. Manning, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Christopher Potts. 2016. A fast unified model for</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">parsing and sentence understanding. In Proceed-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">ings of the 54th Annual Meeting of the Association</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">for Computational Linguistics, ACL 2016, August 7-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">12, 2016, Berlin, Germany, Volume 1: Long Papers.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">http://aclweb.org/anthology/P/P16/P16-1139.pdf.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Jiang, and Diana Inkpen. 2017. Enhanced LSTM for</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">natural language inference. In Proceedings of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">55th Annual Meeting of the Association for Compu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">tational Linguistics, ACL 2017, Vancouver, Canada,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">July 30 -August 4, Volume 1: Long Papers. pages</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">1657-1668. https://doi.org/10.18653/v1/P17-1152.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">ral language inference over interaction space. CoRR</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">abs/1709.04348. http://arxiv.org/abs/1709.04348.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sepp</cell><cell cols="2">Hochreiter</cell><cell>and</cell><cell>J?rgen</cell><cell>Schmidhu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ber. 1997.</cell><cell>Long short-term memory.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Neural</cell><cell>Computation</cell><cell>9(8):1735-1780.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">https:/Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Iyyer, James Bradbury, Ishaan Gulrajani, Victor</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Zhong, Romain Paulus, and Richard Socher.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2016.</cell><cell cols="2">Ask me anything: Dynamic memory</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">networks for natural language processing.</cell><cell>In</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Proceedings of the 33nd International Conference</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">on Machine Learning, ICML 2016, New York</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">City, NY, USA, June 19-24, 2016. pages 1378-1387.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">http://jmlr.org/proceedings/papers/v48/kumar16.html.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">anjing Huang. 2016a.</cell><cell>Deep fusion lstms for</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">text semantic matching. In Proceedings of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">54th Annual Meeting of the Association for Com-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">putational Linguistics, ACL 2016, August 7-12,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">2016, Berlin, Germany, Volume 1: Long Papers.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">http://aclweb.org/anthology/P/P16/P16-1098.pdf.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Wang. 2016b.</cell><cell>Learning natural language</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">inference using bidirectional LSTM model</cell></row><row><cell cols="3">Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by</cell><cell cols="3">and inner-attention. http://arxiv.org/abs/1605.09090. CoRR abs/1605.09090.</cell></row><row><cell cols="3">jointly learning to align and translate. abs/1409.0473. http://arxiv.org/abs/1409.0473. CoRR</cell><cell cols="3">Bill MacCartney and Christopher D. Manning. 2008. Modeling semantic containment and exclusion in</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">natural language inference. In COLING 2008,</cell></row><row><cell cols="3">Samuel R. Bowman, Gabor Angeli, Christopher Potts,</cell><cell cols="3">22nd International Conference on Computational</cell></row><row><cell cols="2">and Christopher D. Manning. 2015.</cell><cell>A large</cell><cell cols="3">Linguistics, Proceedings of the Conference, 18-</cell></row><row><cell cols="3">annotated corpus for learning natural language</cell><cell cols="3">22 August 2008, Manchester, UK. pages 521-528.</cell></row><row><cell>inference.</cell><cell cols="2">In Proceedings of the 2015 Con-</cell><cell></cell><cell></cell></row><row><cell cols="3">ference on Empirical Methods in Natural Lan-</cell><cell></cell><cell></cell></row><row><cell cols="3">guage Processing, EMNLP 2015, Lisbon, Por-</cell><cell></cell><cell></cell></row><row><cell cols="3">tugal, September 17-21, 2015. pages 632-642.</cell><cell></cell><cell></cell></row><row><cell cols="3">http://aclweb.org/anthology/D/D15/D15-1075.pdf.</cell><cell></cell><cell></cell></row></table><note>http://www.aclweb.org/anthology/C08-1066. Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2016. Natural lan- guage inference by tree-based convolution and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>2 in Section 4.3 of the paper). According to</figDesc><table><row><cell></cell><cell>94.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>94.3 94.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Train</cell></row><row><cell></cell><cell>94.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>94.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>89.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>88.7 88.8 88.9 89.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dev</cell></row><row><cell></cell><cell>89.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>88.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell>88.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Number of Models</cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 5: Performance of n ensemble models us-</cell></row><row><cell cols="10">ing majority voting on natural language inference</cell></row><row><cell cols="10">reported for training set (red, top), development</cell></row><row><cell cols="10">set (blue, middle), and test set (green, bottom) of</cell></row><row><cell cols="10">SNLI. The best performance on development set</cell></row><row><cell cols="10">is used as the criteria to determine the final en-</cell></row><row><cell cols="10">semble. The best performance on development set</cell></row><row><cell cols="6">is observed using 6 models.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Examples of original sentences that contain erroneous words (misspelled) in the test set of SNLI along with their corrected counterparts. Erroneous words are shown in bold and italic.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Chi-square test (? 2 test) is used to determine if there is a significant difference between two categorical variables (i.e. models' outputs).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://aclweb.org/anthology/P/P16/P16-2022.pdf" />
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.303</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.303" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Kocisk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>CoRR abs/1509.06664</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading and thinking: Reread LSTM unit for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2870" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2670313" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1511.06361</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N/N16/N16-1170.pdf" />
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/579</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/579" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
	<note type="report_type">Melbourne, Australia</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bowman. 2017. coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno>abs/1704.05426</idno>
		<ptr target="http://arxiv.org/abs/1704.05426" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<ptr target="http://aclanthology.info/papers/E17-1002/neural-tree-indexers-for-text-understanding" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textual entailment with structured attentions and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C/C16/C16-1212.pdf" />
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2248" to="2258" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
