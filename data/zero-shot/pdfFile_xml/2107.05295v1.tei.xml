<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DACY: A UNIFIED FRAMEWORK FOR DANISH NLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Enevoldsen</surname></persName>
							<email>kenneth.enevoldsen@cas.au.dk</email>
							<affiliation key="aff0">
								<orgName type="institution">Interacting Minds Centre &amp; Center for Humanities Computing Aarhus Aarhus University Jens Chr</orgName>
								<address>
									<addrLine>Skous Vej 4, Building 1483, 3rd floor Denmark</addrLine>
									<postCode>8000</postCode>
									<settlement>Aarhus</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Hansen</surname></persName>
							<email>lasse.hansen@clin.au.dk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Clinical Medicine &amp; Center for Humanities Computing Aarhus</orgName>
								<orgName type="institution">Aarhus University</orgName>
								<address>
									<addrLine>Jens Chr. Skous Vej 4, Building 1483, 3rd floor Denmark</addrLine>
									<postCode>8000</postCode>
									<settlement>Aarhus</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristoffer</forename><forename type="middle">L</forename><surname>Nielbo</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Interacting Minds Centre &amp; Center for Humanities Computing Aarhus Aarhus University Jens Chr</orgName>
								<address>
									<addrLine>Skous Vej 4, Building 1483, 3rd floor Denmark</addrLine>
									<postCode>8000</postCode>
									<settlement>Aarhus</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DACY: A UNIFIED FRAMEWORK FOR DANISH NLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Natural Language Processing ? Low-resource NLP ? Data Augmentation ? Danish NLP</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Danish natural language processing (NLP) has in recent years obtained considerable improvements with the addition of multiple new datasets and models. However, at present, there is no coherent framework for applying state-of-the-art models for Danish. We present DaCy: a unified framework for Danish NLP built on SpaCy. DaCy uses efficient multitask models which obtain state-of-the-art performance on named entity recognition, part-of-speech tagging, and dependency parsing. DaCy contains tools for easy integration of existing models such as for polarity, emotion, or subjectivity detection. In addition, we conduct a series of tests for biases and robustness of Danish NLP pipelines through augmentation of the test set of DaNE. DaCy large compares favorably and is especially robust to long input lengths and spelling variations and errors. All models except DaCy large display significant biases related to ethnicity while only Polyglot shows a significant gender bias. We argue that for languages with limited benchmark sets, data augmentation can be particularly useful for obtaining more realistic and fine-grained performance estimates. We provide a series of augmenters as a first step towards a more thorough evaluation of language models for low and medium resource languages and encourage further development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Danish Natural Language Processing (NLP) has seen a recent rise in resources with the introduction of the Danish Gigaword Corpus <ref type="bibr" target="#b10">(Derczynski et al., 2021)</ref>, curated lists of Natural Language Processing (NLP) tools by DaNLP <ref type="bibr" target="#b4">(Brogaard Pauli et al., 2021)</ref> and sprogteknologi.dk, and at least five pretrained neural language models <ref type="bibr" target="#b14">(H?jmark-Bertelsen, 2021;</ref><ref type="bibr" target="#b21">M?llerh?j, 2019;</ref><ref type="bibr" target="#b29">Tamini-Sarnikowski, 2020)</ref>. Datasets and models are available for most common tasks such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, dependency parsing, sentiment analysis, and coreference resolution <ref type="bibr" target="#b4">(Brogaard Pauli et al., 2021;</ref><ref type="bibr" target="#b28">"Sprogteknologi.dk", 2021)</ref>. However, no coherent, efficient and state-of-the-art framework exists for all fundamental NLP tasks. Models are developed and distributed as disjoint projects and often require diverging package versions and have idiosyncratic APIs. These factors complicate workflows and hamper further developments. arXiv:2107.05295v1 [cs.CL] 12 Jul 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">DaCy</head><p>With this motivation we present DaCy: an efficient end-to-end framework for Danish NLP with state-of-the-art performance on POS, NER and dependency parsing. DaCy fills the gap in Danish NLP by providing a consistent interface that is easily extendable and able to integrate other models. DaCy is built on SpaCy v.3 which comes with a range of advantages: the framework is optimized, user-friendly, and well-documented. DaCy includes three fine-tuned language models: DaCy small, based on a Danish Electra (14M parameters) (H?jmark-Bertelsen, 2021); DaCy medium, based on the Danish BERT (110M parameters) <ref type="bibr" target="#b21">(M?llerh?j, 2019)</ref>; and DaCy large, based on the multilingual XLM-Roberta (550M parameters) <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref>. All models have been fine-tuned to do POS tagging, NER, and dependency parsing in a single forward pass, which increases the efficiency of the model and allows for larger models at the same computational cost.</p><p>Besides models fine-tuned for DaCy, the package includes convenient wrappers to add other models to the pipeline. For instance, Danish models for detecting polarity, emotion, and subjectivity classification can be added in a single line of code, and any HuggingFace Transformers <ref type="bibr" target="#b31">(Wolf et al., 2020)</ref> model trained for sentence classification can be conveniently wrapped and included in the pipeline using utility functions. With this functionality, DaCy aims at being a unified framework for Danish NLP. All functionality is well-documented and covered by tutorials. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Robustness &amp; Evaluation</head><p>Fine-tuned language models are commonly evaluated by testing performance on a gold-standard benchmark dataset. The most commonly used benchmark for Danish is the DaNE dataset <ref type="bibr" target="#b16">(Hvingelby et al., 2020)</ref>, which consists of the Danish Dependency Treebank <ref type="bibr" target="#b18">(Johannsen et al., 2015)</ref>, additionally tagged for NER. For languages with few benchmarks datasets, such as Danish, the performance stability and generalizability can not be reliably estimated <ref type="bibr" target="#b25">(Ribeiro et al., 2020)</ref>. For instance, the text included in DaNE was collected in the years 1983-1992 from both written and spoken domains <ref type="bibr" target="#b16">(Hvingelby et al., 2020)</ref>. Given the change of languages over time and the addition of new textual domains such as social media, this dataset is unlikely to be representative of the contemporary domains of application. For instance, models might not be sufficiently exposed to e.g. abbreviated names, spelling errors, or non-standard casing to correctly and robustly classify them. In this sense, the performance obtained on DaNE is unlikely to hold for real-world use cases.</p><p>To provide an additional layer of validation, we propose evaluating models on augmented gold-standard data. Data augmentation entails generating new data by slightly modifying existing data points . Data augmentation techniques such as rotation and cropping are widely used in computer vision to reduce overfitting <ref type="bibr" target="#b27">(Shorten &amp; Khoshgoftaar, 2019)</ref>, and are becoming increasingly common in NLP . The complex syntactic and semantic structure of text complicates the task of finding useful augmentations, but simple manipulations such as synonym replacement and random character swaps and deletions have been found to be particularly useful for supervised learning in low-resource settings <ref type="bibr" target="#b30">(Wei &amp; Zou, 2019)</ref>.</p><p>Although data augmentation is most commonly used for increasing the amount of training data, it can just as well be used for evaluation purposes <ref type="bibr" target="#b25">(Ribeiro et al., 2020)</ref>. By augmenting a gold-standard dataset, we can evaluate model performance when exposed to data that more closely mimics real-life settings by adding spelling errors, more diverse names, or other manipulations. In section 2.2, we introduce a series of augmentations and evaluate the performance of Danish NLP pipelines on them.</p><p>The contributions of this paper are three-fold. 1) We introduce new state-of-art models for Danish dependency parsing, NER and POS. 2) We introduce the DaCy Python library as a unified framework for state-of-the-art NLP in Danish. 3) We evaluate Danish NLP pipelines using data augmentation and provide directions for future model development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training</head><p>To train the candidate models for DaCy, all publicly available language models for Danish were fine-tuned on the DaNE corpus <ref type="bibr" target="#b16">(Hvingelby et al., 2020)</ref> using SpaCy 3.0.3 <ref type="bibr" target="#b15">(Honnibal et al., 2020)</ref>. The models include 2 Danish ELECTRAs <ref type="bibr" target="#b8">(Clark et al., 2020;</ref><ref type="bibr" target="#b14">H?jmark-Bertelsen, 2021;</ref><ref type="bibr" target="#b29">Tamini-Sarnikowski, 2020)</ref>, the Danish ConvBERT <ref type="bibr" target="#b17">(Jiang et al., 2021;</ref><ref type="bibr" target="#b29">Tamini-Sarnikowski, 2020)</ref>, the Danish BERT <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b21">M?llerh?j, 2019)</ref>, and the multilingual XLM-Roberta Large <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref>. All models were trained with an input length of 10 sentences until convergence using similar hyperparameters on a Quadro RTX 8000 GPU. Adam was used as optimizer with hyperparameters ? 1 = 0.9 and ? 2 = 0.999. Further, L2 normalization with ? = 0.01 and gradient clipping with c = 1.0 was employed. For increased efficiency, all models were trained with a multi-task objective <ref type="bibr" target="#b6">(Caruana, 1997;</ref><ref type="bibr" target="#b26">Ruder, 2017)</ref> on NER, POS, and dependency parsing. This allows the training of larger models at the same computational cost, but it is unlikely that multi-task training at this scale improves performance <ref type="bibr" target="#b0">(Aghajanyan et al., 2021;</ref><ref type="bibr" target="#b24">Raffel et al., 2020</ref>). 2 <ref type="table" target="#tab_0">Table 1</ref> shows the performance of all fine-tuned models evaluated on DaNE's test set. The three best performing models in each size category, XLM-Roberta, DaBERT, and AElaectra Cased are included in DaCy as the large, medium and small, respectively. In line with previous findings <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr" target="#b23">Radford et al., 2019;</ref><ref type="bibr" target="#b24">Raffel et al., 2020)</ref>, larger models tend to perform better with XLM-Roberta obtaining the best performance across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>To evaluate the robustness of DaCy and other Danish NLP pipelines, we assessed their performance on multiple augmented version of the DaNE test set. All Danish models are trained on the DaNE corpus which consists of a mix of textual data of both spoken and written origin from the years 1983-1992 <ref type="bibr" target="#b16">(Hvingelby et al., 2020)</ref>, with the exception of Polyglot which is trained on entities extracted from Wikipedia (Al-Rfou <ref type="bibr">' et al., 2013)</ref>. As a consequence, the training data is rarely representative of the domain in which the models will be applied. For example, social media, contemporary news media, and historical texts have domain specific characteristics such as non-standard casing, a higher degree of typos, use of hashtags, and historic spelling such as upper-cased nouns <ref type="bibr" target="#b3">(Baldwin, 2012;</ref><ref type="bibr" target="#b12">Farzindar &amp; Inkpen, 2015;</ref><ref type="bibr">Tahmasebi, 2018)</ref>. While it is infeasible to test the models on all possible domains, some of these characteristics can be modelled using data augmentation which can provide practitioners with an estimate of the potential shortcomings of the model. Further, data augmentation can be used to estimate biases against protected groups such as gender and ethnicity.</p><p>The augmenters presented here are not meant to be exhaustive, but rather a first step towards more thorough validation of new language models. We argue that the bar for inclusion of a new model should be set higher than a slight increase in benchmark performance. Language models are used in a variety of contexts which current benchmarks tasks, especially for low resource languages, do not capture. Our aim with these experiments is to provide an extra layer of insight into the performance of language models that more closely mimics naturalistic use cases, and encourage the development of further augmenters. Augmentation not only provides insights into when model performance breaks down, whether certain models are more suited for specific use-cases than others, but can also be used for identifying specific areas to improve upon.</p><p>The augmenters developed for this paper are designed in accordance with the SpaCy framework, and are thus not necessarily tied to DaCy or Danish in particular and can be used both during model validation and training. Comprehensive tutorials are provided on the DaCy Github repository.</p><p>We tested small, medium, and large SpaCy <ref type="bibr" target="#b15">(Honnibal et al., 2020)</ref>    <ref type="bibr" target="#b20">(Meldgaard, 2005)</ref>, respecting first and last names. Previous evaluations of Danish NLP tools have used the gold-standard tokens instead of using a tokenization module. While this allows for easier comparison of the specific modules it inflates the performance metrics of the models and is unlikely to reflect the metric of interest, namely, the performance during application. 3 All models were tested using both their own tokenizer (if they have one) and the SpaCy tokenizer for Danish. The performance reported in section 3 uses the best peforming tokenization module for each pipeline. For all models except Stanza and Polyglot this was found to be the SpaCy tokenizer. <ref type="table" target="#tab_2">Table 2</ref> shows the overall performance of Danish NLP frameworks on POS, NER, and dependency parsing on the un-augmented DaNE test set. DaCy large obtains a new state-of-the-art on all tasks, most notably on NER and dependency parsing. Regardless of model, performance for POS is stable around 98% accuracy. POS tagging has long been at this level, and obtaining greater accuracy has been argued to require updates to the training data rather than new architectures <ref type="bibr" target="#b19">(Manning, 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Tables 3 to 5 shows a detailed performance breakdown of the models on NER, POS, and dependency parsing on the augmented data described in section 2.2. Overall, spelling variations and abbreviated first names consistently reduce performance of all models on all tasks. Even simple replacements of ae, ?, and ? lead to performance degradation. In general, larger models handle augmentations better than small models with DaCy large performing the best on all augmentations with the exception of lower-casing. DaCy medium, DaNLP's BERT, and NERDA are based on the uncased Danish BERT <ref type="bibr" target="#b21">(M?llerh?j, 2019)</ref>, and are consequently not affected by casing. The BiLSTM-based models (Stanza and Flair) perform competitively under augmentations and are only consistently outperformed by DaCy large.</p><p>On NER specifically, all models with the exception of DaCy large obtain significantly worse performance on Muslim names as compared to Danish names. The robustness of DaCy large likely stems from the multilingual pre-training    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>This paper has introduced the DaCy models and presented a thorough evaluation of Danish NLP models on a battery of augmentations. DaCy models achieve state-of-the-art performance on Danish NER, POS, and dependency parsing, and are robust to augmentations such as keystroke errors, name changes, and lowercasing. The results from training DaCy underline three well-known trends in deep learning and NLP, 1) larger models tend to perform better, 2) higher quality pre-training data leads to better models, as illustrated by the superior performance of AElaectra compared to DaELECTRA, and 3) multilingual models perform competitively with monolingual models <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr" target="#b24">Raffel et al., 2020;</ref><ref type="bibr" target="#b32">Xue et al., 2021)</ref>.</p><p>Our experiments with multiple augmenters revealed different patterns of strengths and weaknesses across Danish NLP models. In general, larger models tend to be more robust to data augmentations. Several models are highly sensitive to casing, which limits their usefulness on certain domains. Evaluating models on augmented data provides a more holistic and realistic estimate of the expected performance, and can reveal in which use cases one model might be more useful than another. For example, it might be better to use DaCy medium on social media as opposed to DaCy large as its performance is not affected by casing.</p><p>The purpose of the data augmentation experiments was to evaluate the robustness of Danish models and to open a discussion on how to present new models going forward. As more models are developed for low and medium resource languages, properly evaluating them becomes vital for securing robustness, transparency, and effectiveness despite limited benchmark sets. We do not posit data augmentation as the only solution, but demonstrate that it can effectively reveal performance differences on important factors such as casing, spelling errors, and biases related to protected groups. As researchers, we bear the responsibility for releasing adequately tested and robust models into the world.</p><p>With the increasing ease of deployment, users must be made aware of the level of performance they can realistically expect to achieve on their problem, and when to choose one model over another. Social media researchers should know that certain models are sensitive to casing, historians should know that some models handle old text variations such as ae, oe, aa poorly, and lawyers should be aware that models might not be able to identify abbreviated names as effectively. In this regard, transparency and openness as to when and how models fail are crucial measures to report. Such evaluation requires the development of infrastructure and tools, but is fast and easy to conduct once in place. For instance, it only takes 8 minutes to test DaCy large on all augmented datasets including bootstrapping. As part of the DaCy library, we provide several augmenters and utility functions for evaluation that integrate with SpaCy and encourage new NLP models to use and expand upon them. For the continued development of low and medium resource NLP in a direction that is beneficial for practitioners, it is vital to conduct more thorough evaluation of new models. We suggest these augmenters not as an evaluation standard, but as preliminary guiding principles for future development of NLP models for low and medium resource languages in particular.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(c) Substitute all names with sampled Danish male names, respecting first and last names. (d) Substitute all names with sampled Danish female names, respecting first and last names. (e) Abbreviate all first names to the first character including a full stop.The stochastic augmentations, i.e. name and keystroke augmentations, were repeated 20 times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of models finetuned for DaCy. Highest scores are in bold and second highest is underlined. WPS indicates words pr. second.</figDesc><table><row><cell></cell><cell></cell><cell>POS</cell><cell></cell><cell>NER</cell><cell></cell><cell cols="3">Dependency Parsing Speed</cell></row><row><cell>Framework</cell><cell>Model</cell><cell cols="2">Accuracy PER</cell><cell cols="3">LOC ORG MISC Avg. F1 UAS</cell><cell>LAS</cell><cell>WPS</cell></row><row><cell>DaCy large</cell><cell>XLM-Roberta</cell><cell>98.39</cell><cell cols="2">95.53 83.90 77.82 80.16</cell><cell>85.20</cell><cell>90.59</cell><cell>88</cell><cell>4311</cell></row><row><cell cols="2">DaCy medium DaBERT</cell><cell>97.93</cell><cell cols="2">89.62 83.09 67.35 70.69</cell><cell>78.47</cell><cell>87.88</cell><cell>85</cell><cell>8335</cell></row><row><cell>DaCy small</cell><cell>AElaectra Cased</cell><cell>97.69</cell><cell cols="2">87.36 81.95 63.83 70.68</cell><cell>76.55</cell><cell>86.45</cell><cell>83</cell><cell>10671</cell></row><row><cell></cell><cell>DaELECTRA</cell><cell>97.40</cell><cell cols="2">82.80 77.39 63.01 66.95</cell><cell>73.16</cell><cell>85.20</cell><cell>82</cell><cell>9855</cell></row><row><cell></cell><cell>DaConvBERT</cell><cell>97.23</cell><cell cols="2">85.08 78.26 61.76 66.93</cell><cell>73.77</cell><cell>84.61</cell><cell>81</cell><cell>10029</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and DaCy models, Stanza<ref type="bibr" target="#b22">(Qi et al., 2020</ref>), Polyglot  (Al-Rfou' et al., 2013, NERDA<ref type="bibr" target="#b19">(Kjeldgaard, 2020)</ref>, Flair<ref type="bibr" target="#b1">(Akbik et al., 2019)</ref>, and DaNLP's BERT<ref type="bibr" target="#b4">(Brogaard Pauli et al., 2021)</ref> on the DaNE test set augmented with the following augmenters:1. Keystroke augmentation: substitute 2%, 5%, or 15% of characters with a neighbouring character on a Danish QWERTY keyboard.</figDesc><table /><note>2. AE?? augmentation: substitute ae/AE with ae/Ae, ?/? with oe/Oe, and ?/? with aa/Aa to simulate some historic text variations in Danish. 3. Lower-case augmentation: convert all text to lower-case.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of Danish NLP pipelines. Wall Time is the time taken by the model to go through the DaNE test set without augmentation. Stanza uses the spacy-stanza implementation. The speed of the DaNLP model is reported as provided by the framework, which does not utilize batch input. However, given the model size it can be expected to reach speeds comparable to DaCy medium. Empty cells indicates that the framework does not include the specific model.</figDesc><table><row><cell></cell><cell>POS</cell><cell></cell><cell></cell><cell>NER</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Dependency Parsing Wall Time</cell></row><row><cell>Model</cell><cell cols="5">Accuracy Person Location Organization Misc</cell><cell>F1</cell><cell cols="2">F1 w/o Misc LAS</cell><cell>UAS</cell><cell>GPU/CPU</cell></row><row><cell>DaCy large</cell><cell>98.37</cell><cell>93.33</cell><cell>84.88</cell><cell>76.49</cell><cell cols="2">80.16 84.39</cell><cell>85.65</cell><cell>88.44</cell><cell>90.85</cell><cell>2.9 / 34.7</cell></row><row><cell>DaCy medium</cell><cell>98.15</cell><cell>89.86</cell><cell>83.96</cell><cell>64.47</cell><cell cols="2">70.09 77.67</cell><cell>79.68</cell><cell>86.65</cell><cell>89.25</cell><cell>1.8 / 9.9</cell></row><row><cell>DaCy small</cell><cell>97.75</cell><cell>87.98</cell><cell>79.23</cell><cell>60.58</cell><cell cols="2">64.82 74.18</cell><cell>76.98</cell><cell>84.03</cell><cell>87.63</cell><cell>1.9 / 2.6</cell></row><row><cell>DaNLP BERT</cell><cell></cell><cell>92.27</cell><cell>83.90</cell><cell>71.13</cell><cell></cell><cell>72.84</cell><cell>83.20</cell><cell></cell><cell></cell><cell>37.4 / -</cell></row><row><cell>Flair</cell><cell>97.80</cell><cell>92.60</cell><cell>84.82</cell><cell>61.29</cell><cell></cell><cell>70.49</cell><cell>81.09</cell><cell></cell><cell></cell><cell>2.0 / -</cell></row><row><cell>NERDA</cell><cell></cell><cell>92.35</cell><cell>81.52</cell><cell>65.96</cell><cell cols="2">72.41 79.04</cell><cell>80.85</cell><cell></cell><cell></cell><cell>2.5 / -</cell></row><row><cell>Polyglot</cell><cell>76.26</cell><cell>79.25</cell><cell>68.06</cell><cell>40.69</cell><cell></cell><cell>56.67</cell><cell>65.32</cell><cell></cell><cell></cell><cell>-/ 3.8</cell></row><row><cell>SpaCy large</cell><cell>96.30</cell><cell>86.17</cell><cell>84.16</cell><cell>63.36</cell><cell cols="2">65.52 75.75</cell><cell>78.57</cell><cell>78.01</cell><cell>81.95</cell><cell>0.9 / 1.4</cell></row><row><cell>SpaCy medium</cell><cell>95.71</cell><cell>84.55</cell><cell>77.29</cell><cell>63.16</cell><cell cols="2">63.25 73.23</cell><cell>76.01</cell><cell>77.73</cell><cell>81.87</cell><cell>1.2 / 1.4</cell></row><row><cell>SpaCy small</cell><cell>94.80</cell><cell>78.92</cell><cell>69.04</cell><cell>53.49</cell><cell cols="2">61.54 67.11</cell><cell>68.61</cell><cell>74.03</cell><cell>78.68</cell><cell>1.4 / 1.5</cell></row><row><cell>Stanza</cell><cell>97.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>83.84</cell><cell>87.34</cell><cell>29.3 / -</cell></row><row><cell cols="7">4. Spacing augmentation: randomly remove 5% of all whitespace.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">5. Name augmentations:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(a) Substitute all names (PER entities) with randomly sampled Danish names, respecting first and last names. (b) Substitute all names with randomly sampled names of Muslim origin used in Denmark</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>NER performance of Danish NLP pipelines reported as average F1 scores excluding the MISC category. Best scores are marked bold and second best are underlined. * denotes that the result is significantly different from the baseline using a significance threshold of 0.05 with Bonferroni correction for multiple comparisons. Danish names is considered the baseline for the augmentation of Muslim, female, and male names. Values in parentheses denote the standard deviation. NERDA limits input size to 128 wordpieces which leads to truncation on long input sizes and high rates of keystroke errors.</figDesc><table><row><cell>Deterministic Augmentations</cell><cell></cell></row><row><cell>Input Length</cell><cell>Names</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>POS performance of Danish NLP pipelines reported as accuracy. Best scores are marked bold and second best are underlined. * denotes that the result is significantly different from baseline using a significance threshold of 0.05 with Bonferroni correction for multiple comparisons. Values in parentheses denote the standard deviation. NERDA limits input size to 128 wordpieces which leads to truncation on long input sizes and with a high degree of keystroke errors.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Deterministic Augmentations</cell><cell></cell><cell cols="2">Stochastic Augmentations</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Input Length</cell><cell></cell><cell>Keystroke Errors</cell></row><row><cell>Model</cell><cell cols="5">Baseline AE?? Lowercase 5 sentences 10 sentences</cell><cell>2%</cell><cell>5%</cell><cell>15%</cell></row><row><cell>DaCy large</cell><cell>98.4</cell><cell>97.5</cell><cell>95.5</cell><cell>98.5</cell><cell>98.4</cell><cell cols="2">95.5 (0.2)* 91.1 (0.2)* 75.4 (0.6)*</cell></row><row><cell>DaCy medium</cell><cell>98.2</cell><cell>96.5</cell><cell>98.1</cell><cell>97.8</cell><cell>97.9</cell><cell cols="2">93.6 (0.3)* 86.5 (0.3)* 63.3 (0.6)*</cell></row><row><cell>DaCy small</cell><cell>97.7</cell><cell>95.4</cell><cell>95.4</cell><cell>97.6</cell><cell>97.7</cell><cell cols="2">93.1 (0.2)* 85.9 (0.4)* 62.5 (0.4)*</cell></row><row><cell>Flair</cell><cell>97.8</cell><cell>95.0</cell><cell>95.0</cell><cell>97.7</cell><cell>97.7</cell><cell cols="2">94.7 (0.2)* 89.8 (0.3)* 72.1 (0.4)*</cell></row><row><cell>Polyglot</cell><cell>76.3</cell><cell>71.6</cell><cell>75.6</cell><cell>75.7</cell><cell>75.6</cell><cell cols="2">71.7 (0.2)* 65.3 (0.3)* 49.4 (0.4)*</cell></row><row><cell>SpaCy large</cell><cell>96.3</cell><cell>92.4</cell><cell>91.5</cell><cell>96.3</cell><cell>96.3</cell><cell cols="2">91.5 (0.2)* 84.8 (0.4)* 66.2 (0.5)*</cell></row><row><cell>SpaCy medium</cell><cell>95.7</cell><cell>92.4</cell><cell>91.6</cell><cell>95.8</cell><cell>95.7</cell><cell cols="2">91.0 (0.3)* 84.5 (0.3)* 66.0 (0.5)*</cell></row><row><cell>SpaCy small</cell><cell>94.8</cell><cell>90.5</cell><cell>90.3</cell><cell>94.8</cell><cell>94.8</cell><cell cols="2">90.7 (0.2)* 85.3 (0.3)* 69.1 (0.4)*</cell></row><row><cell>Stanza</cell><cell>97.6</cell><cell>96.1</cell><cell>95.4</cell><cell>97.7</cell><cell>97.7</cell><cell cols="2">94.8 (0.2)* 90.6 (0.3)* 75.6 (0.5)*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Dependency parsing performance of Danish NLP pipelines reported as LAS. Best scores are marked bold and second best are underlined. * denotes that the result is significantly different from baseline using a significance threshold of 0.05 with Bonferroni correction for multiple comparisons. Values in parentheses denote the standard deviation. DaCy small is robust to spelling errors and outperforms larger models such as DaNLP's BERT and NERDA, this is likely due to its well-curated training data<ref type="bibr" target="#b10">(Derczynski et al., 2021)</ref>. DaNLP's BERT and NERDA models were found to severely under-perform if given longer input lengths. DaCy's models consistently perform slightly better with more context, but are not vulnerable to shorter input. Lastly, as expected, the lack of casing is especially detrimental for NER for the cased models, most notably Flair, the SpaCy models, DaCy large and DaCy small.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Deterministic Augmentations</cell><cell></cell><cell cols="2">Stochastic Augmentations</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Input Length</cell><cell></cell><cell>Keystroke Errors</cell></row><row><cell>Model</cell><cell cols="5">Baseline AE?? Lowercase 5 sentences 10 sentences</cell><cell>2%</cell><cell>5%</cell><cell>15%</cell></row><row><cell>DaCy large</cell><cell>88.4</cell><cell>86.2</cell><cell>87.0</cell><cell>88.3</cell><cell>88.3</cell><cell cols="2">83.7 (0.4)* 76.6 (0.5)* 53.6 (0.8)*</cell></row><row><cell>DaCy medium</cell><cell>86.7</cell><cell>84.6</cell><cell>86.6</cell><cell>85.4</cell><cell>85.3</cell><cell cols="2">79.9 (0.5)* 69.9 (0.7)* 41.1 (0.9)*</cell></row><row><cell>DaCy small</cell><cell>84.0</cell><cell>79.0</cell><cell>82.7</cell><cell>83.5</cell><cell>83.0</cell><cell cols="2">76.8 (0.4)* 66.2 (0.8)* 38.0 (0.6)*</cell></row><row><cell>SpaCy large</cell><cell>78.0</cell><cell>71.0</cell><cell>74.0</cell><cell>77.6</cell><cell>77.6</cell><cell cols="2">69.7 (0.5)* 59.3 (0.7)* 34.8 (0.7)*</cell></row><row><cell>SpaCy medium</cell><cell>77.7</cell><cell>71.2</cell><cell>73.8</cell><cell>77.4</cell><cell>77.4</cell><cell cols="2">69.6 (0.6)* 59.5 (0.6)* 35.3 (0.7)*</cell></row><row><cell>SpaCy small</cell><cell>74.0</cell><cell>65.9</cell><cell>70.4</cell><cell>74.1</cell><cell>74.1</cell><cell cols="2">67.5 (0.4)* 59.1 (0.5)* 38.2 (0.7)*</cell></row><row><cell>Stanza</cell><cell>83.8</cell><cell>80.2</cell><cell>82.5</cell><cell>83.9</cell><cell>83.9</cell><cell cols="2">79.0 (0.4)* 71.9 (0.5)* 49.8 (0.9)*</cell></row><row><cell cols="2">and the model size. Similarly,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See: https://centre-for-humanities-computing.github.io/DaCy/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For a full list of models and training configurations see the config files on Github: https://github.com/ centre-for-humanities-computing/DaCy/tree/main/training</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In our experiments, several of the Danish models performed worse using their own tokenizer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>NER Named Entity Recognition NLP Natural Language Processing POS Part-of-Speech</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11038</idno>
		<title level="m">Muppet: Massive multi-task representations with pre-finetuning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>version: 1. cs</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">FLAIR: An easy-to-use framework for state-of-the-art NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4010</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-4010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<ptr target="https://aclanthology.org/W13-3520" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013-07-07" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Social media: Friend or foe of natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation</title>
		<meeting>the 26th Pacific Asia Conference on Language, Information, and Computation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="58" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DaNLP: An open-source toolkit for danish natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brogaard</forename><surname>Pauli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hvingelby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Nordic Conference on Computational Linguistics</title>
		<meeting>the 23rd Nordic Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
	<note>Publisher: Springer</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical survey of data augmentation for limited data learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07499</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<ptr target="http://arxiv.org/abs/2003.10555" />
		<imprint>
			<date type="published" when="2020-05-03" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<title level="m">Unsupervised cross-lingual representation learning at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The danish gigaword corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ciosici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baglini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Dalsgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fusaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Henrichsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hvingelby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirkedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Kjeldsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ladefoged</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">?</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Rystr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Varab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Nordic Conference on Computational Linguistics</title>
		<meeting>the 23rd Nordic Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural language processing for social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farzindar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="166" />
			<date type="published" when="2015" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03075</idno>
		<ptr target="http://arxiv.org/abs/2105.03075" />
		<title level="m">A survey of data augmentation approaches for NLP</title>
		<imprint>
			<date type="published" when="2021-05-11" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">AElaectra -a step towards more efficient danish natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?jmark-Bertelsen</surname></persName>
		</author>
		<ptr target="https://github.com/MalteHB/-l-ctra/" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">spaCy: Industrial-strength natural language processing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boyd</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1212303</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1212303" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DaNE: A named entity resource for danish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hvingelby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Pauli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Lidegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4597" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02496</idno>
		<title level="m">ConvBERT: Improving BERT with span-based dynamic convolution</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal dependencies for danish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Workshop on Treebanks and Linguistic Theories</title>
		<meeting>the Fourteenth International Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: Is it time for some linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kjeldgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-19400-9_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-19400-9_14" />
	</analytic>
	<monogr>
		<title level="m">Computational linguistics and intelligent text processing</title>
		<editor>A. F. Gelbukh</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Muslimske fornavne i Danmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Meldgaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Publisher: K?benhavns Universitet</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Danish BERT model: BotXO has trained the most advanced BERT model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>M?llerh?j</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-12-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stanza: A python natural language processing toolkit for many human languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07082</idno>
		<ptr target="http://arxiv.org/abs/2003.07082" />
		<imprint>
			<date type="published" when="2020-05-02" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2020-11-25" />
		</imprint>
	</monogr>
	<note>cs, stat. Retrieved</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of NLP models with CheckList</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.442</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.442" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<ptr target="http://arxiv.org/abs/1706.05098" />
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017-07-06" />
		</imprint>
	</monogr>
	<note>cs, stat. Retrieved</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-019-0197-0</idno>
		<ptr target="https://doi.org/10.1186/s40537-019-0197-0" />
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A study on word2vec on a historical swedish newspaper corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Dk</forename><surname>Sprogteknologi</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2084/paper2.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Digital Humanities in the Nordic Countries 3rd Conference</title>
		<meeting>the Digital Humanities in the Nordic Countries 3rd Conference<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03-07" />
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Danish transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Tamini-Sarnikowski</surname></persName>
		</author>
		<ptr target="https://github.com/sarnikowski" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1670</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1670" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">HuggingFace&apos;s transformers: State-of-the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>art natural language processing</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<ptr target="http://arxiv.org/abs/2010.11934" />
		<title level="m">mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<imprint>
			<date type="published" when="2021-07-06" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
