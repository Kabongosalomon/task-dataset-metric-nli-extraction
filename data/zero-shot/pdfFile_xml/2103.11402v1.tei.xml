<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instant-Teaching: An End-to-End Semi-Supervised Object Detection Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
							<email>zhibin.waz@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
							<email>qi.qian@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instant-Teaching: An End-to-End Semi-Supervised Object Detection Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning based object detection frameworks demand plenty of laborious manual annotations, which may not be practical in real applications. Semi-supervised object detection (SSOD) can effectively leverage unlabeled data to improve the model performance, which is of great significance for the application of object detection models. In this paper, we revisit SSOD and propose Instant-Teaching, a completely end-to-end and effective SSOD framework, which uses instant pseudo labeling with extended weak-strong data augmentations for teaching during each training iteration. To alleviate the confirmation bias problem and improve the quality of pseudo annotations, we further propose a co-rectify scheme based on Instant-Teaching, denoted as Instant-Teaching * . Extensive experiments on both MS-COCO and PASCAL VOC datasets substantiate the superiority of our framework. Specifically, our method surpasses state-of-the-art methods by 4.2 mAP on MS-COCO when using 2% labeled data. Even with full supervised information of MS-COCO, the proposed method still outperforms state-of-the-art methods by about 1.0 mAP. On PASCAL VOC, we can achieve more than 5 mAP improvement by applying VOC07 as labeled data and VOC12 as unlabeled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19]</ref> have significantly improved the performance of diverse computer vision applications, e.g., image classification and object detection. In order to avoid overfitting and achieve better performance, a large amount of accurate human-annotated data is needed to train a deep learning model. However, the assumption of having a sufficient amount of accurate labeled data for training may not hold, especially for object detection tasks, which need annotations with accurate class labels and precise bounding box coordinates. Thus, a natural idea is to leverage abundant unlabeled data to facilitate learning in the original task. To relax the dependency of manually labeled data, a promising approach is called semi-supervised learning (SSL) <ref type="bibr" target="#b7">[8]</ref>.</p><p>SSL has recently received increasing attention from the community, since it provides effective methods of using unlabeled data to facilitate model learning with limited annotated data. Most of the existing SSL methods focus on image classification tasks and there are multiple strategies for semi-supervised learning, e.g., self-training <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b51">52]</ref> and co-training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>. Recently, one popular line of research uses consistency losses for semi-supervised learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref>. They either adopt ensemble learning algorithms to enforce the predictions of the unlabeled data to be consistent across multiple models, or constrain the model predictions to be invariant to noise. Another popular line of SSL research focuses on more effective data augmentations to improve the generalization and robustness of the model, in which some learning-based and more complex data augmentation strategies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b43">44]</ref> greatly improve the performance of SSL on image classification tasks.</p><p>Although semi-supervised learning has made great progress in the field of image classification, there is a paucity of literature focus on semi-supervised object detection (SSOD). The recently proposed STAC <ref type="bibr" target="#b44">[45]</ref> performs best among existing SSOD methods and outperforms the supervised model by a large margin, which is of great significance to the research of SSOD. However, we find that STAC still has some problems. First, its training procedure is complicated and inefficient. Before model training, STAC needs to train a teacher model, and then it uses the teacher model to pre-generate pseudo annotations of unlabeled data. Second, during model training, the pregenerated pseudo annotations will no longer be updated, and the constant label will limit its performance. In this paper, to address the above two problems, we propose a novel end-to-end SSOD framework, Instant-Teaching, which uses instant pseudo labeling with extended weak-strong data augmentations for teaching during each training iteration. Specifically, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, during each training iter-ation, Instant-Teaching will first generate pseudo annotations of unlabeled data with weak data augmentations in a mini-batch, and then the predicted annotations will instantly be used as the ground-truth of the same image with strong data augmentations for training. The advantage of Instant-Teaching is that as the model converges during training, the quality of pseudo annotations will be improved instantly. The weak-strong data augmentation scheme is inherited from STAC, which has been proven to be effective in combination with pseudo annotations, and we further extend the strong data augmentations to include Mixup and Mosaic. In addition, the confirmation bias <ref type="bibr" target="#b47">[48]</ref> is a common problem in SSL. To alleviate this issue, we further propose a co-rectify scheme based on Instant-Teaching, denoted as Instant-Teaching * . Instant-Teaching * simultaneously trains two models that have the same structure but share different weights and these two models help each other to rectify false predictions. During inference, we still only use a single model so that it does not increase inference time.</p><p>We test the efficacy of Instant-Teaching * on PASCAL VOC <ref type="bibr" target="#b13">[14]</ref> and MS-COCO <ref type="bibr" target="#b30">[31]</ref> datasets, and follow the experimental protocols used in the latest state-of-the-art SSOD literature STAC <ref type="bibr" target="#b44">[45]</ref> to evaluate the performance. It is worth mentioning that, our Instant-Teaching * framework outperforms state-of-the-art methods at all experimental protocols, and achieves state-of-the-art performance on semi-supervised object detection learning.</p><p>The contributions of this paper are as follows:</p><p>? We propose a novel SSOD framework, called Instant-Teaching, which uses instant pseudo labeling with extended weak-strong data augmentations for teaching during each training iteration. Instant-Teaching is an end-to-end framework and can effectively leverage the unlabeled data.</p><p>? To alleviate the confirmation bias problem and improve the quality of pseudo annotations, we further propose a co-rectify scheme based on Instant-Teaching, denoted as Instant-Teaching * .</p><p>? Our extensive experiments on PASCAL VOC and MS-COCO datasets demonstrate the significant efficacy of our Instant-Teaching * framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object detection is an important computer vision task and has received considerable attention in recent years <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>. One line of research focuses on strong two-stage object detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>, which first generate a sparse set of regions of interest (RoIs) with a Region Proposal Network (RPN), and then perform classification and bounding box regression.</p><p>Another line of research develops fast single-stage object detectors <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50]</ref>. However, these methods train stronger or faster models on a large amount of accurate human-annotated data, which is expensive and timeconsuming to acquire in real applications. In this work, we follow the popular two-stage object detector (Faster-RCNN <ref type="bibr" target="#b38">[39]</ref>) to develop our framework. Different from previous methods training models only on labeled data, we train our object detector on both labeled and unlabeled data with our proposed semi-supervised learning strategy. Semi-supervised learning (SSL) exploits the potential of unlabeled data to facilitate model learning with limited annotated data. Most of the existing SSL methods focus on image classification tasks and most of the works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref> are consistency-based methods, which constrain the model to be invariant to the noise. Pseudo labeling based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b51">52]</ref> improve the performance of SSL by generating high-quality hard labels (i.e., the arg max of the output class probability) of unlabeled data with a predefined threshold and retraining the model. Recently, data augmentations have proven to be a powerful paradigm for boosting SSL on image classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref>. MixMatch <ref type="bibr" target="#b3">[4]</ref> improves SSL by guessing low-entropy labels for data-augmented unlabeled data and mixes labeled and unlabeled data using Mixup. FixMatch <ref type="bibr" target="#b43">[44]</ref>, UDA <ref type="bibr" target="#b50">[51]</ref> and ReMixMatch <ref type="bibr" target="#b2">[3]</ref> have shown that RandAugment <ref type="bibr" target="#b9">[10]</ref> and CTAugment <ref type="bibr" target="#b2">[3]</ref> can significantly facilitate learning of SSL. Semi-supervised object detection (SSOD) applies semisupervised learning to object detection. Recently, a few existing works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b22">23]</ref> propose to train object detectors on both labeled data and unlabeled data by incorporating SSL into object detection. The methods in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref> depend on additional context (e.g., temporal information from video). The method in <ref type="bibr" target="#b35">[36]</ref> proposes data distillation to automatically generate new training annotations by ensembling predictions of multiple transformations of unlabeled data. NOTE-RCNN <ref type="bibr" target="#b14">[15]</ref> proposes to iteratively perform bounding box mining and detector retraining. S 4 OD <ref type="bibr" target="#b27">[28]</ref> proposes a selective net as a heuristic for selecting bounding boxes to improve object detection with unlabeled web images. CSD <ref type="bibr" target="#b21">[22]</ref> proposes a consistency-based SSL method for object detection, which uses flip augmentation and consistency constraints to enhance detection performance. Based on CSD, ISD <ref type="bibr" target="#b22">[23]</ref> proposes to use interpolation regularization to further improve the performance of SSL for object detection. Recently, STAC <ref type="bibr" target="#b44">[45]</ref> develops a SSL framework for object detection that combines selftraining and consistency regularization based on strong data augmentations, which achieves state-of-the-art results. Inspired by these methods, this paper exploits the effective usage of pseudo annotations as well as data augmentations and co-rectify scheme to further improve the perfor- mance of SSL for object detection in a more efficient and simpler way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first give the problem definition of our semi-supervised object detection task (see Section 3.1). Then, we show an overview of our Instant-Teaching * framework (see Section 3.2), which consists of instant pseudo labeling with extended weak-strong data augmentations and co-rectify scheme (see Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition</head><p>In semi-supervised object detection (SSOD), we are given a set of labeled data D l = {(x l i , y l i )} n l i=1 and a set of unlabeled data D u = {x u j } nu j=1 , where x and y denote image and ground-truth annotations (class labels and bounding box coordinates) respectively. The goal of SSOD is to train object detectors on both labeled and unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The overview framework</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our Instant-Teaching * framework is mainly composed of two modules, namely, instant pseudo labeling with weak-strong data augmentations and corectify. It is worth mentioning that, the first module of instant pseudo labeling with weak-strong data augmentations already forms a complete SSOD framework, denoted as Instant-Teaching, which also outperforms state-of-the-art methods.</p><p>These two modules have their own focus, among which instant pseudo labeling with weak-strong data augmentations enables our method to be trained end-to-end, and the quality of pseudo annotations is instantly improved as the model converges. Moreover, weak-strong data augmentations enforce the model to maintain consistent predictions between the weakly augmented and the strongly augmented unlabeled data. In this way, the model can learn useful information from the pseudo annotations generated by itself. The co-rectify scheme trains two models with the same structure simultaneously and these two models help each other to rectify false predictions, thus alleviating the common confirmation bias problem and further improving the model performance.</p><p>Please note that although our Instant-Teaching * trains two models at the same time, we only use a single model (Model-a) during inference, which does not increase inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instant-Teaching *</head><p>Instant pseudo labeling. It is beneficial to update the pseudo annotations with a more precise model during the training process, which motivates us to propose instant pseudo labeling. Instant pseudo labeling performs model training and pseudo-label generation at the same time, which is end-to-end and different from the latest STAC <ref type="bibr" target="#b44">[45]</ref> framework. STAC needs to train a teacher model in advance to generate pseudo annotations of unlabeled data. Moreover, STAC does not update the generated pseudo annotations during training, which limits its performance.</p><p>To be more specific, we decompose each training iteration into two steps. In the first step, we use the current model to generate pseudo annotations of unlabeled data in a mini-batch. Note that in this step, we apply weak augmentations ?(?) to unlabeled data (unless otherwise specified, we only use random flip as weak augmentation in all experiments). In the second step, we apply strong augmentations A(?) to the same unlabeled data with pseudo annotations generated in the first step, and update the model parameters with a entire training objective, which consists of a supervised loss and an unsupervised loss. Note that in this step, to get a fair comparison with STAC, we only apply strong data augmentations to unlabeled data, while still applying weak augmentations to labeled data. In fact, the performance of the model will be relatively poor in the initial training phase. In order to guarantee the quality of the generated pseudo annotations, we always apply non-maximum suppression (NMS) and confidence-based box filtering with a high threshold ? in the first step (unless otherwise specified, we use ? = 0.9 in all experiments).</p><p>Overall, the model is trained by jointly minimizing the supervised loss and unsupervised loss as follows:</p><formula xml:id="formula_0">= s + ? u u ,<label>(1)</label></formula><p>where we use ? u to balance the supervised loss s and the unsupervised loss u . The supervised loss s consists of a classification loss L cls (a standard cross-entropy loss), and a bounding box regression loss L reg (a L 1 loss). s can be computed as:</p><formula xml:id="formula_1">s = l [ 1 N cls i L cls (p(c i | ?(x l )), c * i ) + ? N reg i c * i L reg (p(t i | ?(x l )), t * i )].<label>(2)</label></formula><p>In the above equation, l is the index of labeled images in a mini-batch, i is the index of an anchor in a single image, p(c i | x) is the predicted probability of anchor i being an object in image x, p(t i | x) is the 4-dimensional coordinates of an predicted bounding box, c * i and t * i are the human-annotated ground-truth class label and bounding box coordinates respectively.</p><p>When computing the unsupervised loss u , we first compute the model's predicted class probability distribution and box coordinates based on weakly augmented unlabeled data in a mini-batch: (c u , t u ) = p(c, t | ?(x u )). Then we use the hard label? u = arg max(c u ) as the final class label of pseudo annotations. In addition, the unsupervised loss is computed on strongly augmented unlabeled data and can be written as:</p><formula xml:id="formula_2">u = u [ 1 N cls i L cls (p(c i | A(x u )),? u i ) + ? N reg i (max(c u i ) ? ? )L reg (p(t i | A(x u )), t u i )],<label>(3)</label></formula><p>where u is the index of unlabeled images in a mini-batch,? u i and t u i are pseudo annotations of unlabeled data generated by the model itself, and ? is the confidence threshold.</p><p>Weak-strong data augmentations. How to encourage the model to learn useful information from the pseudo annotations generated by the model itself is essential to all self-training based SSL methods. Weak-strong data augmentation scheme is a promising practice, which has been proven in semi-supervised image classification <ref type="bibr" target="#b43">[44]</ref> and semi-supervised object detection <ref type="bibr" target="#b44">[45]</ref>. Weak-strong data augmentations enforce the model to maintain consistent predictions between the weakly augmented and the strongly augmented unlabeled data, and thus encourage the model to learn useful information from the pseudo annotations.</p><p>Intuitively, the key of weak-strong data augmentation scheme lies in the difference between weak augmentations and strong augmentations. When the weak augmentations remain unchanged, the more complex and appropriate the strong augmentations, the more information the model can learn from the pseudo annotations. Based on this hypothesis, we extend the strong data augmentations of STAC and introduce more complex augmentations for unlabeled data, including Mixup <ref type="bibr" target="#b52">[53]</ref> and Mosaic <ref type="bibr" target="#b5">[6]</ref>. The experimental results also reveal that our extended weak-strong data augmentations can further improve the performance of semisupervised object detection.</p><p>Specifically, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (Mixup), given an unlabeled image x u and its pseudo annotations (b u , c u ), where b u are the 4-dimensional box coordinates and c u are the one-hot class labels of these pseudo boxes (note that we use hard label when the confidence score is larger than the confidence threshold ? ). We first randomly choose one labeled image x l with ground-truth annotations (b l , c l ) from the mini-batch. Next, we mix these two images and their one-hot labels and bounding box coordinates with a mixing coefficient ? m drawn from the Beta(? m , ? m ) distribution, where ? m = 1.0. Finally, we use the mixed image and soft class labels and bounding box coordinates to substitute the image content and pseudo annotations of the unlabeled image x u , which can be computed as:</p><formula xml:id="formula_3">? ? ? ? ? ? ? ? m ? Beta(? m , ? m ), x u = ? m x u + (1 ? ? m )x l , c u = ? m c u ? (1 ? ? m )c l , b u = b u ? b l .<label>(4)</label></formula><p>As for Mosaic, as shown in <ref type="figure" target="#fig_1">Fig. 2 (Mosaic)</ref>, given an unlabeled image x u and a labeled image x l in a mini-batch, we randomly perform two kinds of mixing styles (horizontal mixing and vertical mixing) and mix their corresponding annotations accordingly. By applying Mixup and Mosaic data augmentations to unlabeled data, we can improve the model robustness to pseudo annotation noise and alleviate the overfitting problem in model training.</p><p>Note that, for a fair comparison, we only perform Mixup and Mosaic data augmentations on unlabeled data and keep labeled data with weak data augmentation unchanged in all our experiments, which is the same as STAC <ref type="bibr" target="#b44">[45]</ref>.</p><p>Co-rectify. Confirmation bias <ref type="bibr" target="#b47">[48]</ref> is a common problem in semi-supervised learning. When the model generates incorrect predictions with high confidence, these incorrect predictions will be further strengthened through incorrect pseudo annotations. In other words, the model itself is difficult to rectify these false predictions.</p><p>To alleviate this problem, we propose a co-rectify scheme, which trains two models f a (?) (Model-a) and f b (?) (Model-b) simultaneously. These two models help each other to rectify the false predictions, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The key to the success of co-rectify is that the two models will not converge to the same model. We take two measures to ensure that the two models converge independently. First, although the two models have the same structure, they use different initialization parameters. Second, although the two models share the same data in each mini-batch, their data augmentations and pseudo annotations are also different.</p><p>We take model f a (?) as an example and the rectified pseudo annotations of model f b (?) are constructed in a similar way. When generating pseudo annotations during each training iteration, model f a (?) first predicts class probabilities c i and bounding box coordinates t i on the weakly augmented unlabeled image x u . Then, we use the detection head in model f b (?) to refine the class probabilities c r i and bounding box coordinates t r i by taking the predicted boxes t i as proposals. Finally, the rectified class probabilities c * i are averaged from c i and c r i , and the rectified bounding box coordinates t * i are the weighted average of t i and t r i . The co-rectify process can be computed as:</p><formula xml:id="formula_4">? ? ? ? ? ? ? (c i , t i ) = f a (x u ), (c r i , t r i ) = f b (x u ; t i ), c * i = 1 2 (c i + c r i ), t * i = 1 ci+c r i (t i c i + t r i c r i ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We test our proposed semi-supervised object detection framework Instant-Teaching * on the large-scale dataset MS-COCO <ref type="bibr" target="#b30">[31]</ref> and report the mAP over 80 object categories. For a fair comparison, we use the same SSL experimental settings as STAC. When performing SSL experiments on the MS-COCO dataset, two experimental settings are used. In the first setting, only a small amount of data in the 118k labeled images is selected as the labeled set. The remainder is used as the unlabeled set. Under this setting, we are able to verify the performance of the SSL algorithm when there is only a small amount of labeled data. In the second setting, the entire 118k images are used as the labeled set and the additional 123k unlabeled images are used as the unlabeled set, which enables us to verify whether SSL algorithm can further improve the performance of the detector when large-scale labeled images already exist. In the first experimental setting, we randomly selected 1%, 2%, 5%, and 10% from the 118k labeled images as the labeled set.</p><p>In addition, we also test on PASCAL VOC <ref type="bibr" target="#b13">[14]</ref> following <ref type="bibr" target="#b21">[22]</ref> and report the mAP over 20 object categories. We use the trainval set of VOC07 as labeled data, which consists of ?5k images, and the unlabeled data contains the trainval set of VOC12 (?11k images) and the subset of MS-COCO with the same classes as PASCAL VOC (?95k images). We evaluate the performance on the test set of VOC07 and report the mAPs at IoU=0.5, IoU=0.75, and IoU=0.5:0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We implement our Instant-Teaching * framework based on the MMDetection toolbox <ref type="bibr" target="#b8">[9]</ref>. To get a fair comparison, we follow STAC to use Faster-RCNN <ref type="bibr" target="#b39">[40]</ref> with FPN <ref type="bibr" target="#b28">[29]</ref> as our object detector and use ResNet-50 <ref type="bibr" target="#b18">[19]</ref> as the feature extractor. The feature weights are initialized by the ImageNetpretrained model. Instant-Teaching * mainly contains three hyperparameters: ?, ? u and ? , we set ? = 1.0, ? u = 1.0 and ? = 0.9 unless otherwise specified.</p><p>All our experiments maintain the same training parameters as STAC. Specifically, we train the model using an SGD optimizer on 8 GPUs, with an initial learning rate of 0.01, a momentum of 0.9, a weight decay of 1e?4 and a total training step of 180k. The learning rate decays by 10? at 120k and 165k respectively. Moreover, we fix the minibatch size to 16, in which the ratio between labeled images and unlabeled images is 1:1. Following STAC, for 1%, 2%, 5% and 10% MS-COCO protocols, we use the quick learning schedule. For the 100% protocol, we use the standard learning schedule. The quick schedule adopts multi-scale training and the standard schedule adopts single-scale training, which is depicted in the Appendix A of STAC <ref type="bibr" target="#b44">[45]</ref> and our supplementary materials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We will make a detailed comparison with the supervised baseline and state-of-the-art SSOD methods, including CSD <ref type="bibr" target="#b21">[22]</ref> and STAC <ref type="bibr" target="#b44">[45]</ref>. The detailed results are summarized in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>.</p><p>As depicted in We also observe a similar trend on PASCAL VOC experiments. As depicted in <ref type="table">Table 2</ref>, when compared with STAC, with VOC07 as labeled data and VOC12 as unlabeled data, our Instant-Teaching * improves mAP from 44.64 to 50.00, which demonstrates 5.36 absolute mAP improvement. When there are more unlabeled data introduced (the subset of MS-COCO), Instant-Teaching * can further improve mAP from STAC's 46.01 to 50.80. We also observe that the improvement of AP 0.75 of Instant-Teaching * is more prominent compared to that of AP 0.5 . In other words, the improvement of mAP (AP 0.5:0.95 ) mainly comes from the improvement of predicted high-quality bounding boxes. We also perform ablation studies on our Instant- Teaching * with different backbones in the Appendix <ref type="figure" target="#fig_4">(A.4)</ref>, demonstrating the scalability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Instant pseudo labeling</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we report the average number of annotated instances per image during each training iteration, in which N 1 denotes the number of only human-annotated instances and N 2 denotes the number of total instances including human-annotated and model generated (pseudo annotations). It can be observed that the number of highquality pseudo annotations (N 2 ? N 1 ) gradually increases during the training process. Namely, as the model converges, the quantity of high-quality pseudo annotations can be instantly improved.</p><p>From <ref type="table">Table 3</ref>, we can also observe that at the protocol of 5% MS-COCO with 8? unlabeled data, Instant-Teaching improves mAP from STAC's 23.14 to 24.70 using only color jittering and Cutout <ref type="bibr" target="#b11">[12]</ref> as the strong data augmentations. Without using more strong data augmentations, our Instant-Teaching already outperforms the state-of-theart STAC method. These results prove that our instant pseudo labeling can finally achieve higher performance by continuously improving the pseudo annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Strong data augmentation</head><p>In weak-strong data augmentation scheme, the choice of strong augmentations directly affects the performance of  <ref type="table">Table 3</ref>. Comparison of mAP of Instant-Teaching trained with various data augmentation methods at the protocol of 5% MS-COCO and 8? unlabeled data. ? denotes that we also apply strong augmentations "Color+Cutout" to unlabeled data in the first step during instant pseudo labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Labeled  the final SSOD model. We extend the strong augmentations of STAC from color jittering, geometric transformation and Cutout to include Mixup and Mosaic. Note that, we do not apply geometric transformation, mainly because the online geometric transformation of pseudo annotations is more complicated, and we leave it for future work. As shown in <ref type="table">Table 3</ref>, we first also apply strong augmentations (Color+Cutout) to unlabeled data in the first step during the pseudo labeling phase. This method gives us 1.54 mAP drop compared with STAC. The observation verifies our hypothesis, i.e., the key of weak-strong data augmentation scheme lies in the difference between weak augmentations and strong augmentations. Furthermore, we find that using either Mixup or Mosaic can improve the performance of Instant-Teaching. Instant-Teaching can obtain the best performance by using Mixup and Mosaic data augmentations together, increasing mAP from 23.14 of STAC to 25.60. These observations indicate that our extended weakstrong data augmentations can further improve the performance of SSOD.</p><p>Note that we only use Mixup and Mosaic data augmentations for unlabeled data for a fair comparison with STAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Size of unlabeled data</head><p>In the field of semi-supervised object detection, the importance of the size of unlabeled data should not be ignored. Therefore in this section, we evaluate our method with 5% and 10% labeled data of MS-COCO while varying the size of unlabeled data from 1, 2, 4, and 8 times to that of the labeled data. The results are given in <ref type="table" target="#tab_4">Table 4</ref>. We can observe that our method outperforms the state-ofthe-art method STAC on all scales of unlabeled data. It is    <ref type="figure" target="#fig_4">Fig. 4</ref> we can observe that our Instant-Teaching (without co-rectify) outperforms the supervised model and the state-of-the-art method STAC by a large margin. We also find that as the size of unlabeled data increases, both STAC and Instant-Teaching suffer a "ceiling effect": as the performance gets closer to the ceiling, the improvement becomes smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis of ? and ? u</head><p>We analyze the effect of the confidence threshold ? and the unsupervised loss weight ? u in this section. Our Instant-Teaching method is tested with 10% MS-COCO as labeled data and the remainder as unlabeled data. We first analyze the effect of ? . As shown in <ref type="table" target="#tab_5">Table 5</ref>, we test Instant-Teaching with ? u = 1.0 and ? ? {0.3, 0.5, 0.7, 0.9}. The result shows that the model can achieve better performance by varying the threshold value ? from 0.3 to 0.9, which indicates ? = 0.9 is a better choice to select high-quality pseudo annotations for unlabeled data.</p><p>When analyzing the effect of unsupervised loss weight ? u , we fix ? = 0.9 and vary the value of ? u from 1/4 to 4. As can be seen in <ref type="figure">Fig. 5</ref>, Instant-Teaching achieves the best performance when ? u = 1.0 and the mAP only slightly drop when ? u becomes larger or smaller, which indicates that Instant-Teaching is relatively robust to ? u .</p><p>We can also observe that Instant-Teaching achieves a higher mAP with a smaller value of ? u (e.g., 1/4, 1/2) during the early training iterations. In other words, in the early stages of training, the quality (quantity) of pseudo annotations is low, and the model should pay more attention to the labeled data. In this paper, we use a constant ? u , and take the dynamic adjustment of ? u as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis of co-rectify</head><p>We further propose a co-rectify scheme based on Instant-Teaching to alleviate the confirmation bias problem in SSL, which is shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (Instant-Teaching * ). We analyze the effect of co-rectify using 1% labeled data and the remaining 99% as unlabeled data (1% MS-COCO protocol). The model is trained based on Instant-Teaching with and without our co-rectify scheme respectively. For evaluation, we test on 5k labeled data, which is randomly selected from the 99% unlabeled data of MS-COCO.</p><p>Note that, to verify whether the co-rectify scheme is able to generate more high-quality pseudo annotations, we compare the mAP of predicted pseudo annotations with score larger than 0.9 (same as ? during training). As shown in <ref type="figure">Fig. 6</ref>, we can directly observe that the model trained with co-rectify scheme obtains better performance faster, and is able to consistently improve the performance of our Instant-Teaching along the training iterations. <ref type="figure">Figure 7</ref>. Visualization of predicted pseudo annotations whose confidence scores are larger than 0.9 for unlabeled data. The first row denotes the results of Instant-Teaching (without co-rectify) and the second row denotes the results of Instant-Teaching * .</p><p>In addition, we visualize the pseudo annotations for some unlabeled data in <ref type="figure">Fig. 7</ref>. The results are generated at the same training iteration (120k) with and without the co-rectify scheme respectively. We can observe that Instant-Teaching cooperated with co-rectify scheme can filter out some false predictions and generate more high-quality pseudo annotations at the same time. In summary, the co-rectify scheme is able to alleviate the confirmation bias problem and further improve the performance of Instant-Teaching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we revisit semi-supervised object detection (SSOD) and propose a simple and effective end-to-end SSOD framework -Instant-Teaching, which uses instant pseudo labeling with extended weak-strong data augmentations for teaching during each training iteration. Based on Instant-Teaching, we further propose a co-rectify scheme to alleviate the confirmation bias problem and further improve the performance. Extensive experiments on MS-COCO and PASCAL VOC demonstrate the significant superiority of our method. Although we evaluate with the two-stage detector Faster-RCNN <ref type="bibr" target="#b39">[40]</ref>, our proposed Instant-Teaching * is a general SSOD framework and is not restricted to the object detection models. This means Instant-Teaching * can be directly applied to other detectors, e.g., one-stage detectors SSD <ref type="bibr" target="#b31">[32]</ref> and FCOS <ref type="bibr" target="#b49">[50]</ref>, which we will leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 MS-COCO: Quick</head><p>Training -Batch size: 16.</p><p>-LR decay: [0.01 (?120k), 0.001 (?160k), 0.0001 (?180k)].</p><p>-Data processing: Short edge size is sampled between 500 and 800 if the long edge is less than 1024 after resizing.</p><p>-Batch per image for training Faster-RCNN head: 64.</p><p>Testing -Data processing: Short edge size is fixed to 800 if the long edge is less than 1024 after resizing.</p><p>-Score threshold for testing Faster-RCNN head: 0.001. -Data processing: Short edge size is fixed to 800 if the long edge is less than 1333 after resizing.</p><p>-Batch per image for training Faster-RCNN head: 512.</p><p>Testing -Data processing: Short edge size is fixed to 800 if the long edge is less than 1333 after resizing.</p><p>-Score threshold for testing Faster-RCNN head: 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 PASCAL VOC</head><p>Training -Batch size: 16.</p><p>-LR decay: [0.01 (?120k), 0.001 (?160k), 0.0001 (?180k)].</p><p>-Data processing: Short edge size is fixed to 600 if the long edge is less than 1000 after resizing.</p><p>-Batch per image for training Faster-RCNN head: 256.</p><p>Testing -Data processing: Short edge size is fixed to 600 if the long edge is less than 1000 after resizing.</p><p>-Score threshold for testing Faster-RCNN head: 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Hyperparameters</head><p>In this section, we provide descriptions of the hyperparameters used in our experiments, as shown in Tabel 6. Unless otherwise specified, we use the same hyperparameters in both the MS-COCO and PASCAL VOC experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head><p>Description Value ? The bounding box regression loss weight 1.0 ?u</p><p>The unsupervised loss weight 1.0 ?</p><p>The confidence threshold 0.9 ?m</p><p>The coefficient of Beta distribution 1.0 ?m</p><p>The mixing coefficient of Mixup Beta(?m, ?m) LR</p><p>The initial learning rate 0.01 Momentum</p><p>The momentum used in SGD 0.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight decay</head><p>The weight decay 1e?4 Training steps</p><p>The total training steps 180k Batch size</p><p>The batch size 16 Batch ratio</p><p>The ratio between labeled and unlabeled images in a batch 1:1 <ref type="table">Table 6</ref>. Descriptions of the hyperparameters used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Convergence Analysis</head><p>In this section, we empirically evaluate the convergence of Instant-Teaching * . As shown in <ref type="figure" target="#fig_7">Fig. 8</ref>, we can observe that both the two models trained with our Instant-Teaching * method can reach a steady convergence. These results demonstrate that our Instant-Teaching * can not only achieve state-of-the-art results, it can also be trained easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Ablation Study on Backbone</head><p>In this section, we verify the effect of different backbones on our Instant-Teaching * framework. From Table 7, we replace the ResNet-50 backbone with ResNet-101 and test the efficacy of the supervised baseline, Instant-Teaching, and Instant-Teaching * method on the 2% protocol respectively. We can observe that our Instant-Teaching *  <ref type="table">Table 7</ref>. Comparison of mAP for different semi-supervised methods with different backbones on the 2% MS-COCO protocol. The value in brackets represents the mAP improvement compared to the corresponding supervised model.</p><p>can reach better performance with a more powerful backbone. In other words, it is easy to elevate the performance of our Instant-Teaching * framework by using a more powerful backbone.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The proposed semi-supervised object detection framework. Instant-Teaching includes instant pseudo labeling with extended weak-strong data augmentations. Instant-Teaching * represents Instant-Teaching combined with our co-rectify scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Mixup and Mosaic data augmentations for semisupervised object detection learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Changes in the number of annotations per image during training. N1 refers human-annotated instances and N2 refers total instances including human-annotated and model generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of mAP w.r.t. the size of unlabeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Comparison of mAP with various values of ?u along training iterations. Comparison of mAP of generated pseudo annotations with different training iterations. The model is trained based on Instant-Teaching with and without co-rectify respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>A. 1 . 2 -</head><label>12</label><figDesc>MS-COCO: Standard, [n] ? Training Batch size: 16. -LR decay (1?): [0.01 (?120k), 0.001 (?160k), 0.0001 (?180k)]. -LR decay (2?): [0.01 (?240k), 0.001 (?320k), 0.0001 (?360k)]. -LR decay (3?): [0.01 (?420k), 0.001 (?500k), 0.0001 (?540k)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Value change of loss w.r.t. training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>FPN 10.20?0.15 (+1.15) 13.60?0.10 (+0.90) 18.90?0.10 (+0.43) 24.50?0.15 (+0.64) 38.87 (+1.24) STAC[45] R50-FPN 13.97?0.35 (+4.92) 18.25?0.25 (+5.55) 24.38?0.12 (+5.91) 28.64?0.21 (+4.78) 39.21 (+1.58) Instant-Teaching (ours) R50-FPN 16.00?0.20 (+6.95) 20.70?0.30 (+8.00) 25.50?0.05 (+7.03) 29.45?0.15 (+5.59) 39.60 (+1.97) Instant-Teaching * (ours) R50-FPN 18.05?0.15 (+9.00) 22.45?0.15 (+9.75) 26.75?0.05 (+8.28) 30.40?0.05 (+6.54) 40.20 (+2.57) Comparison of mAP for different semi-supervised methods on MS-COCO. CSD ? is our implementation of the CSD method based on the Faster-RCNN detector. Instant-Teaching * represents our Instant-Teaching framework with co-rectify scheme. The value in brackets represents the mAP improvement compared to the supervised model.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Backbone</cell><cell cols="2">1% COCO</cell><cell>2% COCO</cell><cell>5% COCO</cell><cell>10% COCO</cell><cell>100% COCO</cell></row><row><cell cols="2">Supervised</cell><cell cols="2">R50-FPN</cell><cell cols="2">9.05?0.16</cell><cell>12.70?0.15</cell><cell>18.47?0.22</cell><cell>23.86?0.81</cell><cell>37.63</cell></row><row><cell cols="3">CSD  ? [22] R50-Methods Backbone Unlabeled</cell><cell>AP 0.5:0.95</cell><cell>AP 0.5</cell><cell>AP 0.75</cell><cell></cell></row><row><cell>Supervised (Ours)</cell><cell>R50-FPN</cell><cell></cell><cell>43.60</cell><cell>76.70</cell><cell>44.50</cell><cell></cell></row><row><cell>CSD [22]</cell><cell>R101-R-FCN</cell><cell></cell><cell>-</cell><cell>74.70</cell><cell>-</cell><cell></cell></row><row><cell>STAC [45] Instant-Teaching</cell><cell>R50-FPN R50-FPN</cell><cell>VOC12</cell><cell cols="2">44.64 (+1.04) 77.45 48.70 (+5.10) 78.30</cell><cell>-52.00 (+7.50)</cell><cell></cell></row><row><cell>Instant-Teaching  *</cell><cell>R50-FPN</cell><cell></cell><cell cols="2">50.00 (+6.40) 79.20</cell><cell>54.00 (+9.50)</cell><cell></cell></row><row><cell>CSD [22]</cell><cell>R101-R-FCN</cell><cell>VOC12</cell><cell>-</cell><cell>75.10</cell><cell>-</cell><cell></cell></row><row><cell>STAC [45] Instant-Teaching</cell><cell>R50-FPN R50-FPN</cell><cell>&amp;</cell><cell cols="2">46.01 (+2.41) 79.08 49.70 (+6.10) 79.00</cell><cell>-54.10 (+9.60)</cell><cell></cell></row><row><cell>Instant-Teaching  *</cell><cell>R50-FPN</cell><cell>COCO</cell><cell cols="3">50.80 (+7.20) 79.90 55.70 (+11.20)</cell><cell></cell></row><row><cell cols="7">Table 2. Comparison of mAP for different semi-supervised meth-</cell></row><row><cell cols="7">ods on VOC07. We report the mAP at IoU=0.50:0.95 (AP 0.5:0.95 ),</cell></row><row><cell cols="7">IoU=0.5 (AP 0.5 ) and IoU=0.75 (AP 0.75 ), which are the standard</cell></row><row><cell cols="3">metrics for object detection [31, 7].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>our Instant-Teaching * outperforms state-of-the-art methods by a large margin under all experimental settings of the MS-COCO dataset. Specifically, for the 1% protocol, Instant-Teaching * improves mAP from STAC's 13.97 to 18.05, which achieves 4.08 mAP improvement; for the 2% protocol, Instant-Teaching * improves mAP from STAC's 18.25 to 22.45, which achieves 4.2 mAP improvement. Instant-Teaching * also brings significant improvement in mAP when there are more labeled data: 24.38 to 26.75 on the 5% protocol, 28.64 to 30.40 on the 10% protocol. For the 100% protocol, our Instant-Teaching * still achieves about 1.0 mAP improvement under the high benchmark of 39.21 mAP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Comparison of mAP of Instant-Teaching trained with var- ious scales of unlabeled data on MS-COCO. [n]? denotes the scale of unlabeled data is [n] times larger than that of labeled data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison of mAP with various values of confidence threshold ? .</figDesc><table><row><cell>worth mentioning that, for both 5% and 10% labeled data,</cell></row><row><cell>our Instant-Teaching method trained on 1? unlabeled data</cell></row><row><cell>achieves 23.60 and 28.80 mAP respectively, which are even</cell></row><row><cell>higher than STAC trained on 8? unlabeled data (23.14 and</cell></row><row><cell>27.95). This demonstrates that Instant-Teaching can effi-</cell></row><row><cell>ciently leverage the unlabeled data.</cell></row><row><cell>From</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Learning Schedules</head><p>We provide more details on different learning schedules used in our experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on computational learning theory</title>
		<meeting>the eleventh annual conference on computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised learning. adaptive computation and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">1</biblScope>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Cited in page (s)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Note-rcnn: Noise tolerant ensemble rcnn for semisupervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9508" to="9517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondrej Chum</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Interpolation-based semisupervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsung</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02158</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving object detection with selective self-supervised self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Watch and learn: Semi-supervised learning for object detectors from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3593" to="3602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omnisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive patternrecognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS), 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Proposal learning for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05086</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
