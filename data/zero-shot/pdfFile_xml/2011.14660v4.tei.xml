<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liguang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Deng</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><surname>Lun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Lam</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Yangsheng</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-image classification</term>
					<term>divide networks</term>
					<term>co- training</term>
					<term>deep networks ensemble</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The width of a neural network matters since increasing the width will necessarily increase the model capacity. However, the performance of a network does not improve linearly with the width and soon gets saturated. In this case, we argue that increasing the number of networks (ensemble) can achieve better accuracy-efficiency trade-offs than purely increasing the width. To prove it, one large network is divided into several small ones regarding its parameters and regularization components. Each of these small networks has a fraction of the original one's parameters. We then train these small networks together and make them see various views of the same data to increase their diversity. During this co-training process, networks can also learn from each other. As a result, small networks can achieve better ensemble performance than the large one with few or no extra parameters or FLOPs, i.e., achieving better accuracy-efficiency trade-offs. Small networks can also achieve faster inference speed than the large one by concurrent running. All of the above shows that the number of networks is a new dimension of model scaling. We validate our argument with 8 different neural architectures on common benchmarks through extensive experiments. The code is available at https: //github.com/FreeformRobotics/Divide-and-Co-training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I NCREASING the width of neural networks to pursue better performance is common sense in network engineering <ref type="bibr" target="#b2">[1]</ref>- <ref type="bibr" target="#b4">[3]</ref>. However, the performance of networks does not improve linearly with the width. As shown in <ref type="figure">Figure 1</ref>, in the beginning, increasing width can gain promising improvement in accuracy; at a later stage, the improvement becomes slight and no longer matches the increasingly expensive cost. For example, EfficientNet baseline (w = 5.0, width factor) gains less than +0.5% accuracy improvement compared to EfficientNet baseline (w = 3.8) with nearly doubled FLOPs (floating-point operations). We call this the width saturation of a network. Increasing depth or resolution produces similar phenomena <ref type="bibr" target="#b2">[1]</ref>.</p><p>Besides the width saturation, we also observe that relatively small networks achieve close accuracies to very wide net-   <ref type="figure">Fig. 1</ref>. The width saturation of ResNeXt <ref type="bibr" target="#b4">[3]</ref> and EfficientNet <ref type="bibr" target="#b2">[1]</ref>. The gain does not match the expensive extra cost when the width (w) is large. works, i.e., ResNet-29 (w = 3.0 v.s. w = 5.0) and EfficientNet baseline (w = 2.6 v.s. w = 5.0) in <ref type="figure">Figure 1</ref>. In this case, an interesting question arises, can two small networks with a half width of a large one achieve or even surpass the performance of the latter? Firstly, ensemble is a practical technique and can improve the generalization performance of individual neural networks <ref type="bibr" target="#b5">[4]</ref>- <ref type="bibr" target="#b9">[8]</ref>. Kondratyuk et al. <ref type="bibr" target="#b10">[9]</ref> already demonstrate that the ensemble of several smaller networks is more efficient than a large model in some cases. Secondly, multiple networks can collaborate with their peers and learning from each other during training to achieve better individual or ensemble performance. This is verified by some deep mutual learning <ref type="bibr" target="#b11">[10]</ref>- <ref type="bibr" target="#b13">[12]</ref> and co-training <ref type="bibr" target="#b14">[13]</ref> works.</p><p>Based on the above analysis, we argue that increasing the number of networks (ensemble) can achieve better accuracyefficiency trade-offs than purely increasing the width. A straightforward demonstration is given in this work: we divide one large network into several pieces and show that these small networks can achieve better ensemble performance than the large one with almost the same computation costs.</p><p>The overall framework is shown in <ref type="figure">Figure 2</ref>. 1) dividing: Given one large network, we first divide the network according to its width, more precisely, the parameters or FLOPs of the network. For instance, if we want to divide a network into two small networks, the number of the small one's parameters will become half of the original. During this division process, the regularization components will also be changed as the model capacity degrades. <ref type="bibr">Particularly</ref> decay and drop layers will be divided accordingly with the network width. 2) co-training: After dividing, small networks are trained with different views of the same data <ref type="bibr" target="#b14">[13]</ref> to increase the diversity of networks and achieve better ensemble performance <ref type="bibr" target="#b6">[5]</ref>. This is implemented by applying different data augmentation transformers in practice. At the same time, small networks can also learn from each other <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b13">[12]</ref>, <ref type="bibr" target="#b14">[13]</ref> to further boost individual performance. Thus we add Jensen-Shannon divergence among all predictions, i.e., co-training loss in <ref type="figure">Figure 2</ref>. In this way, one network can learn valuable knowledge about intrinsic object structure information from predicted posterior probability distributions of its peers <ref type="bibr" target="#b15">[14]</ref>.</p><p>3) concurrent running: Small networks can also achieve faster inference speed than the original one through concurrent running on different devices when resources are sufficient. Some different networks and their average latency (inference speed) on ImageNet <ref type="bibr" target="#b16">[15]</ref> are shown in <ref type="figure">Figure 3</ref>.</p><p>We conduct extensive experiments with dividing and cotraining strategies for 8 different networks. Generally, small networks after dividing accomplish better ensemble performance than the original big network with similar parameters and FLOPs, i.e., better accuracy-efficiency trade-offs are achieved. We also reach competitive accuracy with stateof-the-art methods, specifically, 98.31% on CIFAR-10 <ref type="bibr" target="#b17">[16]</ref>, 89.46% on CIFAR-100, and 83.60% on ImageNet <ref type="bibr" target="#b16">[15]</ref>. Furthermore, we validate our method for object detection and demonstrate the generality of dividing and ensemble. All evidence suggests that people should consider the number of networks as a meaningful dimension of network engineering besides commonly known width, depth, and resolution.</p><p>Our contributions are summarized as follows:</p><p>? We post the claim that increasing the number of networks can achieve better accuracy-efficiency trade-offs than purely increasing the network width. ? We propose novel dividing strategies for 8 different networks regarding their parameters and regularizations to achieve better performance with similar costs. ? Practical co-training techniques are developed to help small networks learn from their peers. ? We provide best practices and instructive conclusions for dividing and co-training via extensive experiments and thorough ablations on classification and object detection.  II. RELATED WORKS Neural network architecture design: Since the success of AlexNet <ref type="bibr" target="#b19">[18]</ref> on ImageNet <ref type="bibr" target="#b16">[15]</ref>, deep learning methods dominate the field of computer vision, and network engineering becomes a core topic. Many excellent architectures emerged, e.g., NIN <ref type="bibr" target="#b20">[19]</ref>, VGG <ref type="bibr" target="#b21">[20]</ref>, Inception <ref type="bibr" target="#b23">[21]</ref>, ResNet <ref type="bibr" target="#b24">[22]</ref>, Xception <ref type="bibr" target="#b25">[23]</ref>. They explored different ways to design an effective and efficient model, e.g., 1 ? 1 convolution kernels, stacked convolution layers with small kernels, combination of different convolution and pooling operations, residual connection, depthwise separable convolution. In recent years, neural architecture search (NAS) becomes more and more popular. People hope to automatically learn or search for the best neural architectures for certain tasks with machine learning methods. We name a few here, reinforcement learning based NAS <ref type="bibr" target="#b26">[24]</ref>, progressive neural architecture search (PNASNet <ref type="bibr" target="#b27">[25]</ref>), differentiable architecture search (DARTS <ref type="bibr" target="#b28">[26]</ref>), etc. Implicit ensemble methods: Ensemble methods use multiple learning algorithms to obtain better performance than any of them. Some layer splitting methods <ref type="bibr" target="#b4">[3]</ref>, <ref type="bibr" target="#b23">[21]</ref>, <ref type="bibr" target="#b29">[27]</ref> adopt an implicitly "divide and ensemble" strategy, namely, they divide a single layer in a model and then fuse their outputs to get better performance. Dropout <ref type="bibr" target="#b30">[28]</ref> can also be interpreted as an implicit ensemble of multiple sub-networks within one full network. Slimmable Network <ref type="bibr" target="#b31">[29]</ref>, <ref type="bibr" target="#b32">[30]</ref> derives several networks with different widths from a full network and trains them in a parameter-sharing way to achieve adaptive accuracyefficiency trade-offs at runtime. MutualNet <ref type="bibr" target="#b12">[11]</ref> further trains these sub-networks mutually to make the full network achieve better performance. These methods get several dependent models after implicitly splitting, while our methods obtain independent models in terms of parameters after dividing. They are also compatible with our methods and can be applied to any single model in our system.</p><p>Collaborative learning: Collaborative learning refers to a variety of educational approaches involving the joint intellectual effort by students, or students and teachers together <ref type="bibr" target="#b33">[31]</ref>. It was formally introduced in deep learning by <ref type="bibr" target="#b34">[32]</ref>, which was used to describe the simultaneous training of multiple classifier heads of a network. Actually, according to its original definition, many works involving two or more models learning together can also be called collaborative learning, e.g., DML <ref type="bibr" target="#b13">[12]</ref>, co-training <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b35">[33]</ref>, mutual meanteaching (MMT) <ref type="bibr" target="#b11">[10]</ref>, cooperative learning <ref type="bibr" target="#b36">[34]</ref>, knowledge distillation <ref type="bibr" target="#b15">[14]</ref>. Their core idea is similar, i.e., enhancing the performance of one or all models by training them with some peers or teachers. They inspire our co-training algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>First of all, we recall the common framework of deep image classification. Given a neural model M and N training samples X = {x n } N n=1 from C classes, the objective is cross entropy:</p><formula xml:id="formula_0">L ce (p, y) = ? 1 N N n=1 C c=1 y n,c log(p n,c ),<label>(1)</label></formula><p>where y ? {0, 1} is the ground truth label and p ? [0, 1] is the softmax normalized probability given by M .</p><p>A. Division a) Parameters: In <ref type="figure">Figure 2</ref>, we divide one large network M into S small networks {M 1 , M 2 , . . . , M S }. The principle is keeping the metrics -the number of parameters or FLOPs roughly unchanged before and after dividing. M is usually a stack of convolutional (conv.) layers. Following the definition in PyTorch <ref type="bibr" target="#b37">[35]</ref>, its kernel size is K ?K, numbers of channels of input and output feature maps are C in and C out , and the number of groups is d, which means every Cin d input channels are convolved with its own sets of filters, of size Cout d . In this case, the number of parameters and FLOPs are:</p><formula xml:id="formula_1">Params : K 2 ? C in d ? C out d ? d,<label>(2)</label></formula><p>FLOPs :</p><formula xml:id="formula_2">(2 ? K 2 ? C in d ? 1) ? H ? W ? C out ,<label>(3)</label></formula><p>where H ? W is the size of the output feature map and ?1 occurs because the addition of Cin d ? K 2 numbers only needs ( Cin d ? K 2 ? 1) times operations. The bias is omitted for the sake of brevity. For depthwise convolution <ref type="bibr" target="#b38">[36]</ref>, d = C in .</p><p>Generally, C out = t 1 ? C in , where t 1 is a constant. Therefore, if we want to divide a conv. layer by a factor S, we just need to divide C in by ? S:</p><formula xml:id="formula_3">K 2 ? C in ? C out S ? 1 d = K 2 ? t 1 ? ( C in ? S ) 2 ? 1 d .<label>(4)</label></formula><p>For instance, if we divide a bottleneck . Each small block has a quarter of the parameters or FLOPs of the original block.</p><p>In practice, the numbers of output channels have the greatest common divisor (GCD). The GCD of most ResNet variants <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b4">[3]</ref>, <ref type="bibr" target="#b24">[22]</ref>, <ref type="bibr" target="#b39">[37]</ref> is the C out of the first convolutional layer. For other networks, like EfficientNet <ref type="bibr" target="#b2">[1]</ref>, their GCD is a multiple of 8 or some other numbers. In general, when we want to divide a network by S, we just need to find its GCD, and replace it with GCD/ ? S, then it is done. For networks mainly consisted of group convolutions like ResNeXt <ref type="bibr" target="#b4">[3]</ref>, we keep Cin d fixed, i.e., C in = t 2 ? d, where t 2 is a constant. Namely, the number of channels per group is unchanged during dividing, and the number of groups will be divided. We substitute the C in in Eq. (4) with the above equation and get:</p><formula xml:id="formula_4">K 2 ? t 1 ? t 2 2 ? d S .<label>(5)</label></formula><p>Then the division can be easily achieved by dividing d by S. This way is more concise than the square root operations. For networks that have a constant global factor linearly related to the channel number, we simply divide the factor by ? S, e.g., the widen factor of WRN <ref type="bibr" target="#b3">[2]</ref>, the growth rate of DenseNet <ref type="bibr" target="#b40">[38]</ref>, and the additional rate of PyramidNet <ref type="bibr" target="#b41">[39]</ref>. b) Regularization: After dividing, the model capacity degrades and the regularization components in networks should change accordingly. Under the assumption that the model capacity is linearly dependent with the network width, we change the magnitude of dropping regularization linearly. Specifically, the dropping probabilities of dropout <ref type="bibr" target="#b30">[28]</ref>, Shake-Drop <ref type="bibr" target="#b42">[40]</ref>, and stochastic depth <ref type="bibr" target="#b43">[41]</ref> are divided by ? S in our experiments. As for weight decay <ref type="bibr" target="#b44">[42]</ref>, it is a little complex as its intrinsic mechanism is still vague <ref type="bibr" target="#b45">[43]</ref>, <ref type="bibr" target="#b46">[44]</ref>. We test some dividing manners and adopt two dividing strategiesno dividing and exponential dividing in this paper:</p><formula xml:id="formula_5">wd = wd ? exp( 1 S ? 1.0),<label>(6)</label></formula><p>where wd is the original weight decay value and wd is the new value after dividing. No dividing means the weight decay value keeps unchanged. The above two dividing strategies are empirical criteria. In practice, the best way now is trial and error. Besides, we adopt exponential dividing rather than directly dividing the wd by ? S because the latter may lead to too small weight decay values to maintain the generalization performance when S is large.</p><p>The above dividing mechanism is usually not perfect, i.e., the number of parameters of M i may not be exactly 1/S of M . Firstly, ? S may not be an integer. In this case, we round C in / ? S in Eq. (4) to a nearby even number. Secondly, the division of the first layer and the last fully-connected layer is not perfect because the input channel (color channels) and output channel (number of object classes) are fixed. c) Concurrent running: Small networks can also achieve faster inference speed than the large network by concurrent running on different devices as shown in <ref type="figure">Figure 2</ref>. Typically, a device is an NVIDIA GPU. Theoretically, if one GPU has enough processing units, e.g., streaming multiprocessor, small networks can also run concurrently within one GPU with multiple CUDA Streams <ref type="bibr" target="#b47">[45]</ref>. However, we find this is impractical on a Tesla V100 GPU in experiments. One small network is already able to occupy most of the computational resources, and different networks can only run in sequence. Therefore, we only discuss the multiple devices fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Co-training</head><p>The generalization error of neural network ensemble (NNE) can be interpreted as Bias-Variance-Covariance Trade-off <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b8">[7]</ref>. Let f be the function model learned, g(x) be the target function, and f en (</p><formula xml:id="formula_6">x; X ) = 1 S S i=1 f i (x; X ).</formula><p>Then the expected mean-squared ensemble error is:</p><formula xml:id="formula_7">EX [ fen(x; X ) ? g(x) 2 ] = EX [fen(x) ? g(x)] 2 + EX 1 S 2 S i=1 fi(x; X ) ? EX [fi(x; X )] 2 + EX 1 S 2 S i=1 S j =i fi(x; X ) ? EX [fi(x; X )] ? fj(x; X ) ? EX [fj(x; X )] ,<label>(7)</label></formula><p>where the first term is the square bias (Bias 2 ) of the combined system, the second and third terms are the variance (Var) and covariance (Cov) of the outputs of individual networks. A clean form is</p><formula xml:id="formula_8">E[ 1 S Var +(1 ? 1 S ) Cov + Bias 2 ]</formula><p>. Data noise is omitted here. The detailed proof is given by Ueda et al. <ref type="bibr" target="#b48">[46]</ref>. a) Different initialization and data views: Increasing the diversity of networks without increasing Var or Bias can decrease the correlation (Cov) between networks. To this end, small networks are initialized with different weights <ref type="bibr" target="#b6">[5]</ref>. Then, when feeding the training data, we apply different data transformers D i on the same data for different networks as shown in <ref type="figure">Figure 2</ref>. In this way, {M 1 , M 2 , . . . , M S } are trained on different views {D 1 (x), D 2 (x), . . . , D S (x)} of x. In practice, different data views are generated by the randomness of data augmentation. Besides the commonly used random resize, random crop, and random flip, we introduce random erasing <ref type="bibr" target="#b49">[47]</ref> and AutoAugment <ref type="bibr" target="#b50">[48]</ref> policies. AutoAugment has 14 image transformation operations, e.g., shear, translate, rotate, and auto contrast. It searches tens of different policies which are consisted of two operations and randomly chooses one policy during the data augmentation process. By applying these random data augmentations, we can guarantee that D i (x) produces different views of x across multiple runs. b) Co-training loss: Knowledge distillation and DML <ref type="bibr" target="#b13">[12]</ref> show that one network can boost its performance by learning from a teacher or cohorts. Namely, a deep ensemble system can reduce its Bias in this way. Besides, following the co-training assumption <ref type="bibr" target="#b35">[33]</ref>, small networks are expected to have consistent predictions on x although they see different views of x. From the perspective of Eq. <ref type="formula" target="#formula_7">(7)</ref>, this can reduce the variance of networks and avoid poor performance caused by overly decorrelated networks <ref type="bibr" target="#b8">[7]</ref>. Therefore, we adopt Jensen-Shannon (JS) divergence among predicted probabilities as the co-training objective <ref type="bibr" target="#b14">[13]</ref> :</p><formula xml:id="formula_9">L cot (p 1 , p 2 , . . . , p S ) = H( 1 S S i=1 p i ) ? 1 S S i=1 H(p i ), (8)</formula><p>where p i is the estimated probability of M i , and H(p) = E[? log(p)] is the Shannon entropy of the distribution of p. Through this co-training manner, one network can learn valuable information from its peers, which defines a rich similarity structure over objects. For example, a model classifies an object as Chihuahua may also give high confidence about Japanese spaniel since they are both dogs <ref type="bibr" target="#b15">[14]</ref>. DML uses the Kullback-Leibler (KL) divergence between predictions of every two networks. Their purpose is similar to ours. The overall objective function is a combination of the classification losses of small networks and the co-training loss: <ref type="bibr" target="#b10">(9)</ref> where ? cot = 0.5 is a weight factor of L cot (?) and it is chosen by cross-validation. At the early stage of training, the outputs of networks are full of randomness, so we adopt a warm-up scheme for ? cot <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b51">[49]</ref>. Specifically, we use a linear scaling up strategy when the current training epoch is less than a certain number -40 and 60 for CIFAR and ImageNet, respectively. ? cot = 0.5 is also an equilibrium point between learning diverse networks and producing consistent predictions. During inference, we average the outputs before softmax layers as the final ensemble output.</p><formula xml:id="formula_10">L all = S i=1 L ce (p i , y) + ? cot L cot (p 1 , p 2 , . . . , p S ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT A. Experimental setup</head><p>Datasets We adopt CIFAR-10, CIFAR-100 <ref type="bibr" target="#b17">[16]</ref>, and Ima-geNet 2012 <ref type="bibr" target="#b16">[15]</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight decay</head><p>Generally, weight decay is 1e-4. For {EfficientNet-B3, ResNeXt-29, WRN-28-10, WRN-40-10} on CIFAR datasets, it is 5e-4. Bias and parameters of batch normalization <ref type="bibr" target="#b58">[56]</ref> are left undecayed.</p><p>Besides, we use kaiming weight initialization <ref type="bibr" target="#b59">[57]</ref>. The optimizer is nesterov <ref type="bibr" target="#b60">[58]</ref> accelerated SGD with a momentum 0.9. ResNet variants adopt the modifications introduced in [51], i.e., replacing the first 7 ? 7 conv. with three consecutive 3 ? 3 conv. and put an 2?2 average pooling layer before 1?1 conv. when there is a need to downsample. The code is implemented in PyTorch <ref type="bibr" target="#b37">[35]</ref>. Influence of some settings is shown in <ref type="table" target="#tab_4">Table I.</ref> B. Results on CIFAR and ImageNet dataset 1) CIFAR-100: In <ref type="table" target="#tab_4">Table II</ref>, dividing and co-training achieve consistent improvements with similar or even fewer parameters or FLOPs. Additional cost occurs because the division of a network is not perfect, as mentioned in Sec. III-A. Three conclusions can be drawn from these results 7 networks.</p><p>Conclusion 1: Increasing the number, width, and depth of networks together is more efficient and effective than purely increasing the width or depth.</p><p>For all networks in <ref type="table" target="#tab_4">Table II</ref>, dividing and co-training gain promising improvement. We also notice, ResNet-110 (S = 2) &gt; ResNet-164, SE-ResNet-110 (S = 2) &gt; SE-ResNet-164, EfficientNet-B0 (S = 4) &gt; EfficientNet-B3, and WRN-28-10 (S = 4) &gt; WRN-40-10 with fewer parameters, where &gt; means the former has better performance. By contrast, the latter is deeper or wider. Besides, with wider or deeper networks, dividing and co-training can gain more improvement, e.g., ResNet-110 (+0.64) vs. ResNet-164 (+1.26), SE-ResNet-110 (+0.54) vs. SE-ResNet-164 (+1.06), WRN-28-10 (+1.24) vs. WRN-40-10 (+1.60), and EfficientNet-B0 (+0.65) vs. EfficientNet-B3 (+1.48). All of the above demonstrates the superiority of increasing the number of networks. It is also clear that increasing the number, width, and depth of networks together is a better choice than purely scaling up one single dimension during model engineering.</p><p>Conclusion 2: The ensemble performance is closely related to individual performance. The relationship between the average accuracy and ensemble accuracy is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. When calculating the average accuracy, we separately calculate the accuracies of small networks and average them. From the big picture, the average accuracy is positively correlated with the ensemble accuracy. The higher the average accuracy, the better the ensemble performance. This coincides with the Eq. (7) because the stronger the individual networks the smaller the Bias.</p><p>At the same time, we note that there is a big gap between the average accuracy and ensemble accuracy. The ensemble accuracy is higher than the average accuracy by a large margin. This gap may be filled by two factors according to Eq. (7): the decayed variance of individual networks and learned diverse networks during the co-training process.</p><p>Conclusion 3: A necessary width/depth of networks matters.</p><p>In <ref type="table" target="#tab_4">Table II</ref>, ResNet-110 (S = 4) and SE-ResNet-110 (S = 4) get a drop in performance, 0.63%? and 0.42%?, respectively. In <ref type="figure" target="#fig_3">Figure 4</ref>, these two networks obtain the first two lowest average accuracies. If we take one step further to look at the architecture of these small networks, we will find they have input channels <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b34">32]</ref> at the first layer of their three blocks. These networks are too thin compared to the original one with [16, 32, 64] channels. In this case, the magnitude of the decayed variance of individual networks is smaller than the magnitude of gained accuracy (decayed bias) from increasing the channels (width) of a single network. Consequently, individual networks cannot gain satisfying accuracy and lead to inferior ensemble performance as conclusion 2 said. This demonstrates that a necessary width is essential. The conclusion also applies to PyramidNet-272, which gets a relatively slight improvement with dividing and co-training as its base channel is small, i.e., 16. Increasing the width of networks is still effective when the width is very small.</p><p>In <ref type="table" target="#tab_4">Table II</ref>, ResNet-164 (S = 4) and SE-ResNet-164 (S = 4) still get improvements, although their performance is not as good as applying (S = 2). In <ref type="figure" target="#fig_3">Figure 4</ref>, small networks of ResNet-164 and SE-ResNet-164 with (S = 4) also get better performance than ResNet-110 and SE-ResNet-110 with (S = 4), respectively. This reveals that a necessary depth can help guarantee the model capacity and achieve better ensemble performance. In a word, after dividing, the small networks should maintain a necessary width or depth to guarantee their model capacity and achieve satisfying ensemble performance. This is consistent with some previous works. Kondratyuk et al. <ref type="bibr" target="#b10">[9]</ref> show that the ensemble of some small models with limited depths and widths cannot achieve promising ensemble performance compared to some large models.</p><p>From conclusion 1 -3, we can learn some best practices about effective model scaling. When the width/depth of networks is small, increasing width/depth can still get substantial improvement. However, when width or depth becomes large and increasing them yields little gain <ref type="figure">(Figure 1)</ref>, it is more effective to increase the number of networks. This comes to our core conclusion -Increasing the number, width, and depth of networks together is more efficient and effective than purely scaling up one dimension of them.</p><p>2) CIFAR-10: Results on the CIFAR-10 are shown in <ref type="table" target="#tab_4">Table III</ref>. Although the Top-1 accuracy on CIFAR-10 is already very high, dividing and co-training can also achieve significant improvement. It is worth noting {WRN-28-10, epoch 1800} gets worse performance than {WRN-28-10, epoch 300}, namely, WRN-28-10 may overfit on CIFAR-10 when trained for more epochs. In contrast, dividing and co-training can help WRN-28-10 get consistent improvement with increasing training epochs. This is because we can learn diverse networks from the data. In this way, even if one model overfits, the ensemble of different models can also make a comprehensive and correct prediction. From the perspective of Eq. <ref type="formula" target="#formula_7">(7)</ref>, diverse networks reduce the Var or achieve a negative Cov. The conclusion also applies to {WRN-40-10, 300 epochs} and {WRN-40-10, 1800 epochs} on CIFAR-100. This shows ensembles of neural networks are not only more accurate but also more robust than individual networks.</p><p>3) ImageNet: Results on ImageNet are shown in <ref type="table" target="#tab_4">Table IV</ref>. All experiments on ImageNet are conducted with mixed precision <ref type="bibr" target="#b18">[17]</ref>. WRN-50-2 and WRN-50-3 are 2? and 3? wide as ResNet-50, respectively. The results on ImageNet validate our argument again -Increasing the number, width, and depth of networks together is more efficient and effective than purely scaling one dimension of width, depth, and resolution. Specifically, EfficientNet-B7 (84.4%, 66M, crop 600, wider, deeper) vs.EfficientNet-B6 (84.2%, 43M, crop 528), WRN-50-3 (80.74%, 135.0M) vs.WRN-50-2 (80.66%, 68.9M), the former only produces 0.2%? and 0.08%? gain of accuracy, respectively. However, the cost is nearly doubled. This shows that increasing width or depth can only yield little gain when the model is already very large, while it brings out an expensive extra cost. In contrast, increasing the number of networks rewards with much more tangible improvement.</p><p>C. Discussion a) influence of dividing regularization terms: The influence of dividing the regularization components (weight decay and dropping layers) is shown in <ref type="table">Table V</ref>. The results partially support our assumption: small models generally need less regularization. There are also some counterexamples: dividing the weight decay of WRN-16-8 does not work. Possibly 1e-4 is a small number for WRN-16-8 on CIFAR-100, and it should not be further divided. It is worth noting that 5e-4 and 1e-4 are already appropriate weight decay values selected by cross-validation for WRN-28-10 and WRN-16-8, respectively. b) different ensemble methods: Besides the averaging ensemble manner, we also test max ensemble, i.e., use the most confident prediction of small networks as the final prediction, and the geometric mean of model predictions <ref type="bibr" target="#b10">[9]</ref>: Results are shown in <ref type="table" target="#tab_4">Table VI</ref>. Simple averaging is the most effective way among the test methods in most cases. c) influences of different co-training components: The influences of dividing and ensemble, different data views, and various values of weight factor ? cot of co-training loss in Eq. (9) are shown in <ref type="table" target="#tab_4">Table VII</ref>. The contribution of dividing and ensemble is the most significant, i.e., 1.10%? at best. Using different data transformers (0.30%?) and co-training loss (0.41%?) can also help the model improve performance. Besides, the effect of co-training is more significant when there are more networks, i.e., 0.18%? (S = 2) vs. 0.41%? (S = 4). Considering the baseline is very strong (see <ref type="table" target="#tab_4">Table I</ref>), their improvement is also significant.</p><formula xml:id="formula_11">p = S i p i 1 S .<label>(10)</label></formula><p>We do not divide the large network into 8, 16, or more small networks. Firstly, the large the S, the thinner the small networks. As conclusion 3 in Sec. CIFAR-100 IV-B1 Theoretically, one can abandon the co-training part to achieve an easy one with some sacrifice of performance.</p><p>In this work, we just use a simple co-training method and make no further exploration in this direction. There do exist some other more complex co-training or mutual learning methods. For example, MutualNet <ref type="bibr" target="#b12">[11]</ref> derives several networks with various widths from a single full network, feeds them images at different resolutions, and trains them together in a weight-sharing way to boost the performance of the full  network. We mainly focus on validating the effectiveness and efficiency of increasing the number of networks, more complex ensemble and co-training methods are left as future topics. d) runtime and correlation of small networks after dividing: Runtime and cosine similarity of outputs of different networks are shown in <ref type="table" target="#tab_4">Table VIII</ref>. Corresponding accuracy and FLOPs of these networks can be found in <ref type="table" target="#tab_4">Table II</ref>. For big networks, concurrent running (small networks run in parallel) can get obvious speedup at runtime compared to sequential running. For small networks, additional data loading, data pre-processing (still runs in sequence) and data transferring (CPU?GPU, GPU?GPU) will occupy most of the time. The runtime is also related to the intrinsic structure of networks (e.g., depthwise convolution) and the runtime implementation of the code framework. The speedup of concurrent running with different networks on ImageNet is shown in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>In <ref type="table" target="#tab_4">Table VIII</ref>, we also show the average cosine similarity of outputs of different small networks. There is no obvious relationship between the similarity of outputs and the gain for different networks. For example, (ResNeXt-19, S = 4) has similarity 0.89 and gain 1.55% in accuracy, while (EfficientNet-B0, S = 4) has similarity 0.79 and gain 0.65% in accuracy. (EfficientNet-B3, S = 4) with similarity 0.85 also has a larger gain 1.48% than (EfficientNet-B0, S = 4). Lower similarity scores do not always mean more gains in accuracy. e) Ensemble of ensemble models: The ensemble of divided networks can also achieve better accuracy-efficiency trade-offs (better performance with roughly the same parameters) than the original single model. The testing results are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. For Top-1 accuracy, the ensemble of ensemble models obtains better performance than single models when the number of networks is relatively small. Then the former also reaches the saturation point (plateau) faster than the latter. As for top-5 accuracy, the ensemble of ensemble models always achieves higher accuracy than single models. This shows the robustness of the ensemble of   ensemble models. Like other dimensions of model scaling, i.e., width, depth, and resolution, increasing the number of networks will also saturate. Otherwise, we will get a perfect model by increasing the number of networks; it is not practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DIVIDING FOR OBJECT DETECTION</head><p>We discuss the dividing and co-training strategies for image classification models in the above context. From the experiments on image classification, we can see that dividing and ensemble contributes most to help the system achieve better performance with similar computation costs. In this section, we will explore how dividing and ensemble work on another fundamental computer vision task -object detection.</p><p>The overall architecture of our detection system is shown in <ref type="figure">Figure 7</ref>. Without loss of generality, we choose SSD <ref type="bibr" target="#b67">[65]</ref> as the object detector and replace its original backbone (feature extractor) -VGG16 <ref type="bibr" target="#b21">[20]</ref> with models in <ref type="table">Table.</ref> IV. We then divide the backbone into S small networks, typically, S = 2. For simplicity and compatibility with the bounding box regression process of SSD, the detection predictorsextra feature layers and classifiers in SSD share the same weights. After Non-Maximum Suppression (NMS), we obtain the predicted bounding boxes and their associated confidence scores. Furthermore, these predictions from different small detection models are composed to get the final predicted bounding boxes and corresponding confidence scores. To fully  <ref type="figure">Fig. 7</ref>. The overall architecture of the detection system with dividing. The two different backbones are obtained from dividing a large network. utilize the information provided by different detection models, the ensemble method we adopt here is weighted boxes fusion (WBF) <ref type="bibr" target="#b68">[66]</ref>. Unlike NMS-based methods <ref type="bibr" target="#b69">[67]</ref>, <ref type="bibr" target="#b70">[68]</ref> which will simply abandon part of the predictions, WBF utilizes all predicted boxes of a certain object from different models to construct a more accurate average box based on the confidence scores of boxes. Such dividing and ensemble methods can be easily extended to other object detectors like YOLO <ref type="bibr" target="#b71">[69]</ref>.</p><p>Experiments on object detection are done with SSD300 on MS COCO <ref type="bibr" target="#b73">[71]</ref> dataset. The input image is resized to 300?300. Following training schedule of the open-source implementation of NVIDIA <ref type="bibr" target="#b72">[70]</ref>, we train SSD300 with different backbones on COCO train2017 set for 65 epochs. The COCO train2017 set contains 118K RGB images. The initial learning rate is 0.0026 and is decayed by 10 at epoch 43 and 54. The optimization algorithm is SGD with a momentum of 0.9. The value of weight decay is 5e-4. After training, the model is validated on COCO val2017 set, which contains 5K images. The backbones are all pre-trained on ImageNet as shown in <ref type="table">Table.</ref> IV. No test-time augmentations are used.</p><p>The results of SSD300 with dividing and ensemble are shown in <ref type="table">Table.</ref> IX. The dividing strategy also works for SSD on the object detection task. When the backbone is large, dividing and ensemble can bring significant improvements of AP and AR. Especially for the recall of objects with large areas, dividing and ensemble achieve more than +3% AR for (ResNeXt-101, 64?4d, S = 2) compared to its counterpart. Due to multiple runs of predictors as <ref type="figure">Figure 7</ref> shows, the FLOPs of the whole system will generally increase after dividing. The numbers of parameters are roughly similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, we discuss the accuracy-efficiency trade-offs of increasing the number of networks and demonstrate it is better than purely increasing the depth or width of networks. Along this process, a simple yet effective method -dividing and cotraining is proposed to enhance the performance of a single large model. This work potentially introduces some interesting topics in neural network engineering, e.g., designing a flexible framework for asynchronous training of multiple models, more complex deep ensemble and co-training methods, multiple models with different modalities, introducing the idea to NAS (given a specific constrain of FLOPs, how can we design one or several models to get the best performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Thanks Boxi Wu, Yuejin Li, and all reviewers for their technical support and helpful discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX DETAILS ABOUT DIVIDING A LARGE NETWORK</head><p>S is the number of small networks after dividing. a) ResNet: For CIFAR-10 and CIFAR-100, the numbers of input channels of the three blocks are: original : <ref type="bibr" target="#b34">[32,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr">80,</ref><ref type="bibr">112,</ref><ref type="bibr">192,</ref><ref type="bibr">320,</ref><ref type="bibr">1280]</ref>, S = 2 : <ref type="bibr" target="#b26">[24,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b58">56,</ref><ref type="bibr">80,</ref><ref type="bibr">136,</ref><ref type="bibr">224</ref>, 920], S = 4 : <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b58">56,</ref><ref type="bibr">96,</ref><ref type="bibr">160,</ref><ref type="bibr">640]</ref>. d) WRN: Suppose the widen factor is w, the new widen factor w after dividing is: w = max( w ? S + 0.4 , 1.0). e) ResNeXt: Suppose original cardinality (groups in convolution) is d, new cardinality d is: d = max( d S , 1.0).</p><p>f) Shake-Shake: For Shake-Shake 26 2?96d, the numbers of output channels of the first convolutional layer and three blocks are: and p shake , respectively, we divide them as: g pyramid = gpyramid ? S , p shake = pshake ? S . To pursue better performance, we do not divide the base channel of PyramidNet on CIFAR since it is small -16.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the Shenzhen Institute of Artificial Intelligence and Robotics for Society under Grant AC01202101103. This work was also supported in part by The National Nature Science Foundation of China (Grant Nos: 62036009, 61936006). (Corresponding author: Tin Lun Lam.) Shuai Zhao, Liguang Zhou, Tin Lun LAM, and Yangsheng Xu are with the Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS), The Chinese University of Hong Kong, Shenzhen, 518129, Guangdong, China (e-mail: zhaoshuaimcc@gmail.com; liguangzhou@link.cuhk.edu.cn; tl-lam@cuhk.edu.cn; ysxu@cuhk.edu.cn). Wenxiao Wang and Deng Cai are with the State Key Laboratory of CAD&amp;CG, Zhejiang University, Hangzhou 310027, China (email: wenxi-aowang@zju.edu.cn; dengcai@zju.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The relationship between the numerical average of accuracies and ensemble accuracy of small networks on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The difference of inference latency between sequential and concurrent running. Test on Tesla V100(s) with mixed precision and batch size 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Ensemble of ensemble models. {WRN, S = 2} is treated as a single model. The left is top-1 accuracy, the right is top-5 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>original :<ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b66">64]</ref>, S = 2 :<ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b50">48]</ref>,S = 4 : [8, 16, 32]. b) SE-ResNet: The reduction ratio in the SE module keeps unchanged. Other settings are the same as ResNet. c) EfficientNet: The numbers of output channels of the first conv. layer and blocks in EfficientNet baseline are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, weight arXiv:2011.14660v4 [cs.CV] 6 Sep 2022Fig. 2. Left: divide one large network into several small ones and co-train. Right: concurrent running of small networks on different devices during inference.</figDesc><table><row><cell>output label y</cell><cell>loss ? ( , )</cell><cell>? ( , )</cell><cell>label</cell><cell>ensemble output ?</cell><cell>co-training loss ? ( , )</cell></row><row><cell cols="2">a large network</cell><cell>divide into small networks</cell><cell></cell><cell>?</cell><cell>device: 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>device: 2</cell></row><row><cell></cell><cell cols="2">data transformer</cell><cell></cell><cell>?</cell><cell>device: 4 device: 3</cell></row><row><cell></cell><cell cols="2">training images</cell><cell></cell><cell cols="2">training images</cell></row><row><cell></cell><cell></cell><cell>divide and co-training</cell><cell></cell><cell></cell><cell>concurrent running</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>datasets. CIFAR-10 and CIFAR-100 contain 50K training and 10K test RGB images of size 32?32, labeled with 10 and 100 classes, respectively. ImageNet 2012 contains 1.28 million training images and 50K validation images from 1000 classes. For CIFAR and ImageNet, crop size is 32 and 224, batch size is 128 and 256, respectively. Learning rate and training epochs We apply warm up and cosine learning rate policy [50], [51]. If the initial learning rate is lr and current epoch is epoch, for the first slow epoch steps, the learning rate is lr ? epoch slow epoch ; for the rest epochs, the learning rate is 0.5?lr ?(1+cos(? ? epoch?slow epochs max epoch?slow epoch )). Generally, lr is 0.1; {max epoch, slow epoch} is {300, 20} and {120, 5} for CIFAR and ImageNet, respectively. Models before and after dividing are trained for the same epochs. Data augmentation Random crop and resize, random leftright flipping, AutoAugment [48], random erasing [47], and mixup [54] are used during training. Label smoothing [55] is only applied when training on ImageNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I INFLUENCE</head><label>I</label><figDesc>OF VARIOUS EXPERIMENT SETTINGS ON CIFAR-100.</figDesc><table /><note>STEP-LR MEANS STEP LEARNING RATE POLICY [22]. WHEN THE ERASING PROBABILITY pe = 1.0, RANDOM ERASING ACTS LIKE CUTOUT [52].Method step-lr cos-lr rand. erasing mixup AutoAug. Top-1 err. (%) ResNet-110 [53] original: 26.88% 24.71 ? 0.22 24.15 ? 0.07 , pe = 1.0 23.43 ? 0.01 , pe = 0.5 23.11 ? 0.29 , pe = 0.5 , ? = 0.2 21.22 ? 0.28 , pe = 0.5 , ? = 0.2 19.19 ? 0.23</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II RESULTS</head><label>II</label><figDesc>OF TOP-1 ERROR (%) ON CIFAR-100. THE LAST THREE ROWS ARE TRAINED FOR 1800 EPOCHS. S IS THE NUMBER OF SMALL NETWORKS AFTER DIVIDING. TRAIN FROM SCRATCH, NO EXTRA DATA. WEIGHT DECAY VALUE KEEPS UNCHANGED EXCEPT {WRN-40-10, 1800 EPOCHS}, WHICH APPLIES EQ. (6). THE MAXIMAL REDUCTION OF ERROR OF DIFFERENT NETWORKS IS SHOWN IN BLUE. When training on CIFAR-100, the stride of EfficientNet at the stage 4&amp;7 is set to be 1. Original EfficientNet on CIFAR-100 is pre-trained while we train it from scratch. These results rank #2, #6, and #7 on public leaderboard paperswithcode.com/sota/image-classification-on-cifar-100 (without extra training data) at the time of submission.</figDesc><table><row><cell>Method</cell><cell>original error</cell><cell>error</cell><cell cols="2">re-implementation MParams GFLOPs</cell><cell cols="2">(# Networks) S = 2 error MParams</cell><cell>GFLOPs</cell><cell cols="2">(# Networks) S = 4 error MParams</cell><cell>GFLOPs</cell></row><row><cell>ResNet-110 [53]</cell><cell>26.88</cell><cell>18.96</cell><cell>1.17</cell><cell>0.17</cell><cell>18.32 (0.64 ?)</cell><cell>1.33</cell><cell>0.20</cell><cell>19.56 (0.63 ?)</cell><cell>1.21</cell><cell>0.18</cell></row><row><cell>ResNet-164 [53]</cell><cell>24.33</cell><cell>18.38</cell><cell>1.73</cell><cell>0.25</cell><cell>17.12 (1.26 ?)</cell><cell>1.96</cell><cell>0.29</cell><cell>18.05 (0.33 ?)</cell><cell>1.78</cell><cell>0.26</cell></row><row><cell>SE-ResNet-110 [37]</cell><cell>23.85</cell><cell>17.91</cell><cell>1.69</cell><cell>0.17</cell><cell>17.37 (0.54 ?)</cell><cell>1.89</cell><cell>0.20</cell><cell>18.33 (0.42 ?)</cell><cell>1.70</cell><cell>0.18</cell></row><row><cell>SE-ResNet-164 [37]</cell><cell>21.31</cell><cell>17.37</cell><cell>2.51</cell><cell>0.26</cell><cell>16.31 (1.06 ?)</cell><cell>2.81</cell><cell>0.29</cell><cell>17.21 (0.16 ?)</cell><cell>2.53</cell><cell>0.27</cell></row><row><cell>EfficientNet-B0 [1]  ?</cell><cell>-</cell><cell>18.50</cell><cell>4.13</cell><cell>0.23</cell><cell>18.20 (0.30 ?)</cell><cell>4.28</cell><cell>0.24</cell><cell>17.85 (0.65 ?)</cell><cell>4.52</cell><cell>0.30</cell></row><row><cell>EfficientNet-B3 [1]  ?</cell><cell>-</cell><cell>18.10</cell><cell>10.9</cell><cell>0.60</cell><cell>17.00 (1.10 ?)</cell><cell>11.1</cell><cell>0.60</cell><cell>16.62 (1.48 ?)</cell><cell>11.7</cell><cell>0.65</cell></row><row><cell>WRN-16-8 [2]</cell><cell>20.43</cell><cell>18.69</cell><cell>11.0</cell><cell>1.55</cell><cell>17.37 (1.32 ?)</cell><cell>12.4</cell><cell>1.75</cell><cell>17.07 (1.62 ?)</cell><cell>11.1</cell><cell>1.58</cell></row><row><cell>ResNeXt-29, 8?64d [3]</cell><cell>17.77</cell><cell>16.43</cell><cell>34.5</cell><cell>5.41</cell><cell>14.99 (1.44 ?)</cell><cell>35.4</cell><cell>5.50</cell><cell>14.88 (1.55 ?)</cell><cell>36.9</cell><cell>5.67</cell></row><row><cell>WRN-28-10 [2]</cell><cell>19.25</cell><cell>15.50</cell><cell>36.5</cell><cell>5.25</cell><cell>14.48 (1.02 ?)</cell><cell>35.8</cell><cell>5.16</cell><cell>14.26 (1.24 ?)</cell><cell>36.7</cell><cell>5.28</cell></row><row><cell>WRN-40-10 [2]</cell><cell>18.30</cell><cell>15.56</cell><cell>55.9</cell><cell>8.08</cell><cell>14.28 (1.28 ?)</cell><cell>54.8</cell><cell>7.94</cell><cell>13.96 (1.60 ?)</cell><cell>56.0</cell><cell>8.12</cell></row><row><cell>DenseNet-BC-190 [38]</cell><cell>17.18</cell><cell>14.10</cell><cell>25.8</cell><cell>9.39</cell><cell>12.64 (1.46 ?)</cell><cell>25.5</cell><cell>9.24</cell><cell>12.56 (1.54 ?)</cell><cell>26.3</cell><cell>9.48</cell></row><row><cell>PyramidNet-272 [39]+ShakeDrop [40]</cell><cell>14.96</cell><cell>11.02</cell><cell>26.8</cell><cell>4.55</cell><cell>10.75 (0.27 ?)</cell><cell>28.9</cell><cell>5.24</cell><cell>10.54 (0.48 ?)</cell><cell>32.8</cell><cell>6.33</cell></row><row><cell>WRN-40-10 [2]</cell><cell>18.30</cell><cell>16.02</cell><cell>55.9</cell><cell>8.08</cell><cell>14.09 (1.93 ?)</cell><cell>54.8</cell><cell>7.94</cell><cell>13.10 (2.92 ?)</cell><cell>56.0</cell><cell>8.12</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF TOP-1 ERROR (%) ON CIFAR-10. S IS THE NUMBER OF SMALL NETWORKS AFTER DIVIDING. DIVIDE WEIGHT DECAY AS EQ.<ref type="bibr" target="#b7">(6)</ref>. NO EXTRA DATA AND TRAIN FROM SCRATCH. METHODS COMPARED HERE ARE AUTOAUGMENT<ref type="bibr" target="#b50">[48]</ref>, RANDAUGMENT<ref type="bibr" target="#b61">[59]</ref>, FIXUP-INIT<ref type="bibr" target="#b62">[60]</ref>, CUTOUT<ref type="bibr" target="#b54">[52]</ref>, MIXUP<ref type="bibr" target="#b56">[54]</ref>, SHAKEDROP<ref type="bibr" target="#b42">[40]</ref>, AND FAST AUTOAUGMENT<ref type="bibr" target="#b63">[61]</ref>.</figDesc><table><row><cell>Method</cell><cell></cell><cell>error</cell><cell cols="3">MParams GFLOPs</cell></row><row><cell>ResNet-164 [53]</cell><cell></cell><cell>5.46</cell><cell cols="2">1.73</cell><cell>0.25</cell></row><row><cell>SE-ResNet-164 [37]</cell><cell></cell><cell>4.39</cell><cell cols="2">2.51</cell><cell>0.26</cell></row><row><cell>WRN-28-10 [2]</cell><cell></cell><cell>4.00</cell><cell cols="2">36.5</cell><cell>5.25</cell></row><row><cell>ResNeXt-29, 8?64d [3]</cell><cell></cell><cell>3.65</cell><cell cols="2">34.5</cell><cell>5.41</cell></row><row><cell>WRN-28-10 [2], [48]</cell><cell></cell><cell>2.68</cell><cell cols="2">36.5</cell><cell>5.25</cell></row><row><cell>WRN-28-10 [2], [59]</cell><cell></cell><cell>2.70</cell><cell cols="2">36.5</cell><cell>5.25</cell></row><row><cell>WRN-40-10 [2], [52], [54], [60]</cell><cell></cell><cell>2.30</cell><cell cols="2">55.9</cell><cell>8.08</cell></row><row><cell>Shake-Shake 26 2?96d [48], [62]</cell><cell></cell><cell>2.00</cell><cell cols="2">26.2</cell><cell>3.78</cell></row><row><cell>Shake-Shake 26 2?96d [59], [62]</cell><cell></cell><cell>2.00</cell><cell cols="2">26.2</cell><cell>3.78</cell></row><row><cell>PyramidNet-272 [39], [40], [61]</cell><cell></cell><cell>1.70</cell><cell cols="2">26.2</cell><cell>4.55</cell></row><row><cell>re-implementation</cell><cell cols="2">epochs 300 1800</cell><cell cols="3">MParams GFLOPs</cell></row><row><cell>SE-ResNet-164</cell><cell>2.98</cell><cell>2.19</cell><cell cols="2">2.49</cell><cell>0.26</cell></row><row><cell>ResNeXt-29, 8?64d</cell><cell>2.69</cell><cell>2.23</cell><cell cols="2">34.4</cell><cell>5.40</cell></row><row><cell>WRN-28-10</cell><cell>2.28</cell><cell>2.41</cell><cell cols="2">36.5</cell><cell>5.25</cell></row><row><cell>WRN-40-10</cell><cell>2.33</cell><cell>2.19</cell><cell cols="2">55.8</cell><cell>8.08</cell></row><row><cell>Shake-Shake 26 2?96d</cell><cell></cell><cell>2.00</cell><cell cols="2">26.2</cell><cell>3.78</cell></row><row><cell>SE-ResNet-164, S = 2</cell><cell>2.84</cell><cell>2.20</cell><cell cols="2">2.81</cell><cell>0.29</cell></row><row><cell>ResNeXt-29, 8?64d, S = 2</cell><cell>2.12</cell><cell>1.94</cell><cell cols="2">35.1</cell><cell>5.49</cell></row><row><cell>WRN-28-10, S = 2</cell><cell>2.06</cell><cell>1.81</cell><cell cols="2">35.8</cell><cell>5.16</cell></row><row><cell>WRN-28-10, S = 4</cell><cell>2.01</cell><cell>1.68</cell><cell cols="2">36.5</cell><cell>5.28</cell></row><row><cell>WRN-40-10, S = 4</cell><cell>2.01</cell><cell>1.62</cell><cell cols="2">55.9</cell><cell>8.12</cell></row><row><cell>Shake-Shake 26 2?96d, S = 2</cell><cell></cell><cell>1.75</cell><cell cols="2">23.3</cell><cell>3.38</cell></row><row><cell>Shake-Shake 26 2?96d, S = 4</cell><cell></cell><cell>1.69</cell><cell cols="2">26.3</cell><cell>3.82</cell></row><row><cell cols="6">These results rank #6, #7, and #8 on public leaderboard</cell></row><row><cell cols="4">paperswithcode.com/sota/image-classification-on-cifar-10</cell><cell>(without</cell><cell>extra</cell></row><row><cell cols="2">training data) at the time of submission.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV SINGLE</head><label>IV</label><figDesc>CROP RESULTS ON IMAGENET 2012 VALIDATION SET. NO EXTRA DATA AND TRAIN FROM SCRATCH. S IS THE NUMBER OF SMALL NETWORKS AFTER DIVIDING. ACC. OF M i IS THE ACCURACY OF SMALL NETWORKS. ONLY WRN APPLIES EQ. (6); OTHERS KEEP WEIGHT DECAY UNCHANGED. Acc. MParams / GFLOPs Crop / Batch / Epochs Acc. of Mi TABLE V INFLUENCE OF DIVIDING MANNERS OF WEIGHT DECAY AND DROPPING LAYERS. p shake IS THE INITIAL DROPPING PROBABILITY OF SHAKEDROP.</figDesc><table><row><cell>Method</cell><cell>Method</cell><cell>wd</cell><cell></cell><cell cols="2">Top-1 / Top-5 Top-1 err. (%) CIFAR-10 CIFAR-100</cell></row><row><cell>WRN-28-10</cell><cell></cell><cell>5e-4</cell><cell></cell><cell>2.28</cell><cell>15.50</cell></row><row><cell cols="2">WRN-28-10, S = 2</cell><cell>5e-4</cell><cell></cell><cell>2.15</cell><cell>14.48</cell></row><row><cell cols="2">WRN-28-10, S = 2</cell><cell>2.5e-4</cell><cell></cell><cell>2.15</cell><cell>14.43</cell></row><row><cell cols="2">WRN-28-10, S = 2</cell><cell cols="2">Eq. (6)</cell><cell>2.06</cell><cell>14.16</cell></row><row><cell cols="2">WRN-28-10, S = 4</cell><cell>5e-4</cell><cell></cell><cell>2.36</cell><cell>14.26</cell></row><row><cell cols="2">WRN-28-10, S = 4</cell><cell cols="2">1.25e-4</cell><cell>2.00</cell><cell>14.79</cell></row><row><cell cols="2">WRN-28-10, S = 4</cell><cell cols="2">Eq. (6)</cell><cell>2.01</cell><cell>14.04</cell></row><row><cell>WRN-16-8</cell><cell></cell><cell>1e-4</cell><cell></cell><cell></cell><cell>18.69</cell></row><row><cell cols="2">WRN-16-8, S = 2 WRN-16-8, S = 2</cell><cell>1e-4 0.5e-4</cell><cell></cell><cell>-</cell><cell>17.37 18.11</cell></row><row><cell cols="2">WRN-16-8, S = 2</cell><cell cols="2">Eq. (6)</cell><cell></cell><cell>17.77</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">p shake</cell><cell cols="2">Top-1 err. (%) CIFAR-10 CIFAR-100</cell></row><row><cell cols="2">PyramidNet-272 + ShakeDrop</cell><cell>0.5</cell><cell></cell><cell></cell><cell>11.02</cell></row><row><cell cols="2">PyramidNet-272 + ShakeDrop, S = 2 PyramidNet-272 + ShakeDrop, S = 2</cell><cell>0.5 0.5/ ?</cell><cell>2</cell><cell>-</cell><cell>11.15 10.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI INFLUENCE</head><label>VI</label><figDesc>OF DIFFERENT ENSEMBLE MANNERS. SOFTMAX () MEANS WE ENSEMBLE THE RESULTS AFTER SOFTMAX OPERATION. GENERALLY, WE DO ENSEMBLE OPERATIONS WITHOUT SOFTMAX, i.e., SOFTMAX ()</figDesc><table><row><cell>Method</cell><cell>softmax</cell><cell>ensemble</cell><cell cols="2">Top-1 err. (%) CIFAR-10 CIFAR-100</cell></row><row><cell></cell><cell></cell><cell>max</cell><cell>1.85</cell><cell>15.04</cell></row><row><cell>WRN-28-10, S = 4</cell><cell></cell><cell>avg. max</cell><cell>1.77 1.90</cell><cell>14.45 15.27</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>1.68</cell><cell>14.26</cell></row><row><cell></cell><cell></cell><cell>geo. mean</cell><cell>1.68</cell><cell>14.26</cell></row><row><cell></cell><cell></cell><cell>max</cell><cell>1.94</cell><cell>15.58</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>1.93</cell><cell>15.08</cell></row><row><cell>ResNeXt-29, 8?64d, S = 2</cell><cell></cell><cell>max</cell><cell>2.01</cell><cell>15.78</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>1.94</cell><cell>14.99</cell></row><row><cell></cell><cell></cell><cell>geo. mean</cell><cell>1.94</cell><cell>15.00</cell></row><row><cell>Method</cell><cell>softmax</cell><cell>ensemble</cell><cell cols="2">ImageNet Acc. (%) Top-1 Top-5</cell></row><row><cell></cell><cell></cell><cell>max</cell><cell>81.92</cell><cell>95.82</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>82.03</cell><cell>95.91</cell></row><row><cell>ResNeXt-101, 64?4d, S = 2</cell><cell></cell><cell>max</cell><cell>81.78</cell><cell>95.80</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>82.13</cell><cell>95.98</cell></row><row><cell></cell><cell></cell><cell>geo. mean</cell><cell>82.12</cell><cell>95.93</cell></row><row><cell cols="5">said, after dividing, the small networks may be too thin to</cell></row><row><cell cols="5">guarantee a necessary model capacity and satisfying ensemble</cell></row><row><cell cols="5">performance -unless the original network is very large.</cell></row><row><cell cols="5">Secondly, as mentioned in Sec. Division III-A, small networks</cell></row><row><cell cols="5">run in sequence during training, and training of 8 or 16 small</cell></row><row><cell cols="5">networks may cost too much time. The implementation of</cell></row><row><cell cols="5">an asynchronous training system needs further hard work.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII INFLUENCE</head><label>VII</label><figDesc>OF CO-TRAINING COMPONENTS ON CIFAR-100.</figDesc><table><row><cell>Method</cell><cell cols="2">(# Networks) S = 2 diff. views ?cot Top-1 err. (%)</cell><cell cols="2">(# Networks) S = 4 diff. views ?cot Top-1 err. (%)</cell></row><row><cell></cell><cell></cell><cell>17.85</cell><cell></cell><cell>17.59</cell></row><row><cell>WRN-16-8</cell><cell></cell><cell>17.55</cell><cell></cell><cell>17.48</cell></row><row><cell>original: 20.43%</cell><cell>0.1</cell><cell>17.50</cell><cell>0.1</cell><cell>17.33</cell></row><row><cell>re-impl.: 18.69%</cell><cell>0.5</cell><cell>17.37</cell><cell>0.5</cell><cell>17.07</cell></row><row><cell></cell><cell>1.0</cell><cell>17.48</cell><cell>1.0</cell><cell>17.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII AVERAGE</head><label>VIII</label><figDesc>INFERENCE RUNTIME (MS) PER SAMPLE AND AVERAGE COSINE SIMILARITY OF OUTPUTS OF SMALL NETWORKS ON CIFAR-100. TEST ON RTX 2080TI. SEQ. AND PARA. MEANS THAT SEVERAL NETWORKS RUN IN SEQUENCE AND PARALLEL, RESPECTIVELY. SMALLER TIME IS IN BLUE.</figDesc><table><row><cell></cell><cell>original (S = 1)</cell><cell cols="3">(# Networks) S = 2</cell><cell cols="3">(# Networks) S = 4</cell></row><row><cell>Method</cell><cell>runtime</cell><cell cols="2">runtime seq. para.</cell><cell>cos. sim.</cell><cell cols="2">runtime seq. para.</cell><cell>cos. sim.</cell></row><row><cell>ResNet-110</cell><cell>0.24</cell><cell>0.41</cell><cell>0.52</cell><cell>0.84</cell><cell>0.83</cell><cell>1.08</cell><cell>0.85</cell></row><row><cell>ResNet-164</cell><cell>0.32</cell><cell>0.66</cell><cell>0.71</cell><cell>0.83</cell><cell>1.25</cell><cell>1.27</cell><cell>0.83</cell></row><row><cell>EfficientNet-B0</cell><cell>0.27</cell><cell>0.37</cell><cell>0.39</cell><cell>0.79</cell><cell>0.68</cell><cell>0.78</cell><cell>0.79</cell></row><row><cell>EfficientNet-B3</cell><cell>0.51</cell><cell>0.68</cell><cell>0.63</cell><cell>0.85</cell><cell>1.00</cell><cell>1.15</cell><cell>0.85</cell></row><row><cell>ResNeXt-29, 8?64d</cell><cell>1.13</cell><cell>1.20</cell><cell>0.72</cell><cell>0.89</cell><cell>1.49</cell><cell>0.74</cell><cell>0.89</cell></row><row><cell>WRN-16-8</cell><cell>0.24</cell><cell>0.29</cell><cell>0.24</cell><cell>0.80</cell><cell>0.38</cell><cell>0.38</cell><cell>0.78</cell></row><row><cell>WRN-28-10</cell><cell>0.60</cell><cell>0.67</cell><cell>0.45</cell><cell>0.87</cell><cell>0.91</cell><cell>0.55</cell><cell>0.87</cell></row><row><cell>WRN-40-10</cell><cell>0.87</cell><cell>0.98</cell><cell>0.61</cell><cell>0.88</cell><cell>1.31</cell><cell>0.70</cell><cell>0.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX RESULTS</head><label>IX</label><figDesc>OF SSD300 ON COCO V A L2017 WITH DIVIDING AND ENSEMBLE. AVERAGE PRECISION (AP) AND AVERAGE RECALL (AR) ARE REPORTED. 31.7 10.9 34.1 45.5 27.0 39.5 41.2 16.2 46.9 58.9 WRN-50-2, S = 2 31.7 45.3 29.9 49.3 31.2 10.5 33.3 45.7 27.0 39.9 42.1 17.1 47.6 60.9</figDesc><table><row><cell>Backbone</cell><cell></cell><cell></cell><cell></cell><cell cols="3">MParams GFLOPs 0.5:0.95</cell><cell>AP, IoU: 0.5</cell><cell>0.75</cell><cell>S</cell><cell>AP, Area: M</cell><cell>L</cell><cell>1</cell><cell>AR, #Dets: 10</cell><cell>100</cell><cell>S</cell><cell>AR, Area: M</cell><cell>L</cell></row><row><cell cols="2">ResNet-50 [70]</cell><cell></cell><cell></cell><cell>23.0</cell><cell>42.3</cell><cell>25.0</cell><cell cols="2">42.3 25.7</cell><cell>7.6</cell><cell cols="4">26.9 39.9 23.7 34.2 35.8 11.8 39.4 54.8</cell></row><row><cell cols="14">WRN-50-2 49.7 WRN-50-3 39.3 48.1 30.3 64.1 86.8 30.7 51.2 32.0 11.1 34.8 45.9 27.2 39.6 41.4 16.5 46.5 59.9</cell></row><row><cell cols="2">WRN-50-3, S = 2</cell><cell></cell><cell></cell><cell>64.3</cell><cell>96.3</cell><cell>31.6</cell><cell cols="7">51.5 33.0 11.9 35.6 47.3 28.0 41.2 43.3 18.5 49.2 61.5</cell></row><row><cell cols="2">ResNeXt-101, 64?4d</cell><cell></cell><cell></cell><cell>68.9</cell><cell>90.1</cell><cell>32.6</cell><cell cols="7">53.0 34.1 11.9 36.6 49.6 28.2 41.0 42.8 17.8 48.4 62.4</cell></row><row><cell cols="4">ResNeXt-101, 64?4d, S = 2</cell><cell>69.8</cell><cell>100.5</cell><cell>34.1</cell><cell cols="7">54.7 35.8 13.5 38.5 52.2 29.3 43.0 45.2 20.4 51.0 65.5</cell></row><row><cell>backbone</cell><cell>feature</cell><cell>predictor</cell><cell>bbox score</cell><cell>NMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">weight sharing</cell><cell>ensemble</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>backbone</cell><cell>feature</cell><cell>predictor</cell><cell>bbox score</cell><cell>NMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>original : [16, 96, 96 ? 2, 96 ? 4], S = 2 : [16, 64, 64 ? 2, 64 ? 4], S = 4 : [16, 48, 48 ? 2, 48 ? 4]. g) DenseNet: Suppose the growth rate of DenseNet is g dense , the new growth rate after dividing is g dense = 1 2 ? 2 ? Suppose the additional rate of PyramidNet and final drop probability of ShakeDrop is g pyramid</figDesc><table><row><cell>gdense ? S . h) PyramidNet + ShakeDrop:</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently President and Professor of the Chinese University of Hong Kong, Shenzhen, China. His research interests include robotics, intelligent systems, and electric vehicles.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se-Resnext</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On each worker, batch size is 16 or 32</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Pnasnet</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>AmoebaNet use 100 P100 workers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Snapshot ensembles: Train 1, get M for free</title>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Research and development of neural network ensembles: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous training of negatively correlated neural networks in an ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part B</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ensemble learning using decorrelated neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connection science</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">When Ensembling Smaller Models is More Efficient than Single Large Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00570</idno>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mutualnet: Adaptive convnet via mutual learning from network width and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Willis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep cotraining for semi-supervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge,&quot; IJCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Toronto, Ontario</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 0</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Mixed precision training,&quot; in ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. arXiv.org</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<ptr target="http://arxiv.org/abs/1409.1556" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Slimmable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collaborative Learning: A Sourcebook for Higher Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Macgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Office of Educational Research and Improvement</title>
		<imprint>
			<date type="published" when="1992" />
			<publisher>ERIC</publisher>
		</imprint>
	</monogr>
	<note>ch. What is collaborative learning</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collaborative learning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cooperative learning with visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1705.05512</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shakedrop regularization for deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Three mechanisms of weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>matter little near convergence,&quot; in NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cuda toolkit documentation v11.1.0</title>
		<ptr target="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalization error of ensemble estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Networks</title>
		<meeting>International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1812.01187</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A method for unconstrained convex minimization problem with the rate of convergence o (1/k?2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady an ussr</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno>abs/1705.07485</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Multi-Grain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Weighted boxes fusion: Ensembling boxes from different object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gabruseva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Ensemble methods for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Casado-Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Soft-nmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Deep learning examples for tensor cores</title>
		<ptr target="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context,&quot; in ECCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
