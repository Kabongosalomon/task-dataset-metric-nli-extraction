<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehun</forename><surname>Kim</surname></persName>
							<email>dkim@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology Pohang-si</orgName>
								<address>
									<country>Gyeongsangbuk-do Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyemin</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<addrLine>Pohang-si</addrLine>
									<country>Gyeongsangbuk-do Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<addrLine>Pohang-si</addrLine>
									<country>Gyeongsangbuk-do Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475375</idno>
					<note>ACM Reference Format: Taehun Kim, Hyemin Lee, and Daijin Kim. 2021. UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation. In Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21), October 20-24, 2021, Virtual Event, China. ACM, New York, NY, USA, 9 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Image segmentation; Neural networks KEYWORDS medical image segmentation</term>
					<term>polyp segmentation</term>
					<term>colonoscopy</term>
					<term>self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Uncertainty Augmented Context Attention network (UACANet) for polyp segmentation which considers an uncertain area of the saliency map. We construct a modified version of U-Net shape network with additional encoder and decoder and compute a saliency map in each bottom-up stream prediction module and propagate to the next prediction module. In each prediction module, previously predicted saliency map is utilized to compute foreground, background and uncertain area map and we aggregate the feature map with three area maps for each representation. Then we compute the relation between each representation and each pixel in the feature map. We conduct experiments on five popular polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB and CVC-300, and our method achieves state-ofthe-art performance. Especially, we achieve 76.6% mean Dice on ETIS dataset which is 13.8% improvement compared to the previous state-of-the-art method. Source code is publicly available at https://github.com/plemeri/UACANet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Image segmentation is one of the fundamental and challenging topics in computer vision. It aims to classify each pixel from a given Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM '21, October 20-24, 2021, Virtual Event, China ? 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8651-7/21/10. . . $15.00 https://doi.org <ref type="bibr">/10.1145/3474085.3475375</ref> image. Recent studies adopt image segmentation to the specific domain such as salient object detection which focus on classifying each pixel whether it belongs to the most salient object or not. Similar to the salient object detection, one can apply their techniques to the medical purposes.</p><p>Medical image segmentation is widely used technique such as classifying each organ in the given tomography images like pancreas segmentation <ref type="bibr" target="#b20">[20]</ref>, detecting cells from the microscopy images <ref type="bibr" target="#b22">[22]</ref>, or discriminating abnormal regions from normal regions from the body such as brain tumor <ref type="bibr" target="#b10">[10]</ref> or polyp segmentation <ref type="bibr" target="#b5">[6]</ref>.</p><p>Polyps are an abnormal tissue growth from a surface of our body and can be found in colon, rectum, stomach or even throat. In most cases, polyps are benign, which means they are not indicating illness or maliciousness. However, because polyps are potentially cancerous, so we need a long-term diagnosis including the growth of their sizes or location and whether it became malignant or not. Thus, detecting polyps in the given colonoscopy image is beneficial for aiding early diagnosis of polyp related diseases.</p><p>Previous polyp segmentation networks usually adopt their methodologies from salient object detection (SOD) since they share the main interest, attend more on salient (polyp) region than surrounding scene. Current state-of-the-art methods in SOD which shows decent performance are highly related to the edge guidance <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b29">29]</ref>. However, acquiring additional edge data is often expensive. Reverse attention <ref type="bibr" target="#b4">[5]</ref> suggest using reverse saliency map to obtain boundary cues, but since the boundary region is highly related to the ambiguous saliency score, saliency map without reverse operation already has such boundary information.</p><p>In this paper, we propose Uncertainty Augmented Context Attention network (UACANet), augmenting uncertain area with respect to the saliency map which is highly related to the boundary information. Our method computes the region with ambiguous saliency score and combine with a foreground and a background area for context attention module. On top of a modified version of U-Net <ref type="bibr" target="#b22">[22]</ref> structure network with additional encoders and decoder, we aggregate the feature map based on three areas with weighted summation to obtain a representative context vectors for each area. We then compute the similarity between the context vector and the feature map. We validate our method with five famous polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB and CVC-300 and achieve state-of-the-art performance among previous methods.  <ref type="bibr" target="#b18">[18]</ref> introduced a fully convolutional neural network architecture to efficiently train a model to classify each pixel. They also utilized a multi-scale scheme with aggregating low-level feature maps from the early stages in the neural network. Noh et al. proposed deconvolution network <ref type="bibr" target="#b19">[19]</ref> to compensate a spatial information degradation due to the pooling operation by leveraging transposed convolution and unpooling methods. Pyramid Spatial Pooling Network (PSPNet) utilized grid-wise pooling and comprise multiple sizes of the grid to deal with multi-scale objects. Deeplab <ref type="bibr" target="#b3">[4]</ref> constructed multiple convolution layers with different dilation rates to diversify receptive fields within a single module to extract a multi-scale context information. Dual Attention Network <ref type="bibr" target="#b8">[8]</ref> brought a self-attention mechanism to the semantic segmentation network and comprise a dual path network for spatial and channel dimensions which incorporates local features with their global dependencies. Object Contextual Representation (OCR) <ref type="bibr" target="#b30">[30]</ref> expanded a non-local operation to consider semantic region by aggregating the representation of the pixels from each class and compute a similarity with each pixel from the feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Salient Object Detection</head><p>Salient object detection (SOD) resembles semantic segmentation but has its own criterion. Rather than predicting the entire region in the given image and classifying an object class label, SOD focuses on the object itself, identifying more important regions from the surrounding areas. Since salient object does not have any specific object class, even if the task might seem superficially easier than multi-class segmentation, it is much harder when it comes to the accurate object detection and assuming their priorities among other surrounding objects. Unlike semantic segmentation, it is hard to say that there is a strong baseline model with a high performing yet simple architecture in SOD.</p><p>Current state-of-the-art methods exploit the boundary region of objects for complementary information by multi-task learning strategy which can be incorporated to enhance the quality of saliency map. Edge Guidance Network (EGNet) <ref type="bibr" target="#b29">[29]</ref> used a bottomup stream for edge detection branch and side-out fusion strategy which aggregates each path from the top-down stream for salient object branch. To clarify the terms that we use as bottom-up stream means from high-level feature level to low-level feature level from the backbone network which is commonly used for segmentation or heat-map related tasks such as pose estimation. Also, side-out fusion strategy aggregates multiple feature maps from the backbone with concatenation or summation. Boundary-aware Network (BANet) <ref type="bibr" target="#b24">[24]</ref> oppositely used a side-out fusion for boundary branch and single-stream for object branch, but rather than regarding edge detection as a separate task, they combine edge result and object result to generate saliency map. These methods qualitatively showed competitive results consistently <ref type="bibr" target="#b26">[26]</ref> which proves that edge guidance helps better representation of objects in terms of edge-preserving results. However, acquiring additional edge dataset is often expensive and even with image processing techniques such as Canny edge detection <ref type="bibr" target="#b2">[3]</ref>, it contains redundant edges which are usually unrelated to the object.</p><p>Reverse attention <ref type="bibr" target="#b4">[5]</ref> explicitly multiply a reverse area of prediction in order to capture residual details for the saliency refinement. However, based on both their experimental results and our experiments, the performance with or without reverse attention wasn't very different. However, their suggestion provided an intuition that even without explicit edge guidance, we can access to the edge related context with saliency map. Based on this idea, we broaden a reverse saliency map with additional uncertain area, an ambiguous area that saliency score is neither biased to the foreground nor the background. Also, they suggested an efficient yet practical network architecture. The decoder at the end of the backbone feature map produces a global saliency map, and in the bottom-up stream, only the saliency map from the previous level is used unlike U-Net shape networks concatenated feature maps. The final prediction thus only requires a low-level feature map and saliency map from the previous level as a guidance. While U-Net like networks predicts saliency map at the last bottom-up layer, proposed architecture from Reverse attention <ref type="bibr" target="#b4">[5]</ref> which computes saliency map at the intermediate level feature map and saliency map from the bottom-up stream only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Polyp Segmentation</head><p>Polyp segmentation aims to precisely segment polyps from the given colonoscopy image. One can say that polyp segmentation is a semantic segmentation with binary class problem. While we can adopt such fully convolutional network architecture to solve the problem, since colonoscopy has different image domain compared to the general images, researchers focus on extracting semantic features with detail information. Famous architectures like PSP-Net or Deeplab adopt multi-scale scheme to the backbone network which helps to capture detail information with multiple receptive field sizes, but they usually deploy such methods at the end of the backbone network which is spatially insufficient for recovering precise spatial information. DeeplabV3+ <ref type="bibr" target="#b3">[4]</ref> tried to compensate such problem by concatenate low-level feature map from the backbone network but wasn't sufficient for recovering such details coming from the input image. U-Net <ref type="bibr" target="#b22">[22]</ref> introduced incremental up-sampling of the feature maps alongside the corresponding scales of the low-level feature maps. Such "skip-connections" also appears in Feature Pyramid Networks (FPN) <ref type="bibr" target="#b16">[16]</ref> but they use element-wise addition while U-Net aggregate features with channel-wise concatenation. While FPN is also designed similar to the U-Net in order to extract multiscale features, they do not need a fine-grained detail features since object detection needs to locate a bounding box, not a precise shape of an object. U-Net++ <ref type="bibr" target="#b32">[32]</ref> added extra layers and dense connectivity to reduce the gap between low-level and high-level features.</p><p>As the significance of polyp segmentation has been increased, studies that only dedicated to the polyp datasets have been done recently. ResUNet++ <ref type="bibr" target="#b14">[14]</ref> construct a U-Net shape network with famous CNN modules including residual blocks from ResNet, Atrous Spatial Pyramid Pooling (ASPP) module from Deeplab and Squeeze and Excitation mechanism from SENet <ref type="bibr" target="#b12">[12]</ref>. Selective Feature Aggregation network (SFA) <ref type="bibr" target="#b6">[7]</ref> added another bottom-up path for boundary estimation, similar to SOD networks with edge guidances <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b29">29]</ref>. Parallel Reverse Attention Network (PraNet) <ref type="bibr" target="#b5">[6]</ref> achieved state-of-the-art performance on five different polyp segmentation benchmark by adopting the majority of the network design and techniques from <ref type="bibr" target="#b4">[5]</ref>. They added parallel partial decoder from <ref type="bibr" target="#b28">[28]</ref> to combine low-level feature maps from the backbone network because the architecture from <ref type="bibr" target="#b4">[5]</ref> obviously lack of feature sharing for both top-down stream and bottom-up stream. In our network design, we locate intermediate encoder for low-level features and use the output of encoder and previous decoder features for bottom-up stream to incorporate high-level semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we demonstrate the architecture of our UACANet and the details of comprising modules. We first explain the overall structure of our network, then describe the details of fundamental components including Parallel Axial Attention encoder and decoder, and Uncertainty Augmented Context Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>We design UACANet based on the overall architecture from PraNet <ref type="bibr" target="#b5">[6]</ref> which is a modified version of <ref type="bibr" target="#b4">[5]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> Feature maps from three PAA-e modules are both used for side-out fusion path (purple arrow), Parallel Axial Attention decoder (PAAd) and Uncertainty Augmented Context Attention (UACA). We concatenate three feature maps from PAA-e modules for PAA-d and it predicts the initial saliency map for polyps. Then the feature maps from PAA-e and PAA-d is concatenated for UACA, and the output saliency map from PAA-d is used for context guidance (yellow arrow). We describe detailed information of how UACA incorporates feature maps and context guidance in section 3.3. Then, the output saliency map from UACA is added with previously computed saliency map from the PAA-d. After the first UACA, we concatenate PAA-e feature map and previous UACA feature map for the next UACA (gray and black arrow with concatenation symbol). Also, the saliency map from the previous UACA is used as a context guidance for the next UACA (yellow arrows). The output of UACA is forwarded to the final point-wise convolution and then added with the previous context guidance for the current saliency map. After three consequent UACAs, the final output is computed with bi-linear up-sampling with scale factor of 4 and sigmoid function.</p><p>To sum up, the overall architecture shows that the backbone features are encoded with PAA-e, and encoded features are forwarded to PAA-d for initial saliency map which serves as a initial guidance map, which leads UACA to learn a residual saliency map apart from the initial map. This helps the consequent UACA can focus more on uncertain area like boundaries rather than fairly evident region. </p><p>where ? I refers to a pixel in the output and ground truth, denotes ground truth and?denotes the output. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we add four losses from four prediction from PAA-d and UACA with loss function in Equation 1 (red arrows in <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallel Axial Attention encoder and decoder</head><p>In the era of deep learning, computer vision researchers who study on semantic segmentation and its related tasks like polyp segmentation are dedicated to find a better architecture to extract a finegrained feature maps which have both high-level semantic information and low-level detail information because usually they are both hard to extract and to combine each other. Self-attention mechanism <ref type="bibr" target="#b31">[31]</ref> is one of the major context modules in computer vision but requires heavy computation. Axial attention <ref type="bibr" target="#b11">[11]</ref> solve this problem with performing non-local operation with respect to the single axis and sequentially connected each operation.</p><p>We propose Parallel Axial Attention (PAA) for extracting both global dependencies and local representation. We adopt axial attention strategy by computing non-local operation for both horizontal axis and vertical axis but collocated in parallel. By locating vertical and horizontal attention, both contributes to the final output almost equally compared to the sequential method. While sequentially connected axial attention added trainable positional encoding, we do not use encoding scheme since positional encoding substantially not effective in relatively small scales. Also, we discover that using a parallel connection, element-wise summation is effective to aggregate feature maps rather than concatenation without performance degradation since the same input is used for both horizontal axis and vertical axis and they contributes to the output almost equally by the parallel connection. Also, since a single axis based attention cause unexpected deformation, element-wise summation can help to compensate such artifact. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, we compute two non-local operations with input feature map, one for horizontal axis and the other for vertical axis.</p><p>We choose to actively use this module for encoder and decoder modules for better representation in terms of globally refined feature. First, we design Parallel Axial Attention encoder (PAA-e) which aggregates the low-level feature maps from the top-down stream which will be used for bottom-up stream. Since U-Net structure use the low-level features without channel reduction, redundant information may hinder the performance and the number of channels are quite large since backbone networks are trained for image classification. Not to lose any detail information while reducing the number of channels, we design PAA-e with Receptive Field Block (RFB) <ref type="bibr" target="#b17">[17]</ref> strategy. As shown in <ref type="figure">Figure 3</ref>(a), feature map from the backbone network (green box) is forwarded to each receptive field path. We add PAA for additional global refinement for each scale and concatenate the outputs, then forward to the consecutive convolution layers. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Uncertainty Augmented Context Attention</head><p>While reverse attention <ref type="bibr" target="#b4">[5]</ref> in both SOD and polyp segmentation does not bring the large margin of performance gain, it was evident that it has shown some qualitatively better results. This phenomenon is highly related to the boundary guided SOD networks <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b29">29]</ref>, which show state-of-the-art performances in multiple SOD benchmarks. Boundaries of the object as a extra supervision in SOD networks tends to compensate false negative areas, in other words, object regions with low saliency scores. Reverse attention is thus potentially effective method to bring implicit edge guidance without explicit shape of boundary supervision.</p><p>We focus on both saliency and reverse saliency map from the reverse attention and found that usually the boundary region appears where saliency score shows ambiguity. In other words, boundary region is highly related to the saliency score around 0.5. From this property of saliency map, we assume that both saliency and reverse saliency map has almost equal amount of edge information since reverse saliency map is obtained by simple subtraction from 1. Based on this assumption, extracting ambiguous region from the saliency map as well as foreground and background area would improve attentive methods such as self-attention.</p><p>Based on our research, we propose Uncertainty Augmented Context Attention (UACA) module, a novel self-attention mechanism which incorporates uncertain area for rich semantic feature extraction without extra boundary guidance. We denote previously computed input saliency map as m and generate corresponding foreground map m , background map m and uncertain area map m as follows,  </p><p>We compute foreground and background map with max operation in order to disentangle not only from each other but from the uncertain area map since uncertain area map already represents their joint region which makes a redundant information and may diminish the role of uncertainty. While reverse attention applies explicit channel-wise multiplication to the feature map which resembles Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b27">[27]</ref>, we compose our context module with a set of non-local operations. Similar to OCR <ref type="bibr" target="#b30">[30]</ref> we first compute representative vectors for foreground map, background map and uncertain area map by aggregating the pixel representation with each area map from the input feature map x as follows,</p><formula xml:id="formula_2">v = ?? ?I m x , v = ?? ?I m x , v = ?? ?I m x ,<label>(3)</label></formula><p>where ? I denotes pixels in spatial dimension. We implement Equation 2 with matrix multiplication as shown in <ref type="figure">Figure 4</ref>. Each vector stands for the representative feature vector, so v represents foreground feature and v represents the uncertain area. Then we compute the similarity between each representation vector (v , v and v ) and each pixel from the input feature map x as follows,</p><formula xml:id="formula_3">? = (x ) ? (v ), ? = (x ) ? (v ), ? = (x ) ? (v ), = ? , = ? , = ? , where, N = s ? + s ? + s ? .</formula><p>(4) Finally, we compute context feature map by weighted summation of representation vector v , v and v by similarity score , and as follows,</p><formula xml:id="formula_4">t = ( (v ) + (v ) + (v )).<label>(5)</label></formula><p>Note that (?), (?), (?) and (?) are point-wise convolution. Each pixel in context feature map, t , can be interpreted as a weighted average of three representation vectors v , v and v . The context feature map t and input feature map x are concatenated with respect to the channel axis and feed forward to the point-wise convolution for final output feature map as shown in <ref type="figure">Figure 4</ref>.   <ref type="table">Table 2</ref>: Ablation study for uncertain area on CVC-ClinicDB and ETIS datasets. Red color denotes the best score among the methods, and blue color denotes the second best. ? denotes higher the better and ? denotes lower the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we demonstrate our implementation details, datasets and benchmarks for experiments, and some experimental results including ablation study on uncertain area and comparison with previous state-of-the-art methods on five polyp segmentation benchmarks. We also visualize feature maps and the uncertainty map from UACA and some qualitative results to compare other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We describe most of our model architecture description in Section 3 and the number of channels in convolution layers which appear outside of the backbone network is unified as 32 for small model and 256 for large model. We denote the small model with 32 channels as <ref type="bibr" target="#b22">[22]</ref> 0.818 0.746 0.055 U-Net++ <ref type="bibr" target="#b32">[32]</ref> 0.821 0.743 0.048 ResUNet++ <ref type="bibr" target="#b14">[14]</ref> 0.813 0.793 -SFA <ref type="bibr" target="#b6">[7]</ref> 0.723 0.611 0.075 PraNet <ref type="bibr" target="#b5">[6]</ref> 0.898 0.840 0.030 UACANet-S (Ours) 0.905 0.852 0.026 UACANet-L (Ours) 0.912 0.859 0.025</p><formula xml:id="formula_5">Dataset Method Mean Dice ? Mean IoU ? MAE ? Kvasir U-Net</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVC -ClinicDB</head><p>U-Net <ref type="bibr" target="#b22">[22]</ref> 0.823 0.755 0.019 U-Net++ <ref type="bibr" target="#b32">[32]</ref> 0.794 0.729 0.022 ResUNet++ <ref type="bibr" target="#b14">[14]</ref> 0.796 0.796 -SFA <ref type="bibr" target="#b6">[7]</ref> 0.700 0.607 0.042 PraNet <ref type="bibr" target="#b5">[6]</ref> 0.899 0.849 0.009 UACANet-S (Ours) 0.916 0.870 0.008 UACANet-L (Ours) 0.926 0.880 0.006 <ref type="table">Table 3</ref>: Comparison to the previous state-of-the-art methods and our UACANet on Kvasir and CVC-ClinicDB datasets. Red color denotes the best score among the methods, and blue color denotes the second best. ? denotes higher the better and ? denotes lower the better.</p><p>UACANet-S and the large model with 256 channels as UACANet-L. We use Res2Net <ref type="bibr" target="#b9">[9]</ref> with 26 ? 4 settings as a backbone network.</p><p>Intermediate backbone feature maps are acquired from each stage's last residual block (green box in <ref type="figure" target="#fig_0">Figure 1</ref>. For UACANet-L, similar to the DeeplabV3+ <ref type="bibr" target="#b3">[4]</ref>, we modified strides and dilation rates to increase the spatial size of the feature map. We resize images to 352 ? 352 for both training and inference and resize back to its original size. Unlike PraNet <ref type="bibr" target="#b5">[6]</ref> and other state-of-the-art methods, we adopt additional data augmentation techniques which are fairly  Red color denotes the best score among the methods, and blue color denotes the second best. ? denotes higher the better and ? denotes lower the better.</p><p>popular in semantic segmentation including random flipping on both horizontal and vertical axis, random image scaling from 0.75 to 1.25. We conduct random rotation from 0 to 359 degrees since colonoscopy images may rotate during examination. We also add additional random dilation and erosion for ground truth label to enhance generalization. We use Adam optimizer <ref type="bibr" target="#b15">[15]</ref> and the initial learning rate is set to 10 ?4 with polynomial learning rate decay <ref type="bibr" target="#b3">[4]</ref> with factor (1 ? ( ) 0.9 ). Compared to PraNet which trained only 60 epochs, we increase training epoch to 240 since we apply diverse data augmentation. We use Pytorch <ref type="bibr" target="#b21">[21]</ref> to implement our model and single Titan RTX GPU for train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>Following <ref type="bibr" target="#b5">[6]</ref>, images which have been randomly selected from Kvasir and CVC-ClinicDB is used for training, but we use same training data for fair comparison which has already been extracted from Kvasir and CVC-ClinicDB and it contains 1450 images total. For benchmark dataset, we use five different datasets.</p><p>CVC-ClinicDB CVC-ClinicDB <ref type="bibr" target="#b0">[1]</ref>, also known as CVC-612 contains 612 images from 25 colonoscopy videos and selected 29 sequences from them. The size of images is 384 ? 288. 62 images from this dataset are used for test and remaining images are used for training. CVC-300 CVC-300 is a test dataset from EndoScene <ref type="bibr" target="#b25">[25]</ref>. EndoScene contains 912 images from 44 colonoscopy sequences which were acquired from 36 patients total. Since EndoScene dataset is a combination of CVC-ClinicDB and CVC-300, following D.-P. Fan et al, we use CVC-300 as a test dataset which are 60 samples total. CVC-ColonDB CVC-ColonDB <ref type="bibr" target="#b1">[2]</ref> dataset is collected from 15 different colonoscopy sequences and sampled 380 images from these sequences. ETIS ETIS <ref type="bibr" target="#b23">[23]</ref> dataset contains 196 images which are collected from 34 colonoscopy videos. The size of images is 1225 ? 966 which is the largest among other datasets. Unless polyps in this dataset are vary in size and shape, they are mostly small and hard to find, which makes this dataset more challenging. Kvasir Kvasir <ref type="bibr" target="#b13">[13]</ref> Kvasir dataset consists of 1000 polyp images and corresponding annotations. Unlike the other datasets, images vary in size, from 332?487 to 1920?1072 and also the size of polyps which appear in the images vary in its size and shape. There are 700 large polyps which is larger than 160 ? 160, 48 small polyps smaller than 64 ? 64 and 323 medium polyps within the large and small scale. 900 images are used for training and 100 images are used for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study on Parallel Axial Attention</head><p>To exhibit the validity of PAA module, we conduct an experiment to evaluate UACANet without PAA modules. We design another model whose specific model architecture is identical to the UACANet except PAA modules (yellow box in <ref type="figure">Figure 3</ref>) is excluded. We choose three metrics to evaluate our methods, mean Dice (mDice), mean intersection over union (mIoU) and mean absolute error (MAE). We choose these two datasets for ablation study since CVC-ClinicDB is sampled for training while ETIS isn't. As shown in <ref type="table" target="#tab_1">Table 1</ref>, UACANet-S with PAA module shows better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study on uncertain area</head><p>We conduct an experiment to demonstrate the effectiveness of UACA. We leave all the details identical to the UACA except for excluding the uncertainty map (m in <ref type="figure">Figure 4</ref>), namely Context Attention (CA). We substitute UACA with CA in <ref type="figure" target="#fig_0">Figure 1</ref> to make CANet. We design CANet with same small and large version, namely CANet-S and CANet-L respectively. <ref type="table">Table 2</ref> shows quantitative results on CANet and UACANet on CVC-ClinicDB and ETIS. We also choose these two datasets for same reason as section 4.3. In terms of performance measure, UACANet consistently outperforms CANet on three major metrics.</p><p>We also visualize the output feature map of attention modules in both settings in <ref type="figure" target="#fig_6">Figure 5</ref> to verify the effectiveness of uncertain area qualitatively. UACANet substantially produce more precise results than CANet in terms of quality as well. In first and third row, while visualized feature map and the output of CANet-L shows that even though it detects substantial region of polyps, it fails to detect the ambiguous region which may seems hard to discriminate from mucosa, surface of colon. On the other hand, UACANet consistently segment polyp regions precisely. We also visualize uncertain area of UACA (m in <ref type="figure">Figure 4</ref>), it is easy to recognize that uncertain area is closely related to the boundary of polyps. Especially for the third row, m also helps to detect the ambiguous area denoted as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments with State-of-the-Art methods</head><p>As mentioned above, we compare our method with previous state-ofthe-art methods on five different polyp segmentation benchmarks. Since we train on sampled data from Kvasir and CVC-ClinicDB, even the test results are still unseen, the domain of images from these two datasets is still similar to the training data. Thus, we first demonstrate our results on first two datasets on <ref type="table">Table 3</ref>. On both datasets, our UACANet achieve the best performance among other methods. Especially, our UACANet-L achieves 92.6% mean Dice on CVC-Clinic DB which is 2.7% improvement over PraNet, the latest state-of-the-art method. In <ref type="table" target="#tab_4">Table 4</ref>, we evaluate our method with three completely unseen datasets. We mentioned above that ETIS is the most challenging datasets among other four, nevertheless UACANet-L achieve 76.6% mean Dice which is 13.8% improvement over PraNet.</p><p>We also demonstrate qualitative results on five benchmarks of previous state-of-the-art methods and our method ( <ref type="figure" target="#fig_6">Figure 5</ref>). On Kvasir and CVC-ClinicDB (first and second row), since two datasets are similar to the training dataset, all methods are able to segment the location of polyps, but our method show the most similar results compared to the ground truth. On ETIS dataset (third row), which is the most challenging dataset among other four benchmarks, both UACANet-S and UACANet-L are able to detect a small polyp even if the size of the polyp is very small and hard to notice while other methods has failed to detect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose a novel polyp segmentation network called UACANet which augments uncertain area to the context representation for accurate polyp detection. Without expensive edge annotations, we show that uncertain area is capable of representing boundary information. We propose Parallel Axial Attention for encoder for backbone features and decoder for initial saliency map. We also propose Uncertainty Augmented Context Attention which augments uncertainty area which represents complementary edge information. In a series of both quantitative and qualitative experiments shows that our method outperforms compared to the previous state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture of UACANet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, We add additional encoder network, Parallel Axial Attention encoder (PAA-e) for bottom-up stream and side-out fusion path. This helps to reduce the computational cost for both bottom-up and side-out fusion paths by reducing the number of channels for their input feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The details of Parallel Axial Attention (PAA)We use both binary cross entropy (BCE) loss and intersection over union (IoU) loss. The loss function L is computed as follows,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, the outputs of PAA-e are used for both decoder module and bottom-up stream. Also, we design Parallel Axial Attention decoder (PAA-d) with simple structure yet add additional PAA for final feature aggregation from different level of PAA-e features which are denoted as purple arrows in Figure 1 and Figure 3(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The details of Parallel Axial Attention encoder (PAA-e) (a) and decoder (PAA-d) (b) The details of Uncertainty Augmented Context Attention (UACA) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>m</head><label></label><figDesc>= max(m ? 0.5, 0), m = max(0.5 ? m, 0), m = 0.5 ? abs(m ? 0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of comparison with CANet-L and UACANet-L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results comparison with state-of-the-art methods on five different benchmarks.red boxes. Also, in second row, since ground truth miss another polyp on left bottom corner, UACANet detected the missing polyp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2107.02368v3 [cs.CV] 22 Jul 2021</figDesc><table><row><cell>2 RELATED WORK</cell></row><row><cell>2.1 Semantic Segmentation</cell></row></table><note>Fully Convolutional Network (FCN)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study for Parallel Axial Attention (PAA) on CVC-ClinicDB and ETIS datasets. ? denotes higher the better and ? denotes lower the better.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">Mean Dice ? Mean IoU ? MAE ?</cell></row><row><cell>CVC</cell><cell>UACANet-S (w/o PAA)</cell><cell>0.902</cell><cell>0.858</cell><cell>0.008</cell></row><row><cell>-ClinicDB</cell><cell>UACANet-S</cell><cell>0.916</cell><cell>0.870</cell><cell>0.008</cell></row><row><cell>ETIS</cell><cell>UACANet-S (w/o PAA) UACANet-S</cell><cell>0.684 0.694</cell><cell>0.603 0.615</cell><cell>0.029 0.023</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>mIoU ? MAE ? mDice ? mIoU ? MAE ? mDice ? mIoU ? MAE ?</figDesc><table><row><cell cols="2">Method mDice ? U-Net [22] 0.398</cell><cell>ETIS 0.335</cell><cell>0.036</cell><cell cols="2">CVC-ColonDB 0.512 0.444</cell><cell>0.061</cell><cell>0.710</cell><cell>CVC-300 0.627</cell><cell>0.022</cell></row><row><cell>U-Net++ [32]</cell><cell>0.401</cell><cell>0.344</cell><cell>0.035</cell><cell>0.483</cell><cell>0.410</cell><cell>0.064</cell><cell>0.707</cell><cell>0.624</cell><cell>0.018</cell></row><row><cell>SFA [7]</cell><cell>0.297</cell><cell>0.217</cell><cell>0.109</cell><cell>0.469</cell><cell>0.347</cell><cell>0.094</cell><cell>0.467</cell><cell>0.329</cell><cell>0.065</cell></row><row><cell>PraNet [6]</cell><cell>0.628</cell><cell>0.567</cell><cell>0.031</cell><cell>0.709</cell><cell>0.640</cell><cell>0.045</cell><cell>0.871</cell><cell>0.797</cell><cell>0.010</cell></row><row><cell>UACANet-S (Ours)</cell><cell>0.694</cell><cell>0.615</cell><cell>0.023</cell><cell>0.783</cell><cell>0.704</cell><cell>0.034</cell><cell>0.902</cell><cell>0.837</cell><cell>0.006</cell></row><row><cell>UACANet-L (Ours)</cell><cell>0.766</cell><cell>0.689</cell><cell>0.012</cell><cell>0.751</cell><cell>0.678</cell><cell>0.039</cell><cell>0.910</cell><cell>0.849</cell><cell>0.005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison to the previous state-of-the-art methods and our UACANet on ETIS, CVC-ColonDB and CVC-300 datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Javier</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debora</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Vilari?o</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2015.02.007</idno>
		<ptr target="https://doi.org/10.1016/j.compmedimag.2015.02.007" />
	</analytic>
	<monogr>
		<title level="j">Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards automatic polyp detection with a polyp appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilari?o</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2012.03.002</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2012.03.002" />
	</analytic>
	<monogr>
		<title level="m">Best Papers of Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
		</imprint>
	</monogr>
	<note>IbPRIA&apos;2011</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Computational Approach to Edge Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.1986.4767851</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.1986.4767851" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective Feature Aggregation Network with Area-Boundary Constraints for Polyp Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Yu</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer-Verlag</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-7_34" />
		<imprint>
			<biblScope unit="page" from="302" to="310" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Semantics-enriched Representation via Self-discovery, Self-classification, and Self-restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Reza Hosseinzadeh</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="137" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<title level="m">Axial attention in multidimensional transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Thomas De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">De</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard D</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><forename type="middle">Le</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention u-net: Learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Granado</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-013-0926-3</idno>
		<ptr target="https://doi.org/10.1007/s11548-013-0926-3" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selectivity or invariance: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3799" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of healthcare engineering</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Haibin Ling, and Ruigang Yang. 2021. Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV</title>
		<meeting>the European conference on computer vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Joon-Young Lee, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Edge guided salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mahfuzur Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
