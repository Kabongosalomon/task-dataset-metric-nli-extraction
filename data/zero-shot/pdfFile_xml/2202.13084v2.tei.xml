<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Speech Recognition for Multiple Languages in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Meta AI</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Meta AI</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Speech Recognition for Multiple Languages in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual speech recognition (VSR) aims to recognize the content of speech based on lip movements, without relying on the audio stream. Advances in deep learning and the availability of large audio-visual datasets have led to the development of much more accurate and robust VSR models than ever before. However, these advances are usually due to the larger training sets rather than the model design. Here we demonstrate that designing better models is equally as important as using larger training sets. We propose the addition of prediction-based auxiliary tasks to a VSR model, and highlight the importance of hyperparameter optimization and appropriate data augmentations. We show that such a model works for different languages and outperforms all previous methods trained on publicly available datasets by a large margin. It even outperforms models that were trained on non-publicly available datasets containing up to to 21 times more data. We show, furthermore, that using additional training data, even in other languages or with automatically generated transcriptions, results in further improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Visual speech recognition (VSR) aims to recognize the content of speech based on lip movements, without relying on the audio stream. Advances in deep learning and the availability of large audio-visual datasets have led to the development of much more accurate and robust VSR models than ever before. However, these advances are usually due to the larger training sets rather than the model design. Here we demonstrate that designing better models is equally as important as using larger training sets. We propose the addition of prediction-based auxiliary tasks to a VSR model, and highlight the importance of hyperparameter optimization and appropriate data augmentations. We show that such a model works for different languages and outperforms all previous methods trained on publicly available datasets by a large margin. It even outperforms models that were trained on non-publicly available datasets containing up to to 21 times more data. We show, furthermore, that using additional training data, even in other languages or with automatically generated transcriptions, results in further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual speech recognition (VSR), also known as lipreading, is the task of automatically recognizing speech from video based only on lip movements. In the past, this field has attracted a lot of research attention within the speech recognition community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> but it has failed to meet the initial high expectations. There are two main reasons why the first generation of VSR models fell short: (1) the lack of large transcribed audio-visual datasets resulted in models that could only recognize a limited vocabulary and work only in a laboratory environment and (2) the use of handcrafted visual features, which might not have been optimal for VSR applications, prevented the development of highaccuracy models. Recently, large audio-visual transcribed datasets, like LRS2 <ref type="bibr" target="#b2">[3]</ref> and LRS3 <ref type="bibr" target="#b3">[4]</ref>, have become available, and these have allowed the development of a large vocabulary and robust models. In addition, advances in deep learning have made possible the use of end-to-end models, which learn to extract VSR-related features directly from raw images. These developments have led to a new generation of deep-learning-based VSR models that achieve much higher accuracy than older models and also work in unseen real-life situations.</p><p>The recent advances in VSR models are mainly fuelled by using increasingly larger transcribed datasets and the development of models that work well when trained with huge amounts of data. Some recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> use tens of thousands of hours of non-publicly available training data to achieve state-of-the-art performance on standard benchmarks. In contrast to this recent trend, we demonstrate that carefully designing a model is equally as important as using larger training sets. Our approach revolves around (1) addition of prediction-based auxiliary tasks to a VSR model, <ref type="bibr" target="#b1">(2)</ref> appropriate data augmentations and (3) hyperparameter optimization of an existing architecture. This leads to a great reduction in word error rate (WER) and results in state-of-the-art performance on almost all benchmarks. This is achieved by using only publicly available datasets, which are two orders of magnitude smaller than those used in previous works. We also show that combining multiple datasets further improves the performance (which is in line with the results reported in the literature). Hence, we argue that further progress in the field can be achieved not only by increasing the size of the training data but also by careful model design and optimization.</p><p>The vast majority of existing works focus on improving the performance of English-only VSR models. There are also a few works that design models tailored to a specific language, like Mandarin <ref type="bibr">[7,</ref><ref type="bibr">8,</ref><ref type="bibr">9]</ref>. In contrast to previous works, our approach is evaluated not only on English but also on Mandarin and Spanish (the two other widely spoken languages), Italian, French and Portuguese. Stateof-the-art performance is achieved in all languages.</p><p>Specifically, in this Article, we make the following contributions:</p><p>? We propose a novel method for VSR that outperforms state-of-the-art methods trained on publicly available data by a large margin.</p><p>? We do so with a VSR model with auxiliary tasks that jointly performs VSR and prediction of audio and visual representations.</p><p>? We demonstrate that the proposed VSR model performs well, not only in English, but also in other languages, such as Spanish, Mandarin, Italian, French and Portuguese.</p><p>? We show that enlarging the training sets, even by including unlabelled data with automatically generated transcriptions or videos in other languages, results in improved performance. This provides further evidence for the hypothesis that the recent improvements presented in the literature are probably the result of larger training sets and not necessarily of better models.</p><p>? We discuss challenges for VSR systems that need to be solved and ethical considerations that must be taken into account before this technology can be widely applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Baseline VSR Model</head><p>The baseline VSR model that we extend in this work is based on <ref type="bibr">[10]</ref>.</p><p>The model consists of a threedimensional (3D) convolutional layer with a receptive field of five frames, followed by a 2D ResNet-18 ( <ref type="figure" target="#fig_0">Fig. 1e</ref>) , a 12-layer Conformer model <ref type="bibr">[11]</ref> and a transformer decoder as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>. The model is trained end to end using a combination of the connectionist temporal classification (CTC) loss with an attention mechanism. Data augmentation is also used during training in the form of random cropping and image flipping (applied to all frames in the same sequence). This model achieves state-of-theart VSR performance on the LRS2 and LRS3 datasets, when only publicly available data are used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Baseline ASR Model</head><p>The baseline Automatic Speech Recognition (ASR) model that we use is based on <ref type="bibr">[10]</ref>. The model consists of an 1D ResNet-18 ( <ref type="figure" target="#fig_0">Fig. 1d</ref>), a 12-layer Conformer model and a transformer decoder as shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>. This model also follows the hybrid CTC/attention architecture and is trained end to end. Time-masking is also used as data augmentation during training. At the moment, this is the state-of-the-art ASR model on the LRS2 and LRS3 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Our Approach</head><p>In contrast to previous works, which improve the VSR performance by using increasingly larger training sets, we focus on improving the performance by carefully designing a model without relying on additional data. This is achieved by revising the training strategy and architecture of the state-of-the-art model proposed in <ref type="bibr">[10]</ref>. First, we optimize hyperparameters and improve the language model (LM) with the aim of squeezing extra performance out of the model. Second, we introduce time-masking, which is a temporal augmentation method that is commonly used in ASR models. It substantially improves the VSR performance by forcing the model to rely more on contextual information and, as a consequence, it can better disambiguate similar lip movements that correspond to different phonemes. Finally, we use a VSR model with auxiliary tasks where the model jointly performs VSR and prediction of audio and visual representations extracted from pre-trained VSR and ASR models. This prediction task provides an additional supervisory signal and forces the model to learn better visual representations. A diagram of the architecture of our model is shown in <ref type="figure" target="#fig_0">Fig. 1c</ref>.</p><p>The performance of our model is presented in <ref type="table" target="#tab_1">Tables 1  to 4</ref>. Owing to the random nature of training, we train ten models for each experiment and report the mean and standard deviation of the WER over the ten runs. This is in contrast to previous works, which report just a single value (most probably the best WER) and no standard deviation, and it provides a more robust estimate of the actual performance. However, to facilitate a fair comparison with other works, we also report the best WER of the ten runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Results on LRS2</head><p>The results on LRS2-an English audio-visual dataset-are reported in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Our model outperforms all existing works by a large margin, even when it is trained on smaller amounts of training data. In particular, it outperforms the previous state of the art [10], in terms of the best WER achieved, by 5 %. This is despite the fact that [10] is trained on a larger training set. When we use the same training set size as in [10] our model results in a 9.2 % improvement. When we use additional training data, an even larger improvement of 12.4 % is observed. Similarly, our approach results in a 22.8 % absolute improvement in the best WER over <ref type="bibr" target="#b3">[4]</ref> which uses a training set with similar size to ours and also includes non-publicly available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Results on LRS3</head><p>The results on LRS3-another English audio-visual dataset-are presented in <ref type="table" target="#tab_2">Table 2</ref>. In this case too, our proposed approach substantially outperforms all existing works that are trained using publicly available datasets. In particular, our method leads to an 8.2 % absolute improvement, in terms of the best WER, over the state of the art [10] when the same training data are used. As expected, a smaller absolute improvement of 5.4 % is reported when a smaller training set is used. In the case of additional training data being available, a larger absolute improvement of 11.8 % is achieved.</p><p>There are also some works that rely on very large nonpublicly available datasets for training. As a consequence, it is not clear whether the reported improvement in WER is due to a better model or simply to the large amount of training data. Our approach outperforms all works that use up to 21 times more training data. More specifically, our best model, trained on 1 453 h of video, leads to a 2.1 % absolute improvement over <ref type="bibr">[16]</ref> which uses 31 000 hours of training data. However, it performs worse than <ref type="bibr" target="#b5">[6]</ref>, which presents a model trained on 90 000 hours, which is   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Results on CMLR</head><p>The results on the CMLR dataset-a Mandarin audiovisual dataset-are shown in <ref type="table" target="#tab_3">Table 3</ref>. We report performance in terms of character error rate (CER) instead of WER, because Chinese characters are not separated by spaces. Our approach results in a substantial reduction in the CER over all existing works. We achieve an absolute improvement of 12.9 % over the state of the art [9]. The WER can be further reduced by 1.1 % by first pre-training our model on English and then fine-tuning it on the CMLR training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7">Results on CMU-MOSEAS-Spanish</head><p>The results on the CMU-MOSEAS-Spanish dataset-an audio-visual Spanish dataset-are shown in <ref type="table" target="#tab_4">Table 4</ref>. Given that this is a small dataset, it is not possible to train an accurate model without using additional data.   For this purpose, we first pre-trained the model on English datasets and then fine-tuned it on the training sets of CMU-MOSEAS and TEDx datasets using the Spanish videos only. Because this is a new dataset and there are no results from previous works, we trained the end-toend model presented in [10] to serve as the baseline. We observe that our proposed approach results in a 7.7 % absolute reduction in the WER. A further reduction of 6.5 % can be achieved by using additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.8">Comparison between Mean and Best WER/CER</head><p>In all results shown in Tables 1 to 4 we report both the mean and the best performance over ten runs. We observe that the mean WER, which is more representative of the actual performance, is up to 0.8 % worse than the best WER. The only exception is for the CMLR dataset (Table 3), where the mean and best CER are practically the same, mainly as a result of the large size of the test set. This difference between the mean and best WER is something that should be taken into account when comparing different models, especially when the models are tested on relatively small test sets and the results are too close. There are also a number of applications based exclusively on VSR. Silent speech interfaces (SSIs) <ref type="bibr">[28]</ref>, which can enable speech communication to take place when an audible speech signal is not available, can be developed with the help of VSR systems. This means that a speaker would be able to mouth words instead of vocalizing them. This technology has the potential to transform the lives of speech-impaired people. Individuals who have lost the ability to speak (aphonia) or have difficulty in speaking (dysphonia) due to tracheostomy, laryngectomy, stroke or injury might find it hard to communicate with others. The use of SSI can alleviate this by providing an alternative way of communication and at the same time reduce the stress caused by the sudden loss of their voice. The use of SSI can also be useful in cases where speaking is not allowed, for example, in a meeting, and can provide privacy in public conversations.</p><p>VSR technology also opens up opportunities to automatically transcribe video content that was recorded without audio, like silent movies, CCTV footage or video captured by older webcams, and would otherwise require substantial manual effort or might have even been impossible. It can also be used as a useful tool in face forgery detection <ref type="bibr">[29]</ref>. Most face-manipulation approaches add inconsistencies in mouth movements, which might not always be perceptible by humans, but they can easily be detected by properly trained VSR models. Finally, there is a new form of VSR that has become popular recently and generates audio, instead of text, directly from the input video <ref type="bibr">[30,</ref><ref type="bibr">31]</ref>. This is essentially a combination of a standard VSR model with a text-to-speech model, but it has two important advantages: (1) it does not require any transcribed dataset and can be trained with vast amounts of unlabelled audio-visual data, and (2) it is faster and can potentially be used in real-time applications as it removes the constraint of recognizing a complete word before generating the corresponding speech signal. This new approach is especially useful for audio inpainting applications, because it can automatically fill in audio gaps from video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges</head><p>Despite the great advances in VSR, there are still numerous challenges that need to be solved before the full potential of this technology can be achieved. First, visual ambiguities that arise from the fact that different phonemes correspond to similar lip movements is one of the most important reasons for the substantial performance gap between ASR and VSR models. Designing VSR systems that can resolve some of these ambiguities by relying more on the context, like the time-masking augmentation proposed in this work, might close this gap. In addition, VSR systems are sensitive to visual noise like lighting changes, occlusions, motion blur and compression. Reduced and/or mismatched resolution and frame rate between training and test conditions can also affect performance. There is some evidence that VSR systems are robust to small or moderate amounts of noise and less robust to reduced resolution [32, 33], but further studies are needed to establish the impact of each noise type.</p><p>Another challenge is that a VSR model should be person-independent and pose-invariant. However, it is well known that deep networks rely heavily on texture [34]. This can potentially degrade the performance, because unknown test subjects and head pose can substantially affect the appearance of the mouth. This is typically addressed by training the VSR models on a large number of subjects with varying poses. Some preliminary works on pose-invariant [35] and subject-independent [36] VSR have shown that this can be addressed in a more principled way, and this is another area that deserves further attention. Similarly, multi-view VSR [37] can be beneficial, but it is not yet clear which lip views are optimal and how they should be combined. The availability of multiple cameras in meeting rooms, cars and in modern smartphones opens up a new opportunity for improving VSR systems. The vast majority of VSR systems have focused on plain English speech. However, it is known that lip movements are affected by the context where speech is produced and the type of speech. There is evidence that lip movements tend to increase in silent speech <ref type="bibr">[38]</ref> and also when speech is produced in noise (Lombard effect) <ref type="bibr">[39]</ref>. Despite studies that show a performance drop when VSR models <ref type="bibr" target="#b6">[40,</ref><ref type="bibr" target="#b7">41,</ref><ref type="bibr" target="#b8">42]</ref> are tested on such conditions, this area remains unexplored. Finally, the development of non-English VSR systems that take into account the unique characteristics and accents of each language also remains an open challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ethical Considerations</head><p>It is important to note that VSR is a dual-use technology, which means it can have a positive impact on society as well as a negative one. Although our objective is to build VSR systems that will be beneficial for society, like the applications mentioned above, this technology can also be misused. One example is that it can be deployed for surveillance via CCTV or even via smartphone cameras, which raises privacy concerns <ref type="bibr" target="#b9">[43,</ref><ref type="bibr" target="#b10">44]</ref>. A potential side effect of this is that it might discourage people from speaking in public if they believe that their conversation can be intercepted by anyone carrying a camera <ref type="bibr" target="#b10">[44]</ref>. Sophisticated surveillance using VSR technology might not be possible at the moment, especially via CCTV due to the low quality of CCTV camera images, compared to the high-quality data used during training, but it should not be ignored. Cameras and VSR systems are getting better, so it might become a serious privacy concern rather soon unless automatic blurring of all faces of people who did not provide an explicit consent becomes a new standard.</p><p>Commercial applications of VSR technology are still at a very early stage. One of the very few examples is a smartphone application that aims to help speech-impaired individuals communicate and is currently being trialled in UK NHS hospitals. This is being developed by Liopa <ref type="bibr" target="#b11">[45]</ref>, which also works on keyword spotting from CCTV footage. We thus argue that appropriate government regulations for VSR systems, which address privacy concerns and potential misuse, are necessary at this early stage before the technology is fully commercialized. This will allow the proper auditing of every new application before it reaches the market, so that the risks and merits can be properly communicated to users and the public. Otherwise, VSR systems may have the same fate as face recognition technology, which was commercialized without proper regulation being in place. As a consequence, a ban on using face recognition was introduced in several cities <ref type="bibr" target="#b12">[46,</ref><ref type="bibr">47]</ref> and some companies either stopped offering such services or put restrictions on their use <ref type="bibr" target="#b14">[48,</ref><ref type="bibr" target="#b15">49,</ref><ref type="bibr" target="#b16">50]</ref> when the ethical concerns became widely known.</p><p>It should also be pointed out that VSR technology might be biased against specific age groups, genders, cultural backgrounds or non-native speakers. Most of the publicly available datasets have been collected from TV programmes, TED talks or YouTube videos. Hence, it is very likely that some groups are underrepresented, for example, younger people when data are collected from TV programmes or older people when data are collected from YouTube. Similarly, it is likely that people from specific cultural backgrounds or non-native speakers are also underrepresented. This will lead to VSR models that are less accurate for all these groups. Because demographic information is not available for any publicly available dataset used for training VSR models, it is not easy to verify whether such biases exist. VSR models need to be trained on demographically diverse data, including non-native speakers, to ensure similar performance across different user groups. This will lead to VSR systems whose accuracy is not lower for some users because their age, gender, cultural background or accent is underrepresented in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual Speech Recognition</head><p>Our method outperforms state-of-the-art methods by a large margin for VSR in multiple languages. In what follows we explain the details of our approach and the changes that we have made to the training strategy and architecture that led to this highly improved performance.  and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Wav2Vec2-large-xlsr-53english (https://huggingface.co/jonatasgrosman/ wav2vec2-large-xlsr-53-english), were then used to obtain machine-generated transcriptions for these videos. We only kept the videos where the WER between the two generated transcriptions was below 60 %, resulting in 350,991 videos with a total duration of 641 h. The transcriptions generated by the Wav2Vec2-Base-960h model were used for these videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Metrics</head><p>WER is the most common metric used in speech recognition. This measures how close the predicted word sequence is to the target word sequence. Assuming S is the number of substitutions, D is the number of deletions, I is the number of insertions needed to get from the predicted to the target sequence and N is the number of words in the target sequence, then the metric can be defined as</p><formula xml:id="formula_0">W ER = S + D + I N<label>(1)</label></formula><p>Similarly to WER, we can define the CER, which measures how close the predicted and target character sequences are. In this case, S, D and I are computed at the character level and N is the total number of characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-processing</head><p>We used the RetinaFace <ref type="bibr" target="#b21">[55]</ref> face detector and the Face Alignment Network (FAN) <ref type="bibr" target="#b22">[56]</ref> to detect 68 facial landmarks. The faces were then registered to a neutral reference frame using a similarity transformation to remove translation and scaling variations. A bounding box of 96 ? 96, centred on the mouth centre, was used to crop the mouth region of interest. The cropped patch was further converted to grey-scale and normalized with respect to the overall mean and variance of the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hyperparameter optimization</head><p>Hyperparameter optimization aims to improve the performance of a model by fine-tuning the values of the parameters that are used to control the training process or the model architecture. Some of the most common hyperparameters that are usually optimized are the following: initial learning rate, learning rate decay parameters, number of layers, size of layers, dropout rate and the loss function weights, which are used to combine the different loss terms. Additional hyperparameters related to conformers are the number and size of the self-attention heads. We performed hyperparameter optimization on the LRS2 dataset by attempting to reduce the WER on the validation set. Our conclusion was that the parameters used in the baseline model [10] were already optimal, so no further improvement was observed.</p><p>The next step was to optimize other hyperparameters that might not have been exhaustively optimized, like batch-size-related parameters. Again, the parameters were chosen based on the validation set performance. Further details and results are provided in Supplementary Section S4 and <ref type="table" target="#tab_13">Supplementary Table S8</ref>, respectively. The results on the LRS2 and LRS3 test sets are shown in Supplementary <ref type="table" target="#tab_14">Table S9</ref>. Each hyperparameter was optimized independently based on the WER on the validation set of LRS2. We used the same hyperparameters for all experiments. It is clear that hyperparameter optimization results in a substantial reduction in the WER for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Improving LMs</head><p>A LM determines the probability of a given sequence of characters. It is used during decoding and favours sequences that are more likely to occur. To increase the capacity of the LM we use multiple text corpora for training. We also increase the number of sequences considered during decoding (beam size is set to 40). The impact of these changes is demonstrated in <ref type="table" target="#tab_14">Supplementary Table S9</ref>, where the WER is reduced for both English datasets.</p><p>The score from the LM (S LM ) is incorporated in decoding as follows:</p><formula xml:id="formula_1">S = ?S CT C + (1 ? ?)S att + ?S LM<label>(2)</label></formula><p>where S CT C and S att are the scores of the CTC and decoder branch, respectively, and ? and ? correspond to the CTC and LM score weights. Additional details about the corpora used for training the LM in each language, as well as training details, are presented in Supplementary Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Time Masking</head><p>Data augmentation works by synthesizing additional distorted training data with the goal of reducing over-fitting. In VSR, most existing works make use of image transformations such as random cropping and horizontal flipping <ref type="bibr" target="#b23">[57,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>. These spatial augmentations are helpful, but they do not take into account the temporal nature of visual speech. Only a few works exist that apply temporal augmentations like deleting or duplicating frames <ref type="bibr" target="#b24">[58]</ref> or variable length augmentation <ref type="bibr" target="#b25">[59]</ref>.</p><p>In this Article, we propose the use of time-masking, which is commonly used in training ASR models <ref type="bibr" target="#b26">[60]</ref>. It works by randomly masking n consecutive frames by replacing them with the mean sequence frame. This allows the model to more effectively use contextual information and can better disambiguate similar lip movements that correspond to different phonemes. It also makes the model more robust to short missing segments. Given that there is large variance in the video lengths, especially on the LRS2 and LRS3 datasets, the number of masks used is proportional to the length of the training sequence. Specifically, we use one mask per second and, for each mask, we randomly mask up to 40% of frames, with the masked segments chosen using a uniform distribution. Additional details about this augmentation are provided in Supplementary Section S6.</p><p>The impact of time-masking is shown in the ablation study on the LRS2 and LRS3 datasets shown in <ref type="table" target="#tab_6">Table 5</ref>. Training a model without time-masking results in a substantial increase in the mean WER when compared to the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Prediction-based Auxiliary Tasks</head><p>The standard approach to VSR relies on end-to-end training, which allows the entire model to be optimized towards the desired target. This is an attractive property and has led to impressive results, but also results in substantial challenges in training such a large model. One solution that has recently been proposed is the use of auxiliary tasks in the form of additional losses applied to intermediate layers of the model <ref type="bibr" target="#b27">[61,</ref><ref type="bibr" target="#b28">62,</ref><ref type="bibr">63]</ref>. This acts as regularization, which helps the model learn better representations and leads to better generalization on test data.</p><p>Based on this observation, we propose as an auxiliary task the prediction from intermediate layers of audio and visual representations learned by pre-trained ASR and VSR models <ref type="figure" target="#fig_0">(Fig. 1c</ref>). This is inspired by the recent success of prediction tasks in self-supervised learning. In particular, good audio representations can be learned by predicting handcrafted audio features <ref type="bibr" target="#b29">[64]</ref> or by using joint audio and visual supervision <ref type="bibr" target="#b30">[65]</ref>. Similarly, visual speech representations can be learned by predicting audio features <ref type="bibr" target="#b31">[66]</ref>. Hence, the proposed auxiliary task provides additional supervision to the intermediate layers of the model, which in turns results in better visual representations and improved performance. Mathematically, this is formulated as a regression problem where the goal is to minimize the L1 distance between the predicted and pre-trained visual and audio features. This results in the following loss term added to the loss function:</p><formula xml:id="formula_2">L AUX = ? a h a (f l (x v )) ? g l a (x a ) 1 + ? v h v (f l (x v )) ? g l v (x v ) 1<label>(3)</label></formula><p>where x v and x a are the visual and audio input sequences, respectively, g v and g a are the pre-trained visual and audio encoders, respectively. f is the subnetwork up to layer l whose intermediate representation is used as input to the audio and visual predictors h a and h v , respectively. ? a and ? v are the coefficients for each loss term and ? 1 is the 1 -norm. The model performs VSR and at the same time attempts to predict audio and visual representations from interme-diate layers. Hence, the final loss is simply the addition of the main VSR loss and the auxiliary loss:</p><formula xml:id="formula_3">L = L VSR + L AUX (4) L VSR = ?L CT C + (1 ? ?)L att<label>(5)</label></formula><p>where L V SR is the loss of the hybrid CTC/attention architecture used. L CT C is the CTC loss, L att the loss of the attention mechanism, and ? controls the relative weight of each loss term. Further details about the losses are provided in Supplementary Section S7. We emphasize that the proposed method is not architecture-dependent and can also be used with other more advanced visual front ends <ref type="bibr">[17]</ref>.</p><p>The substantial impact of the auxiliary losses on performance can be observed from <ref type="table" target="#tab_6">Table 5</ref>. Removing either loss, that is, either the first or second term from equation (3), leads to an increase in the mean WER for both datasets. In the case where both losses are removed, that is, no auxiliary loss is used, then the increase in the mean WER is even greater. Finally, the removal of the two losses and time-masking results in a substantial decrease in performance.</p><p>An ablation study on the effect of layer l where the auxiliary loss (equation <ref type="formula" target="#formula_2">(3)</ref>) is attached is shown in Supplementary <ref type="figure" target="#fig_0">Fig. S1</ref>. Layer 6 was found to be the optimal level based on the performance on the validation set. All results reported in all the tables are based on this configuration. Further details are presented in Supplementary Section S9.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Using Additional Training Data</head><p>Using larger and larger training sets with a view to reducing the WER is a recent trend in the literature. To investigate the impact of the amount of training data, we trained models on varying amounts of data. We started by training models using only the training set of each database (seventh row of <ref type="table" target="#tab_1">Table 1</ref> and fourth row of Table 2). It is not possible to train a model from scratch on the LRS2 and LRS3 datasets, so we used curriculum learning. This means that we first used only short utterances and as training progresses we kept adding longer ones. Further details on curriculum learning are provided in Supplementary Section S8. We used a model trained for recognizing 500 English words <ref type="bibr" target="#b25">[59]</ref> on the LRW dataset for initialization, then fine-tuned it on the corresponding training sets of the LRS2 or LRS3 datasets (eighth row of <ref type="table" target="#tab_1">Table 1 and fifth row of Table 2</ref>). Finally, we used the models trained on LRW + LRS3 and LRW + LRS2 as initialization and fine-tuned them further on LRS2 and LRS3, respectively (ninth row of <ref type="table" target="#tab_1">Table 1 and sixth row  of Table 2</ref>). It is clear that, as we use more datasets for training, the performance keeps improving. This is also the case for Spanish and Mandarin (sixth row of <ref type="table" target="#tab_3">Table 3</ref> and third row of <ref type="table" target="#tab_4">Table 4</ref>), even when models trained on English are used for initialization. However, the reduction in WER is smaller than in English, probably due to language mismatch.</p><p>Finally, we used a subset of the AVspeech dataset as additional training data together with the automatically generated English transcriptions. Again, the WER is reduced in all languages (tenth row of <ref type="table" target="#tab_1">Table 1, seventh row  of Table 2, last row of Table 3</ref> and 4), despite using transcriptions that contain errors, with the smallest reduction observed in Mandarin. This is not surprising, because Mandarin is much less similar to English than Spanish. These results are in line with the hypothesis that the reduction in the WER reported in recent works is mainly due to the larger datasets used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Implementation</head><p>Our experiments were implemented using an open-source toolkit, ESPNet <ref type="bibr" target="#b32">[67]</ref>. We trained the models with the Adam optimizer <ref type="bibr" target="#b33">[68]</ref> with ? 1 = 0.9, ? 2 = 0.98 and = 10 ?9 . The learning rate increases linearly in the first 25,000 steps, yielding a peak learning rate of 0.0004 and thereafter decreasing in proportional to the inverse square root of the step number. The network was trained for 50 epochs with a batch size of 16. We used the model averaged over the last ten checkpoints for evaluation. Details regarding the network architecture are provided in Supplementary Section S2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this Article we have presented our approach for VSR and demonstrated that state-of-the-art performance can be achieved not only by using larger datasets, which is the current trend in the literature, but also by carefully designing a model. We have highlighted the importance of hyperparameter optimization, which can further improve the performance of existing architectures. We have then shown the importance of time-masking, which forces the network to focus more in the context. We have also proposed a new architecture based on auxiliary tasks where the VSR model also predicts audio-visual representations learned by pre-trained ASR and VSR models. Finally, we have provided evidence that using larger datasets improves the performance, which is in line with recent works in this field. Our approach outperforms all existing VSR works trained on publicly available datasets in English, Spanish and Mandarin, by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>The datasets used in the current study are available from the original authors on the LRS2 (https: </p><formula xml:id="formula_4">/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 Datasets Details</head><p>Details about the audio-visual datasets used in this study are presented in <ref type="table" target="#tab_1">Supplementary Table S1</ref>. It is clear that the non-publicly available datasets are one to two orders of magnitude larger than the publicly available ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Architecture Details</head><p>The model consists of 4 modules, a front-end encoder, VSR encoder in <ref type="figure" target="#fig_0">Fig. 1c</ref> , a back-end encoder, a hybrid CTC and transformer decoder and two predictors. In particular, the encoder receives as input the raw images and maps them to visual speech representations which are fed to the back-end encoder. This is followed by a CTC and transformer decoder which generates the predicted characters. Finally, the features extracted from the middle position of the back-end encoder flow through two separate predictors to predict visual and acoustic speech representations from pre-trained VSR and ASR models, respectively. The front-end encoder consists of a 3D convolutional layer with a kernel size of 5 ? 7 ? 7 followed by a ResNet-18 <ref type="bibr" target="#b35">[70,</ref><ref type="bibr" target="#b36">71]</ref>. Let B ? T ? H ? W be the input tensor to the visual front-end module, where B, T , H, and W correspond to batch size, number of frames, height and width, respectively. The visual features at the top of the residual blocks are aggregated along the spatial dimension by a global average pooling layer, resulting in a feature output of dimensions B ? C ? T , where C indicates the channel dimensionality. The Swish activation functions is used in all layers. The detailed architecture can be seen in <ref type="table" target="#tab_2">Supplementary Table S2</ref>.</p><p>The back-end encoder starts with a positional embedding module, followed by a stack of 12 conformer blocks. The positional embedding module is a linear layer, which projects the features from the output of ResNet-18 to a 256-dimensional space. The transformed features are further injected with relative position information <ref type="bibr" target="#b38">[73]</ref>. In each conformer block, a feed-forward module, a selfattention module, a convolution module, and a second feed-forward module are stacked in order. Specifically, the feed-forward module is comprised of a linear layer, which projects the features to a higher 2048-dimensional space, followed by a Rectified Linear Unit (ReLU) activation function, a dropout layer with a probability of 0.1, and a second linear layer with output dimension of 256. Half-step residual connections are also used in each feedforward module. The self-attention module is capable of modeling global dependencies among elements. The module maps the query and a set of key-value pairs through an attention map, which focuses on different parts of the input. Instead of performing a single attention function, a multi-head mechanism is leveraged with different linear projections to a lower 64-dimensional space. The attention function is performed in parallel on each head and the outputs are concatenated into a 256-dimensional space and once again projected into the final values. The con- volutional module, which excels at capturing local patterns efficiently, is composed of an 1D point-wise convolutional layer, Gated Linear Units (GLU) <ref type="bibr" target="#b39">[74]</ref>, an 1D depthwise convolutional layer, a batch normalisation layer, a swish activation layer, a 1D point-wise convolutional layer, and a layer normalisation layer. The combination of selfattention and convolution is capable of better capturing both local and global temporal information compared to the standard transformer architecture <ref type="bibr">[11]</ref>. The decoder is composed of an embedding module and a set of residual multi-head attention blocks. It takes as input the encoded sequence and the prefixes of the target sequence. First, the prefixes from index 1 to l -1 are projected to embedding vectors, where l is the target length index. The absolute positional encoding <ref type="bibr" target="#b40">[75]</ref> is also added to the embedding. Next, the embedding is fed to a stack of multi-head attention blocks. Each block consists of a self-attention module, an encoder-decoder attention module and a feed-forward module. Layer normalisation is added before each module. Specifically, the self-attention module is slightly different from the one in the encoder  where future positions at its attention matrix are masked out, followed by an encoder-decoder attention, which helps the decoder to focus on the relevant part of the input. This attention receives the features from the previous selfattention module as Q and the features from the encoder as K and V (K = V ). The features are further fed to a feed-forward module, which is the same as the one used in the encoder. Finally, a layer normalisation and a linear layer are added which predict the posterior distribution of the next generated token.</p><p>A linear layer with a softmax function, which maps the encoded features to the predicted character sequence is also used on top of the back-end encoder . This layer is trained with the CTC loss.</p><p>The predictor is a linear layer which takes as input the features at the middle block (6th) of the back-end encoder and predicts the corresponding audio/visual features from the pre-trained ASR/VSR models. Separate predictors are employed for each prediction task. Both the input and output dimensions of the linear layer are 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Pre-trained VSR and ASR models</head><p>The pre-trained ASR and VSR models are shown in <ref type="figure" target="#fig_0">Fig. 1a</ref> and 1b, respectively. The pre-trained VSR model has exactly the same architecture as the full model described in Supplementary Section S2 but does not include any predictors. The pre-trained ASR model replaces the VSR encoder with an ASR encoder and its architecture can be seen in <ref type="figure" target="#fig_0">Fig. 1d</ref> and <ref type="table" target="#tab_3">Supplementary Table S3</ref>. It should be noted that these models are always trained on the same data as the full model. Then the pre-trained ASR/VSR encoders and some conformer layers are frozen and their internal representations are used as targets for the audio and visual predictors as shown in <ref type="figure" target="#fig_0">Fig. 1c</ref>. The performance of the pre-trained models for all languages can be seen in <ref type="table" target="#tab_4">Supplementary Tables S4, S5</ref>, S6 and S7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4 Hyperparameter Optimization</head><p>The main hyper-parameter that was found to have a significant impact on performance was the batch size. We observed that increasing the batch size from 8 to 16 led to reduced WER on the validation set of the LRS2 dataset (see <ref type="table" target="#tab_13">Supplementary Table S8</ref>). The same pattern is also observed on the LRS2 and LRS3 test sets (see <ref type="table" target="#tab_14">Supplementary Table S9</ref>). There is also one more hyper-parameter which controls the batch size based on the length of the sequences. In other words, if some sequences are too long then the batch is halved. We found that increasing this threshold from 150 to 220 frames also improved the performance. We could not increase these two hyper-parameters even further due to GPU memory constraints but it is likely that the WER will be reduced even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5 Language Models</head><p>We train six monolingual transformer-based language model <ref type="bibr" target="#b41">[76]</ref> for 50 epochs. The English language model is trained by combining the training sets of LibriSpeech (960 h) <ref type="bibr" target="#b42">[77]</ref>, pre-training and training sets of LRS2 <ref type="bibr" target="#b2">[3]</ref> and LRS3 <ref type="bibr" target="#b17">[51]</ref>, TED-LIUM 3 <ref type="bibr" target="#b43">[78]</ref>, Voxforge (English) and Common Voice (English) <ref type="bibr" target="#b44">[79]</ref>, with a total of 166 million characters. The Mandarin language model is trained by combining the CMLR [8] and news2016zh, with a total of 153 million characters. The Spanish language model is trained by combining the Spanish corpus from Multilingual TEDx <ref type="bibr" target="#b19">[53]</ref>, Common Voice <ref type="bibr" target="#b44">[79]</ref> and Multilingual LibriSpeech <ref type="bibr" target="#b45">[80]</ref>, with a total of 192 million characters. The Italian language model is trained by combining the Italian corpus from Multilingual TEDx <ref type="bibr" target="#b19">[53]</ref>, Common Voice <ref type="bibr" target="#b44">[79]</ref> and Multilingual LibriSpeech <ref type="bibr" target="#b45">[80]</ref>, with a total of 252 million characters. The Portuguese language model is trained by combining the Portuguese corpus from Multilingual TEDx <ref type="bibr" target="#b19">[53]</ref>, Common Voice <ref type="bibr" target="#b44">[79]</ref> and Multilingual LibriSpeech <ref type="bibr" target="#b45">[80]</ref>, with a total of 85 million characters. The French language model is trained by combining the French corpus from Multilingual TEDx <ref type="bibr" target="#b19">[53]</ref>, Common Voice <ref type="bibr" target="#b44">[79]</ref> and Multilingual LibriSpeech <ref type="bibr" target="#b45">[80]</ref>, with a total of 945 million characters. In our work, we set ? and ? from equation  <ref type="table" target="#tab_13">Table S8</ref>. Results on the LRS2 and LRS3 test sets can be seen in <ref type="table" target="#tab_14">Supplementary Table S9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6 Time Masking</head><p>We mask n consecutive frames with the mean frame of the video. The duration t n is chosen from 0 to an upper bound n max using a uniform distribution. Since there is a large variance in the video lengths of the LRS2 and LRS3 datasets, we set the number of masks proportional to the sequence length. Specifically, we use one mask per second, and for each mask, the maximum duration n max is set to 0.4 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7 Loss Functions</head><p>To map input sequences x = [x 1 , ..., x T ] such as audio or visual streams to corresponding target characters y = [y 1 , ..., y L ], we consider a hybrid CTC/attention architecture <ref type="bibr" target="#b46">[81]</ref> in this paper, where T , L are the lengths of the input sequence and target character sequence, respectively. The CTC loss assumes conditional independence between the output predictions and the estimated sequence posterior has the form of P CT C (y|x) ? T t=1 p(y t |x). The CTC loss from equation <ref type="formula" target="#formula_3">(5)</ref> is defined as follows:   An attention-based encoder-decoder model gets rid of this assumption by directly estimating the posterior on the basis of the chain rule and has a form of P att (y|x) ? L l=1 p(y l |y &lt;l , x).</p><formula xml:id="formula_5">L CTC = ?logP CT C (y|x) (S1)</formula><p>In this case the L att from equation is:</p><formula xml:id="formula_6">L att = ?logP att (y|x) (S2)</formula><p>The objective function of speech recognition is performed by a linear combination of the CTC loss and a cross-entropy loss as shown in equation <ref type="bibr" target="#b4">(5)</ref>. The ? value used in this work is 0.1.</p><p>A grid search was performed for the parameters ? a and ? v used in the auxiliary loss (equation <ref type="formula" target="#formula_2">(3)</ref>). The values that resulted in the best performance in the validation set of the LRS2 dataset are the following: ? a = 0.4 and ? v = 0.4. These values are used for all experiments. ce-b0 ce-b2 ce-b4 ce-b6 ce-b8 ce-b10 ce-b12 25  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8 Curriculum Learning</head><p>The end-to-end model was trained from scratch, resulting in poor performance on LRS2 and LRS3. This is likely due to the vast amount of very long utterances featured in LRS2 and LRS3, which makes learning from scratch es- pecially challenging. We have found that the issue can be resolved by progressively training the end-to-end model, starting with short utterances and then using longer ones during training. This approach is commonly called curriculum learning (CL). In this paper, the model is initially trained with a subset of labelled training data, consisting of videos shorter than 100 frames. Then this model is used for initialisation when using utterances with up to 150 frames for training. This process is repeated for 3 more rounds where the length of training sequences is 300, 450, and 600 frames, respectively. Results for each round of curriculum learning can be seen in Supplementary Tables S10 and S11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9 Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.1 Ablation Study on the Effect of Layer Position</head><p>We investigate the effect of the layer l where the auxiliary loss (equation <ref type="formula" target="#formula_2">(3)</ref>) is attached. The position of layer varies from 0 to 12 at intervals of 2. Layer 6 was found to be the  optimal level on the validation set of LRS2. Results are presented in <ref type="figure" target="#fig_0">Supplementary Fig. S1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.2 Results on Spanish</head><p>Results on the Multilingual TEDx-Spanish dataset are shown in <ref type="table" target="#tab_1">Supplementary Table S12</ref>. We observe that our proposed approach results in a 5.6 % absolute reduction in the WER. A further reduction of 4.2 % can be achieved by using additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.3 Results on Italian</head><p>We           -Audio auxiliary task, visual auxiliary task, time masking 33.8?0.4 <ref type="table" target="#tab_2">Table S27</ref>: Performance (Mean?Std.) of the pre-trained ASR and VSR Models on the LRS2 dataset. The baseline model pre-trained on LRW and LRS2 has a mean WER of 33.2?0.5. 'RSN' and '1D-RSN' refer to the proposed visual and audio front-end modules, respectively. Details are shown in <ref type="table" target="#tab_2">Supplementary Tables 2 and 3</ref>, respectively. 'SVN' refers to the ShuffleNet v2, where the width multiplier is set to 1. '1D-CNN' refers to the 5-layer CNN module. The detailed architecture of the '1D-CNN' front-end module is presented in <ref type="table" target="#tab_2">Supplementary Table S28</ref>.  <ref type="table" target="#tab_1">Table S17</ref>. We observe that our proposed approach results in a 11.5 % absolute reduction in the WER. Furthermore, as expected, the performance is improved by a large margin of 9.3 % when additional training data is included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.6 Ablation Study on the Effect of Pretrained ASR and VSR Models</head><p>In this section, we investigate the impact of pre-trained ASR and VSR models used in the auxiliary tasks. Results on LRS2 are shown in Supplementary Tables S18 and S19 below. By replacing the ASR model pre-trained on LRW and LRS2 (WER: 3.9%) with a model pre-trained only on LRS2 (WER: 5.4%), we observe that the mean WER increases from 29.5% to 30.9%. Similarly by replacing the VSR model pre-trained on LRW+LRS2 (WER: 33.2%) with a model pre-trained on LRS2 (WER: 52.7%), the mean WER increases from 29.5% to 31.2%. When we use both ASR and VSR models pre-trained on LRS2 (last row of <ref type="table" target="#tab_1">Supplementary Table S18</ref>), a further increase in the mean WER to 33.6% is observed, which indicates that a better pre-trained ASR/VSR model leads to improved performance of the full model. Results on LRS3 are reported in <ref type="table" target="#tab_1">Supplementary Table S19</ref>. In case, when we replace the ASR/VSR model pre-trained on LRW and LRS3 with a model pre-trained on LRS3, the mean WER increases </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.7 Ablation Study on the Effect of Beam Size</head><p>Results on the impact of beam size for multiple languages are presented in <ref type="table" target="#tab_1">Supplementary Tables S20, S21</ref>, S22, S23, S24, and S25. We optimise the beam size with an interval of 5 based on the validation set. In particular, we have optimised the beam size set to 40 on the English corpus (LRS2 and LRS3), 20 on the Mandarin corpus (CMLR), 35 on the Spanish corpus (CM es and MT es ), 25 on the Italian corpus (MT es ), 40 on the French corpus (MT fr ) and 35 on the Portuguese corpus (CM pt and MT pt ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.8 Ablation Study on the Effect of Auxiliary Losses when Using a Large Training Set</head><p>Results of the impact of auxiliary losses and time masking on the performance on LRS3 dataset are shown in Supplementary <ref type="table" target="#tab_2">Table S26</ref>. Note that all models are trained using the LRW, LRS2, LRS3, and AVSpeech datasets, in a total of 1 459 hours. We observe that overall the results are consistent with the ones presented in <ref type="table" target="#tab_6">Table 5</ref>, i.e. removing either auxiliary loss or training a model without using time masking leads to an increase in the mean WER when compared with the full model. To be specific, by removing a visual auxiliary task results, we observe an absolute increase of 0.8% in the mean WER. Then, if we also remove the audio auxiliary task, a further increase of 0.7% in the mean WER is observed. This indicates that training with auxiliary losses can provide a better supervision to the intermediate layer of the model which in turn results in better visual representations and improved performance. Indeed the contribution of the auxiliary losses is smaller when larger sets are used. However, we do believe that this is in line with what we propose in the paper that when don't have access to large training sets then careful design of the model is equally important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.9 Ablation Study on the Effect of Pretrained VSR Models and ASR Models with Different Architectures</head><p>Results of the impact of the pre-trained VSR and ASR models with different architectures are shown in Supplementary <ref type="table" target="#tab_2">Table S27</ref>. Note that all models are trained using the LRW and LRS2 datasets, in a total of 380 hours. To be specific, replacing the proposed visual/audio front-end modules with the ShuffleNet v2 <ref type="bibr" target="#b47">[82]</ref> backbone (see <ref type="table" target="#tab_2">Supplementary Table S28</ref>) leads to an increase of 4.4 % and 0.6%, respectively, in WER. However, we observe that training a model with auxiliary losses, even when the pre-trained VSR and ASR models have different architectures, outperforms the baseline model. This is in line with what we propose in the paper that training with auxiliary losses can provide a better supervision to the intermediate layer of the model which in turn results in better visual representations and improved performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model architecture overview. a-c, Baseline ASR model (a), baseline VSR model (b) and proposed model (c) with prediction-based auxiliary tasks. The frame rate of extracted visual features and audio features is 25. (d), The architecture of the ASR encoder from a. e, The architecture of the VSR encoder from b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>LRS2. [ 3 ]</head><label>3</label><figDesc>describes a large-scale audio-visual English dataset collected from BBC programmes. It consists of 144,482 video clips with a total duration of 224.5 h. The videos are divided into a pre-training set with 96,318 utterances (195 h), a training set with 45,839 utterances (28 h), a validation set with 1,082 utterances (0.6 h) and a test set with 1,243 utterances (0.5 h). LRS3. [51] describes the largest publicly audio-visual English dataset collected from TED talks. It contains 438.9 h with 151,819 utterances. Specifically, there are 118,516 utterances in the 'pre-train' set (408 h), 31,982 utterances in the 'train-val' set (30 h) and 1,321 utterances in the 'test' set (0.9 h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 3 )</head><label>3</label><figDesc>to 0.1 and {English: 0.6, Mandarin: 0.3, Spanish: 0.4, Italian: 0.5, Portuguese: 0.3, French: 0.3}, respectively. The impact of the improved English language model on the validation set of the LRS2 dataset can be seen in Supplementary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure S1 :</head><label>S1</label><figDesc>Performance of visual speech recognition on both the validation set and test set of LRS2 as a function of the layer where the auxiliary loss is attached (see equation 3). "ce-b0" to "ce-b12" refer to the conformer layers from bottom to top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell></cell><cell cols="3">Using Publicly Available Datasets</cell><cell></cell><cell></cell></row><row><cell>MV-WAS [3]</cell><cell>-</cell><cell>LRS2</cell><cell>223</cell><cell>-</cell><cell>70.4</cell></row><row><cell>CTC/Att. [12]</cell><cell>LRW</cell><cell>LRS2</cell><cell>380</cell><cell>-</cell><cell>63.5</cell></row><row><cell>KD + CTC [13]</cell><cell>VoxCeleb2 clean +LRS3</cell><cell>LRS2</cell><cell>995</cell><cell>-</cell><cell>51.3</cell></row><row><cell>KD-seq2seq [14]</cell><cell>LRW+LRS3</cell><cell>LRS2</cell><cell>818</cell><cell>-</cell><cell>49.2</cell></row><row><cell>TDNN [15]</cell><cell>-</cell><cell>LRS2</cell><cell>223</cell><cell>-</cell><cell>48.9</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>LRS2</cell><cell>380</cell><cell>-</cell><cell>37.9</cell></row><row><cell>Ours</cell><cell>-</cell><cell>LRS2</cell><cell>223</cell><cell>33.6?0.5</cell><cell>32.9</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>LRS2</cell><cell>380</cell><cell>29.5?0.4</cell><cell>28.7</cell></row><row><cell>Ours</cell><cell>LRW+LRS3</cell><cell>LRS2</cell><cell>818</cell><cell>27.6?0.2</cell><cell>27.3</cell></row><row><cell>Ours</cell><cell>LRW+LRS3+AVSpeech</cell><cell>LRS2</cell><cell>1 459</cell><cell>25.8?0.4</cell><cell>25.5</cell></row><row><cell></cell><cell cols="3">Using Non-Publicly Available Datasets</cell><cell></cell><cell></cell></row><row><cell>TM-seq2seq [4]</cell><cell>MVLRS+LRS3</cell><cell>LRS2</cell><cell>1 391</cell><cell>-</cell><cell>48.3</cell></row><row><cell cols="3">62 times more training data than the publicly available</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">training data on which our model is trained.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Results on the LRS2 dataset. 'Mean?Std.' refers to the mean word error rate over ten runs and the corresponding standard deviation, while "Best" denotes the best (lowest) WER.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the LRS3 dataset. 'Mean?Std.' refers to the mean word error rate over ten runs and the corresponding standard deviation, while "Best" denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell></cell><cell cols="3">Using Publicly Available Datasets</cell><cell></cell><cell></cell></row><row><cell>KD+CTC [13]</cell><cell>VoxCeleb2 clean</cell><cell>LRS3</cell><cell>772</cell><cell>-</cell><cell>59.8</cell></row><row><cell>KD-seq2seq [14]</cell><cell>LRW+LRS2</cell><cell>LRS3</cell><cell>818</cell><cell>-</cell><cell>59.0</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>LRS3</cell><cell>595</cell><cell>-</cell><cell>43.3</cell></row><row><cell>Ours</cell><cell>-</cell><cell>LRS3</cell><cell>438</cell><cell>38.6?0.4</cell><cell>37.9</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>LRS3</cell><cell>595</cell><cell>35.8?0.5</cell><cell>35.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS2</cell><cell>LRS3</cell><cell>818</cell><cell>34.9?0.2</cell><cell>34.7</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+AVSpeech</cell><cell>LRS3</cell><cell>1 459</cell><cell>32.1?0.3</cell><cell>31.5</cell></row><row><cell></cell><cell cols="3">Using Non-Publicly Available Datasets</cell><cell></cell><cell></cell></row><row><cell>TM-seq2seq [4]</cell><cell>MVLRS+LRS2</cell><cell>LRS3</cell><cell>1 391</cell><cell>-</cell><cell>58.9</cell></row><row><cell>V2P [5]</cell><cell>-</cell><cell>LSVSR</cell><cell>3 886</cell><cell>-</cell><cell>55.1</cell></row><row><cell>RNN-T [16]</cell><cell>-</cell><cell>YT-31k</cell><cell>31 000</cell><cell>-</cell><cell>33.6</cell></row><row><cell>ViT3D-TM [6]</cell><cell>-</cell><cell>YT-90k</cell><cell>90 000</cell><cell>-</cell><cell>25.9</cell></row><row><cell>ViT3D-CM [17]</cell><cell>-</cell><cell>YT-90k</cell><cell>90 000</cell><cell>-</cell><cell>17.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on the CMLR dataset. 'Mean?Std.' refers to the mean character error rate over ten runs and the corresponding standard deviation, while "Best" denotes the best (lowest) CER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell>LipCH-Net [7]</cell><cell>-</cell><cell>CMLR</cell><cell>61</cell><cell></cell><cell>34.0</cell></row><row><cell>CSSMCM [8]</cell><cell>-</cell><cell>CMLR</cell><cell>61</cell><cell>-</cell><cell>32.5</cell></row><row><cell>LIBS [18]</cell><cell>-</cell><cell>CMLR</cell><cell>61</cell><cell>-</cell><cell>31.3</cell></row><row><cell>CTCH [9]</cell><cell>-</cell><cell>CMLR</cell><cell>61</cell><cell>-</cell><cell>22.0</cell></row><row><cell>Ours</cell><cell>-</cell><cell>CMLR</cell><cell>61</cell><cell>9.1?0.05</cell><cell>9.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>CMLR</cell><cell>879</cell><cell>8.2?0.06</cell><cell>8.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+AVSpeech</cell><cell>CMLR</cell><cell>1 520</cell><cell>8.1?0.05</cell><cell>8.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on the CMU-MOSEAS-Spanish (CM es ) dataset. 'Mean?Std.' refers to the mean word error rate over ten runs and the corresponding standard deviation, while "Best" denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>CM es +MT es</cell><cell>244</cell><cell>58.9?0.8</cell><cell>58.1</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>CM es +MT es</cell><cell>244</cell><cell>51.5?0.8</cell><cell>50.4</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>CM es +MT es</cell><cell>905</cell><cell>47.4?0.2</cell><cell>47.2</cell></row><row><cell>Ours</cell><cell cols="2">LRW+LRS2+LRS3+AVSpeech CM es +MT es</cell><cell>1 546</cell><cell>44.6?0.6</cell><cell>43.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the LRS2 dataset and LRS3 dataset. Models are trained on LRW+LRS2 and LRW+LRS3, respectively. To the best of our knowledge, CMLR is the largest publicly available dataset in Mandarin.CMU-MOSEAS.<ref type="bibr" target="#b18">[52]</ref> describes a large-scale dataset that contains multiple languages and was collected from YouTube videos. It consists of 40,000 transcribed sentences and includes Spanish, Portuguese, German and French. We consider the Spanish videos (CM es ) with a total duration of 16.3 h. We divided the data into training and test sets, which contain 8,253 videos (15.7 h) and 329 videos (0.6 h), respectively.Multilingual TEDx.<ref type="bibr" target="#b19">[53]</ref> describes a multilingual corpus collected from TEDx talks. It covers eight languages with manual transcriptions and has a total duration of 765 h. For the purposes of this study, we consider the Spanish videos (MT es ) and use the data split proposed in<ref type="bibr" target="#b19">[53]</ref>. We manually cleaned the dataset to exclude videos where the speaker is not visible, resulting in a total of 44,745 videos (71.4 h) for training, 403 videos (0.7 h) for validation and 302 videos (0.5 h) for testing. It should be noted that we only use the training set in this study.</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>/www.openslr.org/100), and CMU-MOSEAS (http://immortal.multicomp.cs.cmu.edu/cache/ multilingual) repositories. Qualitative results and the list of cleaned videos for the training and test sets of CMU-MOSEAS and Multilingual TEDx are available on the authors' GitHub repository (https://mpc001.github.io/lipreader.html). Recurrent neural network transducer for audio-visual speech recognition. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop, 905-912 (2019). Denby, B. et al. Silent speech interfaces. Speech Communication 52, 270-287 (2010). Lee, J. &amp; Watanabe, S. Intermediate loss regularization for ctc-based speech recognition. In Proceedings of the 46th IEEE International Conference on Acoustics, Speech and Signal Processing, 6224-6228 (2021).</figDesc><table><row><cell cols="2">(https://www.vipazoo.cn/CMLR.html), encoder-decoder based speech recognition. In Pro-Multilingual</cell><cell cols="2">Conference of International Speech Communication</cell></row><row><cell cols="2">ceedings of the 18th Annual Conference of Interna-(http:/Code Availability [17] Serdyuk, D., Braga, O. &amp; Siohan, O. Transformer-(2022). tional Speech Communication Association, 2833-2837 ceedings of the 23rd Annual Conference of Interna-nition for single and multi-person video. In Pro-based video front-ends for audio-visual speech recog-tional Speech Communication Association, 3532-3536 (2017). [63]</cell><cell cols="2">Association, 299-303 (2020). and Pattern Recognition, 5039-5049 (2021). the 34th IEEE/CVF Conference on Computer Vision proach to face forgery detection. In Proceedings of M. Lips don't lie: A generalisable and robust ap-[28] [29] Haliassos, A., Vougioukas, K., Petridis, S. &amp; Pantic,</cell></row><row><cell cols="2">Pre-trained networks and testing code are available on a GitHub repository (https://mpc001.github.io/ lipreader.html) or at Zenodo [69] under an Attribution-[18] Zhao, Y. et al. Hearing lips: Improving lip read-ing by distilling speech recognizers. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, 6917-6924 (2020). NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) licence. [19] McGurk, H. &amp; MacDonald, J. Hearing lips and seeing voices. Nature 264, 746-748 (1976).</cell><cell cols="2">[30] Mira, R. et al. End-to-end video-to-speech synthesis using generative adversarial networks. IEEE Trans-actions on Cybernetics 1-13 (2022). [7] Zhang, X. et al. Understanding pictograph with fa-cial features: End-to-end sentence-level lip reading of [31] Prajwal, K., Mukhopadhyay, R., Namboodiri, V. P. chinese. In Proceedings of the 33rd AAAI Conference &amp; Jawahar, C. Learning individual speaking styles on Artificial Intelligence, 9211-9218 (2019). for accurate lip to speech synthesis. In Proceedings of</cell></row><row><cell></cell><cell></cell><cell cols="2">the 33rd IEEE/CVF Conference on Computer Vision</cell></row><row><cell></cell><cell></cell><cell>and Pattern Recognition, 13796-13805 (2020).</cell></row><row><cell></cell><cell></cell><cell cols="2">[32] Dungan, L., Karaali, A. &amp; Harte, N. The impact of</cell></row><row><cell></cell><cell></cell><cell cols="2">reduced video quality on visual speech recognition.</cell></row><row><cell></cell><cell></cell><cell cols="2">In Proceedings of the 25th IEEE International Con-</cell></row><row><cell></cell><cell></cell><cell cols="2">ference on Image Processing, 2560-2564 (2018).</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>(2020).</cell><cell>-1064</cell><cell cols="2">ings of the 46th IEEE International Conference on [34] Geirhos, R. et al. Imagenet-trained cnns are biased Acoustics, Speech and Signal Processing, 7613-7617 towards texture; increasing shape bias improves ac-(2021). curacy and robustness. In Proceedings of the 7th In-</cell></row><row><cell cols="2">[23] Afouras, T., Chung, J. S. &amp; Zisserman, A. The con-versation: Deep audio-visual speech enhancement. In Proceedings of the 19th Annual Conference of Inter-national Speech Communication Association, 3244-</cell><cell cols="2">ternational Conference on Learning Representations [11] Gulati, A. et al. Conformer: Convolution-augmented (2019). transformer for speech recognition. In Proceedings of the 21st Annual Conference of International Speech [35] Cheng, S. et al. Towards pose-invariant lip-reading. Communication Association, 5036-5040 (2020). In Proceedings of the 45th IEEE International Con-</cell></row><row><cell>3248 (2018).</cell><cell></cell><cell cols="2">[12] Petridis, S., Stafylakis, T., Ma, P., Tzimiropoulos, G. ference on Acoustics, Speech and Signal Processing,</cell></row><row><cell cols="2">[24] Ephrat, A. et al. Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation. ACM Transactions on Graphics 37, 112:1-112:11 (2018).</cell><cell cols="2">4357-4361 (2020). &amp; Pantic, M. Audio-visual speech recognition with a hybrid CTC/attention architecture. In Proceedings [36] Wand, M. &amp; Schmidhuber, J. Improving speaker-of the IEEE Spoken Language Technology Workshop, independent lipreading with domain-adversarial 513-520 (2018). training. In Proceedings of the 18th Annual Con-</cell></row><row><cell cols="2">[25] Yoshimura, T., Hayashi, T., Takeda, K. &amp; Watan-abe, S. End-to-end automatic speech recognition inte-</cell><cell cols="2">[13] Afouras, T., Chung, J. S. &amp; Zisserman, A. ASR is all you need: Cross-modal distillation for lip reading. In Proceedings of the 45th IEEE International Con-ference of International Speech Communication Association, 3662-3666 (2017).</cell></row><row><cell cols="2">grated with ctc-based voice activity detection. In Pro-</cell><cell cols="2">ference on Acoustics, Speech and Signal Processing, [37] Petridis, S., Wang, Y., Li, Z. &amp; Pantic, M. End-to-</cell></row><row><cell cols="2">ceedings of the 45th IEEE International Conference</cell><cell cols="2">2143-2147 (2020). end multi-view lipreading. In Proceedings of the 28th</cell></row><row><cell cols="2">on Acoustics, Speech and Signal Processing, 6999-7003 (2020).</cell><cell cols="2">British Machine Vision Conference (2017). [14] Ren, S., Du, Y., Lv, J., Han, G. &amp; He, S. Learn-ing from the master: Distilling cross-modal advanced [38] Bicevskis, K. et al. Effects of mouthing and interlocu-</cell></row><row><cell></cell><cell></cell><cell cols="2">knowledge for lip reading. In Proceedings of the 34th tor presence on movements of visible vs. non-visible</cell></row><row><cell></cell><cell></cell><cell cols="2">IEEE/CVF Conference on Computer Vision and Pat-articulators. Canadian acoustics= Acoustique cana-</cell></row><row><cell></cell><cell></cell><cell>tern Recognition, 13325-13333 (2021). dienne 44, 17 (2016).</cell></row><row><cell></cell><cell></cell><cell>html),</cell><cell>CMLR</cell></row></table><note>/www.robots.ox.ac.uk/~vgg/data/lip reading/ lrs2.html), LRS3 (https://www.robots.ox.ac.uk/ vgg/data/lip reading/lrs3.[8] Zhao, Y., Xu, R. &amp; Song, M. A cascade sequence-to- sequence model for chinese mandarin lip reading. In Proceedings of the 1st ACM International Conference on Multimedia in Asia, 1-6 (2019).[9] Ma, S., Wang, S. &amp; Lin, X. A transformer-based model for sentence-level chinese mandarin lipreading. In Proceedings of the 5th IEEE International Confer- ence on Data Science in Cyberspace, 78-81 (2020).[10] Ma, P., Petridis, S. &amp; Pantic, M. End-to-end audio- visual speech recognition with conformers. In Proceed[15] Yu, J. et al. Audio-visual recognition of overlapped speech for the LRS2 dataset. In Proceedings of the 45th IEEE International Conference on Acoustics, Speech and Signal Processing, 6984-6988 (2020).[16] Makino, T. et al.[20] Sumby, W. H. &amp; Pollack, I. Visual contribution to speech intelligibility in noise. The Journal of the Acoustical Society of America 26, 212-215 (1954).[21] Yu, W., Zeiler, S. &amp; Kolossa, D. Fusing informa- tion streams in end-to-end audio-visual speech recog- nition. In Proceedings of the 46th IEEE International Conference on Acoustics, Speech and Signal Process- ing, 3430-3434 (2021).[22] Sterpu, G., Saam, C. &amp; Harte, N. How to teach dnns to pay attention to the visual modality in speech recognition. IEEE/ACM Transactions on Au- dio Speech and Language Processing 28, 1052[26] Kim, Y. J. et al. Look who's talking: Active speaker detection in the wild. In Proceedings of the 22nd An- nual Conference of International Speech Communica- tion Association, 3675-3679 (2021).[27] Chung, J. S., Huh, J., Nagrani, A., Afouras, T. &amp; Zisserman, A. Spot the conversation: Speaker diari- sation in the wild. In Proceedings of the 21st Annual[33] Bear, H. L., Harvey, R., Theobald, B.-J. &amp; Lan, Y. Resolution limits on visual speech recognition. In Pro- ceedings of the 21st IEEE International Conference on Image Processing, 1371-1375 (2014).[39]?imko, J., Be?u?,?. &amp; Vainio, M. Hyperarticulation in lombard speech: Global coordination of the jaw, lips and the tongue. The Journal of the Acoustical Society of America 139, 151-162 (2016).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S1 :</head><label>S1</label><figDesc>Details of Audio-Visual Datasets used in this work. CM xx and MT xx denote the particular language parts of the CMU-MOSEAS and Multilingual TEDx datasets, respectively, where xx denotes the standard language codes, conforming to the ISO 639-1 standard.</figDesc><table><row><cell>Dataset</cell><cell>Transcription</cell><cell>Hours</cell></row><row><cell cols="2">Publicly Available Datasets</cell><cell></cell></row><row><cell>LRW [72]</cell><cell></cell><cell>157</cell></row><row><cell>LRS2 [3]</cell><cell></cell><cell>223</cell></row><row><cell>LRS3 [51]</cell><cell></cell><cell>438</cell></row><row><cell>CMLR [8]</cell><cell></cell><cell>61</cell></row><row><cell>MT es [53]</cell><cell></cell><cell>71</cell></row><row><cell>MT it [53]</cell><cell></cell><cell>46</cell></row><row><cell>MT pt [53]</cell><cell></cell><cell>81</cell></row><row><cell>MT fr [53]</cell><cell></cell><cell>85</cell></row><row><cell>CM es [52]</cell><cell></cell><cell>16</cell></row><row><cell>CM pt [52]</cell><cell></cell><cell>18</cell></row><row><cell>CM fr [52]</cell><cell></cell><cell>15</cell></row><row><cell>AVSpeech [24]</cell><cell></cell><cell>641</cell></row><row><cell cols="2">Non-Publicly Available Datasets</cell><cell></cell></row><row><cell>MVLRS [3]</cell><cell></cell><cell>730</cell></row><row><cell>LSVSR [5]</cell><cell></cell><cell>3 886</cell></row><row><cell>YT-31k [16]</cell><cell></cell><cell>31 000</cell></row><row><cell>YT-90k [6]</cell><cell></cell><cell>90 000</cell></row><row><cell>VoxCeleb2 clean [13]</cell><cell></cell><cell>334</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S2 :</head><label>S2</label><figDesc>The architecture of the front-end encoder of the VSR model. The filter shapes are denoted by {Temporal Size ? Spatial Size 2 , Channels} and {Spatial Size 2 , Channels} for 3D convolutional and 2D convolutional Layers , respectively. The sizes correspond to [Batch Size, Channels, Sequence Length, Height, Width] and [Batch Size ? Sequence Length, Channels, Height, Width], for 3D and 2D convolutional layers, respectively. T v denotes the number of input frames.</figDesc><table><row><cell>Component Name</cell><cell>Layer Type</cell><cell></cell><cell>Input Size</cell><cell>Output Size</cell></row><row><cell>Stem 1</cell><cell cols="2">Conv 3D, 5 ? 7 2 , 64 3D Max Pooling, 1 ? 3 2</cell><cell>[B, 1, T v , 88, 88] [B, 64, T v , 44, 44]</cell><cell>[B, 64, T v , 44, 44] [B, 64, T v , 22, 22]</cell></row><row><cell>Reshape</cell><cell>-</cell><cell></cell><cell>[B, 64, T v , 22, 22]</cell><cell>[B?T v , 64, 22, 22]</cell></row><row><cell>Residual Block 2</cell><cell cols="2">? ? Conv 2D, 3 2 , 64 ? Conv 2D, 3 2 , 64 ? ? 2</cell><cell>[B?T v , 64, 22, 22]</cell><cell>[B?T v , 64, 22, 22]</cell></row><row><cell>Residual Block 3</cell><cell>? ? Conv 2D, 3 2 , 128 Conv 2D, 3 2 , 128</cell><cell>? ? ? 2</cell><cell cols="2">[B?T v , 64, 22, 22] [B?T v , 128, 11, 11]</cell></row><row><cell>Residual Block 4</cell><cell>? ? Conv 2D, 3 2 , 256 Conv 2D, 3 2 , 256</cell><cell>? ? ? 2</cell><cell>[B?T v , 128, 11, 11]</cell><cell>[B?T v , 256, 6, 6]</cell></row><row><cell>Residual Block 5</cell><cell>? ? Conv 2D, 3 2 , 512 Conv 2D, 3 2 , 512</cell><cell>? ? ? 2</cell><cell>[B?T v , 256, 6, 6]</cell><cell>[B?T v , 512, 3, 3]</cell></row><row><cell>Aggregation</cell><cell cols="2">2D Global Average Pooling</cell><cell>[B?T v , 512, 3, 3]</cell><cell>[B?T v , 512, 1, 1]</cell></row><row><cell>Reshape</cell><cell>-</cell><cell></cell><cell>[B?T v , 512, 1, 1]</cell><cell>[B, 512, T v ]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S3 :</head><label>S3</label><figDesc>The architecture of the front-end encoder of the ASR model. The filter shapes are denoted by {Temporal Size, Channels} for 1D Convolutional Layers, respectively. The sizes correspond to [Batch Size, Channels, Sequence Length]. T a denotes the length of audio waveforms.</figDesc><table><row><cell>Component Name</cell><cell>Layer Type</cell><cell>Input Size</cell><cell>Output Size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S4 :</head><label>S4</label><figDesc>Performance (Mean?Std.) of the pre-trained ASR and VSR models on the LRS2 dataset.</figDesc><table><row><cell>Method</cell><cell>Training Sets</cell><cell>Full Model</cell><cell>Pre-trained VSR model</cell><cell>Pre-trained ASR model</cell></row><row><cell>Ours</cell><cell>LRS2</cell><cell>33.6?0.5</cell><cell>33.4?0.3</cell><cell>4.0?0.4</cell></row><row><cell>Ours</cell><cell>LRW+LRS2</cell><cell>29.5?0.4</cell><cell>33.2?0.5</cell><cell>3.9?0.2</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>27.6?0.2</cell><cell>29.3?0.4</cell><cell>3.7?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+AVSpeech</cell><cell>25.8?0.4</cell><cell>29.3?0.4</cell><cell>3.7?0.1</cell></row><row><cell cols="5">Table S5: Performance (Mean?Std.) of the pre-trained ASR and VSR models on the LRS3 dataset.</cell></row><row><cell>Method</cell><cell>Training Sets</cell><cell>Full Model</cell><cell>Pre-trained VSR model</cell><cell>Pre-trained ASR model</cell></row><row><cell>Ours</cell><cell>LRS3</cell><cell>38.6?0.4</cell><cell>38.7?0.5</cell><cell>2.3?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS3</cell><cell>35.8?0.5</cell><cell>37.8?0.6</cell><cell>2.2?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>34.9?0.2</cell><cell>35.2?0.2</cell><cell>2.0?0.2</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+AVSpeech</cell><cell>32.1?0.3</cell><cell>35.2?0.2</cell><cell>2.0?0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S6 :</head><label>S6</label><figDesc>Performance (Mean?Std.) of the pre-trained ASR and VSR models on the CMLR dataset.</figDesc><table><row><cell>Method</cell><cell>Training Sets</cell><cell>Full Model</cell><cell>Pre-trained VSR model</cell><cell>Pre-trained ASR model</cell></row><row><cell>Ours</cell><cell>CMLR</cell><cell>9.1?0.05</cell><cell>10.7?0.06</cell><cell>2.5?0.03</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+CMLR</cell><cell>8.2?0.06</cell><cell>9.0?0.05</cell><cell>2.2?0.03</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+AVSpeech+CMLR</cell><cell>8.1?0.05</cell><cell>8.9?0.08</cell><cell>2.2?0.03</cell></row><row><cell cols="5">Table S7: Performance (Mean?Std.) of the pre-trained ASR and VSR models on the CMU-MOSEAS-Spanish (CM es )</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Training Sets</cell><cell>Full Model</cell><cell>Pre-trained VSR model</cell><cell>Pre-trained ASR model</cell></row><row><cell>Ours</cell><cell>LRW+CM es +MT es</cell><cell>51.5?0.8</cell><cell>53.2?0.4</cell><cell>16.3?0.3</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+CM es +MT es</cell><cell>47.4?0.2</cell><cell>47.5?0.6</cell><cell>15.4?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+AVSpeech+CM es +MT es</cell><cell>44.6?0.6</cell><cell>45.3?0.4</cell><cell>15.4?0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S8 :</head><label>S8</label><figDesc>Investigation of the impact of hyperparameters and Language Model (LM) choices on the validation set of the LRS2 dataset.</figDesc><table><row><cell>Method</cell><cell>WER</cell></row><row><cell>CM-seq2seq [10] -Baseline</cell><cell>47.7?0.5</cell></row><row><cell>+ Hyperparameter Optimisation</cell><cell>45.6?0.4</cell></row><row><cell>+ Improved LM</cell><cell>44.1?0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S9 :</head><label>S9</label><figDesc>Investigation of the impact of hyperparameters and Language Model (LM) choices on the LRS2 dataset and LRS3 dataset.</figDesc><table><row><cell>Method</cell><cell>WER on LRS2</cell><cell>WER on LRS3</cell></row><row><cell>CM-seq2seq [10] -Baseline</cell><cell>37.8?0.5</cell><cell>44.9?0.8</cell></row><row><cell>+ Hyperparameter Optimisation</cell><cell>35.9?0.5</cell><cell>40.6?0.8</cell></row><row><cell>+ Improved LM</cell><cell>35.0?0.5</cell><cell>39.1?0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table S10 :</head><label>S10</label><figDesc>Results of curriculum learning experiments on the LRS2 dataset.</figDesc><table><row><cell>Video Length</cell><cell>WER on the</cell><cell>WER on</cell></row><row><cell>in Frames</cell><cell>Validation Set</cell><cell>the Test Set</cell></row><row><cell></cell><cell>Baseline VSR Model</cell><cell></cell></row><row><cell>0-100</cell><cell>65.1?0.2</cell><cell>52.7?0.8</cell></row><row><cell>0-150</cell><cell>54.0?0.7</cell><cell>44.2?0.5</cell></row><row><cell>0-300</cell><cell>46.0?0.6</cell><cell>36.3?0.4</cell></row><row><cell>0-450</cell><cell>43.6?0.5</cell><cell>34.3?0.5</cell></row><row><cell>0-600</cell><cell>42.4?0.4</cell><cell>33.7?0.4</cell></row><row><cell cols="3">VSR Model with Auxiliary Workers</cell></row><row><cell>0-100</cell><cell>51.9?0.3</cell><cell>41.5?0.5</cell></row><row><cell>0-150</cell><cell>46.2?0.4</cell><cell>36.1?0.3</cell></row><row><cell>0-300</cell><cell>43.3?0.2</cell><cell>34.4?0.2</cell></row><row><cell>0-450</cell><cell>42.6?0.3</cell><cell>34.6?0.5</cell></row><row><cell>0-600</cell><cell>42.0?0.3</cell><cell>33.4?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table S11</head><label>S11</label><figDesc></figDesc><table><row><cell cols="2">: Results of curriculum learning experiments on</cell></row><row><cell>the LRS3 dataset.</cell><cell></cell></row><row><cell>Video Length in Frames</cell><cell>WER on the Test Set</cell></row><row><cell cols="2">Baseline VSR model</cell></row><row><cell>0-100</cell><cell>75.2?0.4</cell></row><row><cell>0-150</cell><cell>53.3?0.7</cell></row><row><cell>0-300</cell><cell>43.0?0.4</cell></row><row><cell>0-450</cell><cell>39.9?0.6</cell></row><row><cell>0-600</cell><cell>38.7?0.5</cell></row><row><cell cols="2">VSR Model with Auxiliary Workers</cell></row><row><cell>0-100</cell><cell>57.7?0.4</cell></row><row><cell>0-150</cell><cell>46.8?0.1</cell></row><row><cell>0-300</cell><cell>40.8?0.6</cell></row><row><cell>0-450</cell><cell>39.7?0.4</cell></row><row><cell>0-600</cell><cell>38.6?0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table S12 :</head><label>S12</label><figDesc>Results on the Multilingual TEDx-Spanish (MT es ) dataset. 'Mean?Std.' refers to the mean WER over ten runs and the corresponding standard deviation, while 'Best' denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>CM es +MT es</cell><cell>244</cell><cell>66.4?0.8</cell><cell>65.2</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>CM es +MT es</cell><cell>244</cell><cell>60.8?0.8</cell><cell>60.3</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>CM es +MT es</cell><cell>905</cell><cell>56.9?0.5</cell><cell>56.5</cell></row><row><cell>Ours</cell><cell cols="2">LRW+LRS2+LRS3+AVSpeech CM es +MT es</cell><cell>1 546</cell><cell>56.6?0.3</cell><cell>56.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table S13 :</head><label>S13</label><figDesc>Results on the Multilingual TEDx-Italian (MT it ) dataset. 'Mean?Std.' refers to the mean WER over ten runs and the corresponding standard deviation, while 'Best' denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>MT it</cell><cell>203</cell><cell>71.5?0.4</cell><cell>70.9</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>MT it</cell><cell>203</cell><cell>65.9?0.5</cell><cell>65.2</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>MT it</cell><cell>864</cell><cell>58.7?0.3</cell><cell>58.2</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3+AVSpeech</cell><cell>MT it</cell><cell>1 505</cell><cell>57.9?0.7</cell><cell>57.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Results on the CMU-MOSEAS-Portuguese dataset are shown inSupplementary Table S15. The proposed approach results in a 8.5 % absolute reduction in the WER. Using additional training data leads to a further reduction of 5.6 %.S9.5 Results on FrenchWe manually cleaned the French corpus on Multilingual TEDx to exclude videos where the speaker is not visible, resulting in a total of 58 809 videos (84.9 hours) for training, 333 videos (0.4 hours) for validation and 235 videos (0.3 hours) for testing. Results on the Multilingual TEDx-French dataset are shown inSupplementary Table S16. The proposed approach results in a 9.4 % absolute reduction in the WER. A further reduction of 7.6 % can be achieved by using additional training data.We divide the French corpus on CMU-MOSEAS [52] into 8 880 videos (15.3 hours) for training and 513 videos (0.8 hours) for testing, respectively. Results on the</figDesc><table><row><cell>videos (0.6 hours) for testing. Results on the Multilin-</cell></row><row><cell>gual TEDx-Portuguese dataset are shown in Supplemen-</cell></row><row><cell>tary Table S14. We observe that our proposed approach</cell></row><row><cell>results in a 4.2 % absolute reduction in the WER. A fur-</cell></row><row><cell>ther reduction of 3.9 % can be achieved by using additional</cell></row><row><cell>training data.</cell></row><row><cell>We divide the Portuguese corpus on CMU-</cell></row><row><cell>MOSEAS [52] into 10 658 videos (17.8 hours) for</cell></row><row><cell>training and 412 videos (0.7 hours) for testing, respec-</cell></row><row><cell>tively.</cell></row><row><cell>manually cleaned the Italian corpus on Multilingual</cell></row><row><cell>TEDx to exclude videos without visible speakers, result-</cell></row><row><cell>ing in a total of 26387 videos (45.8 hours) for training,</cell></row><row><cell>252 videos (0.4 hours) for validation and 309 videos (0.5</cell></row><row><cell>hours) for testing. Results on the Multilingual TEDx-</cell></row><row><cell>Italian dataset are shown in Supplementary Table S13.</cell></row><row><cell>Our proposed approach results in an absolute drop of 5.6 %</cell></row><row><cell>in the WER. A further reduction of 8 % can be achieved</cell></row><row><cell>by using additional training data.</cell></row><row><cell>S9.4 Results on Portuguese</cell></row><row><cell>We manually cleaned the Portuguese corpus on Multilin-</cell></row><row><cell>gual TEDx to exclude videos where the speaker is not</cell></row><row><cell>visible, resulting in a total of 52 395 videos (81.3 hours)</cell></row><row><cell>for training, 532 videos (0.7 hours) for validation and 401</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table S14 :</head><label>S14</label><figDesc>Results on the Multilingual TEDx-Portuguese (MT pt ) dataset. 'Mean?Std.' refers to the mean WER over ten runs and the corresponding standard deviation, while 'Best' denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>CM pt +MT pt</cell><cell>256</cell><cell>70.2?0.3</cell><cell>69.7</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>CM pt +MT pt</cell><cell>256</cell><cell>66.0?0.5</cell><cell>65.3</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>CM pt +MT pt</cell><cell>917</cell><cell>62.4?0.4</cell><cell>62.0</cell></row><row><cell>Ours</cell><cell cols="2">LRW+LRS2+LRS3+AVSpeech CM pt +MT pt</cell><cell>1 558</cell><cell>62.1?0.6</cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table S15 :</head><label>S15</label><figDesc>Results on the CMU-MOSEAS-Portuguese (CM pt ) dataset. 'Mean?Std.' refers to the mean WER over ten runs and the corresponding standard deviation, while 'Best' denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>CM pt +MT pt</cell><cell>256</cell><cell>65.7?0.5</cell><cell>65.4</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>CM pt +MT pt</cell><cell>256</cell><cell>57.2?0.7</cell><cell>56.6</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>CM pt +MT pt</cell><cell>917</cell><cell>53.1?0.2</cell><cell>52.8</cell></row><row><cell>Ours</cell><cell cols="2">LRW+LRS2+LRS3+AVSpeech CM pt +MT pt</cell><cell>1 558</cell><cell>51.6?0.2</cell><cell>51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table S16 :</head><label>S16</label><figDesc>Results on the Multilingual TEDx-French (MT fr ) dataset. 'Mean?Std.' refers to the mean WER over ten runs and the corresponding standard deviation, while 'Best' denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>CM fr +MT fr</cell><cell>257</cell><cell>84.0?0.7</cell><cell>83.2</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>CM fr +MT fr</cell><cell>257</cell><cell>74.6?0.6</cell><cell>73.4</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>CM fr +MT fr</cell><cell>918</cell><cell>67.0?0.3</cell><cell>66.7</cell></row><row><cell>Ours</cell><cell cols="2">LRW+LRS2+LRS3+AVSpeech CM fr +MT fr</cell><cell>1 559</cell><cell>67.0?0.6</cell><cell>66.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table S17 :</head><label>S17</label><figDesc>Results on the CMU-MOSEAS-French (CM fr ) dataset. 'Mean?Std.' refers to the mean WER over ten runs and the corresponding standard deviation, while 'Best' denotes the best (lowest) WER.</figDesc><table><row><cell>Method</cell><cell>Pre-training Set</cell><cell>Training Set</cell><cell>Training Sets Total Size (hours)</cell><cell>Mean?Std.</cell><cell>Best</cell></row><row><cell cols="2">CM-seq2seq [10] LRW</cell><cell>CM fr +MT fr</cell><cell>257</cell><cell>79.9?0.4</cell><cell>79.6</cell></row><row><cell>Ours</cell><cell>LRW</cell><cell>CM fr +MT fr</cell><cell>257</cell><cell>68.4?0.5</cell><cell>67.5</cell></row><row><cell>Ours</cell><cell>LRW+LRS2+LRS3</cell><cell>CM fr +MT fr</cell><cell>918</cell><cell>60.1?0.3</cell><cell>59.5</cell></row><row><cell>Ours</cell><cell cols="2">LRW+LRS2+LRS3+AVSpeech CM fr +MT fr</cell><cell>1 559</cell><cell>59.1?0.5</cell><cell>58.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table S18 :</head><label>S18</label><figDesc>Performance (Mean?Std.) of the pre-trained ASR and VSR Models on the LRS2 dataset. The Baseline VSR model pre-trained on LRW and LRS2 has a mean WER of 33.2?0.5.</figDesc><table><row><cell>Method</cell><cell>Training Sets of Full Model</cell><cell>Training Sets of Pre-trained VSR Model</cell><cell>Training Sets of Pre-trained ASR Model</cell><cell>Full Model</cell><cell>Pre-trained VSR Model</cell><cell>Pre-trained ASR Model</cell></row><row><cell>Ours</cell><cell>LRW+LRS2</cell><cell>LRW+LRS2</cell><cell>LRW+LRS2</cell><cell cols="2">29.5?0.4 33.2?0.5</cell><cell>3.9?0.2</cell></row><row><cell>Ours</cell><cell>LRW+LRS2</cell><cell>LRW+LRS2</cell><cell>LRS2</cell><cell cols="2">30.9?0.1 33.2?0.5</cell><cell>5.4?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS2</cell><cell>LRS2</cell><cell>LRW+LRS2</cell><cell cols="2">31.2?0.4 52.7?0.8</cell><cell>3.9?0.2</cell></row><row><cell>Ours</cell><cell>LRW+LRS2</cell><cell>LRS2</cell><cell>LRS2</cell><cell cols="2">33.6?0.3 52.7?0.8</cell><cell>5.4?0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table S19 :</head><label>S19</label><figDesc>Performance (Mean?Std.) of the pre-trained ASR and VSR Models on the LRS3 dataset. The Baseline VSR model pre-trained on LRW and LRS3 has a mean WER of 37.8?0.6.</figDesc><table><row><cell>Method</cell><cell>Training Sets of Full Model</cell><cell>Training Sets of Pre-trained VSR Model</cell><cell>Training Sets of Pre-trained ASR Model</cell><cell>Full Model</cell><cell>Pre-trained VSR Model</cell><cell>Pre-trained ASR Model</cell></row><row><cell>Ours</cell><cell>LRW+LRS3</cell><cell>LRW+LRS3</cell><cell>LRW+LRS3</cell><cell cols="2">35.8?0.5 37.8?0.6</cell><cell>2.2?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS3</cell><cell>LRW+LRS3</cell><cell>LRS3</cell><cell cols="2">36.0?0.3 37.8?0.6</cell><cell>3.8?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS3</cell><cell>LRS3</cell><cell>LRW+LRS3</cell><cell cols="2">37.6?0.3 75.2?0.4</cell><cell>2.2?0.1</cell></row><row><cell>Ours</cell><cell>LRW+LRS3</cell><cell>LRS3</cell><cell>LRS3</cell><cell cols="2">37.9?0.5 75.2?0.4</cell><cell>3.8?0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table S20 :Table S21 :</head><label>S20S21</label><figDesc>Investigation of the impact of beam size choices on the validation set of the LRS2 dataset. Investigation of the impact of beam size choices on the validation set of the CMLR dataset. 8?0.10 10.8?0.10 10.8?0.10 10.8?0.08 10.8?0.08 10.9?0.06 10.9?0.10 11.3?0.06</figDesc><table><row><cell>Model</cell><cell>40</cell><cell>35</cell><cell>30</cell><cell>25</cell><cell>20</cell><cell>15</cell><cell>10</cell><cell>5</cell></row><row><cell>Baseline VSR Model</cell><cell cols="2">43.8?0.3 43.9?0.4</cell><cell>44.0?0.4</cell><cell>44.2?0.5</cell><cell>44.4?0.6</cell><cell>44.6?0.5</cell><cell>45.1?0.5</cell><cell>46.3?0.4</cell></row><row><cell>Model</cell><cell>40</cell><cell>35</cell><cell>30</cell><cell>25</cell><cell>20</cell><cell>15</cell><cell>10</cell><cell>5</cell></row><row><cell>Baseline VSR Model</cell><cell>10.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table S22 :</head><label>S22</label><figDesc>Investigation of the impact of beam size choices on the validation set of the MT es dataset.</figDesc><table><row><cell>Model</cell><cell>35</cell><cell>30</cell><cell>25</cell><cell>20</cell><cell>15</cell><cell>10</cell><cell>5</cell></row><row><cell>Baseline VSR Model</cell><cell>53.9?0.5</cell><cell>54.0?0.3</cell><cell>54.3?0.4</cell><cell>54.7?0.4</cell><cell>55.0?0.4</cell><cell>55.6?0.4</cell><cell>57.2?0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table S23 :</head><label>S23</label><figDesc>Investigation of the impact of beam size choices on the validation set of the MT fr dataset.</figDesc><table><row><cell>Model</cell><cell>40</cell><cell>35</cell><cell>30</cell><cell>25</cell><cell>20</cell><cell>15</cell><cell>10</cell><cell>5</cell></row><row><cell>Baseline VSR Model</cell><cell cols="2">83.1?0.8 83.4?0.9</cell><cell>83.6?0.8</cell><cell>84.2?0.4</cell><cell>84.3?0.7</cell><cell>85.3?0.7</cell><cell>86.5?1.2</cell><cell>88.5?0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table S24 :Table S25 :Table S26 :</head><label>S24S25S26</label><figDesc>Investigation of the impact of beam size choices on the validation set of the MT it dataset. Investigation of the impact of beam size choices on the validation set of the MT pt dataset. Ablation study on the LRS3 dataset. Models are trained on LRW, LRS2, LRS3, and AVSpeech.</figDesc><table><row><cell>Model</cell><cell>30</cell><cell>25</cell><cell></cell><cell>20</cell><cell>15</cell><cell></cell><cell></cell><cell>10</cell><cell>5</cell></row><row><cell>Baseline VSR Model</cell><cell>64.3?0.7</cell><cell cols="2">64.2?0.5</cell><cell>64.6?0.8</cell><cell cols="2">65.0?1.0</cell><cell cols="2">65.5?0.7</cell><cell>67.5?0.7</cell></row><row><cell>Model</cell><cell>40</cell><cell>35</cell><cell>30</cell><cell>25</cell><cell>20</cell><cell>15</cell><cell></cell><cell>10</cell><cell>5</cell></row><row><cell>Baseline VSR Model</cell><cell cols="3">68.6?0.8 68.6?0.8 68.8?0.7</cell><cell>68.9?0.6</cell><cell>69.0?0.6</cell><cell cols="2">69.5?0.6</cell><cell>70.1?0.6</cell><cell>71.5?0.6</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WER on LRS3</cell></row><row><cell>Our model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32.1?0.3</cell></row><row><cell cols="2">-Audio auxiliary task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">33.2?0.2</cell></row><row><cell cols="2">-Visual auxiliary task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32.9?0.3</cell></row><row><cell cols="3">-Audio auxiliary task, visual auxiliary task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">33.6?0.6</cell></row><row><cell>-Time masking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">33.2?0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table S28 :</head><label>S28</label><figDesc>The architecture of the 1D-CNN front-end module.The filter shapes are denoted by {Temporal Size, Channels} for 1D Convolutional Layers, respectively. The sizes correspond to [Batch Size, Channels, Sequence Length]. T a denotes the length of audio waveforms. 8% to 36.0%/37.6%. When replacing both ASR and VSR models to LRS3 for initialisation, the mean WER further increases to 37.9%.</figDesc><table><row><cell>Component Name</cell><cell>Layer Type</cell><cell>Input Size</cell><cell>Output Size</cell></row><row><cell>Conv 1</cell><cell>Conv 1D, 80, 64</cell><cell>[B, 1, T a ]</cell><cell>[B, 64, T a //4]</cell></row><row><cell>Conv 2</cell><cell>Conv 1D, 20, 64</cell><cell>[B, 64, T a //4]</cell><cell>[B, 64, T a //16]</cell></row><row><cell>Conv 3</cell><cell>Conv 1D, 4, 128</cell><cell>[B, 64, T a //16]</cell><cell>[B, 128, T a //32]</cell></row><row><cell>Conv 4</cell><cell>Conv 1D, 4, 256</cell><cell>[B, 128, T a //32]</cell><cell>[B, 256, T a //64]</cell></row><row><cell>Conv 5</cell><cell>Conv 1D, 4, 512</cell><cell>[B, 256, T a //64]</cell><cell>[B, 512, T a //128]</cell></row><row><cell>Aggregation</cell><cell>1D Average Pooling, Stride 5</cell><cell>[B, 512, T a //128]</cell><cell>[B, 512, T a //640]</cell></row><row><cell>from 35.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>All training, testing and ablation studies were conducted at Imperial College London.</p><p>The authors declare no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Information</head><p>Correspondence and requests for materials should be addressed to Pingchuan Ma.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors' Contributions</head><p>The code was written by P.M., and the experiments were conducted by P.M. and S.P. The manuscript was written by P.M., S.P. and M.P. M.P. supervised the entire project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Interests</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio-visual speech modeling for continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference of International Speech Communication Association</title>
		<meeting>the 20th Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4135" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition is worth 32?32?8 voxels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Braga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Siohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>the IEEE Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="796" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Investigating the lombard effect influence on end-to-end audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference of International Speech Communication Association</title>
		<meeting>the 20th Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4090" to="4094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visualonly recognition of normal, whispered and silent speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 43rd IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6219" to="6223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis of the visual lombard effect and automatic recognition experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heracleous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Ishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hagita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="288" to="300" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efforts to Acknowledge the Risks of New A.I. Technology</title>
		<ptr target="https://www.nytimes.com/2018/10/22/business/efforts-to-acknowledge-the-risks-of-new-ai-technology.html" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<ptr target="https://www.vice.com/en/article/bvzvdw/tech-companies-are-training-ai-to-read-your-lips" />
		<title level="m">Tech Companies Are Training AI to Read Your Lips</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Online; accessed 22-December-2021</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Liopa -the world&apos;s only startup focused on automated lipreading via visual speech recognition</title>
		<ptr target="https://liopa.ai" />
		<imprint>
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
	<note>Online; accessed 24</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="https://www.wired.com/story/facial-recognition-laws-are-literally-all-over-the-map/" />
		<title level="m">Facial Recognition Laws Are (Literally) All Over the Map</title>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
	<note>Online; accessed 24</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cities Where Police Are Banned From Using Facial Recognition Tech</title>
		<ptr target="https://innotechtoday.com/13-cities-where-police-are-banned-from-using-facial-recognition-tech/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An Update On Our Use of Face Recognition</title>
		<ptr target="https://about.fb.com/news/2021/11/update-on-use-of-face-recognition/" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="https://edition.cnn.com/2021/05/18/tech/amazon-police-facial-recognition-ban/index.html" />
		<title level="m">Amazon will block police indefinitely from using its facial-recognition software</title>
		<imprint>
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
	<note>Online; accessed 24</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Microsoft won&apos;t sell police its facial-recognition technology, following similar moves by Amazon and IBM</title>
		<ptr target="https://www.washingtonpost.com/technology/2020/06/11/microsoft-facial-recognition" />
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
	<note>Online; accessed 24</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">LRS3-TED: a large-scale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1809.00496" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CMU-MOSEAS: A multimodal language dataset for spanish, portuguese, german and french</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1801" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Multilingual TEDx Corpus for Speech Recognition and Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Salesky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of International Speech Communication Association</title>
		<meeting>the 22nd Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3655" to="3659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VoxLingua107: a dataset for spoken language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alum?e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Spoken Language Technology Workshop</title>
		<meeting>the IEEE Spoken Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Retinaface: Single-stage dense face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 33rd IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the 16th IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lipnet: End-to-end sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.01599" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards practical lipreading with distilled and efficient models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 46th IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7608" to="7612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference of International Speech Communication Association</title>
		<meeting>the 20th Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving rnn transducer based asr with auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Spoken Language Technology Workshop</title>
		<meeting>the IEEE Spoken Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multitask learning with low-level auxiliary tasks for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference of International Speech Communication Association</title>
		<meeting>the 20th Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning speech representations from raw audio by joint audiovisual self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning Workshop</title>
		<meeting>the 37th International Conference on Machine Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Visual Speech Representations from Audio Through Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of International Speech Communication Association</title>
		<meeting>the 22nd Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3011" to="3015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ESPnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual Conference of International Speech Communication Association</title>
		<meeting>the 19th Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">mpc001/visual speech recognition for multiple languages: Visual speech recognition for multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7065080</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7065080" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 29th IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining residual networks with LSTMs for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Conference of International Speech Communication Association</title>
		<meeting>the 18th Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3652" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Asian Conference on Computer Vision</title>
		<meeting>the 13th Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10112</biblScope>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Language modeling with deep transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference of International Speech Communication Association</title>
		<meeting>the 20th Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3905" to="3909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Librispeech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 40th IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ted-Lium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Speech and Computer</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Common voice: A massivelymultilingual speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ardila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4218" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MLS: A large-scale multilingual dataset for speech research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference of International Speech Communication Association</title>
		<meeting>the 21st Annual Conference of International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2757" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hybrid ctc/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ShufflenetV2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Conference on Computer Vision</title>
		<meeting>the 15th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
