<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>21 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>21 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abnormal event detection in video is a challenging vision problem. Most existing approaches formulate abnormal event detection as an outlier detection task, due to the scarcity of anomalous data during training. Because of the lack of prior information regarding abnormal events, these methods are not fully-equipped to differentiate between normal and abnormal events. In this work, we formalize abnormal event detection as a one-versus-rest binary classification problem. Our contribution is two-fold. First, we introduce an unsupervised feature learning framework based on object-centric convolutional auto-encoders to encode both motion and appearance information. Second, we propose a supervised classification approach based on clustering the training samples into normality clusters. A one-versus-rest abnormal event classifier is then employed to separate each normality cluster from the rest. For the purpose of training the classifier, the other clusters act as dummy anomalies. During inference, an object is labeled as abnormal if the highest classification score assigned by the one-versus-rest classifiers is negative. Comprehensive experiments are performed on four benchmarks: Avenue, ShanghaiTech, UCSD and UMN. Our approach provides superior results on all four data sets. On the large-scale ShanghaiTech data set, our method provides an absolute gain of 8.4% in terms of frame-level AUC compared to the state-of-the-art method <ref type="bibr" target="#b33">[34]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Abnormal event detection in video has drawn a lot of attention in the past couple of years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, perhaps because it is considered a challenging task due to the commonly accepted definition of abnormal events, which relies on context. An example that illustrates the importance of context is a scenario in which a truck is being driven on the street (normal event) versus a scenario in which a truck is being driven in a pedestrian area (abnormal event). In addition to the reliance on context, abnormal events rarely occur and are generally dominated by more familiar (normal) events.</p><p>Therefore, it is difficult to obtain a sufficiently representative set of anomalies, making it hard to employ traditional supervised learning methods.</p><p>Most existing anomaly detection approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> are based on outlier detection and learn a model of normality from training videos containing only familiar events. During inference, events are labeled as abnormal if they deviate from the normality model. Different from these approaches, we address abnormal event detection by formulating the task as a multi-class classification problem instead of an outlier detection problem. Since the training data contains only normal events, we first apply k-means clustering in order to find clusters representing various types of normality (see <ref type="figure">Figure 1</ref>). Next, we train a binary classifier following the one-versus-rest scheme in order to separate each normality cluster from the others. During training, normality clusters are treated as different categories, leading to the synthetic generation of abnormal training data. During inference, the highest classification score corresponding to a given test sample represents the normality score of the respective sample. If the score is negative, the sample is labeled as abnormal (since it does not belong to any normality class). To our knowledge, we are the first to treat the abnormal event detection task as a discriminative multi-class classification problem.</p><p>In general, existing abnormal event detection frameworks extract features at a local level <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>, global (frame) level <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, or both <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>. All these approaches extract features without explicitly taking into account the objects of interest. In this paper, we propose an object-centric approach by applying a fast yet powerful single-shot detector (SSD) <ref type="bibr" target="#b18">[19]</ref> on each frame, and learning deep unsupervised features using convolutional auto-encoders (CAE) on top of the detected objects, as shown in <ref type="figure">Figure 1</ref>. This enables us to explicity focus only on the objects present in the scene. In addition, it allows us to accurately localize the anomalies in each frame. Although auto-encoders have been used before for abnormal event detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>, to our knowledge, we are the first to train object-centric auto-encoders. <ref type="figure">Figure 1</ref>. Our anomaly detection framework based on training convolutional auto-encoders on top of object detections. In the training phase (represented in dashed lines), the concatenated motion and appearance latent representations are clustered and a one-versus-rest classifier is trained to discriminate between the formed clusters. In the inference phase, we label a test sample as abnormal if the highest classification score is negative, i.e. the sample is not attributed to any class. Best viewed in color.</p><p>In summary, the novelty of our paper is two-fold. First, we train object-centric convolutional auto-encoders for both motion and appearance. Second, we propose a supervised learning approach by formulating the abnormal event detection task as a multi-class problem. We conduct experiments on the Avenue <ref type="bibr" target="#b22">[23]</ref>, the ShanghaiTech <ref type="bibr" target="#b23">[24]</ref>, the UCSD <ref type="bibr" target="#b24">[25]</ref> and the UMN <ref type="bibr" target="#b25">[26]</ref> data sets, and compare our approach with the state-of-the-art abnormal event detection methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. The empirical results clearly show that our approach achieves superior performance compared to the state-of-the-art methods on all data sets. Furthermore, on the Avenue and the ShanghaiTech data sets, our approach provides considerable absolute gains of 1.5% and 8.4%, respectively, over the state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>We organize the paper as follows. We present related work on abnormal event detection in Section 2. We describe our approach in Section 3. We present the abnormal event detection experiments in Section 4. We draw our final conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Abnormal event detection is commonly formalized as an outlier detection task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, in which the main approach is to learn a model of familiarity from training videos and label the de-tected outliers as abnormal. Several abnormal event detection approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref> learn a dictionary of atoms representing normal events during training, then label the events not represented in the dictionary as abnormal. Some recent approaches have employed locality sensitive hashing <ref type="bibr" target="#b37">[38]</ref> and deep learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> to achieve better results. For instance, Smeureanu et al. <ref type="bibr" target="#b32">[33]</ref> employed a one-class Support Vector Machines (SVM) model based on deep features provided by convolutional neural networks (CNN) pre-trained on the ILSVRC benchmark <ref type="bibr" target="#b29">[30]</ref>, while Ravanbakhsh et al. <ref type="bibr" target="#b26">[27]</ref> combined pre-trained CNN models with low-level optical-flow maps.</p><p>Similar to our own approach, which learns features in an unsupervised fashion, there are a few works that have employed unsupervised steps for abnormal event detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. Interestingly, some recent works do not require training data at all, in order to detect abnormal events <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22]</ref>. More closely-related to our work are methods that employ features learned with autoencoders <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> or extracted from the classification branch of Fast R-CNN <ref type="bibr" target="#b11">[12]</ref>. In order to learn deep features without supervision, Xu et al. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> used Stacked Denoising Auto-Encoders on multi-scale patches. To detect abnormal events, Xu et al. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> used one-class SVM on top of the deep features. Hasan et al. <ref type="bibr" target="#b10">[11]</ref> employed two autoencoders, one that is learned on conventional handcrafted features, and another one that is learned in an end-to-end fashion using a fully convolutional feed-forward network. On the other hand, Sabokrou et al. <ref type="bibr" target="#b30">[31]</ref> combined 3D deep auto-encoders and 3D convolutional neural networks into a cascaded framework. Differences of our approach. Different from these recent related works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, we propose to train autoencoders on object detections provided by a state-of-theart detector <ref type="bibr" target="#b18">[19]</ref>. The most similar work to ours is that of Hinami et al. <ref type="bibr" target="#b11">[12]</ref>. They also proposed an object-centric approach, but our detection, feature extraction and training stages are different. While Hinami et al. <ref type="bibr" target="#b11">[12]</ref> used geodesic <ref type="bibr" target="#b16">[17]</ref> and moving object proposals <ref type="bibr" target="#b9">[10]</ref>, we employ a single-shot detector <ref type="bibr" target="#b18">[19]</ref> based on Feature Pyramid Networks (FPN). In the feature extraction stage, Hinami et al. <ref type="bibr" target="#b11">[12]</ref> fine-tuned the classification branch of the Fast R-CNN model on multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events. In contrast, we learn unsupervised deep features with convolutional auto-encoders. Also differing from Hinami et al. <ref type="bibr" target="#b11">[12]</ref> and all other works, we formalize the abnormal event detection task as a multi-class problem and propose to train a one-versus-rest SVM on top of kmeans clusters. A similar approach was adopted by Caron et al. <ref type="bibr" target="#b3">[4]</ref> in order to train deep generic visual features in an unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Motivation. Since the training data contains only normal events, supervised learning methods that require both positive (normal) and negative (abnormal) samples cannot be directly applied for the abnormal event detection task. However, we believe that including any form of supervision is an important step towards obtaining better performance in practice. Motivated by this intuition, we conceive a framework that incorporates two approaches for including supervision. The first approach consists of employing a singleshot object detector <ref type="bibr" target="#b18">[19]</ref>, which is trained in a supervised fashion, in order to obtain object detections that are subsequently used throughout the rest of the processing pipeline. The second approach consists of training supervised oneversus-rest classifiers on artificially-generated classes representing different kinds of normality. The classes are generated by previously clustering the training samples. Our entire framework is composed of four sequential stages that are described in detail below. These are the object detection stage, the feature learning stage, the model training stage, and the inference stage. Object detection. We propose to detect objects using a single-shot object detector based on FPN <ref type="bibr" target="#b18">[19]</ref>, which offers an optimal trade-off between accuracy and speed. This object detector is specifically chosen because (i) it can accurately detect smaller objects, due to the FPN architecture, <ref type="figure">Figure 2</ref>. Normal and abnormal objects (left) and gradients (right) with reconstructions provided by the appearance (left) and the motion (right) convolutional auto-encoders. The samples are selected from the Avenue <ref type="bibr" target="#b22">[23]</ref>, the ShanghaiTech <ref type="bibr" target="#b23">[24]</ref>, the UCSD Ped2 <ref type="bibr" target="#b24">[25]</ref> and the UMN <ref type="bibr" target="#b25">[26]</ref> test videos, and are not seen during training the auto-encoders. and (ii) it can process about 13 frames per second on a GPU. These advantages are of utter importance for developing a practical abnormal event detection framework. The object detector is applied on a frame by frame basis in order to obtain a set of bounding boxes for the objects in each frame t. We use the bounding boxes to crop the objects. The resulting images are converted to grayscale. Next, the images are directly passed to the feature learning stage, in order to learn object-centric appearance features. At the same time, we use the images containing objects in order to compute gradients representing motion. For this step, we additionally consider the images cropped from a previous and a subsequent frame. As illustrared in <ref type="figure">Figure 1</ref>, we choose the frames at index t ? 3 and t + 3, with respect to the current frame t. Since the temporal distance between the frames is not significant, we do not need to track the objects. Instead, we simply consider the bounding boxes determined at frame t in order to crop the objects at frames t ? 3 and t + 3. For each object, we obtain two image gradients, one representing the change in motion from frame t?3 to frame t and one representing the change in motion from frame t to frame t + 3. Finally, the image gradients are also passed to the feature learning stage, in order to learn object-centric motion features. Feature learning. In order to obtain a feature vector for each object detection, we train three convolutional auto-encoders. One auto-encoder takes as input cropped images containing objects, and it inherently learns latent appearance features. The other two auto-econders take as input the gradients that capture how the object moved before and after the detection moment, respectively. These auto-encoders learn latent motion features. All three auto-encoders are based on the same lightweight architecture, which is composed of an encoder with 3 convolutional and max-pooling blocks, and a decoder with 3 upsampling and convolutional blocks and an additional convolutional layer for the final output. For each CAE, the size of the input is 64 ? 64 ? 1, and the size of the output is the same. All convolutional layers are based on 3 ? 3 filters. Each convolutional layer, except the very last one, is followed by ReLU activations. The first two convolutional layers of the encoder contain 32 filters each, while the third convolutional layer contains 16 filters. The max-pooling layers of the encoder are based on 2 ? 2 filters with stride 2. The resulting latent feature representation of each CAE is composed of 16 activation maps of size 8?8. In the decoder, each resize layer upsamples the input activations by a factor of two, using the nearest neighbor approach. The first convolutional layer in the decoder contains 16 filters. The following two convolutional layers of the decoder contain 32 filters each. The fourth (and last) convolutional layer of the decoder contains a single filter of size 3 ? 3. The main purpose of the last convolutional layer is to reduce the output depth from 64 ? 64 ? 32 to 64 ? 64 ? 1. The auto-encoders are trained with the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> using the pixel-wise mean squared error as loss function:</p><formula xml:id="formula_0">L(I, O) = 1 h ? w h i=1 w j=1 (I ij ? O ij ) 2 ,<label>(1)</label></formula><p>where I and O are the input and the output images, each of size h ? w pixels (in our case, h = w = 64).</p><p>The auto-encoders learn to represent objects detected in the training video containing only normal behavior. When we provide as input objects with abnormal behavior, the reconstruction error of the auto-encoders is expected to be higher. Furthermore, the latent features should represent known (normal) objects in a different and better way than unknown (abnormal) objects. Some input-output CAE pairs selected from the test videos in each data set considered in the evaluation are shown in <ref type="figure">Figure 2</ref>. We notice that the auto-encoders generally provide better reconstructions for normal objects, confirming our intuition. The final feature vector for each object detection sample is a concatenation of the latent appearance features and the latent motion features. Since the latent activation maps of each CAE are 8 ? 8 ? 16, the final feature vectors have 3072 dimensions. Model training. We propose a novel training approach by formalizing the abnormal event detection task as a multiclass classification problem. The proposed approach aims to compensate for the lack of truly abnormal training samples, by constructing a context in which a subset of normal training samples can play the role of dummy abnormal samples with respect to another subset of normal training samples. This is achieved by clustering the normal training samples into k clusters using k-means. We consider that each cluster represents a certain kind of normality, different from the other clusters. From the perspective of a given cluster i, the samples belonging to the other clusters (from the set {1, 2, ...., k} \ i) can be viewed as (dummy) abnormal samples. Therefore, we can train a binary classifier g i , in our case an SVM, to separate the positively-labeled data points in a cluster i from the negatively-labeled data points in clusters {1, 2, ...., k} \ i, as follows:</p><formula xml:id="formula_1">g i (x) = m j=1 w j ? x j + b,<label>(2)</label></formula><p>where x ? R m is a test sample that must be classified either as normal or abnormal, w is the vector of weights and b is the bias term. We note that the negative samples can actually be considered as more closely-related to the samples in cluster i than truly abnormal samples. Hence, the discrimination task is more difficult, and it can help the SVM to select better support vectors. For each cluster i, we train an independent binary classifier g i . The final classification score for one data sample is the highest score among the scores returned by the k classifiers. In other words, the classification score for one data sample is selected according to the one-versus-rest scheme, commonly used when binary classifiers are employed for solving multi-class problems.</p><p>Inference. In the inference phase, each test sample x is classified by the k binary SVM models. The highest classification score is used (with a change of sign) as the abnormality score s for the respective test sample x:</p><formula xml:id="formula_2">s(x) = ? max i {g i (x)}, ?i ? {1, 2, ...., k}.<label>(3)</label></formula><p>By putting together the scores of the objects cropped from a given frame, we obtain a pixel-level anomaly prediction map for the respective frame. If the bounding boxes of two objects overlap, we keep the maximum abnormality score for the overlapping region. To obtain frame-level predictions, we take the highest score in the prediction map as the anomaly score of the respective frame. Finally, we apply a Gaussian filter to temporally smooth the frame-level anomaly scores.  <ref type="bibr" target="#b25">[26]</ref> consists of three independent crowded scenes of different lengths. The three scenes consist of 1453 frames, 4144 frames and 2144 frames, respectively. The resolution of each video frame is 240 ? 320 pixels. The normal behavior is represented by people walking around, while the abnormal behavior is represented by people running in different directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>As evaluation metric, we employ the area under the curve (AUC) computed with regard to ground-truth annotations at the frame-level. The frame-level AUC metric used in most previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> considers a frame as being a correct detection, if it contains at least one abnormal pixel. We adopt the same framelevel AUC definition as these previous works. In order to obtain the final abnormality maps, our pixel-level detection maps are smoothed using a similar technique as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter and Implementation Details</head><p>In the object detection stage, we employ a single-shot detector based on FPN <ref type="bibr" target="#b18">[19]</ref> that is pre-trained on the COCO data set <ref type="bibr" target="#b19">[20]</ref>. The detector is downloaded from the Tensor-Flow detection model zoo. For the training set, we keep the detections with a confidence level higher than 0.5, and for the test set, we keep those with a confidence level higher than 0.4. The convolutional auto-encoders used in the feature learning stage are implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref>. We train the auto-encoders for 100 epochs with the learning rate set to 10 ?3 , and for another 100 epochs with the learning rate set to 10 ?4 . We use mini-batches of 64 samples. We train independent auto-encoders for each of the four data  <ref type="bibr" target="#b23">[24]</ref>, the UCSD Ped2 <ref type="bibr" target="#b24">[25]</ref> and the UMN <ref type="bibr" target="#b25">[26]</ref> data sets. Our framework is compared with several state-of-the-art approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, which are listed in temporal order. The results of Sultani et al. <ref type="bibr" target="#b33">[34]</ref> on ShanghaiTech are based on their pre-trained model. sets considered in the evaluation. To cluster the training samples with k-means, we employ the VLFeat <ref type="bibr" target="#b34">[35]</ref> implementation, which is based on the Lloyd algorithm <ref type="bibr" target="#b7">[8]</ref>. We adopt k-means++ <ref type="bibr" target="#b2">[3]</ref> initialization. We repeat the clustering 10 times, selecting the partitioning with the minimum energy. In all the experiments, we set the number of k-means clusters to k = 10. We set the regularization parameter of the linear SVM (implemented in VLFeat <ref type="bibr" target="#b34">[35]</ref>) to C = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>We evaluate our approach in comparison with a series of state-of-the-art methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> on the Avenue, the ShanghaiTech, the UCSD Ped2 and the UMN data sets. The corresponding results are presented in <ref type="table" target="#tab_1">Table 1</ref>. Avenue. On the Avenue data set, we are able to surpass the results reported in all previous works. Compared to most of the recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>, our method provides an absolute gain of more than 5% in terms of frame-level AUC. With a frame-level AUC of 88.9%, Ionescu et al. <ref type="bibr" target="#b13">[14]</ref> is the best and most recent baseline. We surpass their score by 1.5%. Remarkably, with a frame-level AUC of 90.4%, our approach is the only method that surpasses the threshold <ref type="figure">Figure 3</ref>. Frame-level anomaly detection scores between 0 and 1 (on the horizontal axis) provided by our approach, for various test videos selected from the Avenue <ref type="bibr" target="#b22">[23]</ref>, the ShanghaiTech <ref type="bibr" target="#b23">[24]</ref>, the UCSD Ped2 <ref type="bibr" target="#b24">[25]</ref> and the UMN <ref type="bibr" target="#b25">[26]</ref> data sets. Ground-truth abnormal events are represented in cyan and our scores are illustrated in red. Best viewed in color.</p><p>of 90% on the Avenue data set.</p><p>Notably, Hinami et al. <ref type="bibr" target="#b11">[12]</ref> did not compare with other approaches on the official Avenue test set, arguing that there are five test videos (01, 02, 08, 09 and 10) containing static abnormal objects that are not properly labeled. Therefore, they only evaluated their method on Avenue17, a subset that excludes the respective five videos. We also compare our performance with that reported by Hinami et al. <ref type="bibr" target="#b11">[12]</ref>, being sure to exclude the same five test videos for a fair comparison. Our frame-level AUC score on the Avenue17 subset is 91.6%, which is almost 2% better than the frame-level AUC of 89.8% reported in <ref type="bibr" target="#b11">[12]</ref>. With respect to the complete Avenue test set, we note that our framework attains a better frame-level AUC score on the Avenue17 subset, suggesting that the removed test videos are indeed more problematic than the videos left in Avenue17. As observed by Hinami et al. <ref type="bibr" target="#b11">[12]</ref>, the removed videos include some abnormal objects that are not labeled accordingly. Methods detecting these objects as abnormal are destined to reach higher false positive rates, which is unfair.</p><p>In <ref type="figure">Figure 3 (a)</ref>, we present the frame-level anomaly scores provided by our method on test video 06 from Avenue. According to the ground-truth labels, which are also illustrated in <ref type="figure">Figure 3</ref> (a), we note that there are four ab-normal events in the respective test video. Our approach seems to be able to identify three of the four events, without including any false positive detections. <ref type="figure" target="#fig_0">Figure 4</ref> (top row) illustrates a few examples of true positive and false positive abnormal event detections. From left to right, the true positive detections are a person running, a person walking in the wrong direction, a person picking up an object and a person throwing an object. The first false positive example consists of two people that are detected in the same bounding box by the object detector. The other false positive detection is a person walking in the wrong direction that is labeled as abnormal too soon. ShanghaiTech. Since ShanghaiTech is the newest data set for abnormal event detection, there are only a few recent approaches reporting results on this data set <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. Besides these, Luo et al. <ref type="bibr" target="#b23">[24]</ref> additionally evaluated a previously published method <ref type="bibr" target="#b10">[11]</ref> when they introduced the data set. On the ShanghaiTech data set, the state-of-the-art performance of 72.8% is reported by Liu et al. <ref type="bibr" target="#b20">[21]</ref>. We outperform their approach by a large margin of 12.1%. In order to compare with Sultani et al. <ref type="bibr" target="#b33">[34]</ref> in standard formulation of the abnormal event detection task, we used the open source code provided by Sultani et al. <ref type="bibr" target="#b33">[34]</ref> to compute their anomaly scores for the large-scale ShanghaiTech data set. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the approach of Sultani et al. <ref type="bibr" target="#b33">[34]</ref> obtains a frame-level AUC of 76.5%, outperforming the best existing method <ref type="bibr" target="#b20">[21]</ref>. Our approach significantly outperforms both Sultani et al. <ref type="bibr" target="#b33">[34]</ref> and Liu et al. <ref type="bibr" target="#b20">[21]</ref>, achieving a frame-level AUC of 84.9%. With a frame-level AUC of 84.9%, our approach is the only one to surpass the 80% threshold on ShanghaiTech.</p><p>In <ref type="figure">Figure 3</ref> (b), we display our frame-level anomaly scores against the ground-truth labels on a ShanghaiTech test video with three abnormal events. On this video, we can clearly observe a strong correlation between our anomaly scores and the ground-truth labels. Some localization results from different scenes in the ShanghaiTech data set are illustrated in the second row of <ref type="figure" target="#fig_0">Figure 4</ref>. The true positive abnormal events detected by our framework are (from left to right) two bikers in a pedestrian area, a person robbing another person, a person jumping and two people fighting. The false positive abnormal events are triggered because, in each case, there are two people in the same bounding box and our system labels the unusual appearance and motion generated by the two objects as abnormal. UCSD Ped2. While older approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref> report framelevel AUC scores under 70%, most approaches proposed in the last three years <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> reach frame-level AUC scores between 87% and 94% on UCSD Ped2. For instance, the frameworks based on autoencoders <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> attain results of around 90%. Liu et al. <ref type="bibr" target="#b20">[21]</ref> recently outperformed the previous works, reporting a frame-level AUC of 95.4%. We further surpass their state-of-the-art result, reaching the top frame-level AUC of 97.8% on UCSD Ped2. Our score is 2.4% above the score reported by Liu et al. <ref type="bibr" target="#b20">[21]</ref>, 4.3% above the second-best score reported by Ravanbakhsh et al. <ref type="bibr" target="#b27">[28]</ref>, and more than 7% higher than the scores reported by other frameworks based on auto-encoders <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>As for the other data sets, we compare our frame-level anomaly scores against the ground-truth labels on a test video from UCSD Ped2 in <ref type="figure">Figure 3</ref> (c). On this particular video, our frame-level AUC is above 99%, indicating that our approach can precisely detect the abnormal event. Furthermore, the qualitative results presented in the third row of <ref type="figure" target="#fig_0">Figure 4</ref>, show that our approach can also localize the abnormal events from UCSD Ped2. From left to right, the true positive detections are a biker in a pedestrian area, two bikers in a pedestrian area, two bikers and a skater in a pedestrian area and a biker and a skater in a pedestrian area. As for ShanghaiTech, the false positive abnormal detections are caused by two people in the same bounding box. UMN. It appears that UMN is the easiest abnormal event detection data set, because almost all works report framelevel AUC scores higher than 95%, with some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> even surpassing 99%. The top score of 99.6% is reported by Sabokrou et al. <ref type="bibr" target="#b30">[31]</ref>, and we reach the same performance on the UMN data set. We note that the second scene seems to be slightly more difficult than the other two scenes, since our frame-level AUC score on this scene is 99.1%, while the frame-level AUC scores on the other scenes are 99.9% and 99.8%, respectively. For this reason, we choose to illustrate the frame-level anomaly scores against the ground-truth labels for the second scene from UMN in <ref type="figure">Figure 3 (d)</ref>. Overall, our anomaly scores correlate well with the ground-truth labels, but there are some normal frames with high abnormality scores just before the third abnormal event in the scene.</p><p>In the fourth row of <ref type="figure" target="#fig_0">Figure 4</ref>, we present some localization results provided by our framework. The true positive examples represent people running around in different directions, while the false positive detections are triggered by two people in the same bounding box and a person bending down to pick up an object. We note that the false positive examples are selected from the second scene, as we did not find false positive detections in the other two scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>While the results presented in <ref type="table" target="#tab_1">Table 1</ref> show that our approach can outperform the state-of-the-art methods on four evaluation sets, we also aim to address questions about the robustness of our features and parameter choices, and to discuss the running time of our framework. Parameter selection.</p><p>We present results with various parameter choices on the largest and most difficult evaluation set, namely ShanghaiTech. We first variate the number of clusters k by selecting values in the set {5, 10, 15, 20, 25, 30}. The corresponding frame-level AUC scores are presented in <ref type="figure" target="#fig_1">Figure 5</ref>. The results presented in <ref type="figure" target="#fig_1">Figure 5</ref> indicate that the number of clusters does  not play a significant role in our multi-class classification framework, since the accuracy variations are lower than 1.1%. With only one exception (for k = 25), our results are always higher than 84%. We also variate the regularization parameter of the SVM, by considering values in the set {0.1, 1, 10, 100}. The corresponding frame-level AUC scores are presented in <ref type="figure" target="#fig_2">Figure 6</ref>. The results presented in <ref type="figure" target="#fig_2">Figure 6</ref> show that the performance variation is lower than 0.3%, and the frame-level AUC scores are always higher than 84.6%. We believe that this happens because the classes are linearly separable, since they are generated by clustering the samples with k-means into disjoint clusters. Overall, we conclude that our high improvement (12.1%) over the state-of-the-art approach <ref type="bibr" target="#b20">[21]</ref>, cannot be explained by a convenient choice of parameters. Ablation results. In <ref type="table" target="#tab_2">Table 2</ref>, we present feature ablation results, as well as results for a one-class SVM based on our full object-centric feature set, on the ShanghaiTech data set. When we remove the object detector and train autoencoders at the frame-level, we obtain a frame-level AUC of 72.4%, which demonstrates the importance of extracting object-centric features and using the one-versus-rest SVM. We note that the frame-level auto-encoders have an additional convolutional layer and the input resolution is increased to 192 ? 192. When we replace the one-class SVM with our multi-class approach based on k-means and oneversus-rest SVM, while keeping the features computed on full frames, the frame-level AUC grows to 78.7%. This demonstrates that our approach based on k-means and oneversus-rest SVM is indeed helpful. When we replace the object-centric CAE features with pre-trained SSD features (extracted right before the SSD class predictor), the frame- level AUC is only 81.3%, which shows the importance of learning features with auto-encoders. By removing either the appearance or the motion object-centric CAE features from our model, the results drop by less than 3%. This shows that both appearance and motion features are relevant for the abnormal event detection task. By replacing our multi-class approach based on k-means and one-versusrest SVM with a one-class SVM, while keeping the combined object-centric CAE features, the performance drops by 5.7%. This result indicates that formalizing the abnormal event detection task as a multi-class problem is indeed useful. We conclude that both of our contributions are crucial to obtain superior results. Running time. The single-shot object detector <ref type="bibr" target="#b18">[19]</ref> requires about 74 milliseconds to process a single frame. Hence, it can run at about 13.5 frames per second (FPS). With a reasonable average of 5 objects per frame, our feature extraction and inference stages require about 16 milliseconds per frame. Thus, we can process about 62.5 frames per second. However, the entire pipeline requires about 90 milliseconds to infer the anomaly scores for a single frame, which translates to 11 FPS. We note that more than 80% of the processing time is spent detecting objects on a frame by frame basis. The running time can be improved by replacing the current object detector with a faster one. We note that all running times were measured on an Nvidia Titan Xp GPU with 12 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We introduced a novel method for abnormal event detection in video, which is based on (i) training objectcentric convolutional auto-encoders and on (ii) formalizing abnormal event detection as a multi-class problem. The empirical results obtained on four data sets indicate that our approach outperforms a series of state-of-the-art approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. In future work, we aim to improve our framework by segmenting and tracking objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>True positive (left) versus false positive (right) detections of our framework. Examples are selected from the Avenue [23] (first row), the ShanghaiTech [24] (second row), the UCSD Ped2 [25] (third row) and the UMN [26] (fourth row) data sets. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Frame-level AUC scores on ShanghaiTech obtained by selecting values for the number of clusters k from the set {5, 10, 15, 20, 25, 30}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Frame-level AUC scores on ShanghaiTech obtained by selecting values for the SVM regularization parameter C from the set {0.1, 1, 10, 100}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ShanghaiTech. The ShanghaiTech Campus data set<ref type="bibr" target="#b23">[24]</ref> is among the largest data sets for abnormal event detection. Unlike other data sets, it contains 13 different scenes with various lighting conditions and camera angles. There are 330 training videos and 107 test videos. The test set contains a total of 130 abnormal events annotated at the pixellevel. There are 316154 frames in the whole data set. The resolution of each video frame is 480 ? 856 pixels. UCSD. The UCSD Pedestrian data set<ref type="bibr" target="#b24">[25]</ref> is composed of two subsets, namely Ped1 and Ped2. As Hinami et al.<ref type="bibr" target="#b11">[12]</ref>, we exclude Ped1 from the evaluation, because it has a significantly lower frame resolution of 158 ? 238.</figDesc><table><row><cell>Another</cell></row><row><cell>problem with Ped1 is that some recent works report re-</cell></row><row><cell>sults only on a subset of 16 videos [27, 28, 36], while oth-</cell></row><row><cell>ers [13, 25, 21, 22] report results on all 36 test videos. We</cell></row><row><cell>thus consider only UCSD Ped2, which contains 16 train-</cell></row><row><cell>4. Experiments</cell></row><row><cell>4.1. Data Sets</cell></row></table><note>Avenue. The Avenue data set [23] consists of 16 training videos with a total 15328 frames and 21 test videos with a total of 15324. The resolution of each video frame is 360 ? 640 pixels. For each test frame, ground-truth locations of anomalies are provided using pixel-level masks.ing and 12 test videos. The resolution of each frame is 240 ? 360 pixels. There are 2550 frames for training and 2010 for testing. The videos illustrate various crowded scenes, and anomalies include bicycles, vehicles, skate- boarders and wheelchairs crossing pedestrian areas. UMN. The UMN Unusual Crowd Activity data set</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="4">Avenue Shanghai UCSD UMN</cell></row><row><cell></cell><cell></cell><cell>Tech</cell><cell>Ped2</cell><cell></cell></row><row><cell>Kim et al. [15]</cell><cell>-</cell><cell>-</cell><cell>69.3</cell><cell>-</cell></row><row><cell>Mehran et al. [26]</cell><cell>-</cell><cell>-</cell><cell>55.6</cell><cell>96.0</cell></row><row><cell>Mahadevan et al. [25]</cell><cell>-</cell><cell>-</cell><cell>82.9</cell><cell>-</cell></row><row><cell>Cong et al. [6]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.8</cell></row><row><cell>Saligrama et al. [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.5</cell></row><row><cell>Lu et al. [23]</cell><cell>80.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Dutta et al. [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.5</cell></row><row><cell>Xu et al. [36, 37]</cell><cell>-</cell><cell>-</cell><cell>90.8</cell><cell>-</cell></row><row><cell>Hasan et al. [11]</cell><cell>70.2</cell><cell>60.9</cell><cell>90.0</cell><cell>-</cell></row><row><cell>Del Giorno et al. [7]</cell><cell>78.3</cell><cell>-</cell><cell>-</cell><cell>91.0</cell></row><row><cell>Zhang et al. [38]</cell><cell>-</cell><cell>-</cell><cell>91.0</cell><cell>98.7</cell></row><row><cell>Smeureanu et al. [33]</cell><cell>84.6</cell><cell>-</cell><cell>-</cell><cell>97.1</cell></row><row><cell>Ionescu et al. [13]</cell><cell>80.6</cell><cell>-</cell><cell>82.2</cell><cell>95.1</cell></row><row><cell>Luo et al. [24]</cell><cell>81.7</cell><cell>68.0</cell><cell>92.2</cell><cell>-</cell></row><row><cell>Hinami et al. [12]</cell><cell>-</cell><cell>-</cell><cell>92.2</cell><cell>-</cell></row><row><cell>Ravanbakhsh et al. [28]</cell><cell>-</cell><cell>-</cell><cell>93.5</cell><cell>99.0</cell></row><row><cell>Sabokrou et al. [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.6</cell></row><row><cell>Ravanbakhsh et al. [27]</cell><cell>-</cell><cell>-</cell><cell>88.4</cell><cell>98.8</cell></row><row><cell>Liu et al. [21]</cell><cell>85.1</cell><cell>72.8</cell><cell>95.4</cell><cell>-</cell></row><row><cell>Liu et al. [22]</cell><cell>84.4</cell><cell>-</cell><cell>87.5</cell><cell>96.1</cell></row><row><cell>Sultani et al. [34]</cell><cell>-</cell><cell>76.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Ionescu et al. [14]</cell><cell>88.9</cell><cell>-</cell><cell>-</cell><cell>99.3</cell></row><row><cell>Ours</cell><cell>90.4</cell><cell>84.9</cell><cell cols="2">97.8 99.6</cell></row></table><note>Abnormal event detection results (in %) in terms of frame- level AUC on the Avenue [23], the ShanghaiTech</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Method Score Frame-level CAE features + one-class SVM (baseline) 72.4 Frame-level CAE features + one-versus-rest SVM 78.7 Pre-trained SSD features + one-versus-rest SVM 81.3 CAE appearance features + one-versus-rest SVM 82.2 CAE motion features + one-versus-rest SVM 83.0 Combined CAE features + one-class SVM 79.2 Combined CAE features + one-versus-rest SVM 84.9 Frame-level AUC scores (in %) on ShanghaiTech<ref type="bibr" target="#b23">[24]</ref> obtained by removing various components from our framework versus a baseline based on frame-level features and one-class SVM.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">k-means++: The Advantages of Careful Seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Anomaly Detection in Large Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centroidal Voronoi Tessellations: Applications and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gunzburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="637" to="676" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online Detection of Abnormal Events Using Incremental Coding Length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3755" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Segment Moving Objects in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4083" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting abnormal events in video using narrowed normality clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic Object Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="725" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Classifier Two-Sample Test for Video Anomaly Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection in Videos using Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised Behavior-Specific Dictionary Learning for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepcascade: Cascading 3D Deep Neural Networks for Fast Anomaly Detection and Localization in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Appearance Features for Abnormal Behavior Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIAP</title>
		<meeting>ICIAP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10485</biblScope>
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-World Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">VLFeat: An Open and Portable Library of Computer Vision Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>pages 8.1-8.12</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Detecting Anomalous Events in Videos by Learning Deep Representations of Appearance and Motion. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
