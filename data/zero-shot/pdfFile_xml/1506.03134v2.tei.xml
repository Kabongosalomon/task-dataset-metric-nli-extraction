<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pointer Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pointer Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Networks (RNNs) have been used for learning functions over sequences from examples for more than three decades <ref type="bibr" target="#b2">[3]</ref>. However, their architecture limited them to settings where the inputs and outputs were available at a fixed frame rate (e.g. <ref type="bibr" target="#b3">[4]</ref>). The recently introduced sequence-to-sequence paradigm <ref type="bibr" target="#b0">[1]</ref> removed these constraints by using one RNN to map an input sequence to an embedding and another (possibly the same) RNN to map the embedding to an output sequence. <ref type="bibr">Bahdanau et. al.</ref> augmented the decoder by propagating extra contextual information from the input using a content-based attentional mechanism <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. These developments have made it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems in natural language processing such as translation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> and parsing <ref type="bibr" target="#b6">[7]</ref>, image and video captioning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and even learning to execute small programs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Nonetheless, these methods still require the size of the output dictionary to be fixed a priori. Because of this constraint we cannot directly apply this framework to combinatorial problems where the size of the output dictionary depends on the length of the input sequence. In this paper, we address this limitation by repurposing the attention mechanism of <ref type="bibr" target="#b4">[5]</ref> to create pointers to input elements. We show that the resulting architecture, which we name Pointer Networks (Ptr-Nets), can be trained to output satisfactory solutions to three combinatorial optimization problems -computing planar convex hulls, Delaunay triangulations and the symmetric planar Travelling Salesman Problem (TSP). The resulting models produce approximate solutions to these problems in a purely data driven fash-(a) Sequence-to-Sequence (b) Ptr-Net <ref type="figure">Figure 1</ref>: (a) Sequence-to-Sequence -An RNN (blue) processes the input sequence to create a code vector that is used to generate the output sequence (purple) using the probability chain rule and another RNN. The output dimensionality is fixed by the dimensionality of the problem and it is the same during training and inference <ref type="bibr" target="#b0">[1]</ref>. (b) Ptr-Net -An encoding RNN converts the input sequence to a code (blue) that is fed to the generating network (purple). At each step, the generating network produces a vector that modulates a content-based attention mechanism over inputs ( <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>). The output of the attention mechanism is a softmax distribution with dictionary size equal to the length of the input. ion (i.e., when we only have examples of inputs and desired outputs). The proposed approach is depicted in <ref type="figure">Figure 1</ref>.</p><p>The main contributions of our work are as follows:</p><p>? We propose a new architecture, that we call Pointer Net, which is simple and effective. It deals with the fundamental problem of representing variable length dictionaries by using a softmax probability distribution as a "pointer". ? We apply the Pointer Net model to three distinct non-trivial algorithmic problems involving geometry. We show that the learned model generalizes to test problems with more points than the training problems. ? Our Pointer Net model learns a competitive small scale (n ? 50) TSP approximate solver.</p><p>Our results demonstrate that a purely data driven approach can learn approximate solutions to problems that are computationally intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>We review the sequence-to-sequence <ref type="bibr" target="#b0">[1]</ref> and input-attention models <ref type="bibr" target="#b4">[5]</ref> that are the baselines for this work in Sections 2.1 and 2.2. We then describe our model -Ptr-Net in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence-to-Sequence Model</head><p>Given a training pair, (P, C P ), the sequence-to-sequence model computes the conditional probability p(C P |P; ?) using a parametric model (an RNN with parameters ?) to estimate the terms of the probability chain rule (also see <ref type="figure">Figure 1</ref>), i.e.</p><formula xml:id="formula_0">p(C P |P; ?) = m(P) i=1 p ? (C i |C 1 , . . . , C i?1 , P; ?).<label>(1)</label></formula><p>Here P = {P 1 , . . . , P n } is a sequence of n vectors and C P = {C 1 , . . . , C m(P) } is a sequence of m(P) indices, each between 1 and n.</p><p>The parameters of the model are learnt by maximizing the conditional probabilities for the training set, i.e. ? * = arg max</p><formula xml:id="formula_1">? P,C P log p(C P |P; ?),<label>(2)</label></formula><p>where the sum is over training examples.</p><p>As in <ref type="bibr" target="#b0">[1]</ref>, we use an Long Short Term Memory (LSTM) <ref type="bibr" target="#b10">[11]</ref> to model p ? (C i |C 1 , . . . , C i?1 , P; ?). The RNN is fed P i at each time step, i, until the end of the input sequence is reached, at which time a special symbol, ? is input to the model. The model then switches to the generation mode until the network encounters the special symbol ?, which represents termination of the output sequence.</p><p>Note that this model makes no statistical independence assumptions. We use two separate RNNs (one to encode the sequence of vectors P j , and another one to produce or decode the output symbols C i ). We call the former RNN the encoder and the latter the decoder or the generative RNN.</p><p>During inference, given a sequence P, the learnt parameters ? * are used to select the sequenc? C P with the highest probability, i.e.,? P = arg max C P p(C P |P; ? * ). Finding the optimal sequence? is computationally impractical because of the combinatorial number of possible output sequences. Instead we use a beam search procedure to find the best possible sequence given a beam size.</p><p>In this sequence-to-sequence model, the output dictionary size for all symbols C i is fixed and equal to n, since the outputs are chosen from the input. Thus, we need to train a separate model for each n. This prevents us from learning solutions to problems that have an output dictionary with a size that depends on the input sequence length.</p><p>Under the assumption that the number of outputs is O(n) this model has computational complexity of O(n). However, exact algorithms for the problems we are dealing with are more costly. For example, the convex hull problem has complexity O(n log n). The attention mechanism (see Section 2.2) adds more "computational capacity" to this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content Based Input Attention</head><p>The vanilla sequence-to-sequence model produces the entire output sequence C P using the fixed dimensional state of the recognition RNN at the end of the input sequence P. This constrains the amount of information and computation that can flow through to the generative model. The attention model of <ref type="bibr" target="#b4">[5]</ref> ameliorates this problem by augmenting the encoder and decoder RNNs with an additional neural network that uses an attention mechanism over the entire sequence of encoder RNN states.</p><p>For notation purposes, let us define the encoder and decoder hidden states as (e 1 , . . . , e n ) and (d 1 , . . . , d m(P) ), respectively. For the LSTM RNNs, we use the state after the output gate has been component-wise multiplied by the cell activations. We compute the attention vector at each output time i as follows:</p><formula xml:id="formula_2">u i j = v T tanh(W 1 e j + W 2 d i ) j ? (1, . . . , n) a i j = softmax(u i j ) j ? (1, . . . , n) (3) d i = n j=1 a i j e j</formula><p>where softmax normalizes the vector u i (of length n) to be the "attention" mask over the inputs, and v, W 1 , and W 2 are learnable parameters of the model. In all our experiments, we use the same hidden dimensionality at the encoder and decoder (typically 512), so v is a vector and W 1 and W 2 are square matrices. Lastly, d i and d i are concatenated and used as the hidden states from which we make predictions and which we feed to the next time step in the recurrent model.</p><p>Note that for each output we have to perform n operations, so the computational complexity at inference time becomes O(n 2 ).</p><p>This model performs significantly better than the sequence-to-sequence model on the convex hull problem, but it is not applicable to problems where the output dictionary size depends on the input.</p><p>Nevertheless, a very simple extension (or rather reduction) of the model allows us to do this easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ptr-Net</head><p>We now describe a very simple modification of the attention model that allows us to apply the method to solve combinatorial optimization problems where the output dictionary size depends on the number of elements in the input sequence.</p><p>The sequence-to-sequence model of Section 2.1 uses a softmax distribution over a fixed sized output dictionary to compute p(C i |C 1 , . . . , C i?1 , P) in Equation 1. Thus it cannot be used for our problems where the size of the output dictionary is equal to the length of the input sequence. To solve this problem we model p(C i |C 1 , . . . , C i?1 , P) using the attention mechanism of Equation 3 as follows:</p><formula xml:id="formula_3">u i j = v T tanh(W 1 e j + W 2 d i ) j ? (1, . . . , n) p(C i |C 1 , . . . , C i?1 , P) = softmax(u i )</formula><p>where softmax normalizes the vector u i (of length n) to be an output distribution over the dictionary of inputs, and v, W 1 , and W 2 are learnable parameters of the output model. Here, we do not blend the encoder state e j to propagate extra information to the decoder, but instead, use u i j as pointers to the input elements. In a similar way, to condition on C i?1 as in Equation 1, we simply copy the corresponding P Ci?1 as the input. Both our method and the attention model can be seen as an application of content-based attention mechanisms proposed in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>We also note that our approach specifically targets problems whose outputs are discrete and correspond to positions in the input. Such problems may be addressed artificially -for example we could learn to output the coordinates of the target point directly using an RNN. However, at inference, this solution does not respect the constraint that the outputs map back to the inputs exactly. Without the constraints, the predictions are bound to become blurry over longer sequences as shown in sequence-to-sequence models for videos <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation and Datasets Structure</head><p>In the following sections, we review each of the three problems we considered, as well as our data generation protocol. <ref type="bibr" target="#b0">1</ref> In the training data, the inputs are planar point sets P = {P 1 , . . . , P n } with n elements each, where P j = (x j , y j ) are the cartesian coordinates of the points over which we find the convex hull, the Delaunay triangulation or the solution to the corresponding Travelling Salesman Problem. In all cases, we sample from a uniform distribution in [0, 1] ? [0, 1]. The outputs C P = {C 1 , . . . , C m(P) } are sequences representing the solution associated to the point set P. In <ref type="figure" target="#fig_0">Figure 2</ref>, we find an illustration of an input/output pair (P, C P ) for the convex hull and the Delaunay problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convex Hull</head><p>We used this example as a baseline to develop our models and to understand the difficulty of solving combinatorial problems with data driven approaches. Finding the convex hull of a finite number of points is a well understood task in computational geometry, and there are several exact solutions available (see <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>). In general, finding the (generally unique) solution has complexity O(n log n), where n is the number of points considered.</p><p>The vectors P j are uniformly sampled from [0, 1] ? [0, 1]. The elements C i are indices between 1 and n corresponding to positions in the sequence P, or special tokens representing beginning or end of sequence. See <ref type="figure" target="#fig_0">Figure 2 (a)</ref> for an illustration. To represent the output as a sequence, we start from the point with the lowest index, and go counter-clockwise -this is an arbitrary choice but helps reducing ambiguities during training.</p><p>(a) Input P = {P 1 , . . . , P 10 }, and the output sequence C P = {?, 2, 4, 3, 5, 6, 7, 2, ?} representing its convex hull.</p><formula xml:id="formula_4">P 1 P 2 P 3 P 4 P 5 (b) Input P = {P 1 , .</formula><p>. . , P 5 }, and the output C P = {?, <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5)</ref>, <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5)</ref>, <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3)</ref>, ?} representing its Delaunay Triangulation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Delaunay Triangulation</head><p>A Delaunay triangulation for a set P of points in a plane is a triangulation such that each circumcircle of every triangle is empty, that is, there is no point from P in its interior. Exact O(n log n) solutions are available <ref type="bibr" target="#b15">[16]</ref>, where n is the number of points in P.</p><p>In this example, the outputs C P = {C 1 , . . . , C m(P) } are the corresponding sequences representing the triangulation of the point set P . Each C i is a triple of integers from 1 to n corresponding to the position of triangle vertices in P or the beginning/end of sequence tokens. See <ref type="figure" target="#fig_0">Figure 2</ref> </p><formula xml:id="formula_5">(b).</formula><p>We note that any permutation of the sequence C P represents the same triangulation for P, additionally each triangle representation C i of three integers can also be permuted. Without loss of generality, and similarly to what we did for convex hulls at training time, we order the triangles C i by their incenter coordinates (lexicographic order) and choose the increasing triangle representation 2 . Without ordering, the models learned were not as good, and finding a better ordering that the Ptr-Net could better exploit is part of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Travelling Salesman Problem (TSP)</head><p>TSP arises in many areas of theoretical computer science and is an important algorithm used for microchip design or DNA sequencing. In our work we focused on the planar symmetric TSP: given a list of cities, we wish to find the shortest possible route that visits each city exactly once and returns to the starting point. Additionally, we assume the distance between two cities is the same in each opposite direction. This is an NP-hard problem which allows us to test the capabilities and limitations of our model.</p><p>The input/output pairs (P, C P ) have a similar format as in the Convex Hull problem described in Section 3.1. P will be the cartesian coordinates representing the cities, which are chosen randomly in the [0, 1] ? [0, 1] square. C P = {C 1 , . . . , C n } will be a permutation of integers from 1 to n representing the optimal path (or tour). For consistency, in the training dataset, we always start in the first city without loss of generality.</p><p>To generate exact data, we implemented the Held-Karp algorithm <ref type="bibr" target="#b16">[17]</ref> which finds the optimal solution in O(2 n n 2 ) (we used it up to n = 20). For larger n, producing exact solutions is extremely costly, therefore we also considered algorithms that produce approximated solutions: A1 <ref type="bibr" target="#b17">[18]</ref> and A2 <ref type="bibr" target="#b18">[19]</ref>, which are both O(n 2 ), and A3 <ref type="bibr" target="#b19">[20]</ref> which implements the O(n 3 ) Christofides algorithm. The latter algorithm is guaranteed to find a solution within a factor of 1.5 from the optimal length. <ref type="table" target="#tab_1">Table 2</ref> shows how they performed in our test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture and Hyperparameters</head><p>No extensive architecture or hyperparameter search of the Ptr-Net was done in the work presented here, and we used virtually the same architecture throughout all the experiments and datasets. Even though there are likely some gains to be obtained by tuning the model, we felt that having the same model hyperparameters operate on all the problems would make the main message of the paper stronger.</p><p>As a result, all our models used a single layer LSTM with either 256 or 512 hidden units, trained with stochastic gradient descent with a learning rate of 1.0, batch size of 128, random uniform weight initialization from -0.08 to 0.08, and L2 gradient clipping of 2.0. We generated 1M training example pairs, and we did observe overfitting in some cases where the task was simpler (i.e., for small n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convex Hull</head><p>We used the convex hull as the guiding task which allowed us to understand the deficiencies of standard models such as the sequence-to-sequence approach, and also setting up our expectations on what a purely data driven model would be able to achieve with respect to an exact solution.</p><p>We reported two metrics: accuracy, and area covered of the true convex hull (note that any simple polygon will have full intersection with the true convex hull). To compute the accuracy, we considered two output sequences C 1 and C 2 to be the same if they represent the same polygon. For simplicity, we only computed the area coverage for the test examples in which the output represents a simple polygon (i.e., without self-intersections). If an algorithm fails to produce a simple polygon in more than 1% of the cases, we simply reported FAIL.</p><p>The results are presented in <ref type="table" target="#tab_0">Table 1</ref>. We note that the area coverage achieved with the Ptr-Net is close to 100%. Looking at examples of mistakes, we see that most problems come from points that are aligned (see <ref type="figure">Figure 3</ref> (d) for a mistake for n = 500) -this is a common source of errors in most algorithms to solve the convex hull.</p><p>It was seen that the order in which the inputs are presented to the encoder during inference affects its performance. When the points on the true convex hull are seen "late" in the input sequence, the accuracy is lower. This is possibly the network does not have enough processing steps to "update" the convex hull it computed until the latest points were seen. In order to overcome this problem, we used the attention mechanism described in Section 2.2, which allows the decoder to look at the whole input at any time. This modification boosted the model performance significantly. We inspected what attention was focusing on, and we observed that it was "pointing" at the correct answer on the input side. This inspired us to create the Ptr-Net model described in Section 2.3.</p><p>More than outperforming both the LSTM and the LSTM with attention, our model has the key advantage of being inherently variable length. The bottom half of <ref type="table" target="#tab_0">Table 1</ref> shows that, when training our model on a variety of lengths ranging from 5 to 50 (uniformly sampled, as we found other forms of curriculum learning to not be effective), a single model is able to perform quite well on all lengths it has been trained on (but some degradation for n = 50 can be observed w.r.t. the model trained only on length 50 instances). More impressive is the fact that the model does extrapolate to lengths that it has never seen during training. Even for n = 500, our results are satisfactory and indirectly indicate that the model has learned more than a simple lookup. Neither LSTM or LSTM with attention can be used for any given n = n without training a new model on n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Delaunay Triangulation</head><p>The Delaunay Triangulation test case is connected to our first problem of finding the convex hull. In fact, the Delaunay Triangulation for a given set of points triangulates the convex hull of these points.</p><p>We reported two metrics: accuracy and triangle coverage in percentage (the percentage of triangles the model predicted correctly). Note that, in this case, for an input point set P, the output sequence C(P) is, in fact, a set. As a consequence, any permutation of its elements will represent the same triangulation. Predictions (e) Ptr-Net , m=50, n=50</p><p>Predictions: tour length is 3.523 (f) Ptr-Net , m=5-20, n=20 <ref type="figure">Figure 3</ref>: Examples of our model on Convex hulls (left), Delaunay (center) and TSP (right), trained on m points, and tested on n points. A failure of the LSTM sequence-to-sequence model for Convex hulls is shown in (a). Note that the baselines cannot be applied to a different length from training. Using the Ptr-Net model for n = 5, we obtained an accuracy of 80.7% and triangle coverage of 93.0%. For n = 10, the accuracy was 22.6% and the triangle coverage 81.3%. For n = 50, we did not produce any precisely correct triangulation, but obtained 52.8% triangle coverage. See the middle column of <ref type="figure">Figure 3</ref> for an example for n = 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Travelling Salesman Problem</head><p>We considered the planar symmetric travelling salesman problem (TSP), which is NP-hard as the third problem. Similarly to finding convex hulls, it also has sequential outputs. Given that the Ptr-Net implements an O(n 2 ) algorithm, it was unclear if it would have enough capacity to learn a useful algorithm solely from data.</p><p>As discussed in Section 3.3, it is feasible to generate exact solutions for relatively small values of n to be used as training data. For larger n, due to the importance of TSP, good and efficient algorithms providing reasonable approximate solutions exist. We used three different algorithms in our experiments -A1, A2, and A3 (see Section 3.3 for references). <ref type="table" target="#tab_1">Table 2</ref> shows all of our results on TSP. The number reported is the length of the proposed tour. Unlike the convex hull and Delaunay triangulation cases, where the decoder was unconstrained, in  <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> N/A 5.82 5.27 5.23 5.91 50 <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> N/A 6.46 5.84 5.79 7.66 this example we set the beam search procedure to only consider valid tours. Otherwise, the Ptr-Net model would sometimes output an invalid tour -for instance, it would repeat two cities or decided to ignore a destination. This procedure was relevant for n &gt; 20, where at least 10% of instances would not produce any valid tour.</p><p>The first group of rows in the table show the Ptr-Net trained on optimal data, except for n = 50, since that is not feasible computationally (we trained a separate model for each n). Interestingly, when using the worst algorithm (A1) data to train the Ptr-Net, our model outperforms the algorithm that is trying to imitate.</p><p>The second group of rows in the table show how the Ptr-Net trained on optimal data with 5 to 20 cities can generalize beyond that. The results are virtually perfect for n = 25, and good for n = 30, but it seems to break for 40 and beyond (still, the results are far better than chance). This contrasts with the convex hull case, where we were able to generalize by a factor of 10. However, the underlying algorithms are of far greater complexity than O(n log n), which could explain this phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we described Ptr-Net, a new architecture that allows us to learn a conditional probability of one sequence C P given another sequence P, where C P is a sequence of discrete tokens corresponding to positions in P. We show that Ptr-Nets can be used to learn solutions to three different combinatorial optimization problems. Our method works on variable sized inputs (yielding variable sized output dictionaries), something the baseline models (sequence-to-sequence with or without attention) cannot do directly. Even more impressively, they outperform the baselines on fixed input size problems -to which both the models can be applied.</p><p>Previous methods such as RNNSearch, Memory Networks and Neural Turing Machines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref> have used attention mechanisms to process inputs. However these methods do not directly address problems that arise with variable output dictionaries. We have shown that an attention mechanism can be applied to the output to solve such problems. In so doing, we have opened up a new class of problems to which neural networks can be applied without artificial assumptions. In this paper, we have applied this extension to RNNSearch, but the methods are equally applicable to Memory Networks and Neural Turing Machines.</p><p>Future work will try and show its applicability to other problems such as sorting where the outputs are chosen from the inputs. We are also excited about the possibility of using this approach to other combinatorial optimization problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Input/output representation for (a) convex hull and (b) Delaunay triangulation. The tokens ? and ? represent beginning and end of sequence, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between LSTM, LSTM with attention, and our Ptr-Net model on the convex hull problem. Note that the baselines must be trained on the same n that they are tested on.</figDesc><table><row><cell></cell><cell>METHOD</cell><cell>TRAINED n</cell><cell>n</cell><cell>ACCURACY</cell><cell>AREA</cell></row><row><cell></cell><cell>LSTM [1]</cell><cell>50</cell><cell>50</cell><cell>1.9%</cell><cell>FAIL</cell></row><row><cell></cell><cell>+ATTENTION [5]</cell><cell>50</cell><cell>50</cell><cell>38.9%</cell><cell>99.7%</cell></row><row><cell></cell><cell>PTR-NET</cell><cell>50</cell><cell>50</cell><cell>72.6%</cell><cell>99.9%</cell></row><row><cell></cell><cell>LSTM [1]</cell><cell>5</cell><cell>5</cell><cell>87.7%</cell><cell>99.6%</cell></row><row><cell></cell><cell>PTR-NET</cell><cell>5-50</cell><cell>5</cell><cell>92.0%</cell><cell>99.6%</cell></row><row><cell></cell><cell>LSTM [1]</cell><cell>10</cell><cell>10</cell><cell>29.9%</cell><cell>FAIL</cell></row><row><cell></cell><cell>PTR-NET</cell><cell>5-50</cell><cell>10</cell><cell>87.0%</cell><cell>99.8%</cell></row><row><cell></cell><cell>PTR-NET</cell><cell>5-50</cell><cell>50</cell><cell>69.6%</cell><cell>99.9%</cell></row><row><cell></cell><cell>PTR-NET</cell><cell>5-50</cell><cell>100</cell><cell>50.3%</cell><cell>99.9%</cell></row><row><cell></cell><cell>PTR-NET</cell><cell>5-50</cell><cell>200</cell><cell>22.1%</cell><cell>99.9%</cell></row><row><cell></cell><cell>PTR-NET</cell><cell>5-50</cell><cell>500</cell><cell>1.3%</cell><cell>99.2%</cell></row><row><cell>Ground Truth</cell><cell>Predictions</cell><cell cols="2">Ground Truth</cell><cell cols="2">Ground Truth: tour length is 3.518</cell></row><row><cell cols="2">(a) LSTM, m=50, n=50</cell><cell cols="2">(b) Truth, n=50</cell><cell></cell><cell>(c) Truth, n=20</cell></row><row><cell>Ground Truth</cell><cell>Predictions</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(d) Ptr-Net, m=5-50, n=500</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Tour length of the Ptr-Net and a collection of algorithms on a small scale TSP problem.</figDesc><table><row><cell>n</cell><cell>OPTIMAL</cell><cell>A1</cell><cell>A2</cell><cell>A3</cell><cell>PTR-NET</cell></row><row><cell>5</cell><cell>2.12</cell><cell cols="3">2.18 2.12 2.12</cell><cell>2.12</cell></row><row><cell>10</cell><cell>2.87</cell><cell cols="3">3.07 2.87 2.87</cell><cell>2.88</cell></row><row><cell>50 (A1 TRAINED)</cell><cell>N/A</cell><cell cols="3">6.46 5.84 5.79</cell><cell>6.42</cell></row><row><cell>50 (A3 TRAINED)</cell><cell>N/A</cell><cell cols="3">6.46 5.84 5.79</cell><cell>6.09</cell></row><row><cell>5 (5-20 TRAINED)</cell><cell>2.12</cell><cell cols="3">2.18 2.12 2.12</cell><cell>2.12</cell></row><row><cell>10 (5-20 TRAINED)</cell><cell>2.87</cell><cell cols="3">3.07 2.87 2.87</cell><cell>2.87</cell></row><row><cell>20 (5-20 TRAINED)</cell><cell>3.83</cell><cell cols="3">4.24 3.86 3.85</cell><cell>3.88</cell></row><row><cell>25 (5-20 TRAINED)</cell><cell>N/A</cell><cell cols="3">4.71 4.27 4.24</cell><cell>4.30</cell></row><row><cell>30 (5-20 TRAINED)</cell><cell>N/A</cell><cell cols="3">5.11 4.63 4.60</cell><cell>4.72</cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We release all the datasets at http://goo.gl/NDcOIG.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We choose Ci = (1, 2, 4) instead of (2,4,1) or any other permutation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Rafal Jozefowicz, Ilya Sutskever, Quoc Le and Samy Bengio for useful discussions on this topic. We would also like to thank Daniel Gillick for his help with the final manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An application of recurrent nets to phone probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
	</analytic>
	<monogr>
		<title level="m">ICLEAR 2015</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04681</idno>
	</analytic>
	<monogr>
		<title level="m">ICML 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the identification of the convex hull of a finite set of points in the plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jarvis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="21" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient algorith for determining the convex hull of a finite planar set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="132" to="133" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convex hulls of finite sets of points in two and three dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se June</forename><surname>Preparata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient unstructured mesh generation by means of delaunay triangulation and bowyer-watson algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S1 Rebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational physics</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="138" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic programming treatment of the travelling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="63" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Suboptimal travelling salesman problem (tsp) solver</title>
		<ptr target="https://github.com/dmishin/tsp-solver" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Traveling salesman problem c++ implementation</title>
		<ptr target="https://github.com/samlbest/traveling-salesman" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">C++ implementation of traveling salesman problem using christofides and 2-opt</title>
		<ptr target="https://github.com/beckysag/traveling-salesman" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
