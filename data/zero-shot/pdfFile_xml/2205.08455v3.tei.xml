<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UTTERANCE WEIGHTED MULTI-DILATION TEMPORAL CONVOLUTIONAL NETWORKS FOR MONAURAL SPEECH DEREVERBERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ravenscroft</surname></persName>
							<email>jwravenscroft1@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Goetze</surname></persName>
							<email>s.goetze@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
							<email>t.hain@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UTTERANCE WEIGHTED MULTI-DILATION TEMPORAL CONVOLUTIONAL NETWORKS FOR MONAURAL SPEECH DEREVERBERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech dereverberation is an important stage in many speech technology applications. Recent work in this area has been dominated by deep neural network models. Temporal convolutional networks (TCNs) are deep learning models that have been proposed for sequence modelling in the task of dereverberating speech. In this work a weighted multi-dilation depthwise-separable convolution is proposed to replace standard depthwise-separable convolutions in TCN models. This proposed convolution enables the TCN to dynamically focus on more or less local information in its receptive field at each convolutional block in the network. It is shown that this weighted multi-dilation temporal convolutional network (WD-TCN) consistently outperforms the TCN across various model configurations and using the WD-TCN model is a more parameter-efficient method to improve the performance of the model than increasing the number of convolutional blocks. The best performance improvement over the baseline TCN is 0.55 dB scale-invariant signal-todistortion ratio (SISDR) and the best performing WD-TCN model attains 12.26 dB SISDR on the WHAMR dataset.</p><p>Index Termsspeech dereverberation, temporal convolutional network, speech enhancement, receptive field, deep neural network</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech dereverberation remains an important task for robust speech processing <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Far-field speech signals such as for automatic meeting transcription and digital assistants normally require preprocessing to remove the detrimental effects of interference in the signal <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. A number of methods have been proposed for speech dereverberation for both single channel and multichannel models <ref type="bibr" target="#b6">[7]</ref>. Recent advances in speech dereverberation performance in a number of domains have been driven by deep neural network (DNN) models <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>.</p><p>Convolutional neural network models are commonly used for sequence modelling in speech dereverberation tasks <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. One such fully convolutional model known as the TCN has been proposed for a number of speech enhancement tasks <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. TCNs are capable of monaural speech dereverberation as well as more complex tasks such as joint speech dereverberation and speech separation <ref type="bibr" target="#b16">[17]</ref>. The best performing TCN models for speech dereverberation tasks typically have a larger receptive field for data with higher reverberation times T60 and a smaller receptive field for data with small T60s <ref type="bibr" target="#b18">[19]</ref> which forms the motivation for this paper. In this work, a novel TCN architecture is proposed which is able to focus on specific temporal context within its receptive field. This is achieved by using an additional depthwise convolution kernel in the depthwise-separable convolution with a small dilation factor. Inspired by work in dynamic convolutional networks, an attention network is used to selected how to weight each of the depthwise kernels <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>The remainder of this paper proceeds as follows. Section 2 introduces the signal model and the WD-TCN dereverberation network. Section 3 describes the experimental setup and data and results are presented in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DEREVERBERATION NETWORK</head><p>In this section the monaural speech dereverberation signal model is introduced and the proposed WD-TCN dereverberation model is described. The general WD-TCN model architecture is similar to the reformulation of the Conv-TasNet speech separation model <ref type="bibr" target="#b21">[22]</ref> as a denoising autoencoder (DAE) in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Signal Model</head><p>A reverberant single-channel speech signal is defined as</p><formula xml:id="formula_0">x[i] = h[i] * s[i] = s dir [i] + srev[i]<label>(1)</label></formula><p>for discrete time index i where * denotes the convolution operator, h[i] denotes a room impulse response (RIR) and s[i] denotes the clean speech signal. In this paper the target speech is s dir [i] = ?s[i ? ? ], i.e. the clean signal convolved with the direct path of the RIR from speaker to receiver, expressed by the delay of signal travel from speaker to receiver ? and attenuation factor ?. The mixture signal x[i] is processed in Lx blocks</p><formula xml:id="formula_1">x = [x[0.5( ? 1)LBL], . . . , x[0.5(1 + )LBL ? 1]]<label>(2)</label></formula><p>of LBL samples with a 50% overlap for frame index ? {1, . . . , Lx}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Encoder</head><p>The encoder is a 1D convolutional layer with trainable weights B ? R L BL ?N , where LBL and N are the kernel size and number of output channels respectively. This layer transforms x into a set of filterbank features w such that</p><formula xml:id="formula_2">w = Henc (x B) ,<label>(3)</label></formula><p>where Henc : R 1?N ? R 1?N is a ReLU activation function. </p><formula xml:id="formula_3">v = m w .<label>(4)</label></formula><p>The operator denotes the Hadamard product. A more detailed description of the TCN used as a baseline in this paper is provided in <ref type="bibr" target="#b22">[23]</ref>. Streaming implementations of these models are feasible but for this paper we focus on utterance-level implementations for brevity <ref type="bibr" target="#b21">[22]</ref>. The conventional TCN consists of an initial stage which normalizes the encoded features w and reduces the number of features from N to B for each block using a pointwise convolution (P-Conv) bottleneck layer <ref type="bibr" target="#b21">[22]</ref>. The TCN is composed of a stack of X dilated convolutional blocks that is repeated R times. This structure allows for increasingly larger models with increasingly larger receptive fields <ref type="bibr" target="#b18">[19]</ref>. The depthwise convolution (D-Conv) layer in the blocks has an increasing dilation factor to the power of two for the X blocks in a stack, i.e. the dilation factors f for each block are taken from the set {2 0 , 2 1 , . . . , 2 X?1 } in increasing order. <ref type="figure" target="#fig_1">Fig. 1</ref> (a) depicts the convolutional block as implemented in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. The convolutional block consist primarily of P-Conv and D-Conv layers with parametric rectified linear unit (PReLU) activation functions <ref type="bibr" target="#b24">[25]</ref> and global layer normalization (gLN) layers <ref type="bibr" target="#b21">[22]</ref>. The P-Conv and D-Conv layers are structured to allow increasingly larger models to have larger receptive fields. Combining these two operations is an operation known as depthwise-separable convolution (DS-Conv) <ref type="bibr" target="#b21">[22]</ref>. More detailed definitions of P-Conv, D-Conv and DS-Conv layers are given in Section 2.3.1 before the proposed WD-TCN to replace the DS-Conv operations in TCNs is introduced in Section 2.3.2, denoted in this paper as weighted multi-dilation depthwise-separable convolution (WD-Conv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Depthwise-Separable Convolution (DS-Conv)</head><p>The DS-Conv operation is a factorised version of standard convolutional kernel using a D-Conv layer and a P-Conv layer. The main motivation for using DS-Conv is primarily parameter efficiency where the number of channels is sufficiently larger than the kernel size <ref type="bibr" target="#b21">[22]</ref>. Note that in this section the focus is entirely on 1D convolutional kernels but the same principle can be extended to higher dimensional kernels.</p><p>The D-Conv layer is an entirely sequential convolution with dilation factor f , i.e. each operation operates on each input channel individually. For the matrix of input features Y ? R G?Lx where G is the number of input channels (and consequently also the number of output channels) the D-Conv operation can be defined as</p><formula xml:id="formula_4">D(Y, KD) = (y0 * k0) , . . . , (yG?1 * kG?1)<label>(5)</label></formula><p>where KD ? R G?P is the the D-Conv kernel matrix of trainable weights and P is the kernel size. The gth row of Y and KD are denoted by yg and kg respectively. The P-Conv layer is an entirely channel-wise convolution. This operation in practice is a standard 1D convolutional kernel but with only a kernel size of 1. The P-Conv operation can be defined as</p><formula xml:id="formula_5">P(Y, KP ) = Y KP<label>(6)</label></formula><p>where KP ? R G?H is the P-Conv kernel of trainable weights.</p><p>Combining the definitions for the D-Conv and P-Conv operations, the DS-Conv operation is defined as</p><formula xml:id="formula_6">S (Y, KD, KP ) = P (D (Y, KD) , KP ) .<label>(7)</label></formula><p>The DS-Conv operation as implemented in the baseline system used in <ref type="bibr" target="#b18">[19]</ref> and in this paper can be seen in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref> highlighted by the dashed orange box. with dilation factor f = 2 X?1 . Note that a residual connection around the entire block is omitted for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Weighted Multi-Dilation Depthwise-Separable Convolution (WD-Conv)</head><p>The WD-Conv network structure depicted in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref> is proposed here as an extension to the DS-Conv operation where it is preferable to allow the network to be more selective about the temporal context to focus on without drastically increasing the number of parameters in the model. The proposed WD-Conv layer incorporates additional parallel D-Conv layers that can have a different dilation factor, hence it is referred to as dilation-augmented. The output of the D-Conv layers are weighted in a sum-to-one fashion and summed together. This summed output is then passed as the input to a P-Conv. In its simplest form the WD-Conv operation can be formulated as</p><formula xml:id="formula_7">W Y, KD 1 ,, . . . , KD Q , KP = P Q q=1 aqDq Y, KD q , KP<label>(8)</label></formula><p>where Q is the number of parallel D-Convs in the WD-Conv and aq are their corresponding weights that sum-to-one, i.e. Q q=1 aq = 1. In the model proposed here the number of D-Convs is set to Q = 2 ; one with a dilation factor f = 1 and the other according to the exponentially increasing dilation rule defined previously and used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> where f is increasing in powers of 2 with every successive block in a stack of X blocks. Note that the first convolutional blocks of a stack of X blocks in the proposed implementation use an identical dilation of f = 1 for each of the D-Conv kernels in the WD-Conv operation. Inspired by the dynamic convolution kernel proposed in <ref type="bibr" target="#b20">[21]</ref>, the implementation proposed in this paper computes the weights for each D-Conv layer using an squeeze-and-excite (SE) attention network <ref type="bibr" target="#b25">[26]</ref>. The SE attention network is shown in <ref type="figure" target="#fig_2">Fig. 2</ref> and is composed of a global average pooling layer that reduces the sequence dimension from Lx to 1 producing a vector of dimension H, the same as the feature dimension of the input. This feature vector is then compressed using a linear layer, with a rectified linear unit (ReLU) activation, to a dimension of 4 as in <ref type="bibr" target="#b20">[21]</ref>. The final stage is a linear layer that computes a weight for each of the D-Conv kernels in the WD-Conv structure. In the proposed model there are two D-Conv kernels and so the linear layer has an input dimension of 4 and an output dimension of 2. A softmax activation is used to ensure the sum-to-one constraint on the weights of the D-Conv layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Decoder</head><p>The decoder tansforms the encoded dereverberated signal v back into a time domain signal using a transposed 1D convolutional layer with N input channels, 1 output channel and a kernel size of LBL such that? = v U</p><p>where U ? R N ?L BL is a matrix of trainable convolutional weights and? is an estimated dereverberated signal block in the time domain. The overlap-add method is used for re-synthesis of the signal from the overlapping blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Objective Function</head><p>The objective function used here is the SISDR function <ref type="bibr" target="#b26">[27]</ref> which is the same as that used to train the baseline TCN <ref type="bibr" target="#b18">[19]</ref>. It is reformulated as a loss function by taking the negative SISDR value between the estimated speech segment? and the reference direct path of the signal s dir defined as </p><p>3. EXPERIMENTAL SETUP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model and Training Configuration</head><p>Different model configurations are compared in the following to demonstrate the improvement gained by the proposed WD-TCN model across a range of model sizes. This is done by varying the number of convolutional blocks in a dilated stack X as well as the number of times the dilated stack is repeated R. Based on previous work <ref type="bibr" target="#b18">[19]</ref>, the ranges of X ? {4, 5, 6, 7, 8} and R ? {4, 5, 6, 7, 8} were selected, resulting in 25 different configurations. All other parameters are fixed, i.e. kernel size LBL = 16, number of encoder output channels N = 512, number of bottleneck output channels B = 128, number of channels inside the convolutional blocks H = 512 and the kernel size inside each D-Conv P = 3. For more details on these parameters see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. The same training approach as in <ref type="bibr" target="#b18">[19]</ref> is used for both the baseline TCN and WD-TCN. Each model is trained for 100 epochs. An initial learning rate of 0.001 is used and is halved if there is no improvement for 3 epochs. A batch size of 4 is used. The training was performed using the SpeechBrain speech processing toolkit <ref type="bibr" target="#b23">[24]</ref>. The implementation of the proposed WD-TCN is available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data</head><p>The simulated WHAMR noisy reverberant two speaker speech separation corpus <ref type="bibr" target="#b15">[16]</ref> is used for the following experiments in this section. Only the reverberant and clean first speaker data is used for the input data x[i] and target data s dir [i]. RIRs are simulated using the pyroomacoustics software toolkit <ref type="bibr" target="#b27">[28]</ref> and then convolved with the speech clips to produce the reverberant signal x[i]. The training set contains 20,000 samples for training which are truncated or padded to 4 s in length, to address sample length mismatches in batches and to also speed up training. There are 5000 samples (14.65 hrs) and 3000 samples (9 hrs) in the validation and test sets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Metrics</head><p>A number of metrics are used to assess a variety of properties in the dereverberated speech. The objective function SISDR is also used to measure distortions in signals. Speech-to-reverberation modulation energy ratio (SRMR) <ref type="bibr" target="#b28">[29]</ref> is a measure used to directly measure reverberant effects in the signal. Perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b29">[30]</ref> and extended short-time objective intelligibility (ESTOI) <ref type="bibr" target="#b30">[31]</ref> are objective measures used to assess the quality and intelligibility of signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance Metrics and Model Size</head><p>The average SISDR results on the WHAMR evaluation set for the 25 chosen model configurations of the proposed WD-TCN model are given in <ref type="table" target="#tab_1">Table 1</ref>    <ref type="figure" target="#fig_4">Fig. 3 (top)</ref> shows SISDR performance for all models over the model sizes in number of parameters. It can be seen that using the WD-TCN is a more parameter efficient approach to improving model performance than increasing the number of convolutional blocks (larger X or R values) in a conventional TCN. The SRMR performance against model size ( <ref type="figure" target="#fig_4">Fig. 3, lower panel)</ref> shows the same findings, i.e. that the WD-TCN is a more parameter efficient approach to improving performance. For some larger models (&gt; 6M parameters) performance differs less. However the best performing model in terms of SRMR is still the WD-TCN. <ref type="table" target="#tab_3">Table 2</ref> shows the results of the best performing TCN and WD-TCN models for each of the chosen performance metrics, highlighted in yellow, compared with the respective other model for the same X and R hyper-parameters. The performance in PESQ is inconclusive as many TCN models outperform their corresponding WD-TCN configurations but the best PESQ score of 3.5 is achieved with the WD-TCN model. The WD-TCN models show slightly better performance in ESTOI in line with the trend already observed in SRMR and SISDR. Note that SRMR is considered the most significant metric as it is designed to assess reverberation only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Squeeze-and-Excite Attention Analysis and T60 Variation</head><p>In the following, the attentive weights aq in <ref type="bibr" target="#b7">(8)</ref> in the convolutional blocks are analysed. Note that a1 corresponds to the attention weight applied to the D-Conv layers with the increasing dilation of f ? {1, 2, . . . , 2 X?1 } for all convolutional blocks (cf. <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>) and a2 = is the weight corresponding to the D-Conv layers with the more local fixed dilation f = 1. To analyse whether the SE attention approach was working as intended the attention weights were firstly computed across the entire evaluation set for every WD-TCN model trained in <ref type="table" target="#tab_1">Table 1</ref>. Mean values of the weights for each model and each sample in the evaluation set were then computed and the  evaluation set was in divided into increasing T60 ranges from 0.1s up to 1s. The mean for each weight aq over all models and samples, denoted as?q, q ? {1, 2}, was then computed for each T60 range. <ref type="figure">Figure 4</ref> shows how the mean weight values vary across increasing T60 ranges. As the T60 range increases?1 increases. This demonstrates the SE attention approach is working as intended because the network has a less local focus within its receptive field for speech signals with larger reverberation times. Similarly the mean of the local attention weight?2 decreases as the T60 range increases demonstrating that the network is more focused on local information in its receptive field when the speech has a smaller reverberation time. <ref type="figure">Fig. 4</ref>: Mean values of attention weights?q across six different T60 ranges in the WHAMR evaluation set over all models with X ? {4, . . . , 8} and R ? {4, . . . , 8}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this work, the WD-TCN model was proposed for TCN-based speech dereverberation by replacing depthwise-separable convolutions with weight multi-dilation depthwise-separable convolutions. It was shown that the WD-TCN consistently outperformed a conventional TCN across 25 different model configurations and that using the WD-TCN was a more parameter efficient approach to improving model performance than increasing the number of convolutional blocks in the TCN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. This work was also funded in part by 3M Health Information Systems, Inc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Convolutional block in baseline TCN; (b) Proposed convolutional block. Example for final block in a stack of conv. blocks for Q = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Squeeze and excite attention weighting network. Output dimensionality of each layer is indicated above arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LSISDR(?, s dir ) := ?10 log 10 ?,s dir s dir s dir 2 2? ? ?,s dir s dir s dir 2 2</head><label>1022</label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of baseline TCN and WD-TCN over model size (no. of parameters) in terms of SISDR (top) and SRMR (bottom). receptive field, {X, R} = {8, 8}, shows best overall performance, contrary to the TCN model which gave the best SISDR results with {X, R} = {6, 8}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>with SISDR improvements over the TCN model in the parenthesis. The bold font indicates best performance and highest improvement, respectively. These results show that the WD-TCN outperforms the TCN model across all 25 model configurations. The biggest performance gains are seen around {X, R} = {5, 7} and the WD-TCN model with most parameters and largest 21 (.28) 11.66 (.29) 11.81 (.40) 11.94 (.38) 12.04 (.40) 5 11.51 (.41) 11.86 (.41) 11.94 (.23) 12.11 (.42) 12.11 (.39) R 6 11.64 (.38) 11.95 (.30) 12.08 (.31) 12.09 (.23) 12.11 (.20) 7 11.65 (.20) 12.17 (.44) 12.22 (.30) 12.16 (.13) 12.14 (.16) 8 11.79 (.27) 12.03 (.19) 12.20 (.17) 12.21 (.22) 12.26 (.32) SISDR performance of WD-TCN with SE attention in dB.</figDesc><table><row><cell cols="5">1 Link to WD-TCN model on GitHub: https://github.com/</cell></row><row><cell>jwr1995/WD-TCN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>X</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>4 11.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Numbers in (?) report performance improvement over baseline TCN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Model X R # params SISDR PESQ ESTOI SRMR</figDesc><table><row><cell>TCN</cell><cell>6 7</cell><cell>5.8M</cell><cell cols="3">11.92 3.46 0.930</cell><cell>8.65</cell></row><row><cell cols="2">WD-TCN 6 7</cell><cell>6.0M</cell><cell>12.22</cell><cell>3.5</cell><cell>0.933</cell><cell>8.69</cell></row><row><cell>TCN</cell><cell>6 8</cell><cell>6.6M</cell><cell cols="3">12.03 3.46 0.932</cell><cell>8.70</cell></row><row><cell cols="2">WD-TCN 6 8</cell><cell>6.8M</cell><cell cols="3">12.20 3.43 0.934</cell><cell>8.72</cell></row><row><cell>TCN</cell><cell>8 4</cell><cell>4.5M</cell><cell cols="3">11.63 3.48 0.927</cell><cell>8.60</cell></row><row><cell cols="2">WD-TCN 8 4</cell><cell>4.6M</cell><cell cols="3">12.04 3.45 0.931</cell><cell>8.67</cell></row><row><cell>TCN</cell><cell>8 7</cell><cell>7.7M</cell><cell cols="3">11.98 3.46 0.933</cell><cell>8.79</cell></row><row><cell cols="2">WD-TCN 8 7</cell><cell>7.9M</cell><cell cols="3">12.14 3.45 0.935</cell><cell>8.72</cell></row><row><cell>TCN</cell><cell>8 8</cell><cell>8.8M</cell><cell cols="3">11.94 3.46 0.933</cell><cell>8.71</cell></row><row><cell cols="2">WD-TCN 8 8</cell><cell>9.1M</cell><cell cols="3">12.26 3.45 0.935</cell><cell>8.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Best performing TCN and WD-TCN models compared corresponding models in SISDR, PESQ, ESTOI and SRMR. Bold indicates best performance per configuration, in terms of the X and R hyper-parameters. Results highlighted in yellow indicate best overall results for each model in each metric.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Far-Field Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="148" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Front-end technologies for robust ASR in reverberant environments -spectral enhancement-based dereverberation and auditory modulation filterbank features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anem?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep Learning Based Dereverberation of Temporal Envelopes for Robust Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purushothaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sreeram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
	<note>in Interspeech 2020</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech processing for digital home assistants: Combining signal processing with deep-learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hoffmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Souden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The AMIDA 2009 meeting transcription system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Hannani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huijbregts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Close</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO 2022</title>
		<imprint>
			<date type="published" when="2022-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<title level="m">Speech Dereverberation Using Statistical Reverberation Models</title>
		<meeting><address><addrLine>London, London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural Network-Based Spectrum Estimation for Online WPE Dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech dereverberation using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Chazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EU-SIPCO 2018</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Learning Based Target Cancellation for Speech Dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="941" to="950" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
	<note>in Interspeech 2021</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">UNet++-Based Multi-Channel Speech Dereverberation and Distant Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutive Prediction for Monaural Speech Dereverberation and Noisy-Reverberant Speaker Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3476" to="3490" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">DESNet: A Multi-Channel Network for Simultaneous Speech Dereverberation, Enhancement and Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<idno>SLT 2021</idno>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
	<note>in Interspeech 2020</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WHAMR!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monaural Speech Dereverberation Using Temporal Convolutional Networks With Self Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1598" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TCNN: Temporal Convolutional Neural Network for Real-time Speech Enhancement in the Time Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Receptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO 2022</title>
		<imprint>
			<date type="published" when="2022-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2020</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Att-TasNet: Attending to Encodings in Time-Domain Audio Speech Separation of Noisy, Reverberant Speech Mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Signal Processing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SpeechBrain: A general-purpose speech toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rastorgueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SDR -Half-baked or Well Done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pyroomacoustics: A python package for audio room simulation and array processing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scheibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bezzam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dokmani?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An improved non-intrusive intelligibility metric for noisy and reverberant speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Senoussaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2009" to="2022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
