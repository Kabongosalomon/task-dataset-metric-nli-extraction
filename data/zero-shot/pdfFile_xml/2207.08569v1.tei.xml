<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-manifold Attention for Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Konstantinidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Visual Computing Lab</orgName>
								<orgName type="department" key="dep2">Centre for Research and Technology Hellas</orgName>
								<orgName type="institution">Information Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Papastratis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Visual Computing Lab</orgName>
								<orgName type="department" key="dep2">Centre for Research and Technology Hellas</orgName>
								<orgName type="institution">Information Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosmas</forename><surname>Dimitropoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Visual Computing Lab</orgName>
								<orgName type="department" key="dep2">Centre for Research and Technology Hellas</orgName>
								<orgName type="institution">Information Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Petros</forename><surname>Daras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Visual Computing Lab</orgName>
								<orgName type="department" key="dep2">Centre for Research and Technology Hellas</orgName>
								<orgName type="institution">Information Technologies Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-manifold Attention for Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformer are very popular nowadays due to their state-of-the-art performance in several computer vision tasks, such as image classification and action recognition. Although the performance of Vision Transformers have been greatly improved by employing Convolutional Neural Networks, hierarchical structures and compact forms, there is limited research on ways to utilize additional data representations to refine the attention map derived from the multi-head attention of a Transformer network. This work proposes a novel attention mechanism, called multi-manifold attention, that can substitute any standard attention mechanism in a Transformer-based network. The proposed attention models the input space in three distinct manifolds, namely Euclidean, Symmetric Positive Definite and Grassmann, with different statistical and geometrical properties, guiding the network to take into consideration a rich set of information that describe the appearance, color and texture of an image, for the computation of a highly descriptive attention map. In this way, a Vision Transformer with the proposed attention is guided to become more attentive towards discriminative features, leading to improved classification results, as shown by the experimental results on several well-known image classification datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>T RANSFORMERS have been met with great interest from the research community when they were originally employed for natural language processing in the pioneering work of <ref type="bibr" target="#b0">[1]</ref>. A Transformer is a network architecture that relies on self-attention, which is an attention mechanism that relates different positions of a single sequence in order to compute a new representation of the sequence. Transformers are designed to handle sequential input data, however, unlike Recurrent Neural Networks (RNNs), Transformers process the data in parallel since they receive the entire sequence as input. In this way, Transformers are capable of extracting both shortand long-term dependencies between input and output, while simultaneously achieving increased parallelization and reduced training times with respect to RNNs.</p><p>Self-attention is a major component in Transformers, distinguishing it from the widely utilized convolutional neural networks (CNNs). More specifically, self-attention models shortand long-range interactions and relationships between different image areas. Recently, Vision Transformers (ViTs) <ref type="bibr" target="#b1">[2]</ref> have been introduced for computer vision tasks, leading to state-ofthe-art performance in well-known benchmark datasets and a significant increase in their popularity. However, ViTs lack the local inductive biases of CNNs and do not model efficiently local information <ref type="bibr" target="#b2">[3]</ref>. Recent works aimed to increase local structure modelling by introducing convolutions to the ViTs <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, have redesigned the patch tokenization process and introduced local attention mechanisms <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> or have adopted hierarchical structures similar to CNNs <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>As a result, most literature works concentrate on proposing either more elaborate network architectures or more sophisticated attention mechanisms. However, these works operate only in the Euclidean space of pixel intensity values, overlooking the fact that data representations in other manifolds can be beneficial to the accuracy of a Vision Transformer. Additionally, it has been shown that in the deeper layers of ViTs, the attention maps tend to be similar and their representation stops improving <ref type="bibr" target="#b9">[10]</ref>, thus a need for more discriminative attention maps is imperative. To this end, this work proposes a multi-manifold multihead attention (MMA) that can be introduced to any Vision Transformer. The main advantage of the proposed attention over the standard one is the use of three different manifolds, namely Euclidean, Symmetrical Positive Definite (SPD) and Grassmann manifolds to describe the input sequence. Computing feature representations and distances in manifolds with different statistical and geometrical properties enables a better modelling of the input sequence through the computation of a self-attention matrix that can better enhance the important features of the input sequence for a given computer vision task. In this way, a Transformer-based network with the proposed multi-manifold multi-head attention can produce output feature representations with high discriminative power. More specifically, the contributions of this work are:</p><p>? A novel multi-manifold multi-head attention that better models the underlying structure of the input space through its transformation into feature representations in three different manifolds is proposed. ? The proposed attention can be added to any Transformerbased network for improved performance in computer vision tasks. ? Extensive experimentation is conducted on different image classification datasets to showcase the importance of the proposed attention mechanism. The rest of the paper is organized as follows. Section I presents related work on Vision Transformers and manifolds. Section II describes the proposed methodology in detail, while Section III presents experimental results to support the contributions of this work. Finally, Section IV summarizes the work by drawing conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vision Transformers</head><p>Dosovitskiy et al., in <ref type="bibr" target="#b1">[2]</ref>, were the first to propose Vision Transformer (ViT) as a pure Transformer backbone for image classification. In their work, the input image is segmented into a series of non-overlapping image patches that are then projected into a linear embedding sequence. This sequence is concatenated with a learnable positional encoding that holds arXiv:2207.08569v1 [cs.CV] 18 Jul 2022 information on the spatial location of patches in the image. Thus, an input sequence is formed that is finally fed to the Transformer for classification.</p><p>Following ViT, several variants of Vision Transformers have emerged in the last few years in an attempt to improve their performance on image classification benchmarks. A recent survey and categorisation of Vision Transformers can be found in <ref type="bibr" target="#b10">[11]</ref>. To enhance performance and accelerate convergence, several literature works proposed the combination of CNNs with Transformers in order to leverage the convolutional biases of image data effectively. <ref type="bibr">Touvron et al.,</ref><ref type="bibr" target="#b11">in [12]</ref>, proposed the Data-efficient image Transformer (DeiT) to improve ViT by applying data augmentation and optimisation strategies. Furthermore, based on empirical observations that CNN is a better teacher than Transformer, the authors employed during pre-training a teacher-student strategy, in which the CNN teacher transferred its inductive bias in a soft way to the Transformer student through knowledge distillation.</p><p>In a similar fashion, the authors in <ref type="bibr" target="#b2">[3]</ref> proposed ConViT that attaches a parallel convolution branch to the Transformer branch so as to impose convolutional inductive biases via a Gated Positional Self-Attention (GPSA). The proposed GPSA approximates the locality of convolutional layers with local attention or operates as vanilla self-attention layers depending on the image context. In <ref type="bibr" target="#b3">[4]</ref>, the authors proposed the Convolution-enhanced image Transformer (CeiT) that used a new convolutional module to extract patch embeddings from low-level features and a layer-wise attention for the class embedding to model long-range dependencies.</p><p>Recognizing that a fixed resolution across the entire network neglects fine-grained information and brings heavy computational costs, several literature works proposed hierarchical structures for Vision Transformers. In <ref type="bibr" target="#b5">[6]</ref>, the authors proposed the Tokens-To-Token Vision Transformer (T2T-ViT), which gradually structures the patch embedding sequence by combining neighbouring embeddings into single embeddings, i.e., tokens. This helped the network to learn the local structure of patches and produce hierarchical features. <ref type="bibr">Wang et al.,</ref><ref type="bibr" target="#b7">in [8]</ref>, proposed the Pyramid Vision Transformer (PVT), a Transformer network with the pyramid structure of CNNs. PVT consists of several stages, where the spatial dimensions of the output are reduced in each stage. In addition, a spatialreduction attention (SRA) layer is responsible for learning keys and values of lower dimensionality. In <ref type="bibr" target="#b8">[9]</ref>, Heo et al.proposed the Pooling-based Vision Transformer (PiT), which reduces the spatial dimensions progressively with pooling layers similarly to CNNs.</p><p>To improve local attention and enhance the feature extraction capabilities of Transformer-based networks, <ref type="bibr">Han et al.,</ref><ref type="bibr" target="#b12">in [13]</ref>, proposed the Transformer-iN-Transformer (TNT) network to combine patch-level and pixel-level representations by splitting the patches into sub-patches. Then, the authors adopted and combined Transformer networks at both levels in order to produce better representation with richer local and global information. On the other hand, Liu et al., in <ref type="bibr" target="#b6">[7]</ref>, proposed the Swin Transformer that calculates its representations by shifting windows along the input image to model global and boundary features. More specifically, the self-attention is calculated solely on each window and allows crosswindow interactions. Finally, the Swin Transformer merges patches in each layer to create hierarchical features with linear complexity.</p><p>On the other hand, understanding that Transformers are data hungry models and that they need sufficiently large datasets to perform well, the authors in <ref type="bibr" target="#b4">[5]</ref> proposed three compact Transformer-based models. ViT-Lite, is nearly identical to the original ViT, but with more suitable smaller patch sizing for small datasets. Compact Vision Transformers (CVT) expand on ViT-Lite by using a novel sequential pooling method that pools the sequential based information that results from the Transformer encoder, eliminating the need for the extra Classification Token. Finally, Compact Convolutional Transformers (CCT) further expand on CVT by adding convolutional blocks to the tokenization step, thus preserving local information and being able to encode relationships between patches, unlike the original ViT.</p><p>Traditionally, Vision Transformers process the raw pixel intensities directly in the Euclidean space without considering how different data representations may affect their accuracy. The proposed work can be considered as a method that improves local attention through the use of feature representations in different manifolds to create more descriptive attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Manifold Background</head><p>A manifold is a topological space that locally resembles a Euclidean space near each point <ref type="bibr" target="#b13">[14]</ref>. Essentially, a manifold is a mapping from one space to another, allowing similar features to appear closer to each other, while dissimilar features move further apart. Manifolds have been widely employed in computer vision tasks due to the fact that feature representations in different manifolds carry special statistical and geometrical properties that may provide different discriminative power for a given task <ref type="bibr" target="#b14">[15]</ref>. Two widely employed special types of manifolds used to describe image sets and videos in the literature, are the symmetrical positive definite (SPD) and Grassmann manifolds.</p><p>In <ref type="bibr" target="#b14">[15]</ref>, the authors utilized properties of the Riemmanian geometry on manifolds and proposed a new similarity method based on SPD features for clustering tasks. Yu et al., in <ref type="bibr" target="#b15">[16]</ref> proposed the contour covariance as a region descriptor for accurate image classification. Such a descriptor is a point on the manifold of SPD. In a similar fashion, the authors in <ref type="bibr" target="#b16">[17]</ref> proposed the transformation of the input space to a SPD manifold (i.e., covariance matrix) and the use of a novel continuous manifold neural network, called ODE-RGRU, for action recognition and sleep staging classification. Whereas, the authors in <ref type="bibr" target="#b17">[18]</ref> proposed a collaborative representationbased image set classification algorithm to model the original image set with covariance matrices and learn powerful representations for improved classification performance. In <ref type="bibr" target="#b18">[19]</ref>, the authors proposed SymNet, a SPD manifold deep learning network for image set classification, in which an image set is represented as a non-singular covariance matrix on the SPD manifold. The main drawback of employing covariance features in a deep learning framework is the non-linearity of the SPD manifold that introduces challenging optimization problems.</p><p>On the other hand, Grassmannian geometry has been widely employed in several computer vision tasks, such as sign language recognition, image classification and action recognition. In <ref type="bibr" target="#b19">[20]</ref>, the authors introduced Linear Dynamic System (LDS) features as data representations in the Grassmann manifold for accurate detection of fire and smoke events in video sequences. In <ref type="bibr" target="#b20">[21]</ref>, the authors proposed the use of LDS histograms as points in the Grassmann manifold for accurate sign language recognition. In another work <ref type="bibr" target="#b21">[22]</ref>, the same authors proposed the Grassmannian Pyramid Descriptor (GPD) to extract temporal representations from skeletal sequences for action recognition. In <ref type="bibr" target="#b22">[23]</ref>, the authors performed automated grading of invasive breast carcinoma from medical images by considering each image as a set of multidimensional spatiallyevolving signals that can be efficiently modelled using Vector of Locally Aggregated Descriptors (VLAD) representations on the Grassmann manifold. In a different approach, the authors in <ref type="bibr" target="#b23">[24]</ref> introduced LDS features in a ResNet architecture, achieving state-of-the-art performance in image classification. Finally, the authors in <ref type="bibr" target="#b24">[25]</ref> proposed an unsupervised dimensionality reduction algorithm based on Neighborhood Preserving Embedding (GNPE) to project image sets modelled in high-dimensional Grassmann manifold to a relative lowdimensional one of higher discriminative capabilities, thus dealing with the high computational cost involved with Grassmann manifolds.</p><p>To further leverage the discriminative power of feature representations in different manifolds, other research works have attempted to fuse multiple manifold representations. Recently, Wang et al. employed and fused representations in the SPD and Grassmann manifolds for clustering purposes <ref type="bibr" target="#b25">[26]</ref>. However, Transformer-based networks typically focus only on the Euclidean space of pixel intensities, overlooking alternative data representations. Thus, leveraging the statistical properties of different manifolds, this work proposes a novel multi-manifold multi-head attention for Vision Transformers that combine feature representations from three manifolds (i.e., Euclidean, SPD and Grassmann) to learn a highly descriptive attention map that can better identify the important context of input images. The fusion of representations in different manifolds can guide a Transformer-based network to better model the underlying structure of the input space, leading to improved classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>This section initially introduces the main components of a Vision Transformer and then presents in detail the proposed multi-manifold multi-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vision Transformers</head><p>Initially, a Vision Transformer extracts patch embeddings from an input image and adds position embeddings to them to form the input sequence. This sequence is fed to the Transformer encoder network that comprises alternating multihead attention and multi-layer perceptron (MLP) layers. The output of the Transformer encoder is a feature representation that passes through a linear layer for classification. The general network architecture of a Vision Transformer is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>1) Patch Embeddings: Given an input image x ? R H?W ?C , where H, W , C are the height, width and channels of the image, respectively, a Vision Transformer divides the image into non-overlapping patches, which are then flattened and converted to a sequence of vectors x P ? R L?P 2 ?C , where L = HW P 2 is the sequence length and P is the size of the patch. Afterwards, each vector of the sequence is linearly projected into the space of the hidden dimension of the Transformer encoder through a trainable linear projection (i.e., fully connected) layer W p ? R P 2 ?C?D , where D is the hidden dimension of the layer. Other works combine image patch extraction and linear projection into a single step through a 2D convolution operation <ref type="bibr" target="#b26">[27]</ref>. From an implementation perspective, a 2D convolution is beneficial as GPUs are optimized for such operations, while there is also no need to first split an image into patches (i.e., patches are effortlessly formed due to the 2D convolution operation).</p><p>2) Position Embeddings: Position embeddings are vectors that are added to the patch embeddings and provide positional information to them. In this way, position embeddings provide some sense of order in the input sequence, allowing the Transformer encoder to model both the content of the patches and their spatial location with respect to the other image patches. The most common position embeddings are either vectors with sine and cosine frequencies <ref type="bibr" target="#b1">[2]</ref> or learned embeddings <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p><p>3) Multi-head Attention: The multi-head attention is the most important layer of the Transformer encoder. It is basically a transformation layer that maps an input sequence X ? R L?dx , where L is the sequence length and d x the dimension of the input sequence, to three different vectors, namely the query Q, the key K and the value V. These vectors are generated as:</p><formula xml:id="formula_0">Q = XW q , K = XW k , V = XW v<label>(1)</label></formula><p>where W q ? R dx?dq , W k ? R dx?d k , and W v ? R dx?dv are three different weight matrices with d q , d k and d v being the dimensions of the query, key and value vectors, respectively. Since the dimensions of these vectors are equal to each other, for the rest of the manuscript they will be denoted simply as d. With the query, value and key matrices defined, the scaled dot-product attention is equal to:</p><formula xml:id="formula_1">Attention(Q, K, V) = softmax ( QK T ? d )V<label>(2)</label></formula><p>The obtained attention weights are assigned to the elements of the value vector V and show in which elements the layer attends to so as to produce richer feature representations for a given task. Instead of using a single attention to project the input into a feature subspace with limited modelling capabilities, Vaswani et al., in <ref type="bibr" target="#b0">[1]</ref>, proposed a multi-head selfattention (MHSA) that performs different linear projections of the input at different subspaces. This is achieved by parallel attention layers, called heads, concatenated together. MHSA is computed as:</p><formula xml:id="formula_2">Q i = XW i q , K i = XW i k , V i = XW i v (3) S i = Attention(Q i , K i , V i ), i = 1, 2, . . . , h (4) M HSA(Q, K, V) = concat(S 1 , S 2 , . . . , S h )W o<label>(5)</label></formula><p>where h is the total number of heads, W o ? R hd?d model is the weight projection matrix with d model being the size of the projection space (d = d model /h), S i ? R L?L is the attention matrix of each head and W i q , W i k , W i v ? R d model ?d are the weight matrices for the query, key and value vectors of each head i, respectively. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. This means that the model will be able to gather more positional data because each head will focus on various regions of the input and have a more comprehensive representation after the combination of the vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-manifold Multi-head Attention</head><p>Inspired by the need for more descriptive attention mechanisms and leveraging the fact that different manifolds possess different statistical and geometrical properties, this section introduces the multi-manifold multi-head attention that can be used to replace the standard multi-head attention in any Vision Transformer, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The proposed attention employs three different manifolds, namely Euclidean, Symmetrical Positive Definite (SPD) and Grassmann to produce richer feature representations. By transforming the input image patches to points in the different manifolds and computing distances between them in these manifolds, this work aims to compute an attention matrix with high discriminative power to better model the underlying structure of the input space. Next, each manifold is described in detail, along with how the attention matrices computed in each manifold are fused to achieve more powerful feature representations and thus more accurate classification results in computer vision tasks.</p><p>1) Euclidean Manifold: Currently, a typical approach for Vision Transformers in the literature is to consider that the query and key vectors passed as input to the multi-head attention are points in a high-dimensional feature space, whose geometrical properties are equal to the ones in the Euclidean space, and thus their distance can be computed accordingly. Given query Q ? R L?d and key K ? R L?d vectors, their distance is given by their scaled dot-product:</p><formula xml:id="formula_3">D E (Q, K) = QK T ? d k<label>(6)</label></formula><p>The distance D E ? R h?L?L , with h representing the number of heads in the attention layer, expresses the similarity between query and key vectors, with higher values denoting vectors that are far away from each other in the Euclidean manifold.</p><p>2) SPD Manifold: The SPD manifold is a specific type of Riemann manifold composed of points expressed as square matrices M of size d ? d and it is denoted as:</p><formula xml:id="formula_4">S d ++ = {M ? R d?d : u T Mu &gt; 0 ? u ? R d ? {0 d }} (7)</formula><p>For a matrix to be considered as point in a SPD manifold, it should be symmetrical and have positive eigenvalues. Covariance matrices possess such properties and thus they can be considered points in a SPD manifold. Covariance matrices have been widely employed in the literature to model appearance and texture features in computer vision tasks <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>. As a result, the inclusion of covariance matrices in the computation of the multi-head attention is considered beneficial to the performance of a Vision Transformer due to incorporating additional information about the input, enhancing the discrimination power of the output feature representation. There are several metrics that can be used to measure the distance between points in a SPD manifold <ref type="bibr" target="#b29">[30]</ref>, however, this work employs the Frobenius distance as it is not restricted by the values of the elements in the covariance matrices, unlike log-based distances.</p><p>Given query Q ? R L?d and key K ? R L?d vectors, the covariance matrices of these vectors are initially computed as:</p><formula xml:id="formula_5">C Q = cov(Q) = E[(Q ? E[Q])(Q ? E[Q]) T ]<label>(8)</label></formula><formula xml:id="formula_6">C K = cov(K) = E[(K ? E[K])(K ? E[K]) T ]<label>(9)</label></formula><p>Due to their properties, the covariance matrices C Q , C K ? R L?L lie as points in the SPD manifold. The scaled Frobenius distance between these matrices is then calculated as:</p><formula xml:id="formula_7">D SP D (C Q , C K ) = ||C Q ? C K || F ? d<label>(10)</label></formula><p>where || ? || F denotes the Frobenius norm. Similar to the distance in the Euclidean manifold, the distance D SP D ? R h?L?L in Eq. 10 expresses the similarity between query and key vectors, with higher values denoting vectors that are far away from each other in the SPD manifold.</p><p>3) Grassmann Manifold: The Grassmann manifold is another well-known special type of Riemann manifold that embeds all p-dimensional linear subspaces that lie in a ddimensional Euclidean space. The Grassmann manifold, denoted as G(p, d), can be represented by the set of orthogonal matrices from the orthogonal group O(p) as follows:</p><formula xml:id="formula_8">G(p, d) = {X ? R d?p : X T X = I p }/O(p),<label>(11)</label></formula><p>where X represents any point on the Grassmann manifold. Grassmann manifolds have been widely employed in the literature to model sequential and time-varying signals as any linear dynamic system can be easily transformed to a point in the Grassmann manifold <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. As a result, the transformation of the input space to points in the Grassmann manifold can provide to a Vision Transformer additional information regarding texture and color variations in an image patch, leading to enriched feature representations with high discriminative power.</p><p>Several metrics have been defined to measure the distance between Grassmmanian points. The most common technique is to embed the manifold into the space of symmetric matrices with the mapping ? : G(p, d) ? Sym(d), ?(X) = XX T , which is a one-to-one, continuous and differentiable mapping <ref type="bibr" target="#b30">[31]</ref>. Moreover, to avoid the computation of the coordinates of all projected data and their pairwise distances as well as to improve efficiency, the kernel form of the projection distance [32] is adopted.</p><p>Given query Q ? R L?d and key K ? R L?d vectors, they first need to be transformed into orthogonal matrices so that they can represent points in the Grassmann manifold. To this end, the reduced QR decomposition is applied using the Gram-Schmidt process <ref type="bibr" target="#b32">[33]</ref> to decompose a real matrix into an orthogonal matrix G with orthogonal vectors as columns (G T G = I) and a triangular matrix R. Through the QR decomposition, the orthogonal matrices G Q ? R L?d and G K ? R L?d are derived that represent the corresponding query and key vectors as points in the Grassmann manifold.</p><formula xml:id="formula_9">Q = G Q R (12) K = G K R<label>(13)</label></formula><p>Then, the projection distance <ref type="bibr" target="#b30">[31]</ref> is employed to calculate the scaled distance between the two points in the Grassmann manifold:</p><formula xml:id="formula_10">D G (G Q , G K ) = ||G Q G T Q ? G K G T K || F ? d<label>(14)</label></formula><p>where || ? || F denotes the Frobenius norm. As with the other manifold distances, the distance D G ? R h?L?L in Eq. 14 expresses the similarity between query and key vectors, with higher values denoting vectors that are far away from each other in the Grassmann manifold.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Fusion of Manifolds:</head><p>After the computation of the individual distance matrices in each manifold, a fusion is needed to derive the final distance matrix. Instead of experimenting with fixed weights for each distance matrix, the proposed layer employs a 2D convolution operation to let the Transformer network learn on its own the optimal way to merge the distance matrices. More specifically, given the distance matrices D E , D SP D and D G ? R h?L?L in the Euclidean, SPD and Grassmann manifolds, respectively, the proposed multi-manifold multi-head attention initially concatenates them to derive a new distance matrix D f ? R 3h?L?L . Then, the new distance matrix is passed through a 2D convolutional layer that learns an effective mapping of the distances in the different manifolds and generates a fused distance matrix with size h?L?L. The final refined attention map is derived after a softmax operation is applied. The new feature representation V ? R L?d that is computed after the multiplication of the value vector V with the attention map is equal to:</p><formula xml:id="formula_11">V = softmax(Conv2D(D f ))V (15)</formula><p>Other than the proposed early fusion of manifold representations, a second configuration for the late fusion of manifolds has also been implemented in this work, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. More specifically, this configuration concerns the use of Vision Transformers in parallel and each Transformer receiving as input the representation of the input space in a specific manifold and the extraction of manifold specific feature representations. The different feature representations are then concatenated together and fed to the classifier for image recognition. Given the manifold specific feature representa-</p><formula xml:id="formula_12">tions V E = softmax(D E )V, V SP D = softmax(D SP D )V and V G = softmax(D G )V</formula><p>for the Euclidean, SPD and Grassmann manifolds, respectively, the final feature representation V ? R L?3d is equal to:</p><formula xml:id="formula_13">V = concatenate(V E , V SP D , V G )<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>This section presents experimental results in different datasets to demonstrate the advantages of employing the proposed multi-manifold multi-head attention as a substitute of the standard multi-head attention in Vision Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>The proposed multi-manifold multi-head attention was introduced in several state-of-the-art Transformer-based networks as a replacement to the standard multi-head attention in order to evaluate their performance. More specifically, the ViT-Lite-6/4, CVT-6/4 and CCT-7/3x2 models proposed in <ref type="bibr" target="#b4">[5]</ref>, as well as the Swin-T model proposed in <ref type="bibr" target="#b6">[7]</ref> were employed, giving rise to the MMA-ViT-Lite-6/4, MMA-CVT-6/4, MMA-CCT-7/3x2 and MMA-Swin-T models, respectively, when the proposed multi-manifold multi-head attention was introduced. For the Swin-T model in particular, a patch size of 2, a window size of 4, an embedding dimension of 96, an mlp ratio of 2, depths of (2, 6, 4) and number of heads equal to <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12)</ref> for the different layers were selected.</p><p>Experiments were conducted on three well-known image classification datasets, namely CIFAR-10, CIFAR-100 <ref type="bibr" target="#b33">[34]</ref>  and T-ImageNet <ref type="bibr" target="#b34">[35]</ref> and all models were trained from scratch. The CIFAR-10 dataset consists of 50K training and 10K test images equally distributed among 10 object classes, CIFAR-100 consists of 50K training and 10K test images equally distributed among 100 classes, while T-ImageNet consists of 100K training and 10K validation images equally distributed among 200 classes. In T-ImageNet, the validation images were used to test the performance of the deep models since the provided test images are not annotated. All experiments were run with a fixed batch size of 128 and for 200 epochs for CIFAR-10 and CIFAR-100 and 300 epochs for T-ImageNet. In addition, for CIFAR-10 and CIFAR-100, the input images were of size 32 ? 32 pixels, while for T-ImageNet, the input images were of size 64 ? 64 pixels. The AdamW optimizer <ref type="bibr" target="#b35">[36]</ref> was used with a weight decay of 0.01, a base learning rate of 5e ?4 and a cosine learning rate scheduler that adjusts the learning rate during training <ref type="bibr" target="#b36">[37]</ref>. A warmup of 10 epochs was applied by increasing gradually the learning rate from 0 to the initial value of the cosine learning rate scheduler <ref type="bibr" target="#b37">[38]</ref>. Label smoothing <ref type="bibr" target="#b38">[39]</ref> with a probability = 0.1 was applied during training, where the true label is considered to have a probability of 1 ? and the probability is shared between the other classes. Moreover, a kernel matrix of 1 ? 1 was utilized in the 2D convolutional layer that fuses the multi-manifold distance matrices.</p><p>In addition, extensive data augmentation techniques were applied to enhance the performance of the transformers. Auto-Augment <ref type="bibr" target="#b39">[40]</ref> was adopted in order to transform the training data with adaptive learnable transformations, such as shift, rotation, and color jittering. Moreover, the Mixup strategy <ref type="bibr" target="#b40">[41]</ref> was also employed, which generated weighted combinations of random sample pairs from the training images. The code for the experimental evaluation of the tested Transformers was implemented using the PyTorch framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation study</head><p>Initially, experiments were conducted to determine the contribution of each manifold to the classification results in the CIFAR-10 and CIFAR-100 datasets. To this end, the ViT-Lite-6/4 model, proposed in <ref type="bibr" target="#b4">[5]</ref>, was employed as the backbone network and its multi-head attention was substituted by the proposed multi-manifold multi-head attention, utilizing all possible combinations of manifold distances. The results of the ablation study regarding the early fusion of manifolds are presented in <ref type="table" target="#tab_0">Table I</ref>.</p><p>From the results of <ref type="table" target="#tab_0">Table I</ref>, it can be seen that the Euclidean manifold usually contains more important information than the other two manifolds alone for accurate image classification. More specifically, in CIFAR-10 the use of the SPD manifold leads to a drop of 0.45% in accuracy, while the use of the Grassmann manifold leads to a drop of almost 2% in accuracy.  Similar observations can be made for the CIFAR-100 dataset, although the use of the SPD manifold in this case leads to an increase of about 1.2% in the accuracy when compared to the Euclidean manifold. On the other hand, any combination of two manifolds leads to a significant increase in the accuracy of the ViT-Lite-6/4 model in both CIFAR-10 and CIFAR-100 with respect to employing only the Euclidean manifold. When all three manifolds are fused, an accuracy of 92.41% and 72.5% in CIFAR-10 and CIFAR-100, respectively, is achieved, with the results in the challenging CIFAR-100 dataset being the best, while the results in the CIFAR-10 dataset being slightly inferior than in the case when the Euclidean and the SPD manifolds are employed. Simultaneously, it can be observed that the increase in accuracy is accompanied by a small increase in the floating point operations (FLOPs) and almost no increase in the number of network parameters. Similar observations regarding the early fusion of manifolds can be made when the late fusion of the manifold specific feature representations is performed, as shown in <ref type="table" target="#tab_0">Table II</ref>. In this case, the fusion of all three manifolds leads to a performance of 71.77% in CIFAR-100 and 91.2% in CIFAR-10, with the latter being slightly inferior to the combination of Euclidean and Grassmann or Euclidean and SPD manifold combinations. A comparison between the early fusion of manifolds presented in <ref type="table" target="#tab_0">Table I</ref> and the late fusion of manifolds shown in <ref type="table" target="#tab_0">Table II</ref> shows that the early fusion leads to superior performance in both datasets, while utilizing fewer network parameters and FLOPs. More specifically, the early fusion of manifolds outperforms the late fusion of manifolds by 0.7% in accuracy, while employing more than 62% fewer parameters and FLOPs, showing the importance of fusing the manifold representations inside the Transformer encoder for the computation of a refined attention map. Additionally, from the results above, it can be deduced that the SPD and Grassmann manifolds contain supplementary information to the Euclidean manifold by modelling the appearance, color and texture variations in images. These additional features guide the Transformer network towards a better modelling of the underlying input space, enabling it to achieve improved classification results with a small increase in the number of FLOPs. For the rest of the experiments, it is assumed that the proposed multi-manifold multi-head attention employs the early fusion of all three manifolds since this combination leads to the optimal classification results.</p><p>To further clarify the benefits of employing additional data manifolds, a visualization of attention maps in a few images from the CIFAR-100 dataset is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. It can be observed that the fusion of data representations in different manifolds guides the network to pay attention to additional information that can be beneficial for the classification of an image. For instance, although the Euclidean manifold allows the network to pay attention on the ears of a kangaroo or the hump of a camel, the fusion of the Euclidean with the SPD and Grassmann manifolds enable the network to pay attention to the entire body of the kangaroo and both the hump and the legs of a camel, thus increasing the confidence of the network in its classification results. Similar observations can be made for the rest of the visualization results. Finally, <ref type="figure" target="#fig_4">Fig. 5</ref> depicts the distribution of 20 random object classes from the CIFAR-100 dataset, as formulated by the output of the ViT-Lite-6/4 model with different manifold representations, using the t-distributed stochastic neighbor embedding (t-SNE) algorithm <ref type="bibr" target="#b47">[48]</ref>. t-SNE is a nonlinear dimensionality reduction method, which is very suitable for visualizing high-dimensional data to 2 or 3 dimensions. To achieve its goal, t-SNE constructs a distribution of samples in the high-dimensional space, a similar distribution in the lowdimensional embedding and tries to minimize the Kullback-Leibler (KL) scatter between the two distributions with respect to the location of the embedding points. To this end, the feature vectors from the output of the Transformer encoder prior to the fully-connected layer <ref type="figure" target="#fig_0">(Fig. 1)</ref> are employed as the highdimensional input to the t-SNE algorithm, which then computes a two-dimensional output. From the visualization of <ref type="figure" target="#fig_4">Fig.  5</ref>, it can be observed that the fusion of manifolds leads to data points of the same class and color being closer to each other, while outliers are significantly reduced. These results verify the capability of the proposed MMA-ViT-Lite-6/4 model to better describe the underlying input space and achieve superior classification results with respect to the original ViT-Lite-6/4 model.</p><formula xml:id="formula_14">(a) (b) (c) (d) (e) (f)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with state-of-the-art</head><p>To further demonstrate the benefits of the proposed multimanifold attention, different Transformer network architectures were chosen and their attention was substituted with the proposed multi-manifold attention. <ref type="table" target="#tab_0">Table III</ref> presents a comparison of these Transformers with state-of-the-art CNN-and Transformer-based models in well-known image classification datasets (i.e., CIFAR-10, CIFAR-100 and T-ImageNet).</p><p>The results demonstrate the performance improvement in all tested datasets when the proposed multi-manifold attention was employed. More specifically, the performance of the MMA-ViT-Lite-6/4 model has been improved by 1.47%, 3.2% and 3.98% in CIFAR-10, CIFAR-100 and T-ImageNet, respectively, while the MMA-CVT-6/4 model achieved an improvement of 0.95%, 3.67% and 4.42% in CIFAR-10, CIFAR-100 and T-ImageNet, respectively, with just a small increase in GFLOPs. Similarly, the performance of the MMA-Swin-T model has been improved by 1.06%, 1.36% and %, in CIFAR-10, CIFAR-100 and T-ImageNet, respectively. Finally, the MMA-CCT-7/3x2 model outperformed CCT-7/3x2 in all three datasets, managing to achieve state-of-the-art performance with respect to the other CNN-and Transformerbased models.</p><p>Additional conclusions can be drawn by observing <ref type="figure" target="#fig_5">Fig.  6</ref> that presents the benefits of the proposed multi-manifold attention in terms of model performance and generalization ability. From these results, it can be deduced that all tested Transformers, irrespective of their network architecture, are significantly improved when the proposed multi-manifold attention is employed through a decrease in the training and validation losses and an increase in the validation accuracy with only a small increase in the number of operations (i.e., GFLOPs).</p><p>Finally, a visualization of attention maps for the CVT-6/4, Swin-T and CCT-7/3x2 Transformers, as well as a visualization of the distribution of 20 random classes of the T-ImageNet dataset using t-SNE are presented in <ref type="figure" target="#fig_6">Fig. 7</ref> and <ref type="figure" target="#fig_7">Fig. 8</ref>, respectively. From the attention maps in <ref type="figure" target="#fig_6">Fig. 7</ref>, it can be deduced that the use of the proposed multi-manifold attention guides all tested Transformers to pay more attention to significant parts of the object of interest, such as the legs of a spider, the arch rib of a bridge and the fur of a chimpanzee, thus leading to more accurate classification results. On the other hand, from the distribution of a few of the T-ImageNet classes in <ref type="figure" target="#fig_7">Fig. 8</ref>, it can be observed that the proposed multimanifold attention leads to more compact classes (i.e., points of the same class closer to each other) and less stray points for all tested Transformers.</p><p>These results validate the notion that employing data representations in different manifolds can guide any network, irrespective of its network architecture, to better model the underlying input space and achieve optimal classification results. This is achieved due to the fact that manifolds are governed by different statistical and geometrical rules, allowing a lower intra-class and a higher interclass variance between the representations in a manifold space, with respect to another. Along with the fact that different data representations can model various aspects of an image, such as appearance, color and texture, a Vision Transformer can greatly benefit from the proposed multi-manifold attention. As a result, the proposed multi-manifold attention can effectively substitute the standard attention in Vision Transformers, thus significantly improving their image classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this work, a novel multi-manifold multi-head attention is proposed that can substitute the standard attention in any Transformer-based network irrespective of its architecture. The proposed attention transforms the image representation into points that lie on three distinct manifolds, namely Euclidean, SPD and Grassmann, allowing rich information on an image's content to be captured and processed. Through the calculation and fusion of distances in the three manifolds, a refined attention with highly descriptive and discriminative power is generated, enabling an accurate modelling of the underlying input space. The experimental results with different Transformer-based network architectures and on well-known image classification results verify the effectiveness of the proposed multi-manifold attention in achieving accurate image classification results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the network architecture of a Vision Transformer with its main components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Proposed multi-manifold multi-head attention that replaces the standard multi-head attention, shown in a green rectangle inFig. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Network architecture with manifolds in (a) early and (b) late fusion. The operator ? is used to denote concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of results in CIFAR-100 using the ViT-Lite-6/4 model: (a) Input images and attention maps (heatmaps and overlaid on images) using (b) the Euclidean manifold, (c) the Grassmann manifold, (d) the SPD manifold and (e) all three manifolds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization using t-SNE of 20 random classes of CIFAR-100 using the ViT-Lite-6/4 model when the (a) Euclidean, (b) Grassmann, (c) SPD and (d) all three manifolds are employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Effect of the proposed multi-manifold multi-head attention on the tested Vision Transformers trained on CIFAR-100 (left) and T-ImageNet (right) in terms of (a),(b) model performance and (c),(d) generalization ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of attention maps in T-ImageNet: (a) Input images, attention maps (heatmaps and overlaid on images) using the (b) CVT-6/4, (c) MMA-CVT-6/4, (d) Swin-T, (e) MMA-Swin-T, (f) CCT-7-3x2 and (g) MMA-CCT-7-3x2 models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization using t-SNE of 20 random classes of T-ImageNet using (a) CVT-6/4, (b) Swin-T, (c) CCT-7-3x2, (d) MMA-CVT-6/4, (e) MMA-Swin-T and (f) MMA-CCT-7-3x2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY ON THE VIT-LITE-6/4 MODEL WITH THE EARLY FUSION OF MANIFOLD REPRESENTATIONS TABLE II ABLATION STUDY ON THE VIT-LITE-6/4 MODEL WITH THE LATE FUSION OF MANIFOLD REPRESENTATIONS</figDesc><table><row><cell></cell><cell>Manifolds</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row><row><cell>Euclidean</cell><cell>SPD</cell><cell>Grassmann</cell><cell>Params (M)</cell><cell>FLOPS (G)</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>X</cell><cell></cell><cell></cell><cell>3.20</cell><cell>0.22</cell><cell>90.94</cell><cell>69.2</cell></row><row><cell></cell><cell>X</cell><cell></cell><cell>3.20</cell><cell>0.22</cell><cell>90.49</cell><cell>70.38</cell></row><row><cell></cell><cell></cell><cell>X</cell><cell>3.20</cell><cell>0.23</cell><cell>88.96</cell><cell>67.48</cell></row><row><cell>X</cell><cell>X</cell><cell></cell><cell>3.20</cell><cell>0.23</cell><cell>92.86</cell><cell>71.93</cell></row><row><cell>X</cell><cell></cell><cell>X</cell><cell>3.20</cell><cell>0.24</cell><cell>91.72</cell><cell>72.48</cell></row><row><cell></cell><cell>X</cell><cell>X</cell><cell>3.20</cell><cell>0.24</cell><cell>90.99</cell><cell>71.35</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell>3.20</cell><cell>0.25</cell><cell>92.41</cell><cell>72.5</cell></row><row><cell></cell><cell>Manifolds</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row><row><cell>Euclidean</cell><cell>SPD</cell><cell>Grassmann</cell><cell>Params (M)</cell><cell>FLOPS (G)</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>X</cell><cell></cell><cell>X</cell><cell>6.38</cell><cell>0.44</cell><cell>91.34</cell><cell>71.47</cell></row><row><cell>X</cell><cell>X</cell><cell></cell><cell>6.38</cell><cell>0.44</cell><cell>91.34</cell><cell>71.26</cell></row><row><cell></cell><cell>X</cell><cell>X</cell><cell>6.38</cell><cell>0.45</cell><cell>88.81</cell><cell>67.93</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell>9.56</cell><cell>0.67</cell><cell>91.2</cell><cell>71.77</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III RESULTS</head><label>III</label><figDesc>ON CIFAR-10, CIFAR-100 AND T-IMAGENET</figDesc><table><row><cell>Method</cell><cell>Params (M)</cell><cell>FLOPS (G)</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>T-ImageNet</cell></row><row><cell>ResNet-100 [42]</cell><cell>1.70</cell><cell>0.25</cell><cell>93.39</cell><cell>72.78</cell><cell>-</cell></row><row><cell>ResNet-164 [42]</cell><cell>1.73</cell><cell>0.26</cell><cell>94.54</cell><cell>75.67</cell><cell>-</cell></row><row><cell>EfficientNet-B0 [43]</cell><cell>3.70</cell><cell>0.12</cell><cell>94.66</cell><cell>76.04</cell><cell>-</cell></row><row><cell>Linformer [44]</cell><cell>3.96</cell><cell>0.28</cell><cell>92.45</cell><cell>70.87</cell><cell>-</cell></row><row><cell>Performer [45]</cell><cell>3.85</cell><cell>0.28</cell><cell>91.58</cell><cell>73.11</cell><cell>-</cell></row><row><cell>Reformer [46]</cell><cell>3.39</cell><cell>0.25</cell><cell>90.58</cell><cell>73.02</cell><cell>-</cell></row><row><cell>Couplformer-7 [47]</cell><cell>3.85</cell><cell>0.28</cell><cell>93.44</cell><cell>74.53</cell><cell>-</cell></row><row><cell>ViT-Lite-6/4 [5]</cell><cell>3.20</cell><cell>0.22</cell><cell>90.94</cell><cell>69.2</cell><cell>49.18</cell></row><row><cell>MMA-ViT-Lite-6/4</cell><cell>3.20</cell><cell>0.25</cell><cell>92.41</cell><cell>72.5</cell><cell>53.16</cell></row><row><cell>CVT-6/4 [5]</cell><cell>3.19</cell><cell>0.22</cell><cell>92.58</cell><cell>72.25</cell><cell>51.45</cell></row><row><cell>MMA-CVT-6/4</cell><cell>3.19</cell><cell>0.24</cell><cell>93.53</cell><cell>75.92</cell><cell>55.87</cell></row><row><cell>Swin-T [7]</cell><cell>7.05</cell><cell>0.24</cell><cell>91.88</cell><cell>72.34</cell><cell>60.64</cell></row><row><cell>MMA-Swin-T</cell><cell>7.05</cell><cell>0.36</cell><cell>92.94</cell><cell>73.7</cell><cell>61.57</cell></row><row><cell>CCT-7/3?2 [5]</cell><cell>3.86</cell><cell>0.29</cell><cell>93.65</cell><cell>74.77</cell><cell>61.07</cell></row><row><cell>MMA-CCT-7/3?2</cell><cell>3.86</cell><cell>0.32</cell><cell>94.74</cell><cell>77.5</cell><cell>64.41</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(c)</cell><cell>(d)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work has been supported from General Secretariat for Research and Technology under Grant agreement no. T 6?B?00238 "Q-CONPASS: Dynamic Quality CONtrol on Production lines using intelligent AutonomouS vehicleS".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05704</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deepvit: Towards deeper vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A survey of visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06091</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2021</title>
		<imprint>
			<biblScope unit="page" from="10" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-euclidean universal approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kratsios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bilokopytov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10" to="635" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometry-aware similarity learning on spd manifolds for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2513" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contour covariance: A fast descriptor for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="569" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient continuous manifold learning for time series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Mulyadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03379</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Collaborative representation for spd matrices with application to image-set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08962</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Symnet: A simple symmetric positive definite manifold deep learning method for image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification of multidimensional time-evolving data using histograms of grassmannian points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dimitropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barmpoutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitsikidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grammalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="892" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep learning approach for analyzing video and skeletal features in sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dimitropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on imaging systems and techniques (IST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition based on deep learning and grassmannian pyramids</title>
	</analytic>
	<monogr>
		<title level="m">2018 26th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2045" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grading of invasive breast carcinoma through grassmannian vlad encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dimitropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barmpoutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zioga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patsiaoura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grammalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">185110</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lds-inspired residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dimou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ataloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dimitropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2363" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neighborhood preserving embedding on grassmann manifold for image-set analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">108335</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive fusion of heterogeneous manifolds for subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3484" to="3497" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Vit</title>
		<ptr target="https://amaarora.github.io/2021/01/18/ViT.html" />
		<imprint>
			<biblScope unit="page" from="2022" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Conditional positional encodings for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Covariance of motion and appearance featuresfor spatio temporal recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05355</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Riemannian metric learning for symmetric positive definite matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02393</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dictionary learning and sparse coding on grassmann manifolds: An extrinsic solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3120" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Grassmannian learning: Embedding geometry awareness in shallow and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Heath</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02229</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Solving linear least squares problems by gram-schmidt orthogonalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Bj?rck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS 231N</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ua6zuk0WRH" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno>abs/2001.04451</idno>
		<ptr target="https://arxiv.org/abs/2001.04451" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Couplformer: Rethinking vision transformer with coupling attention map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05425</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
