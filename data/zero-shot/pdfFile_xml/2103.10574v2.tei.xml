<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HOPPER: MULTI-HOP TRANSFORMER FOR SPATIOTEMPORAL REASONING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
							<email>farleylai@nec-labs.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HOPPER: MULTI-HOP TRANSFORMER FOR SPATIOTEMPORAL REASONING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers the problem of spatiotemporal object-centric reasoning in videos. Central to our approach is the notion of object permanence, i.e., the ability to reason about the location of objects as they move through the video while being occluded, contained or carried by other objects. Existing deep learning based approaches often suffer from spatiotemporal biases when applied to video reasoning problems. We propose Hopper, which uses a Multi-hop Transformer for reasoning object permanence in videos. Given a video and a localization query, Hopper reasons over image and object tracks to automatically hop over critical frames in an iterative fashion to predict the final position of the object of interest. We demonstrate the effectiveness of using a contrastive loss to reduce spatiotemporal biases. We evaluate over CATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1 FPS by hopping through just a few critical frames. We also demonstrate Hopper can perform long-term reasoning by building a CATER-h dataset 1 that requires multi-step reasoning to localize objects of interest correctly. * Work done as a NEC Labs intern.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head> <ref type="bibr">(Girdhar &amp; Ramanan, 2020</ref><p>) is an object permanence task where the goal is to classify the final location of the snitch object within a 2D grid space.</p><p>In this paper, we address the problem of spatiotemporal object-centric reasoning in videos. Specifically, we focus on the problem of object permanence, which is the ability to represent the existence and the trajectory of hidden moving objects <ref type="bibr" target="#b3">(Baillargeon, 1986)</ref>. Object permanence can be essential in understanding videos in the domain of: (1) sports like soccer, where one needs to reason, "which player initiated the pass that resulted in a goal?", (2) activities like shopping, one needs to infer "what items the shopper should be billed for?", and (3) driving, to infer "is there a car next to me in the right lane?". Answering these questions requires the ability to detect and understand the motion of objects in the scene. This requires detecting the temporal order of one or more actions of objects. Furthermore, it also requires learning object permanence, since it requires the ability to predict the location of non-visible objects as they are occluded, contained or carried by other objects <ref type="bibr" target="#b38">(Shamsian et al., 2020)</ref>. Hence, solving this task requires compositional, multi-step spatiotemporal reasoning which has been difficult to achieve using existing deep learning models <ref type="bibr" target="#b6">(Bottou, 2014;</ref><ref type="bibr" target="#b25">Lake et al., 2017)</ref>.</p><p>Existing models have been found lacking when applying to video reasoning and object permanence tasks <ref type="bibr">(Girdhar &amp; Ramanan, 2020)</ref>. Despite rapid progress in video understanding benchmarks such as action recognition over large datasets, deep learning based models often suffer from spatial and temporal biases and are often easily fooled by statistical spurious patterns and undesirable dataset biases <ref type="bibr" target="#b23">(Johnson et al., 2017b)</ref>. For example, researchers have found that models can recognize the action "swimming" even when the actor is masked out, because the models rely on the swimming pool, the scene bias, instead of the dynamics of the actor <ref type="bibr" target="#b13">(Choi et al., 2019)</ref>.</p><p>Hence, we propose Hopper to address debiased video reasoning. Hopper uses multi-hop reasoning over videos to reason about object permanence. Humans realize object permanence by identifying key frames where objects become hidden <ref type="bibr" target="#b7">(Bremner et al., 2015)</ref> and reason to predict the motion and final location of objects in the video. Given a video and a localization query, Hopper uses a Multi-hop Transformer (MHT) over image and object tracks to automatically identify and hop over critical frames in an iterative fashion to predict the final position of the object of interest. Additionally, Hopper uses a contrastive debiasing loss that enforces consistency between attended objects and correct predictions. This improves model robustness and generalization. We also build a new dataset, CATER-h, that reduces temporal bias in CATER and requires long-term reasoning.</p><p>We demonstrate the effectiveness of Hopper over the recently proposed CATER 'Snitch Localization' task (Girdhar &amp; Ramanan, 2020) ( <ref type="figure" target="#fig_0">Figure 1</ref>). Hopper achieves 73.2% Top-1 accuracy in this task at just 1 FPS. More importantly, Hopper identifies the critical frames where objects become invisible or reappears, providing an interpretable summary of the reasoning performed by the model. To summarize, the contributions of our paper are as follows: First, we introduce Hopper that provides a framework for multi-step compositional reasoning in videos and achieves state-of-the-art accuracy in CATER object permanence task. Second, we describe how to perform interpretable reasoning in videos by using iterative reasoning over critical frames. Third, we perform extensive studies to understand the effectiveness of multi-step reasoning and debiasing methods that are used by Hopper. Based on our results, we also propose a new dataset, CATER-h, that requires longer reasoning hops, and demonstrates the gaps of existing deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Video understanding. Video tasks have matured quickly in recent years <ref type="bibr">(Hara et al., 2018)</ref>; approaches have been migrated from 2D or 3D ConvNets <ref type="bibr" target="#b20">(Ji et al., 2012)</ref> to two-stream networks <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2014)</ref>, inflated design <ref type="bibr" target="#b9">(Carreira &amp; Zisserman, 2017)</ref>, models with additional emphasis on capturing the temporal structures <ref type="bibr" target="#b61">(Zhou et al., 2018)</ref>, and recently models that better capture spatiotemporal interactions <ref type="bibr">Girdhar et al., 2019)</ref>. Despite the progress, these models often suffer undesirable dataset biases, easily confused by backgrounds objects in new environments as well as varying temporal scales <ref type="bibr" target="#b13">(Choi et al., 2019)</ref>. Furthermore, they are unable to capture reasoning-based constructs such as causal relationships <ref type="bibr">(Fire &amp; Zhu, 2017)</ref> or long-term video understanding <ref type="bibr">(Girdhar &amp; Ramanan, 2020)</ref>.</p><p>Visual and video reasoning. Visual and video reasoning have been well-studied recently, but existing research has largely focused on the task of question answering <ref type="bibr" target="#b22">(Johnson et al., 2017a;</ref><ref type="bibr">Hudson &amp; Manning, 2018;</ref><ref type="bibr" target="#b47">2019a;</ref><ref type="bibr" target="#b58">Yi et al., 2020)</ref>. CATER, a recently proposed diagnostic video recognition dataset focuses on spatial and temporal reasoning as well as localizing particular object of interest. There also has been significant research in object tracking, often with an emphasis on occlusions with the goal of providing object permanence <ref type="bibr" target="#b53">(Wojke et al., 2017;</ref><ref type="bibr" target="#b49">Wang et al., 2019b)</ref>. Traditional object tracking approaches often require expensive supervision of location of the objects in every frame. In contrast, we address object permanence and video recognition on CATER with a model that performs tracking-integrated object-centric reasoning without this strong supervision.</p><p>Multi-hop reasoning. Reasoning systems vary in expressive power and predictive abilities, which include symbolic reasoning, probabilistic reasoning, causal reasoning, etc. <ref type="bibr" target="#b6">(Bottou, 2014)</ref>. Among them, multi-hop reasoning is the ability to reason with information collected from multiple passages to derive the answer <ref type="bibr" target="#b47">(Wang et al., 2019a)</ref>, and it gives a discrete intermediate output of the reasoning process, which can help gauge model's behavior beyond just the final task accuracy . Several multi-hop datasets and models have been proposed for the reading comprehension task <ref type="bibr" target="#b51">(Welbl et al., 2018;</ref><ref type="bibr" target="#b57">Yang et al., 2018b;</ref><ref type="bibr">Dua et al., 2019;</ref><ref type="bibr" target="#b17">Dhingra et al., 2020)</ref>. We extend multihop reasoning to the video domain by developing a dataset that explicitly requires aggregating clues from different spatiotemporal parts of the video, as well as a multi-hop model that automatically extracts a step-by-step reasoning chain, which improves interpretability and imitates a natural way of thinking. We provide an extended discussion of related work in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HOPPER</head><p>Hopper <ref type="figure">(Figure 2</ref>) is a framework inspired from the observation that humans think in terms of entities and relations. Unlike traditional deep visual networks that perform processing over the pixels from which they learn and extract features, object-centric learning-based architecture explicitly separates information about entities through grouping and abstraction from the low-level information <ref type="bibr" target="#b29">(Locatello et al., 2020)</ref>. Hopper obtains representations of object entities from the lowlevel pixel information of every frame (Section 3.2). Additionally, to maintain object permanence, humans are able to identify key moments when the objects disappear and reappear. To imitate that, Hopper computes object tracks with the goal to have a more consistent object representation (Section 3.3) and then achieves multi-step compositional long-term reasoning with the Multi-hop Transformer to pinpoint these critical moments. Furthermore, Hopper combines both fine-grained (object) and coarse-grained (image) information to form a contextual understanding of a video. As shown in <ref type="figure">Figure 2</ref>, Hopper contains 4 components; we describe them below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BACKBONE</head><p>Starting from the initial RGB-based video representation x v ? R T ?3?H0?W0 where T represents the number of frames of the video, 3 is for the three color channels, and H 0 and W 0 denote the original resolution height and width, a conventional CNN backbone would extract the feature map f ? R T ?P ?H?W and for every frame t a compact image representation i t ? R P . The backbone we use is ResNeXt-101 from <ref type="bibr" target="#b30">Ma et al. (2018)</ref>, P = 2048 and H, W = 8, 10. A 1?1 convolution <ref type="bibr" target="#b8">(Carion et al., 2020)</ref> then reduces the channel dimension of f from P to a smaller dimension d (d = 256), and a linear layer is used to turn the dimension of i t from P to d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OBJECT DETECTION AND REPRESENTATION</head><p>We collapse the spatial dimensions into 1 dimension and combine the batch dimension with the temporal dimension for the feature map f . Positional encodings are learned for each time step (T in total) and each spatial location (H ? W in total), which are further added to the feature map in an element-wise manner. The positional encoding-augmented feature map is the source input to the transformer encoder <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> of DETR <ref type="bibr" target="#b8">(Carion et al., 2020)</ref>. DETR is a recently proposed transformer-based object detector for image input; it additionally accepts N embeddings of object queries for every image (assuming every image at most has N objects 2 ) to the transformer decoder. We also combine the batch dimension with temporal dimension for the object queries. Outputs from DETR are transformed object representations that are used as inputs to a multilayer perceptron (MLP) to predict the bounding box and class label of every object. For Snitch Localization, DETR is trained on object annotations from LA-CATER <ref type="bibr" target="#b38">(Shamsian et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRACKING</head><p>Tracking produces consistent object representations as it links the representations of each object through time. We perform tracking using the unordered object representations, bounding boxes and labels as inputs, and applying our Hungarian-based algorithm to match objects between every two consecutive frames. We describe the details as follows.</p><p>Tracking is essentially an association problem <ref type="bibr" target="#b5">(Bewley et al., 2016)</ref>. An association between 2 objects respectively from consecutive 2 frames can be defined by the object class agreement and the difference of the two bounding boxes. Let us denote by? = [? t ] T t=1 the predicted list of objects at all frames in a video, where? t = {? i t } N i=1 denotes the predicted set of objects at frame t. Each object is represented as a 4-</p><formula xml:id="formula_0">tuple? i t = (? i t ,b i t , {p i t (c)|c ? C}, o i t )</formula><p>where? i t denotes the class label that has the maximum predicted likelihood for object i at frame t,b i t ? [0, 1] 4 is a vector that defines the bounding box top left and bottom right coordinates relative to the image size,p i t (c) denotes the predicted likelihood for class c (where C = {large metal green cube, small metal green cube, . . . , ?}), and o i t ? R d denotes the representation vector of this object i at frame t. In order to obtain the optimal bipartite matching between the set of predicted objects at frame t and t + 1, we search for a permutation of N elements ? ? S N with the lowest permutation cost:</p><formula xml:id="formula_1">? = arg min ??S N N i=1 L track ? i t ,? ?(i) t+1</formula><p>(1)</p><p>where L track is a pair-wise track matching cost between predicted object? i t (i.e., object i at frame t) and predicted object at frame t + 1 with index ?(i) from the permutation ?, denoted by? ?(i) t+1 . Following <ref type="bibr" target="#b8">Carion et al. (2020)</ref>, the optimal assignment is computed efficiently with the Hungarian algorithm. The track matching cost at time t for object i is defined as</p><formula xml:id="formula_2">L track ? i t ,? ?(i) t+1 = ?? c 1 {? i t =?}p ?(i) t+1 ? i t + ? b 1 {? i t =?} L box b i t ,b ?(i) t+1</formula><p>( <ref type="formula">2)</ref> where 1 denotes an indicator function such that the equation after the symbol 1 only takes effect when the condition inside the {. . . } is true, otherwise the term will be 0. ? c , ? b ? R weight each term. L box is defined as a linear combination of the L 1 loss and the generalized IoU loss <ref type="bibr" target="#b36">(Rezatofighi et al., 2019)</ref>. When the predicted class label of object i at frame t is not ?, we aim to maximize the likelihood of the class label? i t for the predicted object ?(i) at frame t + 1, and minimize the bounding box difference between the two. The total track matching cost of a video is the aggregation of L track ? i t ,? ?(i) t+1 from object i = 1 to N and frame t = 1 to T ? 1.</p><p>This Hungarian-based tracking algorithm is used due to its simplicity. A more sophisticated tracking solution (e.g. DeepSORT <ref type="bibr" target="#b53">(Wojke et al., 2017)</ref>) could be easily integrated into Hopper, and may improve the accuracy of tracking in complex scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">VIDEO QUERY REPRESENTATION AND RECOGNITION</head><p>The N object tracks obtained from the Hungarian algorithm and a single track of image features from the backbone are further added with the learned positional time encodings to form the source input to our Multi-hop Transformer (which will be introduced in Section 4). Multi-hop Transformer produces the final latent representation of the video query e ? R d . A MLP uses the video query representation e as an input and predicts the grid class for the Snitch Localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTI-HOP TRANSFORMER</head><p>Motivated by how humans reason an object permanence task through identifying critical moments of key objects in the video <ref type="bibr" target="#b7">(Bremner et al., 2015)</ref>, we propose Multi-hop Transformer (MHT). MHT reasons by hopping over frames and selectively attending to objects in the frames, until it arrives at the correct object that is the most important for the task. MHT operates in an iterative fashion, and each iteration produces one-hop reasoning by selectively attending to objects from a collection of frames. Objects in that collection of frames form the candidate pool of that hop. Later iterations are built upon the knowledge collected from the previous iterations, and the size of the candidate pool decreases as iteration runs. We illustrate the MHT in <ref type="figure" target="#fig_0">Figure 16</ref> in Appendix. The overall module is described in Algorithm 1. MHT accepts a frame track T f :</p><formula xml:id="formula_3">[i 1 , i 2 , ? ? ? , i T ], an object track T o : [o 1 1 , o 1 2 , ? ? ? , o 1 T , ? ? ? , o N 1 , o N 2 , ? ? ? , o N T ]</formula><p>, an initial target video query embedding E, the number of objects N and number of frames T . h denotes the hop index, and t is the frame index that the previous hop (i.e., iteration) mostly attended to, in Algorithm 1.</p><p>Overview. Multiple iterations are applied over MHT, and each iteration performs one hop of reasoning by attending to certain objects in critical frames. With a total of H hops, MHT produces refined representation of the video query E (E ? R 1?d ). As the complexity of video varies, H should also vary across videos. In addition, MHT operates in an autoregressive manner to process the incoming frames. This is achieved by 'Masking()' (will be described later). The autoregressive processing in MHT allows hop h+1 to only attend to objects in frames after frame t, if hop h mostly attends to an object at frame t. We define the most attended object of a hop as the object that has the highest attention weight (averaged from all heads) from the encoder-decoder multi-head attention layer in Transformer s (will be described later). The hopping ends when the most attended object is an object in the last frame.</p><p>MHT Architecture. Inside of the MHT, there are 2 encoder-decoder transformer units <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref>: Transformer f and Transformer s . This architecture is inspired by study in cognitive science that reasoning consists of 2 stages: first, one has to establish the domain about which one reasons and its properties, and only after this initial step can one's reasoning happen <ref type="bibr" target="#b41">(Stenning &amp; Van Lambalgen, 2012)</ref>. We first use Transformer f to adapt the representations of the object entities, which form the main ingredients of the domain under the context of an object-centric video task. Then, Transformer s is used to produce the task-oriented representation to perform reasoning.</p><p>We separate 2 types of information from the input: attention candidates and helper information. This separation comes from the intuition that humans sometimes rely on additional information outside of the candidate answers. We call such additional information as helper information H (specifically in our case, could be the coarse-grained global image context, or information related to the previous reasoning step). We define candidate answers as attention candidates U, which are representations of object entities (because object permanence is a task that requires reasoning relations of objects). For each hop, we first extract attention candidates and helper information from the source sequence, then use Transformer f to condense the most useful information by attending to attention candidates via self-attention and helper information via encoder-decoder attention. After that, we use Transformer s to learn the latent representation of the video query by attentively utilizing the information extracted from Transformer f (via encoder-decoder attention). Thus, MHT decides on which object to mostly attend to, given the current representation of the video query E, by reasoning about the relations between the object entities (U), and how would each object entity relate to the reasoning performed by the previous hop or global information (H).</p><p>Algorithm 1 Multi-hop Transformer module.</p><formula xml:id="formula_4">Input: T f ? R T ?d , To ? R N T ?d , E ? R 1?d , N ? R, T ? R Params: LayerNorm, Transformer f , Transformers, Wg, bg 1: h ? 0, t ? 0 2: while t = (T ? 1) do 3: h ? h + 1 4:</formula><p>if h &gt; 1 then 5:</p><p>H ?Extract (To, N , T , t) 6: else 7:</p><p>H ? T f 8: end if 9:</p><p>U ? To 10:</p><p>Uupdate, _ ? Transformer f (U, H) 11:</p><p>Uupdate ?Sigmoid (Wg ? Uupdate + bg) U 12:</p><p>Umask ?Masking (Uupdate, t) 13:</p><p>E, A ? Transformers (Umask, E) 14:</p><p>t ?Softargmax (A) 15: end while 16: e ? LayerNorm (E) Return e Transformer f . Transformer f uses helper information H from the previous hop, to adapt the representations of the object entities U to use in the current reasoning step. Formally, U is the object track sequence T o as in line 9 in Algorithm 1 (U ? R N T ?d , N T tokens), whereas H encompasses different meanings for hop 1 and the rest of the hops. For hop 1, H is the frame track T f (H ? R T ?d , T tokens, line 7). This is because hop 1 is necessary for all videos with the goal to find the first critical object (and frame) from the global information. Incorporating frame representations is also beneficial because it provides complementary information and can mitigate occasional errors from the object detector and tracker. For the rest of the hops, H is the set of representations of all objects in the frame that the previous hop mostly attended to (H ? R N ?d , N tokens, Extract() in line 5). The idea is that, to select an answer from object candidates after frame t, objects in frame t could be the most important helper information. Transformer f produces U update (U update ? R N T ?d , N T tokens), an updated version of U, by selectively attending to H. Further, MHT conditionally integrates helper-fused representations and the original representations of U. This conditional integration is achieved by Attentional Feature-based Gating (line 11), with the role to combine the new modified representation with the original representation. This layer, added on top of Transformer f , provides additional new information, because it switches the perspective into learning new representations of object entities by learning a feature mask (values between 0 and 1) to select salient dimensions of U, conditioned on the adapted representations of object entities that are produced by Transformer f . Please see details about this layer in <ref type="bibr" target="#b31">Margatina et al. (2019)</ref>).</p><p>Transformer s . Transformer s is then used to produce the task-oriented video query representation E. As aforementioned, MHT operates in an autoregressive manner to proceed with time. This is achieved by 'Masking()' that turns U update into U mask (U mask ? R N T ?d ) for Transformer s by only retaining the object entities in frames after the frame that the previous hop mostly attended to (for hop 1, U mask is U update ). Masking is commonly used in NLP for the purpose of autoregressive processing <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref>. Masked objects will have 0 attention weights. Transformer s learns representation of the video query E by attending to U mask (line 13). It indicates that, unlike Transformer f in which message passing is performed across all connections between tokens in U, between tokens in H, and especially across U and H (we use U for Transformer f , instead of U mask , because potentially, to determine which object a model should mostly attend to in frames after t, objects in and before frame t might also be beneficial), message passing in Transformer s is only performed between tokens in E (which has only 1 token for Snitch Localization), between tokens in unmasked tokens in U update , and more importantly, across connections between the video query E and unmasked tokens in U update . The indices of the most attended object and the frame that object is in, are determined by attention weights A from the previous hop with a differentiable 'Softargmax()' <ref type="bibr" target="#b10">(Chapelle &amp; Wu, 2010;</ref><ref type="bibr">Honari et al., 2018)</ref>, defined as, softargmax(x) = i e ?x i j e ?x j i, where ? is an arbitrarily large number. Attention weights A (A ? R N T ?1 ) is averaged from all heads. E is updated over the hops, serving the information exchange between the hops.</p><p>Summary &amp; discussion. H, U mask and E are updated in every hop. E should be seen as an encoding for a query of the entire video. Even though in this dataset, a single token is used for the video query, and the self-attention in the decoder part of Transformer s is thus reduced to a stacking of 2 linear transformations, it is possible that multiple queries would be desirable in other applications. These structural priors that are embedded in (e.g., the iterative hopping mechanism and attention, which could be treated as a soft tree) essentially provide the composition rules that algebraically manipulate the previously acquired knowledge and lead to the higher forms of reasoning <ref type="bibr" target="#b6">(Bottou, 2014)</ref>. Moreover, MHT could potentially correct errors made by object detector and tracker, but poor performance of them (especially object detector) would also make MHT suffer because (1) inaccurate object representations will confuse MHT in learning, and (2) the heuristic-based loss for intermediate hops (will be described in Section 5) will be less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING</head><p>We propose the following training methods for the Snitch Localization task and present an ablation study in Appendix A. We provide the implementation details of our model in Appendix H.</p><p>Dynamic hop stride. A basic version of autoregressive MHT is to set the per-hop frame stride to 1 with 'Masking()' as usually done in NLP. It means that Transformer s will only take in objects in frame t+1 as the source input if the previous hop mostly attended to an object in frame t. However, this could produce an unnecessary long reasoning chain. By using dynamic hop stride, we let the model automatically decide on which upcoming frame to reason by setting 'Masking()' to give unmasked candidates as objects in frames after the frame that the previous hop mostly attended to.</p><p>Minimal hops of reasoning. We empirically set the minimal number of hops that the model has to perform for any video as 5 to encourage multi-hop reasoning with reasonably large number of hops (unless not possible, e.g., if the last visible snitch is in the second last frame, then the model is only required to do 2 hops). This is also achieved by 'Masking()'. E.g., if hop 1 mostly attends to an object in frame 3, 'Masking()' will not mask objects in frames from frame 4 to frame 10 for hop 2, in order to allow hop 3, 4, 5 to happen (suppose 13 frames per video, and frame 4 is computed from 3 + 1, frame 10 is computed as max(3 + 1, 13 ? (5 ? 2))).</p><p>Auxiliary hop 1 object loss. Identifying the correct object to attend to in early hops is critical and for Snitch Localization, the object to attend to in hop 1 should be the last visible snitch (intuitively).</p><p>Hence, we define an auxiliary hop 1 object loss as the cross-entropy of classifying index of the last visible snitch. Inputs to this loss are the computed index of the last visible snitch from T o (with the heuristic that approximates it from predicted object bounding boxes and labels), as well as the attention weights A from Transformer s of hop 1, serving as predicted likelihood for each index.</p><p>Auxiliary hop 2 object loss. Similarly, we let the second hop to attend to the immediate occluder or container of the last visible snitch. The auxiliary hop 2 object loss is defined as the cross-entropy of classifying index of the immediate occluder or container of the last visible snitch. Inputs to this loss are the heuristic 3 computed index and attention weights A from Transformer s of hop 2.</p><p>Auxiliary hop 1&amp;2 frame loss. Attending to objects in the correct frames in hop 1 and 2 is critical for the later hops. A L 1 loss term could guide the model to find out the correct frame index.</p><p>Teacher forcing is often used as a strategy for training recurrent neural networks that uses the ground truth from a prior time step as an input <ref type="bibr" target="#b52">(Williams &amp; Zipser, 1989)</ref>. We use teacher forcing for hop 2 and 3 by providing the ground truth H and U mask (since we can compute the frame index of the last visible snitch with heuristics as described above).</p><p>Contrastive debias loss via masking out. This loss is inspired from the human mask confusion loss in <ref type="bibr" target="#b13">Choi et al. (2019)</ref>. It allows penalty for the model if it could make predictions correctly when the most attended object in the last frame is masked out. However, in contrast to human mask, we enforce consistency between attended objects and correct predictions, ensuring that the model understands why it is making a correct prediction. The idea here is that the model should not be able to predict the correct location without seeing the correct evidence. Technically, the contrastive debias loss is defined as the entropy function that we hope to maximize, defined as follows.</p><formula xml:id="formula_5">L debias = E K k=1 g ? (M neg ; ? ? ? ) (log g ? (M neg ; ? ? ? ))<label>(3)</label></formula><p>where g ? denotes the video query representation and recognition module (Multi-hop Transformer along with MLP) with parameter ? that produces the likelihood of each grid class, M neg is the source sequence to the Multi-hop Transformer with the most attended object in the last hop being masked out (set to zeros), and K denotes the number of grid classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>Datasets. Snitch Localization <ref type="figure" target="#fig_0">(Figure 1</ref>) is the most challenging task in the CATER dataset and it requires maintaining object permanence to solve the task successfully (Girdhar &amp; Ramanan, 2020). However, CATER is highly imbalanced for Snitch Localization task in terms of the temporal cues: snitch is entirely visible at the end of the video for 58% samples, and entirely visible at the second last frame for 14% samples. As a result, it creates a temporal bias in models to predict based on the last few frames. To address temporal bias in CATER, we create a new dataset, CATER-hard (CATER-h for short), with diverse temporal variations. In CATER-h, every frame index roughly shares an equal number of videos, to have the last visible snitch in that frame ( <ref type="figure" target="#fig_0">Figure 13</ref>).</p><p>Baselines &amp; metrics. We experiment TSM , TPN <ref type="bibr" target="#b55">(Yang et al., 2020)</ref>, and SINet <ref type="bibr" target="#b30">(Ma et al., 2018</ref>  Results. We present the results on CATER in <ref type="table" target="#tab_2">Table 1</ref> and on CATER-h in <ref type="table" target="#tab_3">Table 2</ref>. For all methods, we find that the performance on CATER-h is lower than that on CATER, which demonstrates the difficulty of CATER-h. Such performance loss is particularly severe for the tracking baseline and the temporal video understanding methods (TPN and TSM). The DaSiamRPN tracking approach only solves about a third of the videos on CATER and less than 20% on CATER-h. This is because the tracker is unable to maintain object permanence through occlusions and containments, showcasing the challenging nature of the task. Our Hungarian tracking has 46.0% Top-1 on CATER and 37.2% Top-1 on CATER-h. On CATER, TPN and TSM, as two state-of-the-art methods focusing on temporal modeling for videos, achieve a higher accuracy than methods in Girdhar &amp; Ramanan (2020). However, on CATER-h, TPN only has 50.2% Top-1 accuracy and 44.0% for TSM. SINet performs poorly even though SINet reasons about the higher-order object interactions via multiheaded attention and fusion of both coarse-and fine-grained information (similar to our Hopper).</p><p>The poor performance of SINet can be attributed to the less accurate object representations and the lack of tracking whereas effective temporal modeling is critical for the task. Without the object-centric modeling, Transformer that uses a sequence of frame representations performs poorly. However, for both SINet and Transformer, a significant improvement is observed after utilizing our Hopper framework. This shows the benefits of tracking-enabled object-centric learning embedded in Hopper. Since almost 60% of videos in CATER have the last visible snitch in the last frame, we run another experiment by using Transformer in Hopper but only uses the representations of the frame and objects in the last frame. This model variant achieves 61.1% Top-1 on CATER, even higher than the best-performing method in Girdhar &amp; Ramanan (2020), but only 41.8% Top-1 on CATER-h, which proves and reiterates the necessity of CATER-h. The accuracy is relatively high, likely due to other dataset biases. More discussion is available in Appendix G.6. Our method, Hopper with the MHT, outperforms all baselines in Top-1 using only 13 frames per video, highlighting the effectiveness and importance of the proposed multi-hop reasoning. <ref type="bibr" target="#b38">Shamsian et al. (2020)</ref> report slightly better Top-1 (74.8%) and L 1 (0.54) on CATER but they impose strong domain knowledge, operate at 24 FPS (300 frames per video) and require the location of the snitch to be labeled in every frame, even when contained or occluded, for both object detector and their reasoning module. Labeling these non-visible objects for every frame of a video would be very difficult in real applications. Furthermore, their method only has 51.4% Top-1 and L 1 of 1.31 on CATER-h at 24 FPS. Please refer to Appendix where we provide more quantitative results.</p><p>Interpretability. We visualize the attended objects in <ref type="figure" target="#fig_1">Figure 3</ref>. As illustrated, the last visible snitch is in frame 4 in this video, and at frame 5 snitch is contained by the purple cone. At frame 7, the purple cone is contained by the blue cone, but in the end, the blue cone excludes the purple cone. Hop 1 of Hopper-multihop attends to the last visible snitch at frame 4, and hop 2 attends to snitch's immediate container: the purple cone at frame 5. Hop 3 mostly attends to the purple cone's immediate container: the blue cone at frame 7, and secondly attends to the blue cone at frame 11 (by the other head) as the blue cone is just sliding from frame 7 till 12. Hop 4 mostly attends to the blue cone at frame 12. Hop 5 mostly attends to the purple cone (who contains the snitch) at frame 13, and secondly attends to the blue cone at frame 13. The visualization exhibits that Hopper-multihop performs reasoning by hopping over frames and meanwhile selectively attending to objects in the frame. It also showcases that MHT provides more transparency to the reasoning process. Moreover, MHT implicitly learns to perform snitch-oriented tracking automatically. More visualizations are available in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>This work presents Hopper with a novel Multi-hop Transformer to address object permanence in videos. Hopper achieves 73.2% Top-1 accuracy at just 1 FPS on CATER, and demonstrates the benefits of multi-hop reasoning. In addition, the proposed Multi-hop Transformer uses an iterative attention mechanism and produces a step-by-step reasoning chain that improves interpretability.</p><p>Multi-hop models are often difficult to train without supervision for the middle hops. We propose several training methods that can be applied to other tasks to address the problem of lacking a ground truth reasoning chain. In the future, we plan to experiment on real-world video datasets and extend our methods to deal with other complex tasks (such as video QA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research was supported in part by NSF awards: IIS-1703883, <ref type="bibr">IIS-1955404, and IIS-1955365</ref>.  We conduct an ablation study of training methods described in Section 4 in <ref type="table" target="#tab_5">Table 3</ref>. As shown, all proposed training methods are beneficial. 'Dynamic Stride' gives the model more flexibility whereas 'Min 5 Hops' constrains the model to perform a reasonable number of steps of reasoning. 'Hop 1 Loss', 'Hop 2 Loss', 'Frame Loss' and 'Teacher Forcing' stress the importance of the correctness of the first 2 hops to avoid error propagation. 'Debias Loss' is the most effective one by contrastively inducing the latent space to capture information that is maximally useful to the task at hand.  We then study how would different choices of the sub-components of our method affect the Snitch Localization performance. In <ref type="table" target="#tab_7">Table 4</ref>:</p><p>? 'MHT (both Masked)': This refers to using our Hopper-multihop but replacing the original U input to Transformer f with a masked version. In this way, both Transformer f and Transformer s have 'Masking()' applied beforehand.</p><p>? 'MHT (no Gating)': This refers to using our Hopper-multihop but removing the Attentional Feature-based Gating (line 11 in Algorithm 1) inside of MHT.</p><p>? 'MHT (no Tracking)': This refers to using our Hopper-multihop but entirely removing the Hungarian tracking module. Thus, MHT directly takes in unordered object representations as inputs.</p><p>? 'MHT (mask out LAST)': This refers to taking our trained Hopper-multihop, masking out the representation of the most attended object in the last hop by zeros, and then making predictions. This is to verify whether the most attended object in the last hop is important for the final Snitch Localization prediction task.</p><p>? 'MHT (mask out ALL)': Similar to the above, 'MHT (mask out ALL)' refers to taking our trained Hopper-multihop, masking out the representations of the most attended objects in all hops by zeros, and then making predictions. This is to verify how important are the most attended objects in all hops that are identified by our Hopper-multihop. <ref type="table" target="#tab_7">Table 4</ref>, all of these ablations give worse performance, thus, indicating that our motivations for these designs are reasonable (see Section 4). Recall that in <ref type="table" target="#tab_3">Table 2</ref>, 'DETR + Hungarian' (without MHT) has only 37.2% Top-1 accuracy on CATER-h (learning a perfect object detector or tracker is not the focus of this paper). This highlights the superiority of our MHT as a reasoning model, and suggests that MHT has the potential to correct mistakes from the upstream object detector and tracker, by learning more robust object representations during the process of learning the Snitch Localization task. Masking out the most attended object identified by our Hopper-multihop in the last hop only has 32.49% Top-1 accuracy. Masking out all of the most attended objects from all hops only has 11.68% Top-1 accuracy. Such results reassure us about the interpretability of our method.  In <ref type="table" target="#tab_9">Table 5</ref>, we compare the number of parameters of our Hopper-multihop with alternative methods. Our proposed method is the most efficient one in terms of the number of parameters. This is because of the iterative design embedded in MHT. Unlike most existing attempts on using Transformer that stack multiple encoder and decoder layers in a traditional way, MHT only has one layer of Transformer f and Transformer s . As multiple iterations are applied to MHT, parameters of Transformer f and Transformer s from different iterations are shared. This iterative transformer design is inspired by previous work <ref type="bibr" target="#b29">Locatello et al. (2020)</ref>. This design saves parameters, accumulates previously learned knowledge, and adapts to varying number of hops, e.g., some require 1 hop and some require more than 1 hop (e.g, 5 hops). Because for these videos that tentatively only require 1 hop, stacking multiple layers of Transformer such as 5 might be wasteful and not necessary, our design of MHT could address such issue, being more parameter-efficient. We also report the GFLOPs comparison. Given that the FLOPs for MHT depends on the number of hops predicted for a video, we report the average number of FLOPs for the CATER-h test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PARAMETER COMPARISON</head><p>C DIAGNOSTIC ANALYSIS  We evaluate the 'hopping' ability of the proposed Multi-hop Transformer in <ref type="table" target="#tab_11">Table 6</ref>. The prediction is made by our Hopper-multihop that requires at least 5 hops of reasoning unless not possible. For each test video, we compute the ground truth number of hops required by this video and obtain the number of hops that Hopper actually runs. In the table, we provide the ground truth count and predicted count of test videos that require 1 hop, 2 hops, 3 hops, 4 hops, and equal or greater than 5 hops. Since the numbers are close, we further compute Jaccard Similarity (range from 0 to 1 and higher is better) to measure the overlapping between the ground truth set of the test videos and predicted set of the test videos. According to these metrics, our proposed Hopper-multihop functionally performs the correct number of hops for almost all test videos.  In <ref type="table" target="#tab_12">Table 7</ref>, we show the number of times hop h mostly attends to frame t. The results are obtained from Hopper-multihop on CATERh, for those 1025 test videos predicted with ? 5 hops shown in <ref type="table" target="#tab_11">Table 6</ref>. In <ref type="figure">Figure 4</ref>, we plot the index of frame of the most attended object identified by each hop (conveying the same meaning as <ref type="table" target="#tab_12">Table 7</ref>). The transparency of the dot denotes the normalized frequency of that frame index for that particular hop.</p><p>We can observe that: (1) Hop 3 to 6 tend to attend to later frames, and this is due to lacking supervision for the intermediate hops.</p><p>As we discussed in Section 5, multihop model is hard to train in general when the ground truth reasoning chain is missing during training <ref type="bibr">(Dua et al., 2020;</ref><ref type="bibr" target="#b11">Chen &amp; Durrett, 2018;</ref><ref type="bibr" target="#b21">Jiang &amp; Bansal, 2019;</ref><ref type="bibr" target="#b47">Wang et al., 2019a)</ref>. Researchers tend to use ground truth reasoning chain as supervision when they train a multi-hop model <ref type="bibr" target="#b35">(Qi et al., 2019;</ref><ref type="bibr">Ding et al., 2019;</ref>. The results reconfirm that, without supervision for the intermediate steps, it is not easy for a model to automatically figure out the ground truth reasoning chain; (2) MHT has learned to predict the next frame of the frame that is identified by hop 1, as the frame that hop 2 should attend to; (3) there are only 9 videos predicted with more than 5 hops even though we only constrain the model to perform at least 5 hops (unless not possible). Again, this is because no supervision is provided for the intermediate hops. As the Snitch Localization task itself is largely focused on the last frame of the video, without supervision for the intermediate hops, the model tends to "look at" later frames as soon as possible. These results suggest where we can improve for the current MHT, e.g., one possibility is to design self-supervision for each intermediate hop. In <ref type="figure" target="#fig_3">Figure 5</ref>, we present the comparative diagnostic analysis of the performance in terms of when snitch becomes last visible. We bin the test set using the frame index of when snitch becomes last visible in the video. For each, we show the test set distribution with the bar plot, the performance over that bin using the line plot, and performance of that model on the full test set with the dashed line. We find that for Tracking (DaSiamRPN) and TSM, the Snitch Localization performance drops if the snitch becomes not visible earlier in the video. Such phenomenon, though still exists, but is alleviated for Hopper-multihop. We compute the standard deviation (SD) and coefficient of variation (CV). Both are measures of relative variability. The higher the value, the greater the level of dispersion around the mean. The values of these metrics as shown in <ref type="figure" target="#fig_3">Figure 5</ref> further reinforce the stability of our model and necessity of CATER-h dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 COMPARATIVE DIAGNOSTIC ANALYSIS ACROSS FRAME INDEX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXTRA QUALITATIVE RESULTS</head><p>In <ref type="figure">Figure 6</ref>, we visualize the attention weights per hop and per head from Transformer s to showcase the hops performed by Hopper-multihop for video 'CATERh_054110' (the one in <ref type="figure" target="#fig_1">Figure 3</ref>) in details. Please see <ref type="figure" target="#fig_4">Figure 7</ref> <ref type="bibr">, 8, 9, 10, and 11</ref> for extra qualitative results from Hopper-multihop. We demonstrate the reasoning process for different cases (i.e., 'visible', 'occluded', 'contained', 'contained recursively', and 'not visible very early in the video'). <ref type="figure">Figure 6</ref>: Visualization of attention weights &amp; interpretability of our model. In (a), we highlight object(s) attended in every hop from Hopper-multihop (frame border is colored accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). In (b), we visualize the attention weights per hop (the smaller attention weight that an object has, the larger opacity is plotted for that object entity). As shown, Hopper-multihop performs 5 hops of reasoning for the video 'CATERh_054110'. Our model performs reasoning by hopping over frames and meanwhile selectively attending to objects in the frame. Please zoom in to see the details. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E TRACK REPRESENTATION VISUALIZATION</head><p>Please see <ref type="figure" target="#fig_0">Figure 12</ref> for visualization of the object track representations of video 'CATERh_054110' (attention weights from Hopper-multihop for this video are shown in <ref type="figure">Figure 6</ref>). Hopper utilizes tracking-integrated object representations since tracking can link object representations through time and the resulting representations are more informative and consistent. As shown in the figure, the tracks that are obtained from our custom Hungarian algorithm that are competitive. Our model Hopper-multihop takes in the best-effort object track representations (along with the coarsegrained frame track) as the source input to the Multi-hop Transformer, and then further learns the most useful and correct task-oriented track information implicitly (as shown in <ref type="figure">Figure 6</ref>).  Objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F FAILURE CASES</head><p>We present a sample of the failure cases of Hopper-multihop in <ref type="figure" target="#fig_0">Figure 15</ref>. Generally, a video is more difficult if: (1) there are similar looking objects present simultaneously (especially if the object is similar to snitch or another cone in the video); (2) wrong hop 1 or 2 identified; (3) critical moments are occluded (e.g. in <ref type="figure" target="#fig_0">Figure 15</ref>, when the snitch becomes occluded, it is contained by the brown cone); (4) complex object interactions such as recursive containment along with container moving (since such case usually has the last visible snitch very early). Hopper-multihop fails in the first scenario due to the error made by the object representation and detection module, which can be avoided by using a fine-tuned object detector model. Hopper-multihop fails in the second scenario can attribute to the error made by the object detector, tracker, our heuristics, or capability of the inadequately-trained Multi-hop Transformer. The third scenario is not easy even for humans under 1 FPS, thus increasing FPS with extra care might ease the problem. The last scenario requires more sophisticated multi-step reasoning, thus changing the minimal number of hops of the Multi-hop Objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.</p><p>Transformer into a larger number with self-supervision for the intermediate hops to handle the long hops should help in solving this scenario. Overall, an accurate backbone, object detector, tracking method, or the heuristics to determine visibility and the last visible snitch's immediate container (or occluder) will help improve the performance of Hopper-multihop. We would like to focus on enhancing Hopper-multihop for these challenges and verify our hypothesis in our future work. G THE CATER-H DATASET G.1 BASICS: CATER CATER (Girdhar &amp; Ramanan, 2020) provides a diagnostic video dataset that requires spatial and temporal understanding to be solved. It is built against models that take advantage of wrong scene biases. With fully observable and controllable scene bias, the 5, 500 videos in CATER are rendered synthetically at 24 FPS (300-frame 320x240px) using a library of standard 3D objects: 193 different object classes in total which includes 5 object shapes (cube, sphere, cylinder, cone, snitch) in 3 sizes (small, medium, large), 2 materials (shiny metal and matte rubber) and 8 colors. Every video has a small metal snitch (see <ref type="figure" target="#fig_0">Figure 1</ref>). There is a large "table" plane on which all objects are placed. At a high level, the dynamics in CATER videos are in analogy to the cup-and-balls magic routine 4 . A subset of 4 atomic actions ('rotate', 'pick-place', 'slide' and 'contain') is afforded by each object. See Appendix G.2 for definition of the actions. Note that 'contain' is only afforded by cone and recursive containment is possible, i.e., a cone can contain a smaller cone that contains another object. Every video in CATER is split into several time slots, and every object in this video randomly performs an action in the time slot (including 'no action'). Objects and actions vary across videos. The "table" plane is divided into 6 ? 6 grids (36 rectangular cells), and the Snitch <ref type="figure" target="#fig_0">Figure 10</ref>: We visualize the attention weights per hop and per head from Transformers in our Hoppermultihop. Objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.</p><p>Localization task is to determine the grid that the snitch is in at the end of the video, as a single-label classification task. The task implicitly requires the understanding of object permanence because objects could be occluded or contained (hidden inside of) by another object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 DEFINITION OF ACTIONS</head><p>We follow the definition of the four atomic actions in Girdhar &amp; Ramanan (2020). Specifically:</p><p>1. 'rotate': the 'rotate' action means that the object rotates by 90?about its perpendicular or horizontal axis, and is afforded by cubes, cylinders and the snitch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 DATASET GENERATION PROCESS</head><p>The generation of the CATER-h dataset is built upon the CLEVR <ref type="bibr" target="#b22">(Johnson et al., 2017a)</ref> and CATER (Girdhar &amp; Ramanan, 2020) codebases. Blender is used for rendering. The animation setup is the same as the one in CATER. A random number of objects with random parameters are spawned at random locations at the beginning of the video. They exist on a 6 ? 6 portion of a 2D plane with the global origin in the center. Every video has a snitch, and every video is split into several time slots. Each action is contained within its time slot. At the beginning of each slot, objects are randomly selected to perform a random action afforded by that object (with no collision ensured). Please refer to Girdhar &amp; Ramanan (2020) for more animation details.</p><p>In order to have a video dataset that emphasizes on recognizing the effect of the temporal variations on the state of the world, we set roughly equal number of video samples to have the last visible snitch along the temporal axis. In order to obtain such a dataset, we generated a huge number of videos, computed the frame index of the last visible snitch in every video under 1 FPS (13 frames per video). Then, for every frame index i, we obtained the set of videos whose last visible snitch is at frame index i, and finally, we randomly chose 500 and more videos from this set and discarded the rest. Eventually, the total number of videos in CATER-h is 7, 080. We split the data randomly in 70 : 30 ratio into a training and test set, resulting in 5, 624 training samples and 1, 456 testing samples.</p><p>G.4 CATER-H V.S. CATER <ref type="figure" target="#fig_0">Figure 13</ref> compares the CATER-h dataset and the CATER dataset. <ref type="figure" target="#fig_0">Figure 12</ref>: Tracking-integrated object representation visualization. Hopper utilizes tracking-integrated object representations since tracking can link object representations through time and the resulting representations are more informative and consistent. We visualize the object track representations of video 'CATERh_054110' (attention weights from Hopper-multihop for this video are shown in <ref type="figure">Figure 6</ref>). Here, every column is for a track and every row is for a frame. The bounding box and object class label computed from the object representation are plotted (404 is the object class label for ?, i.e., none object, and 140 is the object class label for the snitch). As shown in the figure, the tracks that are obtained from our designed Hungarian algorithm are not perfect but acceptable since having perfect tracking here is not the goal of this paper. Our model Hoppermultihop takes in the (imperfect) object track representations (along with the coarse-grained frame track) as the source input to the Multi-hop Transformer, and then further learns the most useful and correct task-oriented track information implicitly (as shown in <ref type="figure">Figure 6</ref>). Hopper-multihop preforms 5 hops of reasoning for this video; objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color. <ref type="figure" target="#fig_0">Figure 14</ref> shows the data distribution over classes in CATER-h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5 TRAIN/TEST DISTRIBUTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.6 OTHER POTENTIAL DATASET BIAS</head><p>As shown in <ref type="table" target="#tab_3">Table 2</ref>, 'Hopper-transformer (last frame)' still has a relatively high accuracy on CATER-h. We hypothesize that the reason it has 41.8% Top-1 accuracy on CATER-h might be due <ref type="figure" target="#fig_0">Figure 13</ref>: Histogram of the frame index of the last visible snitch of every video. We find that CATER is highly imbalanced for the Snitch Localization task in terms of the temporal cues: e.g., snitch is entirely visible at the end of the video for 58% samples. This temporal bias results in high-accuracy even if it ignores all but the last frame of the video. Our dataset CATER-h addresses this issue with a balanced dataset.</p><p>to other dataset biases (apart from the snitch grid distribution bias, and temporal bias that CATER has). Upon further investigation, we identify one type of additional bias, the "cone bias", i.e., snitch can only be contained by a cone in the videos of CATER and CATER-h.</p><p>In order to verify the existence of the "cone bias", we compute the accuracy if we make a random guess among the grids of cones that are not covered by any other cones, for all test videos whose snitch is covered in the end. This gives us 48.26% Top-1 accuracy. This shows that the "cone bias" does exist in the dataset. The so-called "cone bias" comes from the nature of objects used in CATER and the fact that only the cone can carry the snitch (thus, it is closer to a feature of the dataset, rather than being a "bias" per se). Furthermore, because of the animation rules of CATER, there might exist other dataset biases, such as bias in terms of object size and shape, etc., which are hard to discover and address. This highlights the glaring challenge in building a fully unbiased (synthetic or real) dataset. CATER-h addresses the temporal bias that CATER has. A model has to perform long-term spatiotemporal reasoning in order to have a high accuracy on CATER-h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H IMPLEMENTATION DETAILS H.1 HO P P E R</head><p>We introduce the implementation of Hopper-multihop in this section. For both training and testing, we only used 1 FPS (frames per second) to demonstrate the efficiency of our approach. This means we only have 13 frames per video. Note that we trained the video query representation and recognition part (Multi-hop Transformer along with the final MLP) end to end.</p><p>The CNN backbone we utilized is the pre-trained ResNeXt-101 <ref type="bibr" target="#b54">(Xie et al., 2017)</ref> model from <ref type="bibr" target="#b30">Ma et al. (2018)</ref>. We trained DETR <ref type="bibr" target="#b8">(Carion et al., 2020</ref>) 5 on LA-CATER <ref type="bibr" target="#b38">(Shamsian et al., 2020)</ref> which is a dataset with generated videos following the same configuration to the one used by CATER, but additional ground-truth object bounding box location and class label annotations are available <ref type="bibr" target="#b38">(Shamsian et al. (2020)</ref> predicts the bounding box of snitch in the video given the supervision of the bounding box of the snitch in 300 frames). We followed the settings in <ref type="bibr" target="#b8">Carion et al. (2020)</ref> to set up and train DETR, e.g., stacking 6 transformer encoder layers and 6 transformer decoder layers, utilizing the object detection set prediction loss and the auxiliary decoding loss per decoder layer. d is 256, N is 10 and C is 193. The initial N object query embeddings are learned. The MLP for recognizing the object class label is one linear layer and for obtaining the object bounding box is a MLP with 2 hidden layers with d neurons. After DETR was trained, we tested it on CATER to obtain the object representations, predicted bounding boxes and class labels. For tracking, we set ? c = 1, ? b = 0 because under a low FPS, using the bounding boxes for tracking is counterproductive, and it yielded reasonable results. Then, we trained the video recognition part, i.e., Multi-hop Transformer along with the final MLP, end to end with Adam <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2014)</ref> optimizer. The final MLP we used is one linear layer that transforms the video query representation e of dimension d into the grid class logits. The initial learning rate was set to 10 ?4 and weight decay to 10 ?3 . The batch size was 16. The number of attention heads for DETR was set to 8 and for the Multi-hop Transformer was set to 2. Transformer dropout rate was set to 0.1. We used multi-stage training with the training methods proposed. Moreover, we found that DETR tends to predict a snitch for every cone on the "table" plane when there is no visible snitch in that frame. To mitigate this particular issue of the DETR object detector trained on <ref type="bibr" target="#b38">Shamsian et al. (2020)</ref>, we further compute an object visibility map V ? R N T ?1 , which is a binary vector and determined by a heuristic: an object is visible if the bounding box of the object is not completely contained by any bounding box of another object in that frame. The 'Masking()' function uses V by considering only the visible objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 DESCRIPTION OF REPORTED CATER BASELINES</head><p>For CATER, we additionally compare our results with the ones reported by Girdhar &amp; Ramanan (2020) from TSN , I3D <ref type="bibr" target="#b9">(Carreira &amp; Zisserman, 2017)</ref>, NL  as well as their LSTM variants. Specifically, TSN (Temporal Segment Networks) was the top performing method that is based on the idea of long-range temporal structure modeling before TSM and TPN. Two modalities were experimented with TSN, i.e., RGB or Optical Flow (that captures local temporal cues). I3D inflates 2D ConvNet into 3D for efficient spatiotemproal feature learning. NL (Non-Local Networks) proposed a spacetime non-local operation as a generic building block for capturing long-range dependencies for video classification. In order to better capture the temporal information for these methods, Girdhar &amp; Ramanan (2020) further experimented with a 2-layer LSTM aggregation that operates on the last layer features before the logits. Conclusions from Girdhar &amp; Ramanan (2020) are: (1) TSN ends up performing significantly worse than I3D instead of having similar performance which contrasts with standard video datasets;</p><p>(2) the optical flow modality does not work well as the Snitch Localization task requires recognizing objects which is much harder from the optical flow; (3) more sampling from the video would give higher performance; (4) LSTM for more sophisticated temporal aggregation leads to a major improvement in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 BASELINES</head><p>We introduce the implementation of the baselines that we experimented with in this section. First, for our Hungarian tracking baseline, for every test video, we obtain the snitch track (based on which track's first object has snitch as its label) produced from our Hungarian algorithm, and project the center point of the bounding box of the last object in that track to the 3D plane (and eventually, the grid class label) by using a homography transformation between the image and the 3D plane (same method used in <ref type="bibr">Girdhar &amp; Ramanan (2020)</ref>). We also try with the majority vote, i.e., obtain the snitch track as the track who has the highest number of frames classified as snitch. We report the majority vote result in <ref type="table" target="#tab_2">Table 1</ref> and 2 because it is a more robust method. The results of using the first frame of our Hungarian tracking baseline are 31.8% Top-1, 40.2% Top-5, 2.7 L 1 on CATER, and 28.2% Top-1, 36.3% Top-5, 2.8 L 1 on CATER-h.</p><p>We used the public available implementation provided by the authors <ref type="bibr" target="#b55">(Yang et al., 2020)</ref> for TSM and TPN. Models were defaultly initialized by pre-trained models on ImageNet <ref type="bibr" target="#b16">(Deng et al., 2009</ref>  <ref type="bibr">, 2017)</ref>, which are also the the protocols used in , as well as TPN-101 (I3D-101 backbone, i.e. 3D ResNet-101) with multi-depth pyramid and the parallel flow that they used to obtain results on Kinetics (their best-performing setting of TPN) <ref type="bibr" target="#b9">(Carreira &amp; Zisserman, 2017)</ref>. Specifically, the augmentation of random crop, horizontal flip and a dropout of 0.5 were adopted to reduce overfitting. BatchNorm (BN) was not frozen. A momentum of 0.9, a weight decay of 0.0001 and a synchronized SGD with the initial learning rate 0.01, which would be reduced by a factor of 10 at 75, 125 epochs (150 epochs in total). The weight decay for TSM was set to 0.0005. TPN used auxiliary head, spatial convolutions in semantic modulation, temporal rate modulation and information flow <ref type="bibr" target="#b55">(Yang et al., 2020)</ref>. For SINet, we used the implementation provided by <ref type="bibr" target="#b30">Ma et al. (2018)</ref>. Specifically, image features for SINet were obtained from a pre-trained ResNeXt-101 <ref type="bibr" target="#b54">(Xie et al., 2017)</ref> with standard data augmentation (randomly cropping and horizontally flipping video frames during training). Note that the image features used by SINet are the same as the ones used in our Hopper. The object features were generated from a Deformable RFCN <ref type="bibr" target="#b14">(Dai et al., 2017)</ref>. The maximum number of objects per frame was set to 10. The number of subgroups of higher-order object relationships (K) was set to 3. SGD with Nesterov momentum were used as the optimizer. The initial learning rate was 0.0001 and would drop by 10x when validation loss saturates for 5 epochs. The weight decay was 0.0001 and the momentum was 0.9. The batch size was 16 for these 3 baselines. Transformer, Hopper-transformer, and Hopper-sinet used the Adam optimizer with a total of 150 epochs, a initial learning rate of 10 ?4 , a weight decay of 10 ?3 , and a batch size of 16. Same as our model, the learning rate would drop by a factor of 10 when there has been no improvement for 10 epochs on the validation set. The number of attention heads for the Transformer (and Hopper-transformer) was set to 2, the number of transformer layers was set to 5 to match the 5 hops in our Multi-hop Transformer, and the Transformer dropout rate was set to 0.1. For OPNet related experiments, we used the implementation provided from authors <ref type="bibr" target="#b38">(Shamsian et al., 2020)</ref>. We verified we could reproduce their results under 24 FPS on CATER by using their provided code and trained models.</p><p>For the Random baseline, it is computed as the average performance of random scores passed into the evaluation functions <ref type="bibr">Girdhar &amp; Ramanan (2020)</ref>. For the Tracking baseline, we use the DaSiamRPN implementation from Girdhar &amp; Ramanan (2020) 6 . Specifically, the ground truth information of the starting position of the snitch was first projected to screen coordinates using the render camera parameters. A fixed size box around the snitch is defined to initialize the tracker, and run the tracker until the end of the video. At the last frame, the center point of the tracked box is projected to the 2D plane by using a homography transformation between the image and the 2D plane, and then converted to the class label. With respect to TSN, I3D, NL and their variants, the results were from Girdhar &amp; Ramanan (2020), and we used the same train, val split as theirs when obtaining our results on CATER.   <ref type="bibr" target="#b59">Ng et al., 2015)</ref>. As a natural extension to handle the video data, 3D ConvNets were later proposed <ref type="bibr" target="#b20">(Ji et al., 2012;</ref><ref type="bibr" target="#b45">Taylor et al., 2010)</ref> but with the issue of inefficiency and huge increase in parameters. Using both RGB and optical flow modalities, Twostream networks <ref type="bibr" target="#b40">(Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr">Feichtenhofer et al., 2016)</ref> and Two-Stream Inflated 3D ConvNets (I3D) <ref type="bibr" target="#b9">(Carreira &amp; Zisserman, 2017)</ref> were designed. With the emphasis on capturing the temporal structure of a video, TSN , TRN <ref type="bibr" target="#b61">(Zhou et al., 2018)</ref>, TSM  and TPN <ref type="bibr" target="#b61">(Zhou et al., 2018)</ref> were successively proposed and gained considerate improvements. Recently, attention mechanism and Transformer design <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> have been utilized for more effective and transparent video understanding. Such models include Nonlocal Neural Networks (NL)  that capture long-range spacetime dependencies, SINet <ref type="bibr" target="#b30">(Ma et al., 2018)</ref> that learns higher-order object interactions and Action Transformer (Girdhar et al., 2019) that learns to attend to relevant regions of the actor and their context. Nevertheless, instead of the reasoning capabilities, existing benchmarks and models for video understanding and analysis mainly have focused on pattern recognition from complex visual and temporal input.</p><p>Visual reasoning from images. To expand beyond image recognition and classification, research on visual reasoning has been largely focused on Visual Question Answering (VQA). For example, a diagnostic VQA benchmark dataset called CLEVR <ref type="bibr" target="#b22">(Johnson et al., 2017a)</ref> was built that reduces spatial biases and tests a range of visual reasoning abilities. There have been a few visual reasoning models proposed <ref type="bibr" target="#b37">(Santoro et al., 2017;</ref><ref type="bibr" target="#b34">Perez et al., 2017;</ref><ref type="bibr" target="#b32">Mascharka et al., 2018;</ref><ref type="bibr" target="#b42">Suarez et al., 2018;</ref><ref type="bibr" target="#b0">Aditya et al., 2018)</ref>. For example, inspired by module networks <ref type="bibr" target="#b2">(Andreas et al., 2016b;</ref><ref type="bibr">a)</ref>, <ref type="bibr" target="#b23">Johnson et al. (2017b)</ref> propose a compositional model for visual reasoning on CLEVR that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer; both are implemented by neural networks. Evaluated on the CLEVR dataset, Hu et al. <ref type="formula">(2017)</ref> proposed N2NMNs, i.e., End-to-End Module Networks, which learn to reason by directly predicting the network structures while simultaneously learning network parameters. MAC networks <ref type="bibr">(Hudson &amp; Manning, 2018)</ref>, that approach CLEVR by decomposing the VQA problems into a series of attention-based reasoning steps, were proposed by stringing the MAC cells end-to-end and imposing structural constraints to effectively learn to perform iterative reasoning. Further, datasets for realworld visual reasoning and compositional question answering are released such as GQA <ref type="bibr" target="#b18">(Hudson &amp; Manning, 2019b)</ref>. Neural State Machine (Hudson &amp; Manning, 2019a) was introduced for real-world VQA that performs sequential reasoning by traversing the nodes over a probabilistic graph which is predicted from the image.</p><p>Video reasoning. There has been a notable progress for joint video and language reasoning. For example, in order to strengthen the ability to reason about temporal and causal events from videos, the CLEVRER video question answering dataset <ref type="bibr" target="#b58">(Yi et al., 2020)</ref> was introduced being a diagnostic dataset generated under the same visual settings as CLEVR but instead for systematic evaluation of video models. Other artificial video question answering datasets include COG <ref type="bibr" target="#b56">(Yang et al., 2018a)</ref> and MarioQA <ref type="bibr" target="#b33">(Mun et al., 2017)</ref>. There have been also numerous datasets that are based on real-world videos and human-generated questions such as MovieQA <ref type="bibr" target="#b44">(Tapaswi et al., 2016)</ref>, TGIF-QA <ref type="bibr" target="#b19">(Jang et al., 2017)</ref>, TVQA <ref type="bibr" target="#b26">(Lei et al., 2018)</ref> and Social-IQ <ref type="bibr" target="#b60">(Zadeh et al., 2019)</ref>. Moving beyond the question and answering task, CoPhy <ref type="bibr" target="#b4">(Baradel et al., 2019</ref>) studies physical dynamics prediction in a counterfactual setting and a small-sized causality video dataset <ref type="bibr">(Fire &amp; Zhu, 2017)</ref> was released to study the causal relationships between human actions and hidden statuses. To date, research on the general video understanding and reasoning is still limited. Focusing on both video reasoning and a general video recognition and understanding, we experimented on the recently released CATER dataset (Girdhar &amp; Ramanan, 2020), a synthetic video recognition dataset which is also built upon CLEVR focuses on spatial and temporal reasoning as well as localizing particular object of interest. There also has been significant research in object tracking, often with an emphasis on occlusions with the goal of providing object permanence <ref type="bibr" target="#b5">(Bewley et al., 2016;</ref><ref type="bibr" target="#b53">Wojke et al., 2017;</ref><ref type="bibr" target="#b49">Wang et al., 2019b)</ref>. Traditional object tracking approaches have focused on the fine-grained temporal and spatial understanding and often require expensive supervision of location of the objects in every frame <ref type="bibr" target="#b38">(Shamsian et al., 2020)</ref>. We address object permanence and video recognition on CATER with a model that performs tracking-integrated object-centric reasoning for localizing object of interest.</p><p>Multi-hop reasoning. Reasoning systems vary in expressive power and predictive abilities, which include systems focus on symbolic reasoning (e.g., with first order logic), probabilistic reasoning, causal reasoning, etc <ref type="bibr" target="#b6">(Bottou, 2014)</ref>. Among them, multi-hop reasoning is the ability to reason with information collected from multiple passages to derive the answer <ref type="bibr" target="#b47">(Wang et al., 2019a)</ref>. Because of the desire for chains of reasoning, several multi-hop datasets and models have been proposed for nature language processing tasks <ref type="bibr" target="#b17">(Dhingra et al., 2020;</ref><ref type="bibr">Dua et al., 2019;</ref><ref type="bibr" target="#b51">Welbl et al., 2018;</ref><ref type="bibr" target="#b43">Talmor &amp; Berant, 2018;</ref><ref type="bibr" target="#b57">Yang et al., 2018b)</ref>. For example, <ref type="bibr" target="#b15">Das et al. (2016)</ref> introduced a recurrent neural network model which allows chains of reasoning over entities, relations, and text.  proposed a two-stage model that identifies intermediate discrete reasoning chains over the text via an extractor model and then separately determines the answer through a BERT-based answer module. <ref type="bibr" target="#b47">Wang et al. (2019a)</ref> investigated that whether providing the full reasoning chain of multiple passages, instead of just one final passage where the answer appears, could improve the performance of the existing models. Their results demonstrate the existence of the potential improvement using explicit multi-hop reasoning. Multi-hop reasoning gives us a discrete intermediate output of the reasoning process, which can help gauge model's behavior beyond just final task accuracy . Favoring the benefits that multi-hop reasoning could bring, in this paper, we developed a video dataset that explicitly requires aggregating clues from different spatiotemporal parts of the video and a multi-hop model that automatically extracts a step-by-step reasoning chain. Our proposed Multi-hop Transformer improves interpretability and imitates a natural way of thinking.</p><p>The iterative attention-based neural reasoning <ref type="bibr" target="#b29">(Locatello et al., 2020;</ref><ref type="bibr">Hudson &amp; Manning, 2019a)</ref> with a contrastive debias loss further offers robustness and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J MHT ARCHITECTURE</head><p>We illustrate the architecture of the proposed Multi-hop Transformer (MHT) in <ref type="figure" target="#fig_0">Figure 16</ref>.  <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref>. 'Attentional Feature-based Gating' corresponds to line 11. 'Updated Attention Candidates' is Uupdate ? R N T ?d . 'Masking' corresponds to line 12. Its output, 'Masked Attention Candidates' is Umask ? R N T ?d (however, certain tokens out of these N T tokens are masked and will have 0 attention weights). The last 'Layer Norm' after the Transformers corresponds to line 16. H denotes the total number of hops, i.e., total number of iterations, and it varies across videos. Please refer to Section 4 for details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Snitch Localization in CATER</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative result &amp; interpretability of our model. We highlight the object attended per hop and per head from Hopper-multihop. The frame border of attended object is colored based on the hop index (accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). The bounding box of the most attended object in each hop shares the same color as the color of the hop index. Please zoom in to see the details. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Diagnostic analysis of the performance in terms of when snitch becomes last visible in the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>We visualize the attention weights per hop and per head from Transformers in our Hopper-multihop. Objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>We visualize the attention weights per hop and per head from Transformers in our Hopper-multihop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>We visualize the attention weights per hop and per head from Transformers in our Hopper-multihop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>We visualize the attention weights per hop and per head from Transformers in our Hoppermultihop. Objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) CATER-h Train Set(b) CATER-h Test SetFigure 14: Data distribution over classes in CATER-h.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 :</head><label>16</label><figDesc>Architecture of the Multi-hop Transformer (MHT) that learns a comprehensive video query representation and meanwhile encourages multi-step compositional long-term reasoning of a spatiotemporal sequence. As inputs to this module, the 'Source Sequence' is [T f , To], where [ , ] denote concatenation; and the 'Target Video Query' is E ? R 1?d . 'Final Video Query Representation' is e ? R 1?d . A ? R N T ?1refers to attention weights from the encoder-decoder multi-head attention layer in Transformers, averaged over all heads. To connect this figure with Algorithm 1, 'Obtain Helper Information &amp; Attention Candidates' refers to line 5 for 'Helper Information' H (or line 7 for the first iteration), and line 9 for the 'Attention Candidates' U ? R N T ?d . Dimensionality of 'Helper Information' H is T ? d for hop 1 and N ? d for the rest of the hops. Transformer f and Transformers are using the original Transformer architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An overview of the Hopper framework. Hopper first obtains frame representations from the input video. Object representations and object tracks are then computed to enable tracking-integrated object-centric reasoning for the Multi-hop Transformer (details in Section 4).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">B. obj ect det ect ion and r epr esentat ion</cell><cell></cell><cell>C. t r acking</cell><cell cols="4">D. video Quer y r epr esentat ion and r ecognit ion</cell></row><row><cell></cell><cell></cell><cell></cell><cell>positional encoded feature maps</cell><cell>Target: object queries</cell><cell>Tracking:</cell><cell cols="2">positional time encoding</cell><cell></cell><cell>...</cell><cell>Target: query video</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Source</cell><cell></cell><cell>Hungarian</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>positional</cell><cell>positional spatial</cell><cell></cell><cell></cell><cell>Between</cell><cell>Track 0</cell><cell>Track 1</cell><cell>Track 2</cell><cell>Track N</cell></row><row><cell></cell><cell>time encoding A. backbone</cell><cell>resolution encoding feature maps</cell><cell cols="2">DETR: End-to-end Transformer based object detector</cell><cell>Frame Matching Algorithm</cell><cell>IMG</cell><cell>OBJ</cell><cell>OBJ</cell><cell>...</cell><cell>OBJ</cell></row><row><cell></cell><cell>Time</cell><cell></cell><cell></cell><cell>object</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>representation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Source Sequence</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CNN</cell><cell>MLP</cell><cell>object object bounding box</cell><cell></cell><cell cols="4">Multi-Hop Transformer Ref. Figure 16</cell></row><row><cell></cell><cell cols="2">: the number of frames : the number of objects</cell><cell></cell><cell>class label</cell><cell></cell><cell></cell><cell></cell><cell>MLP</cell></row><row><cell></cell><cell>: batch size : dimension size</cell><cell>image features</cell><cell></cell><cell></cell><cell></cell><cell cols="2">video query representation</cell><cell></cell><cell>Classification</cell></row><row><cell>. . .</cell><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Summary &amp; discussion. The total loss of the model is a linear combination of hop 1 and hop 2 object &amp; frame loss, contrastive debiasing loss for the last hop, and the final grid classification cross-entropy loss. The object &amp; frame loss for hop 1 and 2 are based on heuristics. The motivation is to provide weak supervision for the early hops to avoid error propagation, as multi-hop model can be difficult to train, without intermediate supervision or when ground truth reasoning chain is not present (as in Hopper)(Dua et al., 2020;<ref type="bibr" target="#b35">Qi et al., 2019;</ref> Ding et al., 2019;<ref type="bibr" target="#b47">Wang et al., 2019a;</ref><ref type="bibr" target="#b11">Chen &amp; Durrett, 2018;</ref><ref type="bibr" target="#b21">Jiang &amp; Bansal, 2019)</ref>. One can use similar ideas as the ones here on other tasks that require multi-hop reasoning (e.g., design self-supervision or task-specific heuristic-based weak supervision for intermediate hops, as existing literature often does).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>). Additionally, Transformer that uses the time-encoded frame track as the source sequence is utilized. Both SINet and Transformer are used to substitute our novel MHT in the Hopper framework in order to demonstrate the effectiveness of the proposed MHT. Section 3.3) in order to have a thorough understanding of how well our tracking component performs. For CATER. we also compare with the results inGirdhar &amp;  Ramanan (2020). Details of the baselines are available in Appendix H. We evaluate models using Top-1 and Top-5 accuracy, as well as mean L 1 distance of the predicted grid cell from the ground truth followingGirdhar &amp; Ramanan (2020). L 1 is cognizant of the grid structure and will penalize confusion between adjacent cells less than those between distant cells. CATER Snitch Localization results (on the test set). The top 3 performance scores are highlighted as: First , Second, Third * . Hopper outperforms existing methods under only 1 FPS.</figDesc><table><row><cell>Methods</cell><cell cols="5">FPS # Frames Top 1 ? Top 5 ? L1 ?</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>2.8</cell><cell>13.8</cell><cell>3.9</cell></row><row><cell>DaSiamRPN (Tracking) (Zhu et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>33.9</cell><cell>40.8</cell><cell>2.4</cell></row><row><cell>Hungarian (Tracking -ours)</cell><cell>-</cell><cell>-</cell><cell>46.0</cell><cell>52.7</cell><cell>1.9</cell></row><row><cell>TSN (RGB) (Wang et al., 2016)</cell><cell>-</cell><cell>3</cell><cell>14.1</cell><cell>38.5</cell><cell>3.2</cell></row><row><cell>TSN (RGB) + LSTM (Wang et al., 2016)</cell><cell>-</cell><cell>3</cell><cell>25.6</cell><cell>67.2</cell><cell>2.6</cell></row><row><cell>TSN (Flow) (Wang et al., 2016)</cell><cell>-</cell><cell>3</cell><cell>9.6</cell><cell>32.3</cell><cell>3.7</cell></row><row><cell>TSN (Flow) + LSTM (Wang et al., 2016)</cell><cell>-</cell><cell>3</cell><cell>14.0</cell><cell>43.5</cell><cell>3.2</cell></row><row><cell>I3D-50 (Carreira &amp; Zisserman, 2017)</cell><cell>5</cell><cell>64</cell><cell>57.4</cell><cell>78.4</cell><cell>1.4</cell></row><row><cell>I3D-50 + LSTM (Carreira &amp; Zisserman, 2017)</cell><cell>5</cell><cell>64</cell><cell>60.2</cell><cell>81.8</cell><cell>1.2</cell></row><row><cell>I3D-50 + NL (Wang et al., 2018)</cell><cell>2.5</cell><cell>32</cell><cell>26.7</cell><cell>68.9</cell><cell>2.6</cell></row><row><cell>I3D-50 + NL + LSTM (Wang et al., 2018)</cell><cell>2.5</cell><cell>32</cell><cell>46.2</cell><cell>69.9</cell><cell>1.5</cell></row><row><cell>TPN-101 (Yang et al., 2020)</cell><cell>2.5</cell><cell>32</cell><cell>65.3  *</cell><cell>83.0</cell><cell>1.09</cell></row><row><cell>TSM-50 (Lin et al., 2019)</cell><cell>1</cell><cell>13</cell><cell>64.0</cell><cell>85.7</cell><cell>0.93</cell></row><row><cell>SINet (Ma et al., 2018)</cell><cell>1</cell><cell>13</cell><cell>21.1</cell><cell>47.1</cell><cell>3.14</cell></row><row><cell>Transformer (Vaswani et al., 2017)</cell><cell>1</cell><cell>13</cell><cell>13.7</cell><cell>39.9</cell><cell>3.53</cell></row><row><cell>Hopper-transformer (last frame)</cell><cell>1</cell><cell>13</cell><cell>61.1</cell><cell>86.6</cell><cell>1.42</cell></row><row><cell>Hopper-transformer</cell><cell>1</cell><cell>13</cell><cell>64.9</cell><cell>90.1  *</cell><cell>1.11</cell></row><row><cell>Hopper-sinet</cell><cell>1</cell><cell>13</cell><cell>69.1</cell><cell>91.8</cell><cell>1.02  *</cell></row><row><cell>Hopper-multihop (our proposed method)</cell><cell>1</cell><cell>13</cell><cell>73.2</cell><cell>93.8</cell><cell>0.85</cell></row><row><cell>Methods</cell><cell cols="5">FPS # Frames Top 1 ? Top 5 ? L1 ?</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>2.4</cell><cell>13.6</cell><cell>3.9</cell></row><row><cell>DaSiamRPN (Tracking) (Zhu et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>17.1</cell><cell>26.1</cell><cell>2.9</cell></row><row><cell>Hungarian (Tracking -ours)</cell><cell>-</cell><cell>-</cell><cell>37.2</cell><cell>44.4</cell><cell>2.3</cell></row><row><cell>TPN-101 (Yang et al., 2020)</cell><cell>2.5</cell><cell>32</cell><cell>50.2</cell><cell>88.3  *</cell><cell>1.46</cell></row><row><cell>TSM-50 (Lin et al., 2019)</cell><cell>1</cell><cell>13</cell><cell>44.0</cell><cell>75.7</cell><cell>1.54</cell></row><row><cell>SINet (Ma et al., 2018)</cell><cell>1</cell><cell>13</cell><cell>18.6</cell><cell>44.3</cell><cell>3.24</cell></row><row><cell>Transformer (Vaswani et al., 2017)</cell><cell>1</cell><cell>13</cell><cell>11.6</cell><cell>34.4</cell><cell>3.49</cell></row><row><cell>Hopper-transformer (last frame)</cell><cell>1</cell><cell>13</cell><cell>41.8</cell><cell>79.3</cell><cell>2.10</cell></row><row><cell>Hopper-transformer</cell><cell>1</cell><cell>13</cell><cell>57.6  *</cell><cell>90.1</cell><cell>1.39  *</cell></row><row><cell>Hopper-sinet</cell><cell>1</cell><cell>13</cell><cell>62.8</cell><cell>91.7</cell><cell>1.25</cell></row><row><cell>Hopper-multihop (our proposed method)</cell><cell>1</cell><cell>13</cell><cell>68.4</cell><cell>87.9</cell><cell>1.09</cell></row></table><note>This means that Hopper-transformer and Hopper-sinet use the same representations of image track and object tracks as our Hopper-multihop. Moreover, we report results from a Random baseline, a Tracking baseline used by Girdhar &amp; Ramanan (2020) (DaSiamRPN), and a Tracking baseline based on our Hungarian algorithm (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>CATER-h Snitch Localization results (on the test set). The top 3 performance scores are highlighted as: First , Second, Third</figDesc><table /><note>* . Hopper outperforms existing methods under only 1 FPS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop reading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019. Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2625-2634, 2015. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. Dheeru Dua, Sameer Singh, and Matt Gardner. Benefits of intermediate annotations in reading comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5627-5634, 2020. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016. Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vincent, Christopher Pal, and Jan Kautz. Improving landmark localization with semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1546-1555, 2018. Hop 2 Frame Teacher Debias Top 1 ? Top 5 ? L 1 ?</figDesc><table><row><cell>Stride</cell><cell>Hops</cell><cell>Loss</cell><cell>Loss</cell><cell>Loss</cell><cell>Forcing</cell><cell>Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.41</cell><cell>87.89</cell><cell>1.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.65</cell><cell>87.75</cell><cell>1.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63.32</cell><cell>86.94</cell><cell>1.23</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.88</cell><cell>86.57</cell><cell>1.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.92</cell><cell>88.19</cell><cell>1.20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59.48</cell><cell>84.58</cell><cell>1.37</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59.11</cell><cell>86.20</cell><cell>1.37</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.27</cell><cell>84.43</cell><cell>1.39</cell></row></table><note>Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion for video action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1933-1941, 2016. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE international conference on computer vision, pp. 6202- 6211, 2019. Amy Fire and Song-Chun Zhu. Inferring hidden statuses and actions in video by causal reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 48-56, 2017. Rohit Girdhar and Deva Ramanan. CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning. In ICLR, 2020. Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video action transformer network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 244-253, 2019. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something something" video database for learning and evaluating visual common sense. In ICCV, volume 1, pp. 5, 2017. Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 6546-6555, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In Proceedings of the IEEE International Conference on Computer Vision, pp. 804-813, 2017. Drew Hudson and Christopher D Manning. Learning by abstraction: The neural state machine. In Advances in Neural Information Processing Systems, pp. 5903-5916, 2019a. Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. arXiv preprint arXiv:1803.03067, 2018.A ABLATION STUDY Dynamic Min 5 Hop 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Ablation Study of Hopper training methods. We gradually add training methods described in Section 4, i.e., dynamic hop stride, minimal 5 hops of reasoning, auxiliary hop 1 object loss, auxiliary hop 2 object loss, auxiliary frame loss, teacher forcing, and contrastive debias loss via masking out, onto the base Hopper-multihop model. The results are obtained from the CATER-h test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Ablation study &amp; comparative results of analyzing components of our method (on CATER-h test set).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Parameter and FLOPs comparison of our Hopper-multihop to alternative methods. (M) indicates millions. Results of the methods on CATER-h test set is also listed.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Diagnostic analysis of the Multi-hop Transformer in terms of the 'hopping' ability (# hops performed).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Hop Index vs. Frame Index: the number of times hop h mostly attends to frame t (results are obtained from Hopper-multihop on CATER-h). See details in Appendix C.1. Hop Index vs. Frame Index: we plot the index of frame of the most attended object identified by each hop. Each hop has its unique color, and the transparency of the dot denotes the normalized frequency of that frame index for that particular hop.</figDesc><table><row><cell>Figure 4:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>). The originalResNet (He et al., 2016)  serves as the 2D backbone, and the inflated ResNet(Feichtenhofer et al., 2019)  as the 3D backbone network. We used the default settings of TSM and TPN provided by<ref type="bibr" target="#b55">Yang et al. (2020)</ref>, i.e., TSM-50 (2D ResNet-50 backbone) settings that they used to obtain results on Something-Something (Goyal et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Figure 15: Failure cases. Hopper produces wrong Top-1, Top-5 prediction and terrible L1 results for these failure cases. Similarly, we highlight the attended object per hop and per head (Hop1, Hop2, Hop3, Hop4, and Hop5). Case (a). 'CATERh_048295': the occlusion has made the Snitch Localization task extremely difficult since when the snitch got occluded it was contained by the brown cone. Meanwhile, Hopper fails to attend to the immediate container of the last visible snitch (should be the brown cone at frame 5) in Hop 2. Case (b). 'CATERh_022965': the snitch was not visible very early in the video (at frame 3), the recursive containment, as well as the presence of two similar looking cones have made the task extremely difficult. Hopper fails to attend to the correct object in Hop 3 (should be the yellow cone).</figDesc><table><row><cell>I RELATED WORK (FULL VERSION)</cell></row><row><cell>In this section, we provide detailed discussion of related work. Our work is generally related to the</cell></row><row><cell>following recent research directions.</cell></row><row><cell>Video understanding &amp; analysis. With the release of large-scale datasets such as Kinetics (Carreira</cell></row><row><cell>&amp; Zisserman, 2017), Charades (Sigurdsson et al., 2016), and Something something (Goyal</cell></row><row><cell>et al., 2017), the development of video representation has matured quickly in recent years.</cell></row><row><cell>Early approaches use deep visual features from 2D ConvNets with LSTMs for temporal</cell></row><row><cell>aggregation (Donahue et al., 2015; Yue-Hei</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">?, i.e., none object, will be predicted if the number of objects in an image is less than N .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The heuristic is L1 distance to find out that in the immediate frame which object's bounding box bottom midpoint location is closest to that of the last visible snitch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://en.wikipedia.org/wiki/Cups_and_balls</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. 'pick-place': The 'pick-place' action means the object is picked up into the air along the perpendicular axis, moved to a new position, and placed down. This is afforded by all objects.3. 'slide': the 'slide' action means the object is moved to a new location by sliding along the bottom surface, and is also afforded by all objects.4. 'contain': 'contain' is a special operation, only afforded by the cones, in which a cone is pick-placed on top of another object, which may be a sphere, a snitch or even a smaller cone. This allows for recursive containment, as a cone can contain a smaller cone that contains another object. Once a cone 'contains' an object, the 'slide' action of the cone effectively slides all objects contained within the cone. This holds until the top-most cone is pick-placed to another location, effectively ending the containment for that top-most cone.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/facebookresearch/detr</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/rohitgirdhar/CATER</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Explicit reasoning over end-to-end neural architectures for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somak</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08896</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01705</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representing the existence and the location of hidden objects: Object permanence in 6-and 8-month-old infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renee</forename><surname>Baillargeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cophy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12000</idno>
		<title level="m">Counterfactual learning of physical dynamics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3464" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From machine learning to machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perception of object persistence: The origins of object permanence in infancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Bremner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott P Johnson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development Perspectives</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="13" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient descent optimization of smoothed information retrieval metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="235" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How to learn (and how not to learn) multi-hop reasoning with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-hop question answering via reasoning chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shih-Ting Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02610</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why can&apos;t i dance in the mall? learning to mitigate scene bias in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="853" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01426</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Differentiable reasoning over a virtual knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJxstlHFPH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatiotemporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05803</idno>
		<title level="m">Self-assembling modular networks for interpretable multi-hop reasoning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01696</idno>
		<title level="m">Tvqa: Localized, compositional video question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attend and interact: Higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention-based conditioning methods for external knowledge integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Margatina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03674</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transparency by design: Closing the gap between performance and interpretability in visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mascharka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soklaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4942" to="4950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Marioqa: Answering questions by watching gameplay videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2867" to="2875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07871</idno>
		<title level="m">Visual reasoning with a general conditioning layer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Answering complex open-domain questions through iterative query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07000</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning object permanence from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofri</forename><surname>Kleinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10469</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Human reasoning and cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van Lambalgen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11361</idno>
		<title level="m">Ddrprog: A clevr differentiable dynamic reasoning programmer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09623</idno>
		<title level="m">Repartitioning of the complexwebquestions dataset</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4631" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Do multi-hop readers dream of reasoning chains?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14520</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A dataset and architecture for visual reasoning with a working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Guangyu Robert Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jing</forename><surname>Ganichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sussillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="729" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Clevrer: Collision events for video representation and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkxYzANYDB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Social-iq: A question answering benchmark for artificial social intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8807" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
