<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VTP: Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renshu</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouhan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangyong</forename><surname>Jia</surname></persName>
						</author>
						<title level="a" type="main">VTP: Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents Volumetric Transformer Pose estimator (VTP), the first 3D volumetric transformer framework for multi-view multi-person 3D human pose estimation. VTP aggregates features from 2D keypoints in all camera views and directly learns the spatial relationships in the 3D voxel space in an end-to-end fashion. The aggregated 3D features are passed through 3D convolutions before being flattened into sequential embeddings and fed into a transformer. A residual structure is designed to further improve the performance. In addition, the sparse Sinkhorn attention is empowered to reduce the memory cost, which is a major bottleneck for volumetric representations, while also achieving excellent performance. The output of the transformer is again concatenated with 3D convolutional features by a residual design. The proposed VTP framework integrates the high performance of the transformer with volumetric representations, which can be used as a good alternative to the convolutional backbones. Experiments on the Shelf, Campus and CMU Panoptic benchmarks show promising results in terms of both Mean Per Joint Position Error (MPJPE) and Percentage of Correctly estimated Parts (PCP). Our code will be available.</p><p>Index Terms-3D human pose estimation, Transformer, Multiperson pose estimation, Voxel-based estimator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ULTI-VIEW multi-person 3D pose estimation <ref type="bibr" target="#b0">[1]</ref> has been an essential task that benefits many real-world applications, such as surveillance, intelligent sports, virtual/augmented reality, and human-computer interaction. It aims to localize the 3D joints for all people from multi-view cameras. Compared to single-view approaches, multi-view cameras can provide complementary information to effectively alleviate projective ambiguities.</p><p>One of the biggest challenges in multi-view multi-person 3D pose estimation is that the identity of the persons from each camera view is unknown. Previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> tried to tackle this problem as an association task. First, the same persons from different views are identified and associated across multiple views. Subsequently, each person's pose is estimated by triangulation or optimization-based pictorial structure models. Establishing cross-view correspondences is often crucial for this line of research. Recently, some approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> bypass the cross-view matching problem and perform both human localization and pose estimation in the 3D volumetric space. VoxelPose <ref type="bibr" target="#b3">[4]</ref> proposes to use a 3D object detection design that avoids making incorrect decisions in each camera view, and enables a collaborative decision from all camera views. Nevertheless, while the 3D Convolutional Network (CNN) in VoxelPose is good at extracting local features, CNN has limitations when capturing long-range dependencies. Additionally, the volumetric representation calls for heavy memories and computations, which is an unnecessary overhead for sparse scenes. This paper attempt to explore better representation learning for voxel-based methods in multi-view multi-person 3D pose estimation.</p><p>Transformer has demonstrated promising performance in many vision tasks. In this paper, we propose a transformer framework that learns volumetric representations for multiview multi-person 3D pose estimation. Inheriting the advantages of VoxelPose <ref type="bibr" target="#b3">[4]</ref>, VTP aggregates features from 2D keypoints in all camera views and directly learns the spatial relationships in the 3D voxel space. The aggregated 3D features are passed through 3D convolutions before being flattened into sequential embeddings and fed into a transformer. A residual structure is designed to further improve the network performance.</p><p>A major bottleneck of transformer-based models lies in that the self-attention computation is quadratic and the computational cost is unbearable if applied directly to voxels. Towards this end, VTP overcomes this bottleneck from two aspects: (1) VTP adopts a coarse-to-fine routine, where persons are first localized in 3D and transformers are applied to the 3D region of interest (ROI) to regress the 3D poses in the second stage;</p><p>(2) VTP exploits the sparse Sinkhorn attention which computes quasi-global attention with only local windows based on differentiable sorting of the representations, thus improving the memory efficiency.</p><p>Our contributions can be summarized as follows:</p><p>? We present the first transformer framework that learns volumetric representations for multi-view multi-person 3D pose estimation, named VTP, which empowers the transformer to perform pose estimation in the 3D volumetric space instead of explicit cross-view matching in 2D. <ref type="bibr">?</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Multi-view 3D Pose Estimation</head><p>To exploit information from different viewpoints, many methods have been proposed for multi-view human pose estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Establishing cross-view correspondences is key to many existing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Iskakov et al. <ref type="bibr" target="#b5">[6]</ref> propose to fuse 3D voxel feature maps and learn 3D pose via differentiable triangulation. Qiu et al. <ref type="bibr" target="#b6">[7]</ref> introduce a crossview fusion scheme into CNN to jointly estimate 2D poses for multiple views, and present a recursive Pictorial Structure Model to recover the 3D pose. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> propose a deep-learning-free 4D association method that unifies per-view parsing, cross-view matching, and temporal tracking into a single optimization framework. In 2020, Tu et al. propose a volumetric paradigm VoxelPose <ref type="bibr" target="#b3">[4]</ref> and bypass the cross-view matching problem, thus effectively reducing the impact of incorrectly established cross-view correspondences. Inspired by VoxelPose, we present the first transformer framework for 3D volumetric representation for multi-view 3D pose estimation. MvP <ref type="bibr" target="#b0">[1]</ref> views multi-person 3D pose estimation as a direct regression problem and does not perform any intermediate task including 2D keypoint detection. Yet, we argue that the intermediate 2D keypoint detection makes methods more robust to environmental change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vision Transformer</head><p>Transformer, first introduced in Attention is All You Need <ref type="bibr" target="#b8">[9]</ref>, is a common model in the field of natural language processing (NLP) and has swept various tasks of NLP with its outstanding performance. Thanks to its known advantages in capturing long-range dependencies, the computer vision (CV) field has also recently seen a soaring number of papers using transformers in various tasks, such as object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, image classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, video understanding <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, and Super-Resolution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. There is also some recent research on the use of transformers in human pose estimation tasks such as 2D pose detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. PRTR <ref type="bibr" target="#b25">[26]</ref> proposes a top-down regression-based method of human pose estimation network based on cascade transformer structure. POET <ref type="bibr" target="#b26">[27]</ref> proposes the first end-toend trainable multi-instance 2D pose estimation model, which combines CNN and transformer to directly predict the pose in parallel using the relationship between the detected person and the whole image environment.</p><p>For the 3D pose estimation, METRO <ref type="bibr" target="#b27">[28]</ref> presents a method for reconstructing 3D human pose and mesh vertices from a single image. The attention encoder is used to jointly model vertex-vertex and vertex-joint interactions and to output 3D joint coordinates and mesh vertices simultaneously. However, no one has tried to exploit transformers for volumetric representations in multi-view 3D pose estimation, due to its high computational cost, especially for large sparse scenes. In fact, MvP <ref type="bibr" target="#b0">[1]</ref> designs a multi-view pose transformer that is free of volumetric representations.</p><p>In contrast, to explore further potentials of the volumetric representations and benefit from its elegant and simple formation, we propose the first volumetric transformer pose estimator (VTP) for multi-view 3D pose estimation. In this paper, we prove that the transformer framework for 3D volumetric representations shows outstanding performance compared to other voxel-based methods. <ref type="figure" target="#fig_1">Figure 1</ref> is an overview of the proposed framework. The detection process is a top-down, two-stage approach. First, The 2D heatmaps at each viewpoint are obtained by pretrained 2D pose backbone. Second, the entire public space will be voxelized, using the Cuboid Proposal Network (CPN) network, the same as VoxelPose <ref type="bibr" target="#b3">[4]</ref>, to predict each persons' center in the space. Third, the space around each predicted human center will be voxelized in a more detailed space, and the VTP network is used to predict all the 3D joint positions of that person. VTP takes the CPN predicted human center's surrounding voxel grids as input, containing the multi-view fused information about a person's surrounding space. The input will go through a separate convolution-based network(marked as <ref type="bibr" target="#b1">(2)</ref> in Figure1) and a transformer-based network(including (1) in Figure1 and following transformer encoder), finally concatenating their outputs and putting them through the convolutional layer (marked as (3) in <ref type="figure" target="#fig_1">Figure1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Specifically, given a voxel space V ? R j?h?w?l , where h, w, l are the height, width, and length of certain grid space surrounding the center of the target person, j denotes the number of predicting joints per person which is predefined in different datasets. In a single grid, there are j numbers, each representing the j-th joint's average possibility predicted from multi-view 2D heatmaps. These grids are fed into the transformer t (?) and convolutional layer c (?), which will both extract the j channels into high-level features</p><formula xml:id="formula_0">X t = t (V ) ? R ft?h?w?l , X c = c (V ) ? R fc?h?w?l .</formula><p>These two outputs are concatenated together and fed into the 3D convolutional network to convert the high-level features into voxel grids V o ? R j?h?w?l which have the same channels as the initial input. Finally, the coordinates of each joint node can be obtained by weighting and summing the coordinates of the voxel grids with a special trick <ref type="bibr" target="#b28">[29]</ref> A. Voxel Transformer Pose (VTP) Estimator</p><p>Input of the network. The target people are first localized by a number of Cuboid Proposal Network proposals, following VoxelPose <ref type="bibr" target="#b3">[4]</ref>. The 2D pose heatmaps in all camera views are projected to a common discretized 3D space to construct a 3D feature volume. The space is divided into discrete grids of X ? Y ? Z, expressed as {V x,y,z }. Denote the 2d heatmap under 3D Conv and Residual block. There are three places where the 3D convolution is used in this network: (1) the transformer's embedding, (2) the initial input voxel grids are extracted into high-level features, and (3) after the output of two branches are concatenated together, the high-level features voxel grids are recovered to original channels. The 3D Conv used here has four parameters: c in , c out , kernelSize, padding c in , c out represent the input voxel channels and the target output channels, KernelSize is the size of the 3D convolution kernel, here we make padding = (kernelSize ? 1)/2. The main function is to transform the channels of each voxel element while keeping the dimensions of the voxel grids constant.</p><p>Using three 3D convolutions will form a residual block, where c res in , c res out are the two parameters of the residual block. The input of the residual block will go through two branches, one consisting of c in = c res in , c out = c res out and c in = c res out , c out = c res out , two 3D convolution layer with kernel size set to 3. And the other one consists of one 3D convolutional layer with c in = c res in , c out = c res out and kernel size of 1. The two paths will result in four-dimensional vectors of the same scale, which are summed and then passed through the ReLU activation function to obtain the final output of the residual block.</p><p>Position Embedding. Positional embeddings are there to give a transformer knowledge about the position of the input vectors. They are added (or concatenated) to corresponding input vectors. The learnable positional embedding, same as in Bert <ref type="bibr" target="#b29">[30]</ref>, is used in the proposed framework which lets the network derive the effective position embedding in the training data by itself. The authors also tried Sinusoidal Position Encoding in three dimensions: length, width, and height of voxel grids to make the transformer perceive the position of the current grid in 3D space. However, the experimental results were not satisfactory.</p><p>Sinkhorn Sparse Transformer on voxel grids. The transformer module t (?) accepts sequential input and the process of converting four-dimensional voxel grids into two-dimensional</p><formula xml:id="formula_1">sequences is described below. First, let a h ? w ? l 3D grid with j dimensional vectors be V = v 0,0,0 , . . . , v h,w,l ? R j?h?w?l ,</formula><p>where v x,y,z represents a grid vector with coordinates (x, y, z), the dimension j is equal to the number of human joint nodes defined in the dataset. Second, feeding V into a 3D convolutional network with c in = j, c out = e yields a voxel grid with more features extracted as V e = v 0,0,0 e , . . . , v h,w,l e</p><p>, v e ? R e?h?w?l , then each v e will be added to the position embedding of the same length e. This completes the embedding process of the initial input. Third, the voxel grids with four dimensions h, w, l, e are transformed into a two-dimensional sequence S = {s 0 , . . . s L } , s i ? R e?L , where the mapping relation is s x+(h?y)+(h?w?z) = v x,y,z e , as shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>For an input sequence S of length L, it will be divided into N b bins in the way of one bin for every B elements. Let the partition function be ? (?), Bins = ? (S) , where Bins represents all the blocks that are partitioned out,</p><formula xml:id="formula_2">Bins = {b 1 , . . . , b N b } , b i ? R B,e .</formula><p>If each element can only focus on other elements in the same bucket as itself, this approach is equivalent to a block-based local attention mechanism that lacks macroscopic dependence on other elements far away from itself, whereas for 3D pose estimation, it is important to infer the likelihood of a joint in the current position based on cues from other distant joints. The following section describes how the Sinkhorn sparse transformer reorders the bins so that the current region can focus on distant regions related to itself, making it an approximate global attention process.</p><p>The self-attention mechanism in transformers relies on the calculation of the Query, Key, and Value of each element, which can be written as:</p><formula xml:id="formula_3">Attention (Q, K, V ) = Score (Q, K) V = sof tmax QK T ? d V (1)</formula><p>The relevance of element j to element i can be obtained by calculating the dot product of the Query of element i and the Key of element j. After the weights are calculated for all other elements, the Value values are summed up according to their relevance. This requires the computation process to store the QK T matrix quadratic to the length of the input sequence, and here is where the bottleneck of the original transformer lies. For voxel grids of human pose estimation, the grid density is positively correlated with the accuracy, and if the original transformer is used then the grid density will be very sparse and cannot achieve valuable prediction accuracy. Nevertheless, voxel grids and transformers are both sparse, and most of the space in voxel grids does not have human joint nodes. The elements that have a large impact on the final result of the transformer are often very few, so if we can make each element focus only on the most relevant elements with itself, we can reduce the quadratic complexity. The main idea of Sinkhorn sparse attention is to divide the sequence into blocks, calculate the correlation between blocks and learn the appropriate block sorting order. After sorting, each block in the original order will only focus on the elements in the corresponding block in the new sequence. This alleviates the problem caused by the quadratic complexity of transformers and makes it possible to exploit transformers for volumetric representations.</p><p>First, each element in each bin b i is multiplied by three different linear transformation matrices W Q , W K , W V respectively to obtain three new matrices b  for all bins can be reduced from the quadratic of the input sequence length L to N 2 b , and the attention matrix is obtained as follows, where a i,j denotes the overall correlation between bin i and bin j:</p><formula xml:id="formula_4">Q i , b K i , b V i , which</formula><formula xml:id="formula_5">[i] = 1 B B j=1 b Q i [j] [i], i ? [1, ..., e] (2) b Kmean i [i] = 1 B B j=1 b K i [j] [i], i ? [1, ..., e]<label>(3)</label></formula><formula xml:id="formula_6">R = ? ? ? a 0,0 ? ? ? a 0,N b . . . . . . . . . a N b ,0 ? ? ? a N b ,N b ? ? ?<label>(4)</label></formula><p>If the matrix R is doubly stochastic (matrix elements are all non-negative and all row sums and column sums are one), it becomes a permutation matrix. Permutation matrices, which reorder the elements of a vector, are special cases of doubly stochastic matrices. Each permutation matrix is a convex combination of doubly stochastic matrices, so learning doubly stochastic matrices can be considered a form of relaxed permutation matrices. Here a method is needed to compute an arrangement based on the already-computed bin-to-bin correlation matrix, i.e. to compute a permutation matrix with which to multiply the input sequence of bins for sorting.</p><p>An optimal assignment could be obtained using the Hungarian algorithm. However, this operation of selecting the optimal assignment is not differentiable. In this case, since deep learning models rely on derivative gradient descent for parameter updating, we would not be able to use neural networks for this problem. Can we use a differentiable operation to approximate the operation of picking the optimal assignment so that it can be learned? The answer is yes, and the solution is to use the iterative Sinkhorn normalization.</p><p>For a matrix R the normalization is performed for rows and columns repeatedly and independently, which is called the Sinkhorn normalization process <ref type="bibr" target="#b30">[31]</ref>. k represents the number of user-defined iterations. This process is expressed in the following equations <ref type="bibr" target="#b31">[32]</ref>:</p><formula xml:id="formula_7">S 0 = exp (R)<label>(5)</label></formula><formula xml:id="formula_8">S k = F c F r S k?1 (R)<label>(6)</label></formula><formula xml:id="formula_9">S (R) = lim k?? S k (R)<label>(7)</label></formula><p>where F r , F c denote the row and column regularization functions with the following formula:</p><formula xml:id="formula_10">F k c (X) = F k?1 c (X) ? log exp (X1 l ) 1 N (8) F k r (X) = F k?1 r (X) ? log 1 l 1 N exp (X)<label>(9)</label></formula><p>Sinkhorn <ref type="bibr" target="#b32">[33]</ref> proves that repeating the above two steps eventually converges to a doubly stochastic matrix if R has support. Each of these steps is derivable so that the chain of derivations required to update the parameters for deep learning is not interrupted by the computation of the permutation matrix, thus enabling end-to-end training.</p><p>The matrix obtained after the Sinkhorn operation will be used to reorder the initial bin sequence b</p><formula xml:id="formula_11">Q i , b K i , b V i , i ? (1, N b ) into a new sequence b Q sorted i , b K sorted i , b V sorted i , i ? (1, N b )</formula><p>, as shown in <ref type="figure" target="#fig_5">Figure 3</ref>. Subsequently, the Query, Key, and Value of the two sequences are concatenated into a bigger bin, which results in a Query, Key, and Value with 2B elements. Here the bigger bin contains both locally adjacent elements and other relevant elements from afar.    <ref type="bibr" target="#b33">[34]</ref> output from the 3D convolutional network. At this point, the channels of each voxel element will become e + c. Finally, the features are fed into the last layer of 3D convolutional network, where input channels equals e + c and output channels is j, where j is the number of human joints. It will change the high-dimensional feature into the probability of the existence of each joint in the current voxel space.</p><p>Final output. After the above steps, voxel grids with joints possibility will be obtained. However, this is not enough to deduce the final position of each joint node, because if we just pick the most probable value from the used grids, then the 2000mm ? 2000mm ? 2000mm space will be divided into 32 ? 32 ? 32 grids, and each grid will have the length of 62.5mm, the accuracy would be limited here. Similar to <ref type="bibr" target="#b3">[4]</ref>, we don't find the voxel grid which has the maximum probability as prediction, instead we calculate the weighted average from all voxel grids, which is a common technique in previous works <ref type="bibr" target="#b28">[29]</ref>.</p><p>Let P j (x, y, z) represent the probability map that the j-th joint node is in the voxel grid V x,y,z , and then the final position of this joint can be derived from the following equation:</p><formula xml:id="formula_12">J j = X x=1 Y y=1 Z z=1 (x, y, z) ? P j (x, y, z)<label>(10)</label></formula><p>In this way, the prediction probabilities of all voxel grids are used as weights, and the 3D coordinates of the voxel grids are summed together to obtain prediction results with smaller deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT A. Dataset and evaluation metrics</head><p>We conducted experiments on three publicly available multiview multi-person human pose estimation benchmarks: CMUpanoptic, Shelf, and Campus.</p><p>CMU Panoptic. CMU Panoptic provides some examples with large social interaction. It uses 480 synchronized VGA cameras, 31 synchronized HD cameras (temporally aligned with VGA cameras), and 10 RGB-D sensors for motion capture. All of the 521 cameras are calibrated by the Structure From Motion approach. Following <ref type="bibr" target="#b3">[4]</ref>, the training set consists of the following sequences: "60422 ultima-tum1","16022 4 haggling1","160226 haggling1","161202 hag-gling1","160906 ian1","160906 ian2","160906 ian3","160906 band1","160906 band2","160906 band3". The testing set consists of :"160906 pizza1","160422 haggling1","160906 ian5",and "160906 band4".</p><p>Shelf and Campus. The Shelf dataset has annotated the body joints of four actors interacting with each other using cameras 2, 3 and 4. Triangulation is performed using the three camera views for deriving the 3D ground truth. Actor 4 (Vasilis) is occluded in most of the camera views and thus excluded from the evaluation. The Campus dataset has annotated the body joints of the main three actors performing different actions for the frames that are observed from the first two cameras. The ground-truth for the third camera view is the result of the triangulation (between cameras 1 and 2), and then projected to camera 3.</p><p>We have used the following evaluation metrics on these datasets.</p><p>PCP3D(Percentage of Correctly estimated Parts)The PCP metric measures the percentage of correctly predicted parts. A body part is considered correct by the algorithm if:</p><formula xml:id="formula_13">|s n ? s n | + |e n ? e n | 2 ? ? |s n ? e n |<label>(11)</label></formula><p>where s n and e n are the ground truth start and end location of part n, s n and e n are the corresponding estimated locations, and ? is a threshold parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE(Mean Per Joint Position Error) This metric is calculated by:</head><formula xml:id="formula_14">E M P JP E (f, S) = 1 N s Ns i=1 |P (f ) f,S (i) ? P (f ) gt,S (i) | 2<label>(12)</label></formula><p>where f denotes a frame and s denote the corresponding skeleton. P </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We test the performance on these three datasets under two scenarios, one is using the CPN network to predict the human center, the other is using the ground truth of the human center. The pretrained 2d human pose network backbone is used in all three datasets and require gradient is set to false. The embedding size of the Sinkhorn Transformer and the channels that 3D Conv extract are both set to 256. We set the depth (number of attention layers) to 1, size of the bin to 128, and number of attention heads for the Sinkhorn Transformer to 2. The input grid size is 32 ? 32 ? 32. We trained our model on a Nvidia GTX 3090 GPU.</p><p>For the Campus and Shelf datasets, we also adopt the same approach as <ref type="bibr" target="#b3">[4]</ref>, directly using the 2D pose estimator trained on the COCO dataset. These two datasets are only used as test sets, and the training process only uses synthetic data from the CMU panoptic dataset. We apply the Adam optimizer and train the model for 30 epochs. The learning rate is set to be 1e-4. The PCP3D metric is used to evaluate the predicted poses. For each ground truth pose, the most similar predicted pose is found and the percentage of correct limbs is calculated. However, we argue that this metric does not penalize false positive predictions, and does not provide an accurate analysis of errors in terms of distance. Therefore, we also measure the MPJPE (Mean Per Joint Position Error).</p><p>For the CMU Panoptic dataset, it is partitioned into a training set and a test set as described in the previous section, we apply the Adam optimizer and train the model for 10 epochs. The learning rate is also set to be 1e-4. The AP K metric in <ref type="bibr" target="#b3">[4]</ref> is used for evaluation on this dataset to facilitate comparison of performance. A prediction was considered to be estimated correctly when its MPJPE was less than K mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental results</head><p>Shelf and Campus. The two datasets are evaluated using the PCP3D metric for actor 1 (A1), actor 2 (A2), and actor 3 (A3), respectively, and the MPJPE results are also provided. <ref type="table" target="#tab_1">Table I presents the result for the Shelf dataset and Table II</ref> presents the result for the Campus dataset. In the Shelf dataset, we surpassed VoxelPose in both PCP3D average performance and MPJPE, achieving PCP3D average performance of 97.3 and MPJPE of 56.3mm, respectively, compared to VoxelPose's PCP3D average performance of 97.0 and MPJPE of 57.3mm. When the ground truth of the body center is used in the first stage, our lead in PCP3D average performance comes to 0.4%, and MPJPE reduces by 1.7mm, showing more significant improvement than under the condition of using CPN to predict the human center. This indicates that accurate localization of the targets can lead to highly accurate pose estimation results. For the Campus dataset, our results in terms of AP is on par with the state-of-the-art methods including VoxelPose. The MPJPE of our method is 2mm smaller than VoxelPose when using the ground truth in the center of the body, reaching It should be noted that since VoxelPose did not test the performance when using the ground truth of human center, the results here are obtained by us using the official code, after 30 epochs of training, we get the result PCP3D average performance of 96.5. We speculate that it might be because of the smaller number of views in the Campus dataset, which only has 3 cameras (one camera's ground truth is the result of triangulation from the other two cameras), compared with 5 cameras from the other two datasets.</p><p>CMU Panoptic. We compared our proposed method VTP and VoxelPose under the Panoptic dataset. As shown in <ref type="table" target="#tab_1">Table  III</ref>, our network improves by 1.28 mm compared to VoxelPose when using the ground truth of the body center. When using the CPN network as the first stage to predict the body center, our network improves 0.06mm in MPJPE compared to the already very accurate voxelpose, and our model has a higher correct rate under the threshold of AP 25 . Here we can find that after using the CPN network for prediction, our lead for voxelpose shrinks, and the same happens for the other two datasets. Our speculation is that because the body center predicted by the CPN network has some error compared to the ground truth, the predicted value may be randomly distributed in the vicinity of the ground truth, resulting in ambiquity in the relative relationship between the learned embedding of the transformer for voxel grid in the expression space. To this end, more accurate body center estimation is still needed. <ref type="figure" target="#fig_10">Figure 6</ref> shows the visualization of the pose prediction using the ground truth of the human center in two different scenes. On the left, the original images with labeled 2D joints are shown in five views; on the right, the ground-truth poses of each person are represented by various colors, and the red color represents the predicted poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation studies</head><p>In this section,ablative studies are conducted to analyze the essential components in our proposed model in detail. Various parameters in our proposed VTP model are tested on the Shelf dataset, and the results are shown in <ref type="table" target="#tab_1">Table IV</ref>.</p><p>Two branch structure. Our proposed network can be divided into three parts, a separate convolution-based network(marked as (2) in <ref type="figure" target="#fig_1">Figure 1</ref>) to increase the number of channels, a transformer-based network(including (1) in <ref type="figure" target="#fig_1">Figure  1</ref> and following transformer encoder), the Embedding phase in Transformer can be regarded as an increase to the number of channels, finally is the part that reduces the number of channels from the above two parts jointed features to the number of joint nodes through convolutional layer(marked as (3) in <ref type="figure" target="#fig_1">Figure 1</ref>). (a) and (b) of <ref type="table" target="#tab_1">Table IV</ref> show the effect of using only CNN to increase the number of channels and using only the transformer network to increase the number of channels. It can be found that the effect of using these two branches alone is much worse than that of using the features of both branches in (d). <ref type="figure">Figure 7</ref> shows a result using different branches.</p><p>The limited receptive field of CNN leads to difficulty in capturing global information, while Transformer can capture long-range dependencies, so combining the two enables the network structure to inherit the advantages of CNN and Transformer and retain the global and local features to the greater extent.</p><p>Embedding size. We study the effect of choosing the embedding size. As shown in the Table IV (c), (d), (g), we change the embedding size of the transformer branch and the number of output channels of the CNN branch, while all other parameters are fixed. The MPJPE increases to 68.1mm and the PCP3D average performance is only 94.8 when the embedding size is set to 32. The performance in terms of these two metrics is improving as the embedding size increases from 32 to 256. However, when the embedding size increases from 256 to 512, the improvement of MPJPE is not obvious, also the PCP3D Average performance decreases by 0.6%. This may be owing to that the 512 embedding size made the model too complex, overfitting the training set, thus deteriorating the generalization ability. <ref type="figure">Figure 9</ref> shows the performances for five different embedding sizes. The performance improvement is more obvious in the process of increasing the embedding size from 32 to 128, and the effect starts to slow down from 128 to 256. <ref type="figure">Figure 8</ref> shows a result using 32 and 256 embedding sizes. <ref type="figure">Fig. 9</ref>. Relation between embedding size and PCP3D Average / MPJPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of layers and attention heads.</head><p>One of the major bottlenecks of using transformers on voxel grids is the complexity, where the increase in granularity of the 3D space will bring a cubic increase in the length of the transformer sequence. Increasing the number of layers will consume more memory resources linearly, unless the reversible layers like the one in reformer <ref type="bibr" target="#b36">[37]</ref> are used, which saves space to store the intermediate gradients but significantly reduce the training speed. In Table IV(d) and (h), we compare the results of 1 transformer layer versus 2 transformer layers. In (h) we train 40 epochs in case the network does not converge. The MPJPE decreases slightly after increasing the transformer to two layers, and the PCP3D average performance decreases by 0.3. Increasing the number of layers to 2 behaves similarly to increasing the embedding size to 512.</p><p>For the multi-headed attention mechanism, we did not find any obvious pattern. As shown in <ref type="table" target="#tab_1">Table IV</ref>(d-f), we test the effects under 2, 4, and 8 attention heads, respectively. The MPJPE and PCP3D average performance of 2 heads are the highest, achieving 56.3mm and 97.3 respectively, while under 4 and 8 heads, the PCP3D average decreases continuously, while the MPJPE decreases and then increases. Therefore, we choose the 2 head attention mechanism for our final architecture.</p><p>Grid size. By comparing (d),(i) in <ref type="table" target="#tab_1">Table IV</ref>, we can see that the performance of both MPJPE and PCP3D is improved when the grid size (in other words, the number of grids that the space is divided into) is increased from 24 ? 24 ? 24 to 32 ? 32 ? 32. It shows that a more detailed division of space brings better results, which always holds true for voxelbased methods. However, as mentioned before, a more detailed division of the space will lead to a significant increase in computation. We choose 32 ? 32 ? 32 in order to strike the balance between complexity and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose the Volumetric Transformer Pose estimator (VTP), a network consisting of transformer and CNNs, for volumetric representation learning in multi-view multi-person 3D pose estimation scenarios. In the two-stage process of localizing multiple people and estimating the pose for each person, VTP contributes to better volumetric representation learning in the second stage, where the pose of the localized person is estimated. A residual structure and the Sinkhorn attention are applied to further improve the efficiency and accuracy. Experiments on popular benchmarks show that VTP is on par with the state-of-the-art methods in terms of PCP3D and MPJPE, and outperforms previous voxel-based methods when the target person localization is accurate. In future works, we will explore the potential of the proposed framework in the first stage of localizing the persons as well, which will likely increase the overall accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received xxx. The work was supported by the the National Key Research and Development Program under Grant No. 2019YFC0118404, National Natural Science Foundation of China under Grant No.U20A20386, Zhejiang Provincial Science and Technology Program in China under Grant No. LQ22F020026, Fundamental Research Funds for the Provincial Universities of Zhejiang under Grant No.GK219909299001-028, Zhejiang Key Research and Development Program under Grant No. 2020C01050, the Key Laboratory fund general project under Grant No. 6142110190406, the key open project of 32 CETC under Grant No. 22060207026 (Corresponding authors: Renshu Gu; Gangyong Jia.) The authors are with the Hangzhou Dianzi University, China. (email: chenyuxing@hdu.edu.cn, renshugu@hdu.edu.cn, huangouhan@hdu.edu.cn, gangyong@hdu.edu.cn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the network structure. the viewpoint v as M v ? R J?H?W , where J is the number of joints. For each V x,y,z , the projection position P x,y,z v under viewpoint v can be calculated by the camera geometry, then the value of the 2d heatmap at this projection position can be expressed as M x,y,z v . At this point, for a V x,y,z , the average value of the 2d heatmaps can be calculated for all the views, F x,y,z = 1 V V v=1 M x,y,z v , V being the number of views that can observe this space. At this point, every feature vector of {V x,y,z } can be obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Flatten and dividing process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>represent the set of Query, Key, and Value of each element in i-th bin. Next, the mean of Query and Key b Qmean i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Here the obtained b Qmean i , b</head><label>b</label><figDesc>Kmean i for dot product can be used to estimate the degree of correlation between two bins, approximating the element-to-element relationship in the original transformer. At this time, the complexity of computing Score b Qmean i , b Kmean i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Reordering process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Combining two sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5</head><label>5</label><figDesc>represents the attention matrix of sparse Sinkhorn transformer. The orange area represents the relevant area after the original sequence is chunked, and the blue area represents the elements in the corresponding position of the sequence after the permutation matrix transformation:The whole process requires computing the attention matrix twice, once for a sequence in bins, with a complexity of O N 2 b ; the second time for a bin containing 2B elements, with a complexity of O B 2 . So this attentional approach is changed from the original complexity of O L 2 to O B 2 + N 2 b , where B = L N b . After the sparse Sinkhorn transformer encoder, the output S o = {s 0 , . . . s L } will be the same channels as the input, each element being a vector of e channels. the sequence is unflattened into 3D space again and concatenated with the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Attention matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(i) is the estimated position of joint I and P (f ) gt,S (i) is the corresponding ground truth position. All joints are considered, N s means the number of joints. Finally, the MPJPEs are averaged over all frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Estimated 3D poses using ground truth of human center under CMU Panoptic dataset 71.3mm compared with 73.9mm from VoxelPose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>:2205.12602v1 [cs.CV] 25 May 2022 the sparse Sinkhorn attention which computes quasiglobal attention with only local windows based on differentiable sorting. Extensive experiments on benchmark datasets show that our approach is on par with existing state-of-the-art methods in terms of Percentage of Correctly estimated Parts (PCP), and shows superior Mean Per Joint Position Error (MPJPE) performance compared to previous volumetric representation based methods.</figDesc><table /><note>We design a residual block structure with 3D convolu- tions for VTP to further improve accuracy and perfor- mance.? To overcome heavy computations of voxels, we exploitarXiv?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON SHELF. (t) MEANS METHOD WITH TEMPORAL INFORMATION. (gt) MEANS USING THE GROUND TRUTH BODY CENTERS IN PEOPLE LOCALIZATION. ? MEANS THE LARGER THE BETTER, AND ? MEANS THE LOWER THE BETTER.</figDesc><table><row><cell>Shelf</cell><cell>A1 ?</cell><cell>A2 ?</cell><cell>A3 ?</cell><cell>Avg. ?</cell><cell>MPJPE (mm) ?</cell></row><row><cell>(t) 4D Association [8]</cell><cell>99.0</cell><cell>96.2</cell><cell>97.6</cell><cell>97.6</cell><cell>-</cell></row><row><cell>(t) Part-aware Pose [35]</cell><cell>99.14</cell><cell cols="2">95.41 97.64</cell><cell>97.39</cell><cell>-</cell></row><row><cell>(t) VoxelTrack [36]</cell><cell>98.6</cell><cell>94.9</cell><cell>97.7</cell><cell>97.1</cell><cell>-</cell></row><row><cell>Wu et al. [5]</cell><cell>99.3</cell><cell>96.5</cell><cell>97.3</cell><cell>97.7</cell><cell>-</cell></row><row><cell>Dong et al. [2]</cell><cell>98.8</cell><cell>94.1</cell><cell>97.8</cell><cell>96.9</cell><cell>-</cell></row><row><cell>VoxelPose [4]</cell><cell>99.3</cell><cell>94.1</cell><cell>97.6</cell><cell>97.0</cell><cell>57.3</cell></row><row><cell>Ours</cell><cell>99.3</cell><cell>95.1</cell><cell>97.4</cell><cell>97.3</cell><cell>56.3</cell></row><row><cell>(gt) VoxelPose [4]</cell><cell>99.2</cell><cell>95.1</cell><cell>97.8</cell><cell>97.4</cell><cell>52.8</cell></row><row><cell>(gt) Ours</cell><cell>99.5</cell><cell>96.2</cell><cell>97.6</cell><cell>97.8</cell><cell>51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON CAMPUS. ? MEANS THE LARGER THE BETTER, ? MEANS THE LOWER THE BETTER.</figDesc><table><row><cell>Campus</cell><cell>A1 ?</cell><cell>A2 ?</cell><cell>A3 ?</cell><cell>Avg. ?</cell><cell>MPJPE (mm) ?</cell></row><row><cell>(t) Part-aware Pose [35]</cell><cell>98.37</cell><cell cols="2">93.76 98.26</cell><cell>96.79</cell><cell>-</cell></row><row><cell>(t) VoxelTrack [36]</cell><cell>98.1</cell><cell>93.7</cell><cell>98.3</cell><cell>96.7</cell><cell>-</cell></row><row><cell>Dong et al. [2]</cell><cell>97.6</cell><cell>93.3</cell><cell>98.0</cell><cell>96.3</cell><cell>-</cell></row><row><cell>VoxelPose [4]</cell><cell>97.6</cell><cell>93.8</cell><cell>98.8</cell><cell>96.7</cell><cell>78.2</cell></row><row><cell>Ours</cell><cell>97.6</cell><cell>93.1</cell><cell>98.1</cell><cell>96.3</cell><cell>80.1</cell></row><row><cell>(gt) VoxelPose [4]</cell><cell>97.6</cell><cell>93.3</cell><cell>98.7</cell><cell>96.5</cell><cell>73.9</cell></row><row><cell>(gt) Ours</cell><cell>97.8</cell><cell>93.8</cell><cell>97.3</cell><cell>96.3</cell><cell>71.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>ON CMU PANOPTIC.</figDesc><table><row><cell>CMU Panoptic</cell><cell cols="3">AP25 ? AP50 ? AP100 ?</cell><cell cols="2">AP150 ? MPJPE (mm) ?</cell></row><row><cell>VoxelTrack [36]</cell><cell>85.88</cell><cell>98.31</cell><cell>99.54</cell><cell>-</cell><cell>16.97</cell></row><row><cell>Wu et al. [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.84</cell></row><row><cell>VoxelPose [4]</cell><cell>83.59</cell><cell>98.33</cell><cell>99.76</cell><cell>99.91</cell><cell>17.68</cell></row><row><cell>Ours</cell><cell>83.79</cell><cell>97.14</cell><cell>98.15</cell><cell>98.40</cell><cell>17.62</cell></row><row><cell>(gt) VoxelPose [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16.94</cell></row><row><cell>(gt) Ours</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON THE SHELF DATASET.</figDesc><table><row><cell></cell><cell>Structure</cell><cell cols="2">Attention head Embedding size</cell><cell>Grid size</cell><cell>A1</cell><cell>A2</cell><cell>A3</cell><cell>Avg. MPJPE (mm)</cell></row><row><cell>(a)</cell><cell>CNN</cell><cell>-</cell><cell>256</cell><cell cols="4">32 ? 32 ? 32 97.3 83.7 97.5</cell><cell>92.9</cell><cell>64.3mm</cell></row><row><cell>(b)</cell><cell>Transformer</cell><cell>2</cell><cell>256</cell><cell cols="4">32 ? 32 ? 32 98.2 91.8 97.5</cell><cell>95.8</cell><cell>61.3mm</cell></row><row><cell>(c)</cell><cell>CNN + Transformer</cell><cell>2</cell><cell>32</cell><cell cols="4">32 ? 32 ? 32 97.6 89.7 97.0</cell><cell>94.8</cell><cell>68.1mm</cell></row><row><cell>(d)</cell><cell>CNN + Transformer</cell><cell>2</cell><cell>256</cell><cell cols="4">32 ? 32 ? 32 99.5 94.9 97.6</cell><cell>97.3</cell><cell>56.3mm</cell></row><row><cell>(e)</cell><cell>CNN + Transformer</cell><cell>4</cell><cell>256</cell><cell cols="4">32 ? 32 ? 32 99.2 94.3 97.5</cell><cell>97.0</cell><cell>57.2mm</cell></row><row><cell>(f)</cell><cell>CNN + Transformer</cell><cell>8</cell><cell>256</cell><cell cols="4">32 ? 32 ? 32 99.3 93.0 97.6</cell><cell>96.6</cell><cell>57.1mm</cell></row><row><cell>(g)</cell><cell>CNN + Transformer</cell><cell>2</cell><cell>512</cell><cell cols="4">32 ? 32 ? 32 99.4 93.2 97.5</cell><cell>96.7</cell><cell>56.0mm</cell></row><row><cell cols="2">(h) CNN + Transformer(2 layers)</cell><cell>2</cell><cell>256</cell><cell cols="4">32 ? 32 ? 32 98.8 94.5 97.6</cell><cell>97.0</cell><cell>56.4mm</cell></row><row><cell>(i)</cell><cell>CNN + Transformer</cell><cell>2</cell><cell>256</cell><cell cols="4">24 ? 24 ? 24 99.2 94.3 97.7</cell><cell>97.0</cell><cell>58.1mm</cell></row><row><cell cols="3">Fig. 7. result on Shelf with different branch structure</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Fig. 8. Results on Shelf with different embedding size.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Direct Multi-view Multi-person 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end dynamic matching network for multi-view multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzhentao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="477" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="197" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph-based 3d multi-person pose estimation using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Size</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="11148" to="11157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7718" to="7727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4d association graph for realtime multi-person motion capture using multiple video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1324" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voxel transformer for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="3164" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">M3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022</meeting>
		<imprint>
			<biblScope unit="page" from="772" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">BViT: Broad Attention based Vision Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06268</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Parameter efficient multimodal transformers for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04124</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nested Dense Attention Network for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yirong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Multimedia Retrieval. 2021</title>
		<meeting>the 2021 International Conference on Multimedia Retrieval. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="250" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">FreqNet: A Frequency-domain Image Super-Resolution Network with Dicrete Cosine Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyuan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10800</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Poseur: Direct Human Pose Regression with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weian</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07412</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tfpose: Direct human pose estimation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weian</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15320</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pose recognition with cascade transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1944" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">End-to-end trainable multi-instance pose estimation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Stoffl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mathis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12115</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-toend human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ranking via sinkhorn propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Prescott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.1925</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A relationship between arbitrary positive matrices and doubly stochastic matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="876" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A practical survey on faster and lighter transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?tan</forename><surname>Marceau Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Aloise</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Part-aware measurement for robust multi-view multi-human 3d pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hau</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1472" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Voxeltrack: Multi-person 3d human pose estimation and tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
