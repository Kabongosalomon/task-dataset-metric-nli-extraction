<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label-Wise Message Passing Graph Neural Network on Heterophilic Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enyan</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimeng</forename><surname>Guo</surname></persName>
							<email>zhimeng@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Label-Wise Message Passing Graph Neural Network on Heterophilic Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>ACM Reference Format: Enyan Dai, Shijie Zhou, Zhimeng Guo, Suhang Wang. 2022. Label-Wise Mes-sage Passing Graph Neural Network on Heterophilic Graphs. In Proceedings of ACM Conference (Conference&apos;17). ACM, New York, NY, USA, 11 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have achieved remarkable performance in modeling graphs for various applications. However, most existing GNNs assume the graphs exhibit strong homophily in node labels, i.e., nodes with similar labels are connected in the graphs. They fail to generalize to heterophilic graphs where linked nodes may have dissimilar labels and attributes. Therefore, in this paper, we investigate a novel framework that performs well on graphs with either homophily or heterophily. More specifically, we propose a label-wise message passing mechanism to avoid the negative effects caused by aggregating dissimilar node representations and preserve the heterophilic contexts for representation learning. We further propose a bi-level optimization method to automatically select the model for graphs with homophily/heterophily. Theoretical analysis and extensive experiments demonstrate the effectiveness of our proposed framework for node classification on both homophilic and heterophilic graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-structured data is very pervasive in the real-world such as knowledge graphs, traffic networks, and social networks. Therefore, it is important to model the graphs for downstream tasks such as traffic prediction <ref type="bibr" target="#b31">[32]</ref>, recommendation system <ref type="bibr" target="#b10">[11]</ref> and drug generation <ref type="bibr" target="#b2">[3]</ref>. Inspired by the homophily of graphs, i.e., linked nodes tend to have similar features and belong to the same class <ref type="bibr" target="#b18">[19]</ref>, Graph Neural Networks (GNNs) <ref type="bibr" target="#b27">[28]</ref> adopt a message-passing mechanism which learns a node's representation by iteratively aggregating the representations of its neighbors. This can enrich the node features and preserve the node attributes and local topology for various downstream tasks.</p><p>Despite the great success of GNNs in modeling graphs, most GNNs design their message-passing mechanisms based on the homophily assumption of graphs; while heterophilic graphs also widely exist in the real world due to "opposite attract". For instance, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  in dating website, people are more likely to connect with people of different genders. And in the trading network, fraudsters tend to contact accomplices instead of other fraudsters <ref type="bibr" target="#b20">[21]</ref>.</p><p>In heterophilic graphs, the neighbors of central nodes can have different labels and features with each other. The heterophily of graphs challenges many existing GNNs with homophily assumption <ref type="bibr" target="#b33">[34]</ref>. For example, message-passing of GCN <ref type="bibr" target="#b14">[15]</ref> will smooth the representations of central node and its heterophilic neighbors, inducing similar representations for nodes in different classes. <ref type="figure" target="#fig_1">Fig. 1</ref> gives an example, where the color of a node denotes the node label and the number on the node means the node attribute. For node and with different labels, adopting one step aggregation of GCN will result in similar representations of and , i.e., 3.5 and 3.0, which could worsen the classification. Our theoretical analysis in Sec. 3.2 also shows that GCN will result in non-discriminative representations on heterophilic graphs. Attention mechanism such as GAT <ref type="bibr" target="#b25">[26]</ref> cannot address the issue due to the high heterophily of the graphs. For example, in <ref type="figure" target="#fig_1">Fig. 1</ref>, for node , all of its neighbors are of different classes with . Applying attention won't be able to assign larger weights to nodes of the same label as to help learn better representation of .</p><p>Though heterophilic graphs challenge existing GNNs, we observe that the heterophilic neighborhood contexts itself provides useful information. Generally, two nodes of the same class are more likely to have similar heterophilic neighborhood contexts; while two nodes of different classes are more likely to have different heterophilic neighborhood contexts. Thus, a heterophilic contextpreserving mechanism can lead to more discriminative representations. One promising way to preserve heterophilic context is to conduct label-wise aggregation, i.e., separately aggregate neighbors in each class. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, for node , with label-wise aggregation, will be represented as [1.0, 5.0, 3.0, non-existence], in the order of 's attribute, blue, green, and orange neighbors, respectively. Compared with , 's representations of central node and neighborhood context differ significantly with . In other words, we obtain more discriminative features with label-wise aggregation, which is also verified by our analysis in Theorem 3.3. Though promising, there is no existing work exploring label-wise message passing to address the challenge of heterophilic graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>. Therefore, in this paper, we investigate a novel label-wise massage passing for node classification on heterophilic graphs. In essence, we are faced with two challenges: (i) the label-wise aggregation needs the label of each node; while for node classification, we are only given a small set of labeled nodes. How to design label-wise message passing to facilitate node classification on sparsely labeled heterophilic graphs? (ii) Generally, we don't know the homophily level of the given graph. The label-wise message passing might not work well on graphs with high homophily. How to ensure the performance on both heterophilic and homophilic graphs? In an attempt to address these challenges, we propose a novel framework Label-Wise GNN (LW-GNN) <ref type="bibr" target="#b0">1</ref> . LW-GNN adopts a pseudo label predictor to predict pseudo labels and designs a novel label-wise message-passing to preserve the heterophilic contexts with pseudo labels. To handle both heterophilic and homophilic graphs, apart from label-wise message passing GNN, LW-GNN also utilizes a GNN for homophilic graphs, and adopts bi-level optimization on the validation data to automatically select the better model for the given graph. The main contributions are: ? We theoretically show the limitation of GCN and the superiority of label-wise aggregation on heterophilic graphs; ? We design a novel label-wise message passing mechanism and add an automatic model selection module to ensure the performance of LW-GNN on both heterophilic and homophilic graphs; and ? We conduct extensive experiments on real-world graphs with heterophily and homophily to demonstrate the effectiveness of LW-GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph neural networks (GNNs) have shown great success for various applications such as social networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, financial transaction networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> and traffic networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Based on the definition of the graph convolution, GNNs can be categorized into two categories, i.e., spectral-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> and spatialbased <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. Spectral-based GNN models are defined according to spectral graph theory. Bruna et al. firstly generalize convolution operation to graph-structured data from spectral domain. GCN <ref type="bibr" target="#b14">[15]</ref> simplifies the graph convolution by first-order approximation. For spatial-based graph convolution, it aggregates the information of the neighbors nodes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. For instance, spatial graph convolution that incorporates the attention mechanism is applied in GAT <ref type="bibr" target="#b25">[26]</ref> to facilitate the information aggregation. Graph Isomorphism Network (GIN) <ref type="bibr" target="#b28">[29]</ref> is proposed to learn more powerful representations of the graph structures. Recently, to learn better node representations, deep graph neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18</ref>] and <ref type="bibr" target="#b0">1</ref> The code will be released upon acceptance self-supervised learning methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> have been investigated. However, the aforementioned methods are generally designed based on the homophily assumption of the graph. Low homophily level in some real-word graphs can largely degrade their performance <ref type="bibr" target="#b33">[34]</ref>. Some initial efforts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref> have been taken to address the problem of heterophilic graphs. For example, H2GCN <ref type="bibr" target="#b33">[34]</ref> investigates three key designs for GNNs on heterophilic graphs. SimP-GCN <ref type="bibr" target="#b11">[12]</ref> adopts a node similarity preserving mechanism to handle graphs with heterophiliy. FAGCN <ref type="bibr" target="#b1">[2]</ref> adaptively aggregates low-frequency and high-frequency signals from neighbors to learn representations for graphs with heterophily. Our LW-GNN is inherently different from these methods: (i) we propose a novel label-wise aggregation to better capture the neighbors' information in heterophilic graphs; and (ii) Automatic model selection is deployed to achieve state-of-the-art performance on both homophilic and heterophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we first analyze the GCN to show the challenge of heterophilic graphs. We then prove the advantages of label-wise aggregation and give formal problem definition of node classification on graphs with homophily/heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Definition</head><p>Let G = (V, E, X) be an attributed graph, where V = { 1 , ..., } is the set of nodes, E ? V ? V is the set of edges, and X = {x 1 , ..., x } is the set of node attributes. A ? R ? represents the adjacency matrix of the graph G, where A = 1 indicates that there is an edge between nodes and ; otherwise, A = 0. In the node classification task, each node belongs to one of classes. We use to denote label of node . Based on how likely edges link nodes in the same class, graphs can be categorized to homophilic and heterophilic graphs. Specifically, the graph homophily level can be evaluated by the following measure: Definition 3.1 (Homophily Ratio). It is the fraction of edges in a graph that connect nodes of the same class, i.e., intra-class edges. The homophily ratio ? is calculated as:</p><formula xml:id="formula_0">? = |{( , ) ? E : = }| |E | .<label>(1)</label></formula><p>When the homophily ratio is small, most of the edges will link nodes from different classes, which indicates a heterophilic graph. In homophilic graphs, connected nodes are more likely to belong to the same class, which will lead to a homophily ratio close to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of GCN on Heterophilic Graphs</head><p>GCN <ref type="bibr" target="#b14">[15]</ref> is one of the most widely used graph neural networks. The operation in each layer of GCN can be written as:</p><formula xml:id="formula_1">H ( +1) = (?H ( ) W ( ) ),<label>(2)</label></formula><p>where H ( ) is the node representation matrix of the output of the -th layer and? is the normalized adjacency matrix. Generally, the symmetric normalized form</p><formula xml:id="formula_2">D ? 1 2 AD ? 1 2 or D ?1 A is used as?, where D is a diagonal matrix with D =</formula><p>A . The adjacency matrix can be augmented with a self-loop. is an activation function such as ReLU. In a single layer of GCN, the process can be split to two steps. First, GCN layer averages the neighbor features with Z =?X. Then, a non-linear transformation (ZW) is applied to obtain intermediate features or final predictions. The step of averaging the neighbor features can benefit the node classification when the neighbors have similar features. However, in heterophilic graphs, the operation of mixing neighbors that possess different features will result in poor representations for node classification. This could be justified by the following theorem.</p><p>Assumptions. We first discuss the assumptions of the heterophilic graphs: (i) Following previous works <ref type="bibr" target="#b33">[34]</ref>, the graph G is considered as a -regular graph, i.e.. each node have neighbors; For each node , its neighbors' features and class labels { : ? N ( )} are conditionally independent given , and , where and represent the means. and denote the standard deviations. Intuitively, though nodes in N ( ) and N ( ) belong to the same class , they are connected to nodes of different classes because of their different properties. For example, in the molecule, the atom in the same class will exhibit different features, when they are linked to different atoms. Therefore, this assumption is valid. And it is also verified by the empirical analysis on large real-world heterophilic graphs in Appendix D. Let</p><formula xml:id="formula_3">( = | ) = ?, ( = | ) = 1?? ?1 , ? ? .</formula><formula xml:id="formula_4">= ?? 1 =1 ( ??) ( ??), where?= 1 =1</formula><p>and represents the element-wise product. We can have the following theorem. . . }, as the decrease of homophily ratio ? with ? ? 1 , the representations obtained by the averaging process in GCN layer, i.e. Z = D ?1 AX, will be less discrimative. When ? = 1 and &gt; ? | ? |, the representations after averaging process will be nearly non-discrimative.</p><p>The detailed proof can be found in Appendix B. Since the intraclass distance is often much smaller than inter-calss distance, the condition | ? | &gt; | ? | is generally meet in real-world graphs. And computes the standard deviations of mean neighbor features in different classes. As a result, is usually much larger than the and | ? |. Considering ? is generally small in the real graphs, e.g.,</p><formula xml:id="formula_5">? = 1.3 in the Texas dataset, &gt; ? | ? |, ? ? {1, .</formula><p>. . } can also hold in most of real-world graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of Label-Wise Aggregation</head><p>In heterophilic graphs, we observe that the heterophilic neighbor contexts itself provides useful information. As shown in Appendix D, for two nodes and of the same class, i.e., = , the features of nodes in N ( ) are likely to be similar to that of nodes in N ( ); while for nodes and with ? , the features of nodes in N ( ) are likely to be different from that in N ( ). Thus, we can preserve the heterophily context of neighbors by separately aggregating the neighbor features in each class, i.e., a , = ?N ( ) 1 |N ( ) | x . This is further proved by the following theorem. The detailed proof is presented in Appendix C. The difference between the groups of neighbors is naturally larger than the intragroup variance. Since</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>??</head><p>is usually small, e.g. around 1.8 in the Texas graph, the condition | ? | &gt; ?? is generally satisfied in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Problem Definition</head><p>In real world, we are usually given a graph with unknown homophily level. Based on the analysis above, in heterophilic graphs, label-wise aggregation can be used to capture the local context information; while simply averaging neighbors is very effective in homophilic graphs. In addition, only a part of nodes V ? V are labeled in real-world graphs, which can be split into a training set V and a validation set V for semi-supervised learning. Thus, we aim to develop a framework that works for semi-supervised node classification on graphs with any homophily level. The problem is defined as: Problem 1. Given an attributed graph G = (V, E, X) with a small set of labels Y for node set V , the homophily ratio ? of G is unknown, we aim to learn a GNN which accurately predicts the labels of the unlabeled nodes, i.e., (G, Y ) ?? , where is the function we aim to learn and? is the set of predicted labels for unlabeled nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we give the details of our proposed framework LW-GNN. For heterophilic graphs, the basic idea is to separately aggregate the neighbors in different classes to well preserve the local contextual information. There are two main challenges: (i) how to design and conduct label-wise aggregation on heterophilic graphs with a small number of labeled nodes; and (ii) how to make it work for both heterophilic and homophilic graphs. To address the challenges, LW-GNN estimates the labels of unlabeled nodes and designs a novel label-wise message-passing mechanism with the estimated labels. To ensure the performance on graphs with any homophily level, LW-GNN trains a label-wise aggregation model for heterophily and a GNN model for homophily, and can automatically select the model for the graph with unknown homophily ratio.</p><p>An illustration of the proposed LW-GNN is shown in <ref type="figure" target="#fig_4">Fig. 2</ref>, which is composed of an MLP-based pseudo label predictor , a GNN with label-wise message passing, a GNN for homophilic graph, and an automatic model selection module. The predictor takes the node attributes as input and predicts the node labels. utilizes the estimated pseudo labels from to conduct label-wise aggregation on G for node classification. , which is the GNN for homophilic graphs also makes predictions from the input graph G. The predictions of and are combined with model weights. A bi-level optimization on validation set is applied to learn the model weights for model selection. Next,we give the details of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Label-Wise Message Passing</head><p>Though the message-passing mechanisms in current GNNs have achieved remarkable success in homophilic graphs, they generally fail to deal with heterophilic graphs. This is because nodes and their neighbors have different labels and features in heterophilic graphs, while existing GNNs directly mix the neighbors in various classes together with the central node. Therefore, the aggregated representations can poorly preserve the neighbors' information, resulting in worse performance <ref type="bibr" target="#b33">[34]</ref>.</p><p>Based on the motivation above and the theoretical analysis in Sec. 3.3, we propose to conduct label-wise aggregation, i.e., neighbors in different classes are separately aggregated to update node representations. However, only a small number of nodes are provided with labels. Thus, a pseudo label predictor is deployed to estimate class labels for label-wise aggregation. Next, we first introduce the pseudo label prediction followed by the design of label-wise message passing for heterophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Pseudo Label Prediction.</head><p>The pseudo label predictor takes the node attributes as input and estimates the class labels of unlabeled nodes. Specifically, a MLP is utilized to obtain pseudo label of node with 's attributes a?</p><formula xml:id="formula_6">y = (x ),<label>(3)</label></formula><p>where x denotes the attributes of node . Note that, we use MLP as the predictor because message passing of the GNNs can lead to poor representations on heterophilic graphs. The loss function for training the pseudo label predictor is:</p><formula xml:id="formula_7">min L = 1 |V | ?? ?V (?, ),<label>(4)</label></formula><p>where V is the set of nodes in the training set, denote the true label of node , represents the parameters of the predictor ,and (?) is the cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Design of Label-Wise Message</head><p>Passing. With , we can get the pseudo labels? for the unlabeled nodes V = V\V . Combining it with the provided Y , we have labels?? (? ? Y ) necessary for label-wise aggregation. Since in label-wise aggregation, we will separately aggregate the neighbors of each class, which could result in large dimension of node representations. Thus, in each layer of the label-wise message passing, a linear transformation is firstly applied to process the input features to learn a good feature space and reduce the dimensionality to as</p><formula xml:id="formula_8">z ( ) = W ( ) h ( ) ,<label>(5)</label></formula><p>where h ( )</p><p>is the representation vector of node ? V at -th layer with h (0) = x , and W ( ) denotes the parameters. We then aggregate the neighbors of the nodes label-wisely with estimated labels. Let a , be the aggregated representation of neighbors whose estimated labels are class , the label-wise aggregation process can be formally written as:</p><formula xml:id="formula_9">a , = ?? ?N ( ) 1 ( , ) z ( ) ,<label>(6)</label></formula><p>where N ( ) = { : ( , ) ? E ??= } is node 's neighbors with estimated label . ( , ) = ?? , ? , is the normalization term with , = |N ( )|. When there is no neighbor belonging to class , we can assign its value with zero embedding or the average embedding of class .</p><p>Finally, to preserve the heterophily context, instead of averaging the neighborhood representations, for each node , we get 's representation at layer +1 by concatenating its label-wise neighborhood representations a , , = 1, . . . , , together with z , followed by the ReLU as the activation function. This can be formally written as</p><formula xml:id="formula_10">h ( +1) = ReLU(CONCAT(z ( ) , a ( ) ,1 , ..., a ( ) , )),<label>(7)</label></formula><p>where is the number of classes. h ( +1) ? R ( +1) ? is 's representation at layer + 1. With Eq. <ref type="formula" target="#formula_10">(7)</ref>, we are able to obtain representations that preserve the target node's features and capture the information of their neighbors in heterophilic graphs. Thus, for two nodes and belonging to the same class, their contexts should be similar, which results in similar representations. Multiple layers of label-wise message passing can be applied to incorporate more hops of neighbors in representation learning. In heterophilic graphs, different hops of neighbors may exhibit different class distributions which can provide useful information for node classification. Therefore, the final node prediction is based on the intermediate representations of all layers. Inspired by <ref type="bibr" target="#b29">[30]</ref>, max pooling is applied to combine the intermediate representations for a model with layers:</p><formula xml:id="formula_11">y = softmax[W ? MaxPooling(h (1) , ..., h ( ) )],<label>(8)</label></formula><p>where W ? R ?( +1) is a learnable weight matrix,? is predicted label probabilities of node .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Model Selection</head><p>As described in Sec. 4.1, we rely on the pseudo labels to conduct label-wise message passing. In heterophilic graphs, the homophily ratio is very small and even can be around 0.2 <ref type="bibr" target="#b21">[22]</ref>. With a reasonable pseudo label predictor, the label-wise aggregation will mix much less noise than the general GNN aggregation. In contrast, for homophilic graphs such as citation networks, their homophily ratios are close to 1. In this situation, label-wise aggregation may not be effective as (i) the pseudo-labels contain noises, which can bring more negative effects in homophilic graphs; and (ii) most N ( ), = 1, . . . , , might be empty. Thus, for homophilic graphs, directly aggregating all the neighbors may introduce less noise in representations than aggregating label-wisely. Therefore, it is necessary to determine whether to apply the label-wise message passing or the state-of-the-art GNN for homophilic graphs. One straightforward way is to select the model based on the homophily ratio. However, the graphs are generally sparsely labeled which makes it difficult to estimate the real homophily ratio. Moreover, an additional hyperparameter i.e., threshold of homophily ratio will be required for model selection. To address this problem, we propose to utilize the validation data to automatically select the model with bi-level optimization.</p><p>To achieve good performance on graphs with different homophily ratios, we can combine predictions of the label-wise aggregation model for heterophilic graphs and traditional GNN models for homophilic graphs. Predictions from the GNN for homophilic graphs are given by:</p><formula xml:id="formula_12">? = GNN(A, X),<label>(9)</label></formula><p>where the GNN is flexible to various models for homophilic graphs such as GAT and GIN. Here, we select GCNII <ref type="bibr" target="#b5">[6]</ref> which achieves state-of-the-art results on homophilic graphs. The model selection can be achieved by assigning higher weight to the corresponding model prediction. The combined prediction is given as:</p><formula xml:id="formula_13">y = exp ( 1 ) 2 =1 exp ( )? + exp ( 2 ) 2 =1 exp ( )? ,<label>(10)</label></formula><p>where? ?? is the prediction of node from . 1 and 2 are the learnable weights to control the contributions of two models in final prediction. 1 and 2 can be obtained by finding the values that lead to good performance on validation set. More specifically, this goal can be formulated as the following bi-level optimization problem:</p><formula xml:id="formula_14">min 1 , 2 L ( * ( 1 , 2 ), * ( 1 , 2 ), 1 , 2 ) . . * , * = arg min , L ( , , 1 , 2 )<label>(11)</label></formula><p>where L and L are the average cross entropy loss of the combined predictions {?: ? V } and {?: ? V } on validation set and training set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">An Optimization Algorithm of LW-GNN</head><p>Computing the gradients for 1 and 2 is expensive in both computational cost and memory. To alleviate this issue, we use an alternating optimization schema to iteratively update the model parameters and the model selection weights. </p><formula xml:id="formula_15">+1 = ? ? L ( , , 1 , 2 ), +1 = ? ? L ( , , 1 , 2 ),<label>(12)</label></formula><p>where and are model parameters after updating steps. and are the learning rates for and . Updating Upper Level 1 and 2 . Here, we use the updated model parameters and to approximate * and * . Moreover, to further speed up the optimization, we apply first-order approximation <ref type="bibr" target="#b9">[10]</ref> to compute the gradients of 1 and 2 :</p><formula xml:id="formula_16">+1 1 = 1 ? ? 1 L (?,?, 1 , 2 ), +1 2 = 2 ? ? 2 L (?,?, 1 , 2 ),<label>(13)</label></formula><p>where?and?means stopping the gradient. is the learning rate for 1 and 2 .</p><p>More details of the training algorithm are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct experiments to demonstrate the effectiveness of LW-GNN. In particular, we aim to answer the following research questions:</p><p>? RQ1 Is our proposed LW-GNN effective in node classification on both homophilic and heterophilic graphs. ? RQ2 Can our label-wise aggregation learn representations that well capture information for label prediction? ? RQ3 How do the quality of pseudo labels and the automatic model selection affect LW-GNN?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets. To evaluate the performance of our proposed LW-GNN, we conduct experiments on three homophilic graphs and four heterophilic graphs. For homophilic graphs, we choose the widely used benchmark datasets, Cora, Citeseer, and Pubmed <ref type="bibr" target="#b14">[15]</ref>. The dataset splits of homophilic graphs are the same as the cited paper. As for heterophilic graphs, we use a webpage dataset Texas <ref type="bibr" target="#b21">[22]</ref>, and three subgraphs of wiki, i.e., Squirrel, Chameleon, and Crocodile <ref type="bibr" target="#b23">[24]</ref>. Following <ref type="bibr" target="#b33">[34]</ref>, 10 splits are used in each heterophilic graph for evaluation. The statistics of the datasets are presented in <ref type="table" target="#tab_0">Table 1</ref>. Compared Methods. We compare LW-GNN with the following representative and state-of-the-art GNNs:  ? MLP: Multilayer perceptron is applied here to give predictions without using the graph structure information. ? GCN <ref type="bibr" target="#b14">[15]</ref>: This is a popular spectral-based Graph Convolutional Network. ? MixHop [1]: It adopts a graph convolutional layer with powers of the adjacency matrix. ? SuperGAT <ref type="bibr" target="#b12">[13]</ref>: Apart from the classification loss on provided labels, a self-supervised learning task is deployed to further guide the learning of attention for better information propagation based on GAT <ref type="bibr" target="#b25">[26]</ref>. ? GCNII <ref type="bibr" target="#b5">[6]</ref>: Based on GCN, residual connection and identity mapping are applied in GCNII to have a deep GNN for better performance.</p><p>We also compare LW-GNN with the following state-of-the-art GNN models for heterophilic graphs:</p><p>? FAGCN [2]: FAGCN adaptively aggregates low-frequency and high-frequency signals from neighbors to improve the performance on heterophilic graphs. ? SimP-GCN <ref type="bibr" target="#b11">[12]</ref>: A feature similarity preserving aggregation is applied to facilitate the representation learning on graphs with homophily and heterophily. ? H2GCN <ref type="bibr" target="#b33">[34]</ref>: H2GCN investigates the limitations of GCN on graphs with heterophily. And it accordingly adopts three key designs for node classification on heterophilic graphs.</p><p>Implementation Details. For experiments on each heterophilic graph, we run the 10 public dataset splits and report the average results and standard deviations. For homophily graphs, we run each experiment 5 times on the provided public dataset split. A one-hideen layer MLP with 64 filters is applied as the pseudo label predictor . We adopt two layers of label-wise message passing on all the datasets. More discussion about the impacts of the depth on LW-GNN is given in Sec. 5.5. The hidden dimension of is fixed as 64 for all graphs. As for the which deploys GCNII <ref type="bibr" target="#b5">[6]</ref> as the backbone, the hyperparameter settings are the same as the cited paper. During the training phase, the learning rate is set as 0.01 for all the parameters and model selection weights. The inner iteration step is set as 2. All the hyperparameters of the baselines are tuned on the validation set for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Node Classification Performance</head><p>To answer RQ1, we conduct experiments on both heterophilic graphs and homophilic graphs with comparison to the state-of-theart GNN models. Performance on Heterophilic Graphs. We conduct experiments on 10 dataset splits on each heterophilic graphs. The average accuracy and standard deviations on heterophilic graphs are reported in <ref type="table" target="#tab_1">Table 2</ref>. The model selection weight for label-wise aggregation GNN is shown along with the results of LW-GNN. Note that this model selection weight ranges from 0 to 1. When the weight is close to 1, the label-wise aggregation model is selected. When the weight for is close to 0, the GNN for homophilic graph is selected. From this table, we can observe that:</p><p>? GNN models designed for homophily graphs such as GCN and SuperGAT perform poorly on heterophilic graphs. For example, MLP outperforms GCN and other GNNs for homophilic graphs by a large margin on Texas. This indicates the necessity of investigating messaging-passing mechanism for graphs with heterophily. ? The model selection weight for is close to 1 for heterophilic graphs, which verifies that the proposed LW-GNN can correctly select the label-wise aggregation GNN for heterophilic graphs. ? Compared with SimP-GCN which also aims to preserve node features, our LW-GNN performs significantly better on heterophilic graphs. This is because SimP-GCN only focuses on the similarity of central node attributes. In contrast, our label-wise aggregation can preserve both the central node features and the information of their neighbors for node classification on heterophilic graphs. LW-GNN also outperforms other GNNs for heterophilic graphs by a large margin. This further demonstrates the effectiveness of label-wise aggregation.</p><p>Performance on Homophilic Graphs. We report the average results of 5 runs in <ref type="table" target="#tab_2">Table 3</ref>. Similarly, the model selection weight for is also presented. From the table, we can observe that existing GNNs for heterophilic graphs generally perform worse than stateof-the-art GNNs on homophilic graphs such as GCNII. In contrast, LW-GNN achieves comparable results with the the best model on homophilic graphs. This is because LW-GNN combines the GNN using label-wise message passing and a state-of-the-art GNN for homophilic graph. And it can automatically select the right model for the given homophilic graph.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of Node Representations</head><p>To answer RQ2, we compare the representation similarity of intraclass node pairs and inter-class node pairs. Results of GCN is also reported as the reference. For both GCN and LW-GNN, representations learned by the last layer are used for analysis. Since we have similar observations on other heterophilic graphs, we only show the results on Texas in <ref type="figure" target="#fig_6">Fig. 3</ref>. From this figure, we can observe that the learned representations of GCN are very similar for both intra-class pairs and inter-class pairs. This verifies that simply aggregating the neighbors will make the node representations less discrimative. With label-wise aggregation, the similarity scores of intra-class pairs are significantly higher than inter-class node pairs. This demonstrates that the representations learned by label-wise message passing can well preserve the target nodes' features and their contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>To answer RQ3, we conduct ablation studies to understand the contributions of each component to LW-GNN. To investigate how the quality of pseudo labels can affect LW-GNN, we train a variant LW-GNN\P by replacing the MLP-based label predictor with a GCN model. To show the importance of the automatic model selection, we train a variant LW-GNN\G which removes the GNN for homophilic graphs and only uses label-wise aggregation GNN. Finally, we replace the GCNII backbone of to GCN, denoted as LW-GNN , to show LW-GNN is flexible to adopt various GNNs for . Experiments are conducted on both homophilic and heterophilic graphs. The results are shown in <ref type="table" target="#tab_3">Table 4</ref> and ablation studies on the rest datasets are shown in Appendix E. We can observe that:</p><p>? On homophilic graphs, LW-GNN\P shows comparable results with LW-GNN, because GCNII will be selected given a homophilic graph. On the heterophilic graph Texas, the performance of LW-GNN\P is significantly worse than LW-GNN. This is because GNNs can produce poor pseudo labels on heterophilic graph, which degrades the label-wise message passing. ? Compared with MLP, LW-GNN\G performs much better. This shows label-wise aggregation can capture structure information. However, LW-GNN\G performs worse than GCNII and LW-GNN on homophilic graphs. This indicates the necessity of combining GNN for homophilic graphs with label-wsie aggregation. ? LW-GNN achieves comparable results with GCN on homophilic graphs. On heterophilic graphs, LW-GNN performs similarly with LW-GNN. This shows the flexibility of LW-GNN in adopting traditional GNN models designed for homophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Impacts of Label-Wise Aggregation Layers</head><p>In this subsection, we explore the sensitivity of LW-GNN on the depth of , i.e., the number of layers of label-wise message passing. Since LW-GNN will not select for homophilic graphs. We only conduct the sensitivity analysis on heterophilic graphs. We vary the depth of as {2, 3, . . . , 6}. The other experimental settings are the same as that described in Sec. 5.1. The results on Chameleon and Squirrel are shown in <ref type="figure" target="#fig_7">Fig. 4</ref>. More results are in Appendix F. From the figure, we find that our LW-GNN is insensitive to the number of layers, while the performance of GCN will drop with the increase of depth. This is because (i) aggregation of LW-GNN is performed label-wisely. Embeddings of nodes in different classes will not be similar even after many iterations; and (ii) the intermediate representations for different hops of neighbors are combined together to give the final prediction, which further reduces the potential risk brought by poor representations in deep model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we develop a novel label-wise message-passing mechanism to learn representations that preserve the node features and their neighbors' information for heterophilic graphs. An automatic model selection module is applied to ensure the performance of the proposed framework on graphs with any homophily ratio. Analysis on node representations demonstrates the effectiveness of the labelwise aggregation. Extensive experiments shows that our proposed LW-GNN can achieve sate-of-the-art results on both homophilic and heterophilic graphs. There are several interesting directions need further investigation. First, since better pseudo labels will benefit the label-wise message passing, it is promising to incorporate the predictions of LW-GNN in label-wise message passing. Second, in some applications such as link prediction, labels are not available. Therefore, we will investigate how to generate useful pseudo labels for label-wise aggregation for applications where no labeled nodes are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TRAINING ALGORITHM OF LW-GNN</head><p>The training algorithm of LW-GNN is shown in Algorithm 1. In line 1 and 2, we firstly train the to obtain the required pseudo labels for label-wise message passing. From line 4 to 6, we get the combined predictions from and and update the model selection weights with Eq. <ref type="bibr" target="#b12">(13)</ref>. From line 7 to 10, we update the model parameters and by minimizing L with Eq. <ref type="bibr" target="#b11">(12)</ref>. The updating of model selection weights and model parameters are conducted iteratively until convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF THEOREM 1</head><p>Proof. In this proof, we focus on nodes in class and class , where ? . Since dimensions of the node feature are independent to each other, without loss of generality, we consider one dimension of the feature and aggregated representation for node , which is denoted as and . For node in class , the aggregated representation in GCN layer is rewritten as:</p><formula xml:id="formula_17">= ?? ?N ( ) 1 |N ( )| .<label>(14)</label></formula><p>With assumptions in Sec. 3.2, the expectation of aggregated representations of nodes in class can be written as:</p><formula xml:id="formula_18">E( | = ) = ? ? + 1 ? ? ? 1 ?? =1, ? ,<label>(15)</label></formula><p>Similarly, we can get the expectation of aggregated nodes representations in class , i.e., E( | = ). Then, the difference between E( | = ) and E( | = ) is</p><formula xml:id="formula_19">? , = |E( | = ) ? E( | = )| = |? ? ( ? ) + 1 ? ? ? 1 ( ? )<label>(16)</label></formula><formula xml:id="formula_20">+ 1 ? ? ? 1 ?? =1, ? , ( ? )| = | ? ? 1 ? 1 ( ? ) + 1 ? ? ? 1 ( ?? =1 ( ? ))|</formula><p>When ? ? 1 , we can infer the upper bound of ? , as:</p><formula xml:id="formula_21">? , ? ? ? 1 ? 1 | ? | + 1 ? ? ? 1 ?? =1 | ? | = ? ? 1 (| ? | ? 1 ?? =1 | ? |) + 1 ? 1 ( ?? =1 | ? | ? | ? |),<label>(17)</label></formula><p>And the lower bound of ? , is: </p><formula xml:id="formula_22">? , ? ? ? 1 ? 1 | ? | ? 1 ? ? ? 1 ?? =1 | ? | = ? ? 1 (| ? | + 1 ?? =1 | ? |) ? 1 ? 1 ( ?? =1 | ? | + | ? |),<label>(18)</label></formula><p>Thus, when | ? | &gt; | ? |, ? ? {1, ... } and ? ? 1 , both the upper bound and lower bound of ? , will decrease with the decrease of ?,</p><p>Next, we will show that lower ? will lead to higher variance of aggregated nodes. According to Eq.(14), the variance of { : = } can be written as:</p><formula xml:id="formula_23">( | = ) = ( ?? ?N ( ) 1 |N ( )| | = )</formula><p>According to the assumption 1, the neighbor features are conditional independent to each other given the label of the center node. And for each neighbor node ? N ( ), we have ( = | ) = ?, ( = | ) = 1?? ?1 , ? ? . Therefore, for neighbor node ? N ( ) of node whose label is , its features follow a mixed distribution:</p><formula xml:id="formula_24">( | = ) = ?? =1 ( = | = ) ( | = ) =? ? ( , ) + 1 ? ? ? 1 ?? =1, ? ( , )<label>(19)</label></formula><p>Using the variance of mixture distribution, the variance of node in class can be derived as</p><formula xml:id="formula_25">( | = ) = 1 ( | = ) = 1 (E[ ( | , = )] + [E( | , = )]) = 1 ? 2 + 1 ? ? ? 1 ?? =1, ? 2 + ? 2 + 1 ? ? ? 1 ?? =1, ? 2 ? (? + 1 ? ? ? 1 ?? =1, ? ) 2<label>(20)</label></formula><p>Let?= 1 =1 and 2 = 1 =1 ( ??) 2 . Then Eq.(20) can be rewritten as the following equation:</p><formula xml:id="formula_26">( | = ) = 1 ( ? ? 1 ? 1 2 + ? ? ? 1 ( 1 ?? =1 2 + 2 )+ + ? ? 1 ? 1 2 + ? ? ? 1?2 ? (? + 1 ? ? ? 1 ?? =1, ? ) 2 )<label>(21)</label></formula><p>As ? ? 1 , we can set = ? ?1 ?1 , 0 ? ? 1 and ?? ?1 = 1 ? . For the last three terms of Eq.(21), we have:</p><formula xml:id="formula_27">? ? 1 ? 1 2 + ? ? ? 1?2 ? (? + 1 ? ? ? 1 ?? =1, ? ) 2 = 2 + (1 ? )?2 ? ( + (1 ? )?) 2 = (1 ? ) ( ??) 2 ? 0<label>(22)</label></formula><p>Combining Eq.(21) and Eq.(22), we are able to get the lower bound of the variance as:</p><formula xml:id="formula_28">( | = ) ? ? ? 1 ( ? 1) 2 + ? ? ( ? 1) ( 1 ?? =1 2 + 2 ) = ? ( ? 1) ( 2 ? 2 ? 1 ?? =1 2 ) + 1 ( ? 1) ( 2 + ?? =1 2 ? 2 )<label>(23)</label></formula><p>When &gt; , we know that with the decrease of ?, the lower bound of ( | = ) will increase. Similarly, ( | = ) will also increase with a lower ?. Combining with |E( | = ) ? E( | = )| will decrease with the decrease of ?, we can conclude that when ? ? 1 , the graph with lower ? will lead to less discrimative aggregate representations.</p><p>When ? = 1 , we can get and ensure the high discriminability regardless the homophily ratio. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ANALYSIS ON HETEROPHILIC GRAPHS</head><p>In this section, we conduct empirical analysis to verify Assumption 2 in Sec.3.2. Specifically, we aim to show (i) For nodes in the same class, features of their neighbors in the same class are similar; (ii) For nodes in different classes, features of their neighbors in the same class follow different distributions. Let X = { : = , = , ? N ( ), ? V} be the set of neighbors which belong to class and are linked by the central node in class . For neighbors in class , We analyze the average similarity scores between X and X to investigate whether neighbors in class that are linked by center nodes in different classes follow different distributions. The results on Crocodile, Chameleon, and Squirrel for representative neighbor classes are presented in <ref type="figure" target="#fig_9">Fig. 5-7</ref>, where ( , )-th element in the similarity matrix denotes the average node feature cosine similarity between X and X . From this figure, we can observe that:</p><p>? For X , ? ? 1, . . . , , its intra-group similarity score is very high.</p><p>This proves that the heterophilic neighbors' features are similar when the nodes are in the same class. ? The similarity scores between X and X are very small when ? . This indicates that for nodes in different classes their heterophilic neighbors belonging to the same class still differs a lot.</p><p>With the above observations, Assumption 2 is justified.  <ref type="table" target="#tab_5">Table 5</ref> gives additional ablation studies on Pubmed, Chameleon, and Squirrel. The observations are similar to that of <ref type="table" target="#tab_3">Table 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL ABLATION STUDIES</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Conference' 17 ,</head><label>17</label><figDesc>July 2017, Washington, DC, USA ? 2022 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of different message-passing mechanisms on the heterophilic graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>And dimensions of node feature are independent to each other; (ii) For nodes in different classes, their heterophilic neighbors' features follow different distributions. Specifically, let N ( ) denote node 's neighbors of class . For two nodes and in classes and ( ? ), the features of their heterophilic neighbors N ( ) and N ( ) in class ? {1, ..., } follow two different normal distributions ( , ) and ( , )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 3 . 2 .</head><label>32</label><figDesc>For an attributed graph G = (V, E, X) that follows the above assumptions in Sec. 3.2, if | ? | &gt; | ? | and &gt; , ? ? {1, .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>The overall framework of our LW-GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 3 . 3 .</head><label>33</label><figDesc>We consider an attributed graph G = (V, E, X) that follows the aforementioned assumptions in Sec. 3.2. If | ? | &gt; ?? , the heterophilic context representations learned by label-wise aggregation, i.e., a , = ?N ( ) 1 |N ( ) | x will keep its discriminability regardless the value of homophily ratio ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Representation similarity distributions on Texas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Classification accuracy with different model depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(a) Neighbors in Class 1 (b) Neighbors in Class 2 Figure 5 :Figure 6 :</head><label>1256</label><figDesc>Similarity matrices of neighbors linked with nodes in different classes on Crocodile (a) Neighbors in Class 1 (b) Neighbors in Class 2 Similarity matrices of neighbors linked with nodes in different classes on Chameleon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Similarity matrices of neighbors linked with nodes in different classes on Squirrel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 8</head><label>88</label><figDesc>Performance with different model depth. F ADDITIONAL RESULTS ABOUT THE NUMBER OF LAYERS OF LABEL-WISE AGGREGATION gives additional analysis of the impacts of model depth on two heterophilic graphs, i.e., crocodile and Texas. The observations in Figure 8 are similar to Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">Nodes Edges</cell><cell cols="2">Classes Hom. Ratio</cell></row><row><cell>Texas</cell><cell></cell><cell>183</cell><cell>309</cell><cell>5</cell><cell>0.11</cell></row><row><cell cols="3">Chameleon 2,277</cell><cell>36,101</cell><cell>5</cell><cell>0.24</cell></row><row><cell cols="2">Squirrel</cell><cell>5,201</cell><cell>217,073</cell><cell>5</cell><cell>0.22</cell></row><row><cell cols="2">Crocodile</cell><cell cols="2">11,631 360,040</cell><cell>5</cell><cell>0.25</cell></row><row><cell>Cora</cell><cell></cell><cell>2,708</cell><cell>5,429</cell><cell>6</cell><cell>0.81</cell></row><row><cell cols="2">Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>7</cell><cell>0.74</cell></row><row><cell cols="2">Pubmed</cell><cell cols="2">19,717 44,338</cell><cell>3</cell><cell>0.8</cell></row><row><cell cols="3">Updating Lower Level</cell><cell cols="3">and . Instead of calculating  *  and</cell></row><row><cell cols="6">*  per outer iteration, we fix 1 and 2 and update the mode pa-</cell></row><row><cell>rameters</cell><cell>and</cell><cell cols="2">for steps by:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Node classification performance (Accuracy(%) ? Std.) on heterophilic graphs.</figDesc><table><row><cell>Method</cell><cell>Texas</cell><cell cols="3">Chameleon Squirrel Crocodile</cell></row><row><cell>MLP</cell><cell>78.1 ?6.0</cell><cell>48.0 ?1.5</cell><cell cols="2">32.3 ?1.8 65.8 ?0.7</cell></row><row><cell>GCN</cell><cell>57.6 ?5.9</cell><cell>63.5 ?2.5</cell><cell cols="2">46.7 ?1.5 66.7 ?1.0</cell></row><row><cell>MixHop</cell><cell>60.6 ?7.7</cell><cell>61.2 ?2.2</cell><cell cols="2">44.1 ?1.1 67.6 ?1.3</cell></row><row><cell>SuperGAT</cell><cell>58.6 ?7.7</cell><cell>59.4 ?2.5</cell><cell cols="2">38.9 ?1.5 62.6 ?0.9</cell></row><row><cell>GCNII</cell><cell>68.6 ?9.8</cell><cell>63.5 ?2.5</cell><cell cols="2">49.4 ?1.7 69.0 ?0.7</cell></row><row><cell>FAGCN</cell><cell>79.5 ?4.8</cell><cell>63.9 ?2.2</cell><cell cols="2">43.3 ?2.5 67.1 ?0.9</cell></row><row><cell>SimP-GCN</cell><cell>80.5 ?5.9</cell><cell>63.7 ?2.3</cell><cell cols="2">42.8 ?1.4 63.7 ?2.3</cell></row><row><cell>H2GCN</cell><cell>83.7 ?6.0</cell><cell>54.2 ?2.3</cell><cell cols="2">36.0 ?1.1 66.7 ?0.5</cell></row><row><cell>LW-GNN</cell><cell>84.5 ?5.1</cell><cell>69.4 ?1.4</cell><cell cols="2">55.8 ?1.1 77.4 ?0.6</cell></row><row><cell>Weight for</cell><cell>0.960</cell><cell>0.986</cell><cell>0.987</cell><cell>0.999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Node classification performance (Accuracy(%) ? Std.) on homophilic graphs.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>MLP</cell><cell>58.6 ?0.5</cell><cell>60.3 ?0.4</cell><cell>72.7 ?0.4</cell></row><row><cell>GCN</cell><cell>81.6 ?0.7</cell><cell>71.3 ?0.3</cell><cell>78.4 ?1.1</cell></row><row><cell>MixHop</cell><cell>80.6 ?0.2</cell><cell>68.7 ?0.3</cell><cell>78.9 ?0.5</cell></row><row><cell>SuperGAT</cell><cell>82.7 ?0.4</cell><cell>72.2 ?0.8</cell><cell>78.4 ?0.5</cell></row><row><cell>GCNII</cell><cell>84.2 ?0.5</cell><cell>72.0 ?0.8</cell><cell>80.2 ?0.2</cell></row><row><cell>FAGCN</cell><cell>83.1 ?0.6</cell><cell>71.7 ?0.6</cell><cell>78.8 ?0.3</cell></row><row><cell>SimP-GCN</cell><cell>82.8 ?0.1</cell><cell>71.8 ?0.8</cell><cell>80.3 ?0.2</cell></row><row><cell>H2GCN</cell><cell>81.6 ?0.4</cell><cell>71.0 ?0.5</cell><cell>79.5 ?0.2</cell></row><row><cell>LW-GNN</cell><cell>84.3 ?0.3</cell><cell>72.3 ?0.4</cell><cell>80.4 ?0.3</cell></row><row><cell>Weight for</cell><cell>0.001</cell><cell>0.006</cell><cell>0.005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study ?0.3 72.3 ?0.4 84.5 ?5.1 77.4 ?0.6</figDesc><table><row><cell>Datasets</cell><cell cols="2">Homophilic</cell><cell cols="2">Heterophilic</cell></row><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Texas</cell><cell>Crocodile</cell></row><row><cell>MLP</cell><cell cols="4">58.7 ?0.5 60.3 ?0.4 78.1 ?6.0 65.8 ?0.7</cell></row><row><cell>GCN</cell><cell cols="4">81.6 ?0.7 71.3 ?0.3 57.6 ?5.9 66.7 ?1.0</cell></row><row><cell>GCNII</cell><cell cols="4">84.2 ?0.5 72.0 ?0.8 68.6 ?9.8 69.0 ?0.7</cell></row><row><cell>LW-GNN\P</cell><cell cols="4">84.2 ?0.3 72.3 ?0.5 80.2 ?3.8 76.1 ?0.8</cell></row><row><cell>LW-GNN\G</cell><cell cols="4">75.3 ?0.4 65.1 ?0.5 84.3 ?4.0 77.3 ?0.7</cell></row><row><cell>LW-GNN</cell><cell cols="4">81.9 ?0.2 71.6 ?0.3 83.8 ?3.8 77.2 ?1.1</cell></row><row><cell>LW-GNN</cell><cell>84.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Training Algorithm of LW-GNN</figDesc><table><row><cell cols="4">Input: G = (V, E, ), Y , , , ,</cell><cell>and</cell></row><row><cell cols="4">Output: , , , 1 and 2</cell></row><row><cell cols="4">1: Train by optimizing Eq.(4) w.r.t</cell></row><row><cell cols="4">2: Obtain pseudo labels? by Eq.(3)</cell></row><row><cell cols="2">3: repeat</cell><cell></cell><cell></cell></row><row><cell>4:</cell><cell cols="4">Get combined predictions of and</cell><cell>on V</cell></row><row><cell>5:</cell><cell cols="3">Calculate the upper level loss L</cell></row><row><cell>6:</cell><cell cols="4">Update 1 and 2 according to Eq.(13)</cell></row><row><cell>7:</cell><cell cols="2">for = 1 to do</cell><cell></cell></row><row><cell>8:</cell><cell cols="3">Obtain the lower level loss L</cell></row><row><cell>9:</cell><cell>Update</cell><cell>and</cell><cell>by Eq.(12)</cell></row><row><cell>10:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell cols="3">11: until convergence</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Additional ablation studies.</figDesc><table><row><cell>Datasets</cell><cell>Homophilic</cell><cell cols="2">Heterophilic</cell></row><row><cell></cell><cell>Pubmed</cell><cell>Chameleon</cell><cell>Squirrel</cell></row><row><cell>MLP</cell><cell>72.7 ?0.4</cell><cell>48.0 ?1.5</cell><cell>32.3 ?1.8</cell></row><row><cell>GCN</cell><cell>78.4 ?1.1</cell><cell>63.5 ?2.5</cell><cell>46.7 ?1.5</cell></row><row><cell>GCNII</cell><cell>80.2 ?0.2</cell><cell>63.5 ?2.5</cell><cell>49.4 ?1.7</cell></row><row><cell>LW-GNN\P</cell><cell>77.6 ?0.7</cell><cell>64.1 ?1.9</cell><cell>54.6 ?1.8</cell></row><row><cell>LW-GNN\G</cell><cell>72.4 ?0.6</cell><cell>69.1 ?1.7</cell><cell>55.8 ?1.5</cell></row><row><cell>LW-GNN</cell><cell>79.2 ?0.8</cell><cell>68.3 ?1.6</cell><cell>55.7 ?1.2</cell></row><row><cell>LW-GNN</cell><cell>80.4 ?0.3</cell><cell>69.4 ?1.4</cell><cell>55.8 ?1.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>If &gt; ? | ? |, ? ? {1, . . . , }, we can get ( | = ) &gt; ? 2 , . So when ? = 1 and &gt; ? | ? |, ? ? {1, . . . , } , the representations after the averaging process will be undiscrimative. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF OF THEOREM 2</head><p>Proof. In this proof, we also consider a center node in class . And we focus on one dimension of the node feature and aggregated representation. Specifically, for each dimension, the label-wise aggregation can be written as:</p><p>where , denotes the aggregated feature of neighbors in class .</p><p>Since ? N ( ), we know node 's features follows distribution as ? ( , ). The mean of , in Eq.(26) is given as:</p><p>Then the absolute difference between E( , | = ) and E( , | = ) will be:</p><p>Given the assumption that the features are conditionally independent given the label of center node, the variance of , can be written as:</p><p>In label-wise aggregation, we generally concatenate the { , : ? {1, . . . }} for further classification. Therefore, the lower bound of discriminability can be given by the representation of the class that are most discriminative, which can be formally written as:</p><p>When ? ? 1 , we can get:</p><p>As for ? ? 1 , let ? we can infer that:</p><p>With the two above equations, regardless of the value of homophily ratio ?, the following inequality holds:</p><p>Therefore, we can get that when | ? | &gt; ?? ,</p><p>( , * | = ) &gt; 1. This shows that label-wise aggregation can preserve the context</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixhop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Molecular generative graph neural networks for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bongini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Say no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing graph neural network-based fraud detectors against camouflaged fraudsters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Node similarity preserving graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cayleynets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geom-Gcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gcc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-stage self-supervised learning for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A semi-supervised graph attentive network for financial fraud detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised graph-to-graph translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1863" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11468</idno>
		<title level="m">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-supervised training of graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02380</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
