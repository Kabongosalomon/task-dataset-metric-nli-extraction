<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Seizure Detection using EEG: A Comprehensive Comparison of Recent Approaches under a Realistic Setting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanhyung</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyewon</forename><surname>Jeong</surname></persName>
							<email>hyewonj@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghwa</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
							<email>edwardchoi@kaist.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">AITRICS</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">National Health Insurance Service Ilsan Hospital, Republic of Korea Severance Children&apos;s Hospital, Yonsei University College of Medicine, Republic of Korea Hoon-Chul Kang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Severance Children&apos;s Hospital</orgName>
								<orgName type="department" key="dep2">Yonsei University College of Medicine</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Seizure Detection using EEG: A Comprehensive Comparison of Recent Approaches under a Realistic Setting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Massachusetts Institute of Technology, USA AITRICS, Republic of Korea The Cooper Union for the Advancement of Science and Art, USA AITRICS, Republic of Korea * These authors contributed equally ? This work was done while the authors were in AITRICS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Electroencephalogram (EEG) is an important diagnostic test that physicians use to record brain activity and detect seizures by monitoring the signals. There have been several attempts to detect seizures and abnormalities in EEG signals with modern deep learning models to reduce the clinical burden. However, they cannot be fairly compared against each other as they were tested in distinct experimental settings. Also, some of them are not trained in real-time seizure detection tasks, making it hard for ondevice applications. In this work, for the first time, we extensively compare multiple state-ofthe-art models and signal feature extractors in a real-time seizure detection framework suitable for real-world application, using various evaluation metrics including a new one we propose to evaluate more practical aspects of seizure detection models. * These authors contributed equally ? This work was done while the authors were in AITRICS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Dataset and Code Availability</head><p>The data used for our work can be found from the Temple University Hospital (TUH) EEG Seizure Corpus (TUSZ) dataset V1.5.2 (https://isip.piconepress. com/projects/tuh eeg/html/downloads.shtml). Our code is available at https://github.com/AITRICS/ EEG real time seizure detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Introduction</head><p>Epilepsy is a neurologic disorder characterized by epileptic seizures <ref type="bibr">(Fisher et al., 2014</ref>) and the precise frequency and type of seizures of the patient should be identified in order to diagnose epilepsy. This affects the decisions of pharmacological treatment, dietary therapy, and surgical treatment <ref type="bibr">(Scheffer et al., 2017)</ref>. In this process, electroencephalogram (EEG) is an essential diagnostic tool for seizure detection. Inpatient video-EEG recording is a routine diagnostic method for capturing seizure events for deciding proper future treatment. However, many of the event onsets are prone to be missed on-site as clinicians cannot always be present by the patient, and they cannot rewind the whole 24-hour video to find out the actual onset <ref type="bibr">(Nordli Jr, 2006)</ref>. Family members of a patient can press a button to denote the onset of the seizure event, but non-experts (i.e. family member) can make false positive or false negative decision.</p><p>In order to accurately detect seizures for prompt intervention, we need an on-site seizure detector that can be used while EEG is being recorded. This task can, in theory, be done by a well-trained neurologist where they routinely scan the recorded EEG signals to find out the characteristic signal for the potential diagnoses. This manual process, however, can take up to several hours, making it labor-intensive as well as untimely. To improve the timeliness of the seizure detection and reduce the burden of clinicians, we need a machine that accurately detects the seizure signal in real-time to help physicians diagnose and prescribe.</p><p>Due to the need of an accurate seizure detector, several hospital-based research organizations released public benchmark datasets <ref type="bibr">(Obeid and Picone, 2016;</ref><ref type="bibr">Shah et al., 2018;</ref><ref type="bibr">Shoeb, 2009;</ref><ref type="bibr">Goldberger et al., 2000;</ref><ref type="bibr">Andrzejak et al., 2001)</ref> for the seizure detection task. There have been several attempts to detect seizure and abnormalities with these public EEG signal datasets using deep neural network models <ref type="bibr" target="#b0">(Acharya et al., 2018;</ref><ref type="bibr">Roy et al., 2019b;</ref><ref type="bibr">Mohsenvand et al., 2020;</ref><ref type="bibr">Golmohammadi et al., 2017;</ref><ref type="bibr">Akbarian and Erfanian, 2020;</ref><ref type="bibr">Zabihi et al., 2015)</ref>. More recent works have focused on multi-class seizure <ref type="bibr">detection (Sriraam et al., 2019;</ref><ref type="bibr">Daoud and Bayoumi, 2019;</ref><ref type="bibr">Ahmedt-Aristizabal et al., 2020;</ref><ref type="bibr">Priyasad et al., 2021;</ref><ref type="bibr">Jia et al., 2022)</ref>, and real-time seizure detection <ref type="bibr">(Shawki et al., 2020;</ref><ref type="bibr">Bomela et al., 2020;</ref><ref type="bibr">Thyagachandran et al., 2020;</ref><ref type="bibr">Fan and Chou, 2018)</ref>. Although previous studies have explored multiple model architectures for seizure detection, we cannot fairly compare each of the model as they used distinct experimental settings and evaluation metrics. Also, not all the studies adopted the real-time seizure detection tasks, limiting the practicality of the models.</p><p>In this paper, for the first time, we present and compare the performance of various state-of-the-art models and feature extraction strategies under the same experimental setting of real-time binary seizure detection ( <ref type="figure" target="#fig_0">Figure 1</ref>). We performed the seizure (ictal) onset detection task, where the model receives EEG signal and detects whether the patient is currently in ictal phase or not, given that the entire process from feature extraction to detection must be completed within the stride time of the sliding window in order to guarantee real-time process. We also proposed margin-based evaluation metric, which is an evaluation metric providing more useful information to physicians by assessing whether a model detects seizure events within the margin from the seizure onset and offset. Our work thus presents a comprehensive summary of various models' performance under a strictly practical and realistic setting, such that researchers and practitioners can directly refer to our study for deploying their own models in practice, or use our study as a benchmark for developing a new seizure detection model in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Signal Feature Extractor</head><p>Signal processing and frequency extraction are useful tools for extracting features from raw signal, which has been used for preprocessing speech signal or biomedical signals (Electrocardiogram (ECG), Heart sound, or EEG signal). One of the popular feature extractors for medical signals is Short Time Fourier Transform (STFT) <ref type="bibr">(Sejdi? et al., 2009;</ref><ref type="bibr">Springer et al., 2015;</ref><ref type="bibr">Sriraam et al., 2019)</ref>, which applies a sequence of Fast Fourier transforms (FFT) to signals using a sliding window. Some of the previous works used specific frequency bands out of extracted important frequency features (e.g., delta wave (0-4 Hz), theta wave (4-8 Hz), or alpha wave <ref type="bibr">(8)</ref><ref type="bibr">(9)</ref><ref type="bibr">(10)</ref><ref type="bibr">(11)</ref><ref type="bibr">(12)</ref>) that are related to brain activity <ref type="bibr">(Parvez and Paul, 2014;</ref><ref type="bibr">Liu et al., 2019)</ref>. Other seizure detection models <ref type="bibr">(Shawki et al., 2020;</ref><ref type="bibr">Thyagachandran et al., 2020;</ref><ref type="bibr">Golmohammadi et al., 2019)</ref> used linear scale filter bank on STFT output matrix, which is called Linear frequency cepstral coefficients (LFCC). A recent work <ref type="bibr">(Priyasad et al., 2021) utilized SincNet (Ravanelli and</ref><ref type="bibr">Bengio, 2018)</ref> filters onto channel-wise one dimensional sigal input for seizure detection task. SincNet extracts signal features with Convolutional Neural Networks (CNN) Sinc function filters, without traditional feature extracion methods. As recent deep learning architectures such as CNN provide flexible and powerful feature extraction, recent works presented high performing deep learning models using raw signal without feature extraction in various tasks including speech recognition (Wav2vec2.0 <ref type="bibr">(Baevski et al., 2020)</ref>) and EEG seizure detection task <ref type="bibr">(Roy et al., 2019b;</ref><ref type="bibr">Mohsenvand et al., 2020)</ref>). <ref type="bibr">Mohsenvand et al. (2020)</ref> downsampled the raw input signal to multiple frequencies to extract more diverse feature information. We extracted signal features from EEG input with these diverse signal feature extraction methods  <ref type="figure" target="#fig_2">Figure 2</ref>). The models detect whether ictal/non-ictal signal appears within the 4-second sliding window input. We present an example case with a Raw EEG signal but other signal feature extractors can also be applied to the pipeline.</p><p>( <ref type="figure" target="#fig_2">Figure 2</ref>) that were frequently used for biomedical signal or speech processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Seizure Detection</head><p>Recent studies introduced deep neural network models to binary and multi-class EEG seizure detection and classification. In this paper, we are focusing on binary seizure detection for simpler setting. A simple seizure detector with CNN was proposed to detect seizure <ref type="bibr">(Andrzejak et al., 2001;</ref><ref type="bibr" target="#b0">Acharya et al., 2018;</ref><ref type="bibr">Yuan et al., 2018b)</ref>. Other works used Long Short Term Memory (LSTM) Hochreiter and Schmidhuber (1997) based architectures to capture temporal dependencies. <ref type="bibr">Golmohammadi et al. (2017)</ref> compared Gated Recurrent Unit (GRU) and LSTM for real-time binary seizure detection. Another work encoded the graph theoretic relationship between EEG leads into features to detect seizure onset time <ref type="bibr">(Akbarian and Erfanian, 2020;</ref><ref type="bibr">Zabihi et al., 2015)</ref> on CHB-MIT dataset <ref type="bibr">(Shoeb, 2009;</ref><ref type="bibr">Goldberger et al., 2000)</ref>. Extending binary detection models, there has been recent attempts to classify multiple seizure types using CNN architecture <ref type="bibr">(Asif et al., 2020;</ref><ref type="bibr">Sriraam et al., 2019;</ref><ref type="bibr">Daoud and Bayoumi, 2019;</ref><ref type="bibr">Ahmedt-Aristizabal et al., 2020;</ref><ref type="bibr">Priyasad et al., 2021;</ref><ref type="bibr">Jia et al., 2022</ref><ref type="bibr">), including AlexNet (Krizhevsky et al., 2012</ref><ref type="bibr">Sriraam et al., 2019</ref><ref type="bibr">), ResNet (He et al., 2016</ref><ref type="bibr">), DenseNet (Huang et al., 2017</ref><ref type="bibr">), and MobileNetV3 (Howard et al., 2019</ref>. Some of the works detected abnormal signals in input EEG signal <ref type="bibr">(Roy et al., 2019b;</ref><ref type="bibr">Mohsenvand et al., 2020)</ref>. Rather than classifying the signal segment to single or multiple seizure types, Daoud and Bayoumi (2019) predicted seizure events before it occurs. However, many of the previous models cannot be applied for real-time seizure detection because they are trained to detect seizure events on the whole EEG signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Real-Time Seizure Detector</head><p>More recent works focused on building real-time seizure detectors, using various experimental settings. Channel-wise LSTM was used in combination with LFCC as feature extractor <ref type="bibr">(Shawki et al., 2020)</ref>, and Time Delay Neural Networks (TDNN) with LSTM using LFCC (Thyagachandran et al., 2020) was applied on Temple University Hospital (TUH) dataset (Obeid and Picone, 2016), one of the public benchmark EEG dataset for seizure detection task. Some of the real-time seizure detectors are inappropriate for clinical use as they are slow detector. <ref type="bibr">Shawki et al. (2020)</ref> achieved computation speed of 1.81 seconds per 1 second sliding window with CPU processor, which is too slow for real-time processing. Another work (Bomela et al., 2020) did not report the latency of a detector. Also some used small-scale CHB-MIT dataset that includes EEG signal of only 25 patients <ref type="bibr">(Fan and Chou, 2018;</ref><ref type="bibr">Wang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Metric for Seizure Detection</head><p>The performance of a model on EEG seizure detection can be measured by evaluating how well a model detected seizure events compared to the event label. Therefore, the evaluation metric should be valid for the seizure detection task <ref type="bibr">(Ziyabari et al., 2017)</ref>, distinct from the accuracy measure used in time-series data or the word error rate used in the speech recognition task <ref type="bibr">(Wang et al., 2003;</ref><ref type="bibr">Mostefa et al., 2006)</ref>. Scoring the performance of a model based on the event is called term-based scoring, and metrics evaluating per channel and per unit window is called the epoch-based method. Any-Overlap (OVLP) <ref type="bibr">(Wilson et al., 2003;</ref><ref type="bibr">Gotman et al., 1997)</ref> is a term-based scoring method that measures True Positive (TP) by counting the overlapping seizure detection hypothesis and label. OVLP is thus more permissive, outputs high sensitivity as it only considers the event-based scoring, not considering the duration of the event. Time-Aligned Event Scoring (TAES) improves this method by weighting the amount of overlap between label and model prediction. These two methods assess the model prediction within the whole signal, which is not suitable for real-time seizure detection task. Epoch-Based Sampling (EPOCH) (Baldassano et al., 2016) provides a more relevant metric for the real-time seizure detection, which measures the overlap between label and prediction within a unit time window (that is a data point within an epoch), per EEG channel. In this work, we propose a new metric called MARGIN which evaluates the detection performance of the seizure detection model within onset/offset margin. Other than these four evaluation metric, we additionally measured average onset latency time and calculated the average onset latency time of seizure events. Summary of each metric is in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We used the public benchmark dataset, the Temple University Hospital EEG Seizure Data Corpus (V1.5.2) (Obeid and Picone, 2016) (TUH v 1.5.2), which contains the largest number of seizure types and patient instances. We provide better experimental setting with the largest dataset as some of the prior works used private datasets (Daoud and Bayoumi, 2019) and small-scale public datasets <ref type="bibr">(Akbarian and Erfanian, 2020;</ref><ref type="bibr">Fan and Chou, 2018;</ref><ref type="bibr">Zabihi et al., 2015)</ref> which makes it difficult to reproduce and generalize the results.</p><p>The number of EEG signal types and patient number are summarized in <ref type="table" target="#tab_0">Table 1 and Appendix Table 5</ref>. Among total 7,034 signal streaks from 642 patients, there were total 304 patients with 3,047 signal streaks which each includes one or more seizure episode (Patients group in <ref type="table" target="#tab_0">Table 1</ref>). The dataset also included 338 patients in Normal Control group, which is the group without any seizure events. There were total 1,838 normal signals from Normal Control, and 2,149 normal signals from Patients group. We have 589 subjects in the training dataset and 51 subjects in the development dataset (640 subjects), which we divide into validation (25 subjects) and test set (26 subjects) according to patient instances. In order to prevent dataset shift, we randomly sample signal types within a batch so that each batch contains equal number of signal type <ref type="table" target="#tab_0">(Table 1</ref>, Appendix <ref type="figure" target="#fig_6">Figure 5</ref>). Additional details of the dataset, including slight modification from the original TUH 1.5.2 open data, can be found in Appendix Section A.1. The original EEG signal data was uniformly resampled with the sampling rate of 200 Hz to standardize the sampling rate of all signals (Section 4.3 6) for more detail). We sampled the EEG signals from the leads listed in <ref type="figure" target="#fig_7">Figure 6</ref> for different types of EEG montage (Unipolar, Transverse Bipolar Montage). We simply subtracted the voltage between Unipolar leads to acquire Bipolar montage signal data, following Bipolar lead info from SeizureNet (Asif et al., 2020) (Appendix <ref type="figure" target="#fig_7">Figure 6</ref>, Table (a)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Real-Time Seizure Detection</head><p>The real-time seizure detector explained in <ref type="figure" target="#fig_0">Figure 1</ref> can be directly used for clinical setting, where the model receives signal input of 4 seconds window ondevice every 1 second (i.e. window shift length). Thus, we would want a model to process each window within the shift length (1 second). We prepare each downsampled dataset into the size of window length (4 seconds), and slided the window with 1 second window shift length. We found the optimal window length by considering processing speed, and optimal shift length by considering the performance (Appendix Section B.2).</p><p>After preparing EEG input, we extracted features with signal feature extractors (Section 4.3, <ref type="table" target="#tab_2">Table 3</ref>), and fed the extracted signal into each model (Section 4.4) to obtain the detected output. The models were mainly evaluated with EPOCH metric and MAR-GIN as it provides per-window evaluation (EPOCH) and measures practicality of the model as real-time seizure detector (MARGIN); we also explored other evaluation metrics on all 15 models (Appendix <ref type="table" target="#tab_0">Table  13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Signal Feature Extractor</head><p>We explore six different signal feature extraction techniques to generate input data for CNN and LSTM combined models ( <ref type="figure" target="#fig_2">Figure 2</ref>). All six methods affect the final performance of a model, and each has different processing speed and feature representation ( <ref type="table" target="#tab_2">Table 3</ref>). Note that different types of first CNN2D block was applied to the output of each feature extractor, which is described in detail in each section below. For more detailed information on signal feature extractor (Section A.2), CNN1D input ( <ref type="figure" target="#fig_8">Figure  7</ref>), experimental result with various models <ref type="table" target="#tab_0">(Table  13)</ref>, please see Appendix.</p><p>1) Raw: Downsampled raw EEG signal (without feature extraction) passed to a CNN2D block with kernel shape 1 ? 51 with stride 1 ? 4.</p><p>2) SincNet (Ravanelli and Bengio, 2018): We performed time-domain convolution between the input EEG signal and SincNet filters in the first CNN2D layer to preserve more information from the input signal frequency. We used 7 distinct SincNet bands with trainable frequency band range with the stride of 2. CNN2D block with kernel shape 7 ? 21 with stride of 7 ? 2 was applied to channel-wise 2D input.</p><p>3) Short Time Fourier Transform (STFT): Fast Fourier Transform (FFT) was applied with sliding window of frame length 0.125 seconds and frame shift length (hop length or overlapping size) across 50% of frame length, following the Hann window function <ref type="bibr">(Oppenheim, 1999)</ref>. The extracted feature represents information from both frequency and temporal domain. STFT can be also further processed with various steps, such as Frequency Bands and LFCC. 4) Frequency Bands: STFT was applied to extract frequency time features over 7 frequency bands: <ref type="bibr">1-4, 4-8, 8-12, 12-30, 30-50, 50-70, 70-100</ref> Hz. Features on each band is then averaged over a signal. We follow the parameters from <ref type="bibr">Liu et al. (2019)</ref> and <ref type="bibr">Mirowski et al. (2009)</ref>, which used predefined frequency ranges. We applied the same CNN2D block from SincNet: kernel shape 7?21 with stride of 7?2. 5) Linear frequency cepstral coefficients (LFCC): We used 0.3 seconds window length and 0.15 seconds window shift length to generate 8 absolute LFCC features out of the feature extractor, following setup from <ref type="bibr">Shawki et al. (2020)</ref>. 6) Downsampled Raw <ref type="bibr">(50,</ref><ref type="bibr">100,</ref><ref type="bibr">200Hz)</ref> We resampled the original 200Hz raw signal data to 100Hz and 50Hz raw signals. The three differently sampled signals <ref type="bibr">(50,</ref><ref type="bibr">100,</ref><ref type="bibr">and 200 Hz)</ref> are then fed to CNN block separately and concatenated for further training. Sampling rates of 50, 100, 200Hz are enough to represent the features as it is more than twice the characteristic frequency range of seizure signals (0.5-25 Hz) <ref type="bibr">(Shoeb, 2009</ref>). Furthermore, anti-aliased and downsampled signal does not improve the performance of our model <ref type="table" target="#tab_0">(Table 18</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Models</head><p>We extensively searched over 15 architectures for realtime seizure onset detection on both Unipolar (Appendix <ref type="table" target="#tab_0">Table 11</ref>) and Bipolar (Table 2) montage. We also measured the model size (Appendix <ref type="table" target="#tab_7">Table 7</ref>) and inference speed on CPU and GPU to analyze the feasibility of each model as the real time detector (Appendix section A.4, <ref type="table" target="#tab_1">Table 2</ref>). Details on the parame-ters and architectures of each model are summarized in Appendix Section A.3, B.1, and <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">CNN2D with LSTM</head><p>CNN2D with LSTM consists of three parts: CNN encoder for feature extraction, Recurrent Neural Network (RNN) module for temporal feature representation, and classifier ( <ref type="figure" target="#fig_0">Figure 1</ref>). After signal feature extraction, we applied CNN2D layer with 1D kernel (filters ? number of channels ? timestep) so that this shape filter is applied to each EEG channel separately, such that we can preserve channel-wise information within EEG signal. This also brings a model the faster feature extraction compared to the computation-heavier model with multiple channelwise 1D CNN kernels <ref type="bibr">(Priyasad et al., 2021;</ref><ref type="bibr">Mohsenvand et al., 2020)</ref>. We tested the following five different CNN2D + LSTM models where CNN2D layer is followed by 2D adaptive average pooling module with RNN models, and then classifier block ( <ref type="figure">Figure  3</ref>). 1) CNN2D+LSTM: As mentioned above, channelwise CNN2D layer and LSTM are used for both channel-wise signal shape feature and temporal feature for CNN2D+LSTM model. 2) CNN2D+BLSTM: Same model architecture with 1) but alternatively with Bidirectional Long Short-Term Memory (BLSTM) in RNN module for temporal feature representation.</p><p>3) ResNet-short+LSTM: CNN2D+LSTM model where CNN blocks are from ResNet model (He et al., 2016) helping a model learn both simple and rather complex shape for each seizure type with flexible gradient flow through skip connections. 4) ResNet-short+Dilation+LSTM: Almost same model architecture with 3) but three parallel Dilated Convolution (dilation of 1 ? 1, 1 ? 2, 1 ? 4) was used in the first CNN2D block. The three outputs of the Dilated Convolution blocks are then concatenated and forwarded to CNN blocks from ResNet model as in 3). We expect the three dilated convolution would enable CNN block to receive diverse range of signals, which is similar to down-sampled raw (Mohsenvand et al., 2020) but with higher temporal resolution as the filter is shared across temporal axis. The more dilation the kernel has, the wider range of lower frequency signals the CNN model receives. to perform on par with other CNN models but with small size, available for the off-line portable device. In this model, MobileNetV3 CNN blocks are used for feature extraction and connected to LSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">CNN1D with LSTM</head><p>CNN1D with LSTM models follow the same structure as CNN2D + LSTM models (CNN, RNN, followed by Classifier layer) but with CNN1D block. Unlike previous EEG seizure type classification models <ref type="bibr">(Priyasad et al., 2021;</ref><ref type="bibr">Mohsenvand et al., 2020)</ref> which used multiple channel-wise CNN1D modules, our model does not have channel-wise CNN encoder. Our model rather has a CNN1D encoder with the filter size following the number of channels ? timestep in order to increase the speed for real-time seizure detection. 6) CNN1D+LSTM: This model consists of a CNN 1D feature extraction block, followed by 1D adaptive average pooling module, LSTM layers, and classifier block ( <ref type="figure" target="#fig_8">Figure 7)</ref>. 7) CNN1D+BLSTM: Same model architecture with 6) but with BLSTM instead of LSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Benchmark Models</head><p>We adopted the original architecture of six different benchmark models frequently used for EEG seizure detection, abnormality detection, and seizure classi-  <ref type="figure">Figure 9</ref>). Unlike previous works using attention <ref type="bibr">module (Priyasad et al., 2021;</ref><ref type="bibr">Yuan et al., 2018b,a)</ref> that calculate importance of each EEG channel, we can extract the relationship between the channels from the measured channel-to-channel similiarity. 15) Guided Feature Transformer: The model considers the neighboring connection between all pairs of EEG leads by simply multiplying the channel adjacency mask (channel relation mask) to the self attention map (Appendix <ref type="figure">Figure 9</ref>). Examples of adjacent EEG leads with C3 are summarized in Appendix <ref type="figure" target="#fig_7">Figure 6</ref>, where we denote neighboring leads to be connected in Unipolar montage and directly used the connection between leads in Bipolar montage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation Metrics</head><p>We calculate the binary classification performance (e.g. AUROC, AUPRC) of the models on the test dataset, using four different methods to determine correct and incorrect classifications <ref type="figure" target="#fig_5">(Figure 4</ref>). To evaluate sensitivity (True Positive Rate, TPR) and specificity (True Negative Rate, TNR) using OVLP, TAES, EPOCH and onset/offset accuracy of MAR-GIN, we measured values over the whole EEG signal data of a patient where the model is trained with sliding window. In addition to four evaluation metrics introduced below, we measured the onset latency, which is the average latency from the label onset to hypothesis onset. 1) Any-Overlap (OVLP): OVLP provides sensitivity permissive metric which measures whether the prediction overlaps with the label.</p><p>2) Time-Aligned Event Scoring (TAES): TAES measures the amount of overlap between seizure label and prediction, by calculating the portion of correctly predicted among total duration of seizure label.</p><p>3) Epoch-Based Sampling (EPOCH): EPOCH outputs a summary score per each unit window, which evaluates the accuracy of prediction by comparing with the ground truth label. Each epoch contains the preprocessed EEG signals within a fixed time window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Margin-Based Boundary Evaluation (MARGIN): We propose a new evaluation metric for EEG seizure detection task. MARGIN evaluates whether a model has detected the seizure within a fixed margin (3 seconds, 5 seconds) before and after the onset / offset time of seizure. Thus, a model with higher MARGIN accuracy would more accurately detect the onset and offset of seizure event, thus would help prompt clinical intervention. MARGIN also compensates for the inherent measurement error in scalp EEG signal, as the signal itself is an indirect measure of neural activity. Because of this uncertainty in the measured signal, actual seizure can often be initiated earlier than the onset diagnosed by physicians. We thus give the model a safety margin to take into account situation where a model detects a seizure event before the onset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we report comparative experimental result on 15 model architectures <ref type="table" target="#tab_0">(Table 2, Appendix  Table 11</ref>), 6 signal feature extractors <ref type="table" target="#tab_2">(Table 3)</ref>, and 4 evaluation metrics <ref type="table" target="#tab_3">(Table 4</ref>). We first evaluate the performance and speed of 15 models <ref type="table" target="#tab_1">(Table 2)</ref> to find out the most favorable model for the real-time seizure detection task. We used the raw input feature as only the raw input showed the fastest processing speed on both CPU and GPU <ref type="table" target="#tab_2">(Table 3)</ref>, among all the other signal feature extractors. As model evaluation only used raw signal, we explored other well-known signal feature extractors so as to achieve a better performance given the CNN2D+LSTM model. We also explored four different evaluation metrics previously used for EEG detection task, as <ref type="table" target="#tab_1">Table 2</ref> only reports EPOCH and MARGIN. We used CNN2D+LSTM among the 15 models for comparing signal feature extractors and evaluation metrics as this model provided accurate predictions as well as fast inference speed. We also provide the same set of experiments with ResNet-short+LSTM, which achieved the highest performance on raw EEG signals with acceptable real-time seizure detection inference speed <ref type="table" target="#tab_1">(Table 2)</ref>, for cross-reference <ref type="table" target="#tab_0">(Table 15</ref>, 16, 17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Real-Time Seizure Detection</head><p>We compare the performance of real-time binary seizure detectors on both bipolar <ref type="table" target="#tab_1">(Table 2</ref>) and unipolar lead (Appendix <ref type="table" target="#tab_0">Table 11</ref>) EEG signal datasets. We used a window size of 4 seconds and a shift  Compared to CNN1D+LSTM, CNN2D+LSTM showed better performance as it considers independently extracted channel-wise feature information.</p><p>ChronoNet <ref type="bibr">(Roy et al., 2019b)</ref> performed the worst probably because we used the raw signal and did not follow the feature extractor (LFCC) from the original paper for fair comparison. Also, the model was originally designed for detecting abnormal signals (seizure or abnormal noisy signals) which is rather easier task compared to seizure detection. Feature Transformer also showed severely poor performance but gained significant performance increase after we applied the channel adjacency mask (i.e. Guided Feature Transformer). Thus, channel relation mask helps the model factor in similarity between channels on top of attention scaling, so that model can capture a seizure signal spreading out to the neighborhood leads.</p><p>Model performance on the Unipolar dataset showed similar trends as in the Bipolar dataset (Appendix <ref type="table" target="#tab_0">Table 11</ref>), although models suffered slight decrease in performance compared to the Bipolar dataset. This might be generally because Bipolar montage is effective in localizing and lateralizing seizure <ref type="bibr">(Beniczky and Schomer, 2020)</ref>. CNN2D + LSTM model performed on par with ResNet18 based models, and was fast enough to meet the real-time constraint (i.e. a model must process each window within shift length) ( <ref type="table" target="#tab_1">Table 2)</ref>. Taking into account the trade-offs between model size (Appendix <ref type="table" target="#tab_7">Table 7</ref>) and performance, we used CNN2D+LSTM model for further evaluation on bipolar dataset with various evaluation methods (Table 4), window size (Appendix <ref type="table" target="#tab_8">Table 8</ref>), shift size (Appendix <ref type="table" target="#tab_9">Table 9</ref>), signal feature extractors (Appendix <ref type="table" target="#tab_2">Table 3</ref>), and seizure-type binary detection (Appendix <ref type="table" target="#tab_0">Table 12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Seizure Detection with various signal processing methods</head><p>We compared the performance and speed of traditionally and frequently used signal processing methods <ref type="table" target="#tab_2">(Table 3)</ref>. Frequency Bands and STFT showed better performance compared to raw feature extraction, and SincNet (Ravanelli and Bengio, 2018) showed the worst performance among all feature extractors. The raw signal showed the fastest CPU/GPU processing speed with CNN2D+LSTM, and showed similar or better MARGIN and TAES performance compared to frequency bands and STFT with both CNN2D+LSTM and Resnet-short+LSTM models (Appendix <ref type="table" target="#tab_0">Table 14</ref>, 17). We used raw data for data preparation as this provides original data without any information loss, and maintains fast processing speed suitable to be used in conjunction with EEG test in clinical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Seizure Detection Evaluation Metrics</head><p>We then evaluated the binary seizure detection with CNN2D+LSTM and Resnet-short+LSTM using various evaluation methods <ref type="table" target="#tab_3">(Table 4,</ref>  <ref type="bibr">14,</ref><ref type="bibr">17)</ref>. Sensitivity (TPR) measured with OVLP was the highest among all evaluation metric as it is more permissive in terms of sensitivity than other evaluation metrics <ref type="bibr">(Ziyabari et al., 2017)</ref>. We also present our novel seizure evaluation method MARGIN (seizure boundary detection) on onset/offset considering margin from the onset/offset time. Our example patient signal streak shows that MARGIN provides additional assessment on the relibility of a model whether the real-time detection output near the onset/offset of seizure is trustworthy or not <ref type="figure" target="#fig_5">(Figure 4</ref>).</p><p>In this sample patient EEG trajectory <ref type="figure" target="#fig_5">(Figure 4</ref>) and from the result <ref type="table" target="#tab_3">(Table 4)</ref>, OVLP showed high performance as it provides loose evaluation on the overlapping signal duration between ground truth and prediction. On the other hand, TAES provides stricter result as it considers overlapping time between label and hypothesis. Onset latency itself could not also be a favorable metric for real-time seizure event detection as we need to further consider the total length of seizure events (Appendix <ref type="table" target="#tab_6">Table 6</ref>) which is is not possible to be done real-time.</p><p>EPOCH presents evaluation result on every windows, temporally detecting unreliable prediction of a model. We used EPOCH for other experiments as it has been used in previous literature and provides the strict, per epoch evaluation. EPOCH might be of concern if the dataset contains subjects presenting with long and short seizure events <ref type="bibr">(Ziyabari et al., 2017)</ref>, as it weigh long seizure events heavily. We thus provide additional metric more useful for seizure onset detection (MARGIN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We provide a comprehensive analysis of the experimental settings on binary EEG seizure detection task for future benchmarking. While each existing models were successful in detecting seizures, the tasks cannot be compared against each other as the tasks were distinctly designed and tested. We set a standard real-time seizure detection task to fairly compare each  </p><formula xml:id="formula_0">- - MARGIN(3sec) - - - - - 0.41, 0.5 - MARGIN(5sec) - - - - - 0.56, 0.51 - Onset Latency - - - - - - 10.55</formula><p>models and experimental settings. We validate models on real-time binary seizure detection tasks with raw sampled signal under the constraint that each window signal is processed within shift length. This work could be extended to multi-class seizure classification tasks and localization of seizure onset with the help of attention modules. Our extensive research would help researchers initiate research in EEG based seizure detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Institutional Review Board (IRB)</head><p>Our work does not require Institutional Review Board approval as we did not collect patient dataset for our research. We slightly modified the given TUH V1.5.2 data in order to detect each seizure types with ample number of signals. We merged the original Tonic Seizure (TNSZ) and Tonic-Clonic Seizure (TCSZ) in TUH-EEG Corpus according to 2017 ILAE guideline <ref type="bibr">(Scheffer et al., 2017)</ref>. Also, we removed Myoclonic Seizure (MYSZ) seizure type data since there are only two patients, and transferred one SPSZ patient and one TNSZ patient data to test data from original TUH train data in order to allow all train, validation, and test set to have equal number of patient data for each type of seizure. Normal signals in TUH dataset includes when a patient presents with Eye Movement (EYEM), Chewing (CHEW), Shivering (SHIV), muscle artifact (MUSC), Electrode Pop (ELPP), and Electrostatic Artifact (ELST). Among 7, 034 of seizure signals, 583 in Focal Non-Specific seizure (FNSZ) class, 1, 836 signal streaks in Generalized Seizure (GNSZ), 52 in Simple Partial Seizure (SPSZ), 367 in Complex Partial Seizure (CPSZ), 99 in Absence Seizure (ABSZ), 62 in Tonic Seizure (TNSZ), and 48 in Tonic Clonic Seizure (TCSZ) ( <ref type="table" target="#tab_5">Table 5)</ref>.</p><p>Based on the original label from the original EEG, we re-defined the label for each input window. If a signal window contains ictal period of more than the length of window shift length, we set the signal window is ictal, and nonictal, otherwise.</p><p>We also summarize the length of the seizure events for each seizure types in <ref type="table" target="#tab_6">Table 6</ref>. Background noise (normal signal types) showed the longest event duration, and abscence seizure showed the shortest event length. Tonic Clonic Seizure showed the longest event duration among all seizure types.</p><p>A.2. Signal Feature Extractor 1) Raw 64 ? 1 ? 51 CNN2D filter is convolved to the raw input signal with 20 channels and 800 data points across the time dimension for channel-wise feature extraction.</p><p>2) SincNet We used 7 distinct SincNet filters of size 1 ? 80 to extract information from each signal frequency. Once the SincNet filters are convoluted with input EEG signal, the output size becomes 7 ? 20 ? 400, 7 different frequency information of length 400 for all 20 channels. It is then flattened to 1 ? 140 ? 400 so that every 7 row corresponds to 7 different frequencies per each channel. We designed the first CNN layer for SincNet to have the filter size of 64 ? 7 ? 21 and stride size 7 ? 2 for channel-wise feature extraction.</p><p>3) Frequency bands The output of frequency bands is 20 ? 7 ? 100, 7 averaged frequency bands of length 100 for each channel. This output is flattened to 1 ? 140?100 so that every 7 row corresponds to frequency band information for each channel. We used the first CNN layer with filter size 64 ? 7 ? 21 and stride size 7 ? 2 for channel-wise convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) STFT</head><p>Output of STFT (20 ? 100 ? 100) includes information of 100 frequencies of length 100 for all 20 channels. The output is flattened to size 1 ? 2, 000 ? 100 so that every row holds information about 100 frequencies per each channel. We also used   CNN layer with filter size 64 ? 100 ? 3 and stride 100 ? 1 for channel-wise convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) LFCC</head><p>Output of LFCC (20 ? 8 ? 27) includes information of 8 size frequency domain for all 20 channels. The output is flattened to size 1 ? 160 ? 27 so that every row holds information about 8 frequencies per each channel. We also used CNN layer with filter size 64 ? 8 ? 21 and stride 8 ? 2 for channel-wise convolution.</p><p>6) Down-sampled The 800 data points of raw data is re-sampled in 200, 100, 50Hz then converted to size of 800, 400, and 200 data points samples. Each sample goes to different stride size CNN2D modules (stride size of 1x4, 1x2, 1x1) and outputs of the three CNN2D modules are concatenated to size 64 ? 20 ? 200. We additionally implement and compare experiments of Down-sampled feature extraction with and without anti-aliasing filter (bandpass filter) ( <ref type="table" target="#tab_0">Table 18</ref>). The anti-aliasing filter does not improve the performance of our model.  <ref type="figure">Figure 9)</ref>: The EEG input signal first passes the CNN filter and then positional encoding was to time dimension and then passed to transformer encoder <ref type="bibr">(Vaswani et al., 2017)</ref>. In Guided Feature Transformer, the channel adjacency matrix is multiplied element-wise to the sofmax output of key query of multi-head self attention layer to calculate the similarity between connected channels. Transformer encoder output then passes through average pooling, LSTM, and linear classifier layer for seizure binary detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Model Speed</head><p>GPU and CPU speed were measured with Titan Xp and Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz. We measured both inference and learning time of each signal feature extraction methods with CNN2D+LSTM model on raw signal. The inference time was calculated per each sliding window on one CPU, where we measured the total time it took to infer a 30 second input signal segment. We report the total time divided by the number of windows (4 second window, 26 windows) within the whole 30 second signal. 1) CPU Processing Speed: CPU speed is the time it takes for a model to process one sliding window using only one CPU with batch size 1. This is the realistic model evaluation in a hospital where the EEG data from one patient is processed by one microprocessor.</p><p>2) GPU Processing Speed: GPU speed is the time it takes for a model to process one sliding window using one GPU with the support of 5 CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Supplementary Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Model Size</head><p>We summarize the model size according to each model <ref type="table" target="#tab_7">(Table 7)</ref>. Chrononet and TDNN + LSTM are the two smallest models; AlexNet is the biggest model, followed by the ResNet based model. The sizes of the models were all measured using torchinfo API. We evaluate the performance of CNN2D+LSTM and ResNet-short+LSTM using various input window size <ref type="bibr">(1,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr">8,</ref><ref type="bibr">10</ref>, 12 sec) ( <ref type="table" target="#tab_8">Table 8</ref>, 15) and window shift length (1, 2, 3, 4, 5 sec) ( <ref type="table" target="#tab_9">Table 9</ref>, 16). The model performs similar regardless of the window size, and we got the best performance with the largest window size (12 second). With increased window size, we can utilize more information within the input signal but at the same time, the processing speed decreases gradually (increasing processing time in <ref type="table" target="#tab_8">Table 8</ref>) and also might be inconvenient to users as they have to wait for the device to gather information for the time equivalent to the window size. Also, wider window size decreases the temporal resolution of the seizure detection alarm raised by the real-time seizure detector as the model detects the seizure only after it acquires the signal within the full window. We used the 4 second window setting for other experiments as this setting shows the second-best performance and still includes some input signal information than shorter window settings.</p><p>The model showed worse performance as the shift size increases. As shift length increases model process speed increases, but a model can provide more inference results by shifting the window every second. We used the best performing shift length (1 second) <ref type="figure">Figure 9</ref>: Architecture of Feature Transformer and Guided Transformer. Feature Transformer and Guided Transformer shares same architecture, but the channel adjacency matrix is multiplied to the key and query output of multi-head self attention module in Guided Transformer transformer encoder. for our final setting as it can utilize and learn more windows than any other setting, closer to a real-world setting, and provides the highest performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Model Speed</head><p>The batch size of EEG training input might affect the processing speed of each model. As batch size increases, the processing time of each STFT based sig- We trained 15 models on raw Unipolar EEG dataset and report the test result in <ref type="table" target="#tab_0">Table 11</ref>. Training on Unipolar dataset showed similar trends as Bipolar dataset, where ResNet-short variant of CNN2D+LSTM model showed the best performance and CNN2D+LSTM showed performance on par with ResNet based CNN2D+LSTM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Seizure Type Binary Detection</head><p>We report the binary detector model inference result with each seizure type binary detection task, on two different signal feature extractor settings ( <ref type="table" target="#tab_0">Table 12</ref>). The binary seizure type detection with CNN2D+LSTM model showed that the model performed well with AUC over 0.9 on detecting most seizure types <ref type="table" target="#tab_0">(Table 12)</ref> except Tonic-Clonic Seizure task. Tonic-Clonic Seizure (TCSZ) detection task showed the AUROC of 0.84 <ref type="table" target="#tab_0">(Table 12</ref>) which is lower than binary seizure detection performance of CNN2D+LSTM <ref type="table" target="#tab_1">(Table 2)</ref>. TCSZ detection task showed better performance with other frequency bands <ref type="table" target="#tab_0">(Table 12</ref>, Frequecy Bands row), as extracted signal features might capture the slowly evolving TCSZ better than the raw input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Seizure Evaluation Metric</head><p>We present seizure detector evaluation metric with multiple models <ref type="table" target="#tab_0">(Table 13,</ref>  <ref type="bibr">14,</ref><ref type="bibr">17)</ref>, which can further be used as the reference value for future studies.      0.92 ? 0.00 0.91 ? 0.00 0.941 6 0.9 ? 0.01 0.9 ? 0.01 1.2196 7 0.91 ? 0.01 0.9 ? 0.01 1.3012 8 0.9 ? 0.02 0.9 ? 0.00 1.3785 10 0.93 ? 0.01 0.92 ? 0.01 1.9242 12 0.92 ? 0.01 0.91 ? 0.01 2.0114   <ref type="table" target="#tab_0">Table 18</ref>: Evaluation on anti-aliasing effect: 0 to 100Hz Band-pass filtering is applied before down-sampling feature extraction on raw bipolar TUH EEG V1.5.2 dataset to see the effect of anti-aliasing on performance. The results were averaged over 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>Model Type AUROC AUPRC TPR TNR Downsampled feature extraction CNN2D+LSTM 0.88 ? 0.04 0.87 ? 0.04 0.74 ? 0.06 0.87 ? 0.00 after bandpass filter ResNet-short + LSTM 0.91 ? 0.00 0.9 ? 0.00 0.83 ? 0.01 0.82 ? 0.02</p><p>Downsampled feature extraction CNN2D+LSTM 0.88 ? 0.02 0.87 ? 0.02 0.78 ? 0.04 0.83 ? 0.03 ResNet-short + LSTM 0.92 ? 0.01 0.87 ? 0.02 0.85 ? 0.05 0.86 ? 0.04</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Real-Time Seizure Detection with CNN (+LSTM) models. We downsample the EEG signal and extract features (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Signal Feature Extractors and input preprocessing. Upon downsampling EEG signal with 200Hz, the input undergoes one of the signal feature extractors and fed into the first CNN2D layer designed for each extractors. C: number of EEG signal channels, T window: length of one sliding window, K: Kernel size, F: Frequency bands, S: Stride</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>5) MobileNetV3-short+LSTM: We used Mo-bileNetV3 Howard et al. (2019) which is designed Figure 3: Architecture of CNN2D + LSTM seizure detector with raw EEG input. K: Kernel size, S: Stride</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>fication(Jia et al., 2022; Ahmedt-Aristizabal et al.,  2020; Roy et al., 2019a; Priyasad et al., 2021; Asif  et al., 2020; Sriraam et al., 2019; Roy et al., 2019b;  Thyagachandran et al., 2020).8) ResNet18 (He et al., 2016): We used ResNet18 among all ResNet variations for its small model size suitable for real time detection. 9) MobileNetV3 (Howard et al., 2019): CNN model with squeeze-and-excite block after expansion layer, designed to perform well on mobile devices. 10) AlexNet (Krizhevsky et al., 2012): One of the early CNN architecture revolutionized CNN by using ReLU, dropout, overlapping pooling, and data augmentation which are now easily found in many CNN architectures. 11) DenseNet (Huang et al., 2017): CNN model that connects feature map of every layer with the ones with following layers, and concatenate them. 12) ChronoNet (Roy et al., 2019b): This model uses a CNN1D layer parallelized with various kernels, which is then connected to GRU. 13) TDNN with LSTM (Thyagachandran et al., 2020): A combination of Time Delay Neural Networks (TDNN) and LSTM. TDNN layer performs temporal convolution by convolving each channelwise EEG signal segment across timestep; LSTM layer then combines the channel-wise contextual representation to model sequence information. TDNN is commonly used for signal detection (EEG person identification (Kumar et al., 2019), voice activity detection, anomaly heart sound detection). 4.4.4. Feature Transformers 14) Feature Transformer: The model captures similarity between signals via self-attention on the channel-wise CNN features (Appendix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Evaluation Methods on example raw EEG signal. OVLP, TAES, MARGIN evaluates the model on the whole raw signal. OVLP outputs whether the hypothesis overlaps with the label, and TAES outputs the duration of how much hypothesis overlaps with label. MARGIN evaluates whether the onset/offset of event hypothesis is within set margin window. EPOCH outputs evaluation on hypothesis based on each window. Then final TP is measured by counting the number of windows where the model hypothesis was correct, divided by the total number of windows within each batch. TP -True Positive; FN -False Negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Data Sampler Inputs We equally distribute Non-Ictal, Ictal Onset/Offset, Ictal, and Alternated Ictal/Non-Ictal signal segments within each batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Lead Information for TUH Dataset We summarize the channels in unipolar and bipolar montage (left figure and Table (a) Unipolar / Bipolar Montage). We used the neighborhood leads for Unipolar Montage and interconnected Bipolar channels to generate adjacency matrix in Guided Feature Transformer (Figure 9). Table (b) examplifies the channel pairs with C3 for Unipolar and Bipolar Montage, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Architecture of CNN1D + LSTM seizure detector according to each feature extractors. K: Kernel size, , ResNet and MobileNetV3 based CNN2D+LSTM models (ResNet-short+LSTM, ResNet-short+Dilation+LSTM, and MobileNetV3-short+LSTM model, Figure 8), and feature transformers (Figure 9). CNN1D+LSTM (Figure 7): Upon feature extraction, the feature passes CNN1D layers and MaxPool layer, LSTM layer and then linear layer for binary classification. ResNet/MobileNetV3-based CNN2D+LSTM (Figure 8): ResNet short+LSTM and ResNet short+ Dilation+LSTM models both have ResNet blocks from the original ResNet18 encoder which feed output to LSTM and linear classifier. ResNet short+Dilation+LSTM model additionally have dilation and concatenation before ResNet block. MobileNet short+LSTM layer have Inverted Residual blocks from original MobileNetV3 architec-ture, which also feeds the output into LSTM and linear classifier. Feature Transformer, Guided Feature Transformer (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Architecture of ResNet-short+LSTM, ResNet-short+Dilation+LSTM, and MobileNetV3-short+LSTM Models K: Kernel size, S: Stride B.2. Seizure Detection with Multiple Window Size and Shift Length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Organization. The number of signal types in the whole dataset after slicing the whole signal segment into 30 second signal segments.</figDesc><table><row><cell>Signal Types</cell><cell>Sample No.</cell></row><row><cell></cell><cell>(Train/Val/Test)</cell></row><row><cell>Non-ictal (Patients)</cell><cell>44,599 / 9,221 / 6,907</cell></row><row><cell>Non-ictal (Normal Control)</cell><cell>37,017 / 1,051 / 822</cell></row><row><cell>Ictal, Non-ictal Signal</cell><cell>3,636 / 636 / 541</cell></row><row><cell>Ictal</cell><cell>3,671 / 964 / 456</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Result of real-time seizure detection on raw bipolar TUH EEG dataset trained with each architecture, averaged over 5 runs. Please see Appendix section A.4 for CPU and GPU settings. We report TPR, TNR, and measured MARGIN when TNR is above 0.95.</figDesc><table><row><cell>Methods</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>TPR</cell><cell>TNR</cell><cell>MARGIN(5sec)</cell><cell>CPU/GPU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Acc(Onset/Offset)</cell><cell>Speed (sec)</cell></row><row><cell>CNN2D + LSTM</cell><cell>0.89 ? 0.01</cell><cell>0.88 ? 0.01</cell><cell>0.81 ? 0.03</cell><cell>0.83 ? 0.03</cell><cell>0.56 / 0.51</cell><cell>0.079 / 0.004</cell></row><row><cell>CNN2D + BLSTM</cell><cell>0.87 ? 0.01</cell><cell>0.86 ? 0.02</cell><cell>0.76 ? 0.05</cell><cell>0.85 ? 0.04</cell><cell>0.57 / 0.64</cell><cell>0.224 / 0.005</cell></row><row><cell>ResNet-short + LSTM</cell><cell>0.92 ? 0.00</cell><cell>0.91 ? 0.00</cell><cell>0.83 ? 0.02</cell><cell>0.85 ? 0.01</cell><cell>0.62 / 0.65</cell><cell>0.941 / 0.013</cell></row><row><cell>ResNet-short+Dilation + LSTM</cell><cell>0.91 ? 0.01</cell><cell>0.90 ? 0.01</cell><cell>0.84 ? 0.01</cell><cell>0.83 ? 0.03</cell><cell>0.58 / 0.61</cell><cell>0.682 / 0.031</cell></row><row><cell>MobileNetV3-short + LSTM</cell><cell>0.89 ? 0.01</cell><cell>0.87 ? 0.01</cell><cell>0.83 ? 0.03</cell><cell>0.78 ? 0.03</cell><cell>0.53 / 0.59</cell><cell>0.266 / 0.009</cell></row><row><cell>CNN1D + LSTM</cell><cell>0.80 ? 0.03</cell><cell>0.79 ? 0.02</cell><cell>0.69 ? 0.05</cell><cell>0.75 ? 0.07</cell><cell>0.33 / 0.4</cell><cell>0.009 / 0.003</cell></row><row><cell>CNN1D + BLSTM</cell><cell>0.80 ? 0.01</cell><cell>0.78 ? 0.01</cell><cell>0.73 ? 0.03</cell><cell>0.71 ? 0.03</cell><cell>0.56 / 0.51</cell><cell>0.021 / 0.004</cell></row><row><cell>ResNet18</cell><cell>0.81 ? 0.02</cell><cell>0.78 ? 0.03</cell><cell>0.75 ? 0.04</cell><cell>0.75 ? 0.02</cell><cell>0.3 / 0.34</cell><cell>1.446 / 0.040</cell></row><row><cell>MobileNetV3</cell><cell>0.85 ? 0.01</cell><cell>0.83 ? 0.01</cell><cell>0.77 ? 0.04</cell><cell>0.76 ? 0.01</cell><cell>0.47 / 0.55</cell><cell>2.588 / 0.055</cell></row><row><cell>AlexNet</cell><cell>0.83 ? 0.02</cell><cell>0.81 ? 0.02</cell><cell>0.70 ? 0.03</cell><cell>0.81 ? 0.02</cell><cell>0.56 / 0.64</cell><cell>0.079 / 0.015</cell></row><row><cell>DenseNet</cell><cell>0.82 ? 0.01</cell><cell>0.79 ? 0.01</cell><cell>0.78 ? 0.04</cell><cell>0.73 ? 0.03</cell><cell>0.37 / 0.45</cell><cell>0.356 / 0.043</cell></row><row><cell>ChronoNet</cell><cell>0.59 ? 0.03</cell><cell>0.57 ? 0.02</cell><cell>0.58 ? 0.18</cell><cell>0.56 ? 0.14</cell><cell>0.2 / 0.34</cell><cell>0.0136 / 0.0061</cell></row><row><cell>TDNN + LSTM</cell><cell>0.80 ? 0.02</cell><cell>0.78 ? 0.02</cell><cell>0.72 ? 0.07</cell><cell>0.73 ? 0.05</cell><cell>0.39 / 0.47</cell><cell>3.49 / 0.24</cell></row><row><cell>Feature Transformer</cell><cell>0.6 ? 0.13</cell><cell>0.6 ? 0.12</cell><cell>0.46 ? 0.24</cell><cell>0.72 ? 0.16</cell><cell>0.08 / 0.13</cell><cell>0.1231 /0.0074</cell></row><row><cell>Guided Feature Transformer</cell><cell>0.82 ? 0.01</cell><cell>0.81 ? 0.01</cell><cell>0.7 ? 0.03</cell><cell>0.8 ? 0.163</cell><cell>0.49 / 0.55</cell><cell>0.1646 / 0.0072</cell></row><row><cell cols="3">length of 1 second after extensively searching for op-</cell><cell cols="4">input signal. However, the processing speed on CPU</cell></row><row><cell cols="3">timal window and shift size (Appendix Section B.2).</cell><cell cols="4">exceeds or similar to the shift length (1 second for the</cell></row><row><cell cols="3">ResNet-short+LSTM and ResNet-short + Dilation</cell><cell cols="4">experiments in Table 2) indicating that larger models</cell></row><row><cell cols="3">+ LSTM both showed the best performance among</cell><cell cols="2">are relatively slow.</cell><cell></cell><cell></cell></row><row><cell cols="3">various models (Table 2). ResNet-short+LSTM per-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">formed better than the ResNet18 model as the LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">module can capture the temporal dependency of the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Real-time seizure detection on bipolar TUH EEG dataset trained with each different signal process feature extractor on CNN2D + LSTM, averaged over 5 runs. Please see Appendix section A.4 for CPU and GPU settings. Detailed results can be found inAppendix Table 14.</figDesc><table><row><cell>Methods</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>TPR</cell><cell>TNR</cell><cell>MARGIN(5sec)</cell><cell>CPU/GPU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Acc(Onset / Offset)</cell><cell>Speed (sec)</cell></row><row><cell>Raw</cell><cell>0.89 ? 0.01</cell><cell>0.88 ? 0.01</cell><cell>0.81 ? 0.03</cell><cell>0.83 ? 0.03</cell><cell>0.56 / 0.51</cell><cell>0.079 / 0.004</cell></row><row><cell>SincNet</cell><cell>0.83 ? 0.01</cell><cell>0.81 ? 0.02</cell><cell>0.72 ? 0.03</cell><cell>0.78 ? 0.01</cell><cell>0.5 / 0.52</cell><cell>0.293 / 0.017</cell></row><row><cell>STFT</cell><cell>0.91 ? 0.01</cell><cell cols="2">0.90 ? 0.01 0.85 ? 0.03</cell><cell>0.82 ? 0.03</cell><cell>0.55 / 0.59</cell><cell>0.538 / 0.167</cell></row><row><cell>Frequency Bands</cell><cell>0.92 ? 0.01</cell><cell>0.91 ? 0.01</cell><cell>0.85 ? 0.02</cell><cell>0.83 ? 0.01</cell><cell>0.56 / 0.57</cell><cell>0.131 / 0.254</cell></row><row><cell>LFCC</cell><cell>0.86 ? 0.03</cell><cell>0.84 ? 0.024</cell><cell>0.73 ? 0.07</cell><cell>0.84 ? 0.03</cell><cell>0.52 / 0.51</cell><cell>0.051 / 0.301</cell></row><row><cell>Downsampled Raw</cell><cell>0.88 ? 0.02</cell><cell>0.87 ? 0.02</cell><cell>0.78 ? 0.04</cell><cell>0.83 ? 0.03</cell><cell>0.53 / 0.55</cell><cell>0.143 / 0.006</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Exploration on four evaluation methods on real-time seizure detection with CNN2D+LSTM, average over 5 runs. We selected TPR, TNR and FAs/24hours that maximizes TPR + TNR, and selected MARGIN and Onset Latency when TNR is above 0.95.</figDesc><table><row><cell></cell><cell>Metrics</cell><cell cols="7">AUROC AUPRC TPR TNR FAs / 24 hrs Acc(Onset, Offset) Time(Sec)</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.75</cell><cell>0.82</cell><cell>47.06</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.33</cell><cell>1.0</cell><cell>1.03</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN2D+LSTM</cell><cell>EPOCH</cell><cell>0.89</cell><cell>0.88</cell><cell>0.81</cell><cell>0.83</cell><cell>-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Umar Asif, Jianbin Tang, and Stefan Harrer. Machine learning for seizure type classification: Setting the benchmark. CoRR, abs/1902.01012, 2019a. URL http://arxiv.org/ abs/1902.01012.</figDesc><table><row><cell>region and brain state. Physical Review E, 64(6): 061907, 2001. Umar Asif, Subhrajit Roy, Jianbin Tang, and Ste-fan Harrer. Seizurenet: Multi-spectral deep fea-ture learning for seizure type classification. In Machine Learning in Clinical Neuroimaging and Subspace techniques for task-independent eeg per-son identification. In 2019 41st Annual In-ternational Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pages 4545-4548. IEEE, 2019. Chien-Liang Liu, Bin Xiao, Wen-Hoar Hsaio, and Anand Thyagachandran, M Kumar, Mriganka Sur, Rajeswari Aghoram, and H Murthy. Seizure detec-tion using time delay neural networks and lstms. In ology Symposium (SPMB), pages 1-5. IEEE, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Appendix A. Detailed information on Experimental Setting 2020 IEEE Signal Processing in Medicine and Bi-A.1. Dataset</cell><cell>Meysam Golmohammadi, Saeedeh Ziyabari, Vinit Shah, Eva Von Weltin, Christopher Campbell, Iyad Obeid, and Joseph Picone. Gated recurrent net-works for seizure detection. In 2017 IEEE Sig-nal Processing in Medicine and Biology Symposium (SPMB), pages 1-5. IEEE, 2017. Subhrajit Roy, Subhrajit Roy, Isabell Kiral-Kornek, and Stefan Har-</cell></row><row><cell>Radiogenomics in Neuro-oncology, pages 77-87. Vincent S Tseng. Epileptic seizure prediction with Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz</cell><cell>Meysam Golmohammadi, Amir Hossein Harati Ne-rer. Chrononet: a deep recurrent neural network</cell></row><row><cell>Springer, 2020. multi-view convolutional neural networks. IEEE Kaiser, and Illia Polosukhin. Attention is all you</cell><cell>jad Torbati, Silvia Lopez de Diego, Iyad Obeid, and for abnormal eeg identification. In Conference on</cell></row><row><cell>Access, 7:170352-170361, 2019. need. In Advances in neural information processing Alexei Baevski, Henry Zhou, Abdelrahman Mo-systems, pages 5998-6008, 2017. hamed, and Michael Auli. wav2vec 2.0: A frame-work for self-supervised learning of speech repre-Piotr Mirowski, Deepak Madhavan, Yann LeCun, Hongda Wang, Weiwei Shi, and Chiu-Sing Choy. and Ruben Kuzniecky. Classification of patterns sentations. arXiv preprint arXiv:2006.11477, 2020. of eeg synchronization for seizure prediction. Clin-Hardware design of real time epileptic seizure de-</cell><cell>Joseph Picone. Automatic analysis of eegs using Artificial Intelligence in Medicine in Europe, pages big data and hybrid deep learning architectures. 47-56. Springer, 2019b. Frontiers in human neuroscience, 13:76, 2019. Ingrid E Scheffer, Samuel Berkovic, Giuseppe J Gotman, D Flanagan, J Zhang, and B Rosenblatt. Capovilla, Mary B Connolly, Jacqueline French,</cell></row><row><cell>ical neurophysiology, 120(11):1927-1940, 2009. tection based on stft and svm. IEEE Access, 6: Steven Baldassano, Drausin Wulsin, Hoameng Ung, 67277-67290, 2018. Tyler Blevins, Mesha-Gay Brown, Emily Fox, and Brian Litt. A novel seizure detection algorithm in-formed by hidden markov model event states. Jour-nal of neural engineering, 13(3):036011, 2016. Mostafa Neo Mohsenvand, Mohammad Rasool Izadi, and Pattie Maes. Contrastive representation learn-ing for electroencephalogram classification. In Ma-chine Learning for Health, pages 238-253. PMLR, Ye-Yi Wang, Alex Acero, and Ciprian Chelba. Is word error rate a good indicator for spoken lan-guage understanding accuracy. In 2003 IEEE</cell><cell>Automatic seizure detection in the newborn: meth-Laura Guilhoto, Edouard Hirsch, Satish Jain, ods and initial evaluation. Electroencephalography Gary W Mathern, Solomon L Mosh?, et al. Ilae and clinical neurophysiology, 103(3):356-362, 1997. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. classification of the epilepsies: position paper of the ilae commission for classification and terminol-ogy. Epilepsia, 58(4):512-521, 2017.</cell></row><row><cell>2020. Workshop on Automatic Speech Recognition and S?ndor Beniczky and Donald L Schomer. Electroen-Understanding (IEEE Cat. No. 03EX721), pages cephalography: basic biophysical and technological Djamel Mostefa, Olivier Hamon, and Khalid Choukri. 577-582. IEEE, 2003. aspects important for clinical applications. Epilep-tic Disorders, 22(6):697-715, 2020. Walter Bomela, Shuo Wang, Chun-An Chou, and Jr-Shin Li. Real-time inference and detection of dis-Evaluation of automatic speech recognition and Scott B Wilson, Mark L Scheuer, Cheryl Plummer, speech language translation within tc-star: Re-Bryan Young, and Steve Pacia. Seizure detection: sults from the first evaluation campaign. In LREC, correlation of human experts. Clinical Neurophys-pages 149-154. Citeseer, 2006. iology, 114(11):2156-2164, 2003. ruptive eeg networks for epileptic seizures. Scien-tific Reports, 10(1):1-10, 2020. Hisham Daoud and Magdy A Bayoumi. Efficient Douglas R Nordli Jr. Usefulness of video-eeg moni-toring. Epilepsia, 47:26-30, 2006. Ye Yuan, Guangxu Xun, Kebin Jia, and Aidong Zhang. A multi-view deep learning framework for</cell><cell>In Proceedings of the IEEE conference on com-Ervin Sejdi?, Igor Djurovi?, and Jin Jiang. Time-puter vision and pattern recognition, pages 770-frequency feature representation using energy con-778, 2016. centration: An overview of recent advances. Digital Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997. signal processing, 19(1):153-183, 2009. Vinit Shah, Eva von Weltin, Silvia Lopez, James Ri-ley McHugh, Lillian Veloso, Meysam Golmoham-madi, Iyad Obeid, and Joseph Picone. The temple Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasude-university hospital seizure detection corpus. Fron-tiers in Neuroinformatics, 12:83, 2018.</cell></row><row><cell>epileptic seizure prediction based on deep learn-Iyad Obeid and Joseph Picone. The temple university eeg seizure detection. IEEE journal of biomedical</cell><cell>van, et al. Searching for mobilenetv3. In Proceed-Nabila Shawki, Tarek Elseify, Thao Cap, Vinit Shah,</cell></row><row><cell>ing. IEEE transactions on biomedical circuits and hospital eeg data corpus. Frontiers in neuroscience, and health informatics, 23(1):83-94, 2018a.</cell><cell>ings of the IEEE/CVF International Conference Iyad Obeid, and Joseph Picone. A deep learning-</cell></row><row><cell>systems, 13(5):804-813, 2019. Miaolin Fan and Chun-An Chou. Detecting abnor-mal pattern of epileptic seizures via temporal syn-10:196, 2016. Alan V Oppenheim. Discrete-time signal processing. Pearson Education India, 1999. Ye Yuan, Guangxu Xun, Fenglong Ma, Qiuling Suo, Hongfei Xue, Kebin Jia, and Aidong Zhang. A novel channel-aware attention framework for multi-</cell><cell>David Ahmedt-Aristizabal, Tharindu Fernando, Si-mon Denman, Lars Petersson, Matthew J Aburn, on Computer Vision, pages 1314-1324, 2019. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected con-based real-time seizure detection system. In 2020 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), pages 1-6. IEEE, 2020.</cell></row><row><cell>chronization of eeg signals. IEEE Transactions on Biomedical Engineering, 66(3):601-608, 2018. Robert S Fisher, Carlos Acevedo, Alexis Arzi-channel eeg seizure detection via multi-view deep Mohammad Zavid Parvez and Manoranjan Paul. Eeg learning. In 2018 IEEE EMBS International Con-signal classification using frequency band analy-ference on Biomedical &amp; Health Informatics (BHI), sis towards epileptic seizure prediction. In 16th pages 206-209. IEEE, 2018b. manoglou, Alicia Bogacz, J Helen Cross, Chris-tian E Elger, Jerome Engel Jr, Lars Forsgren, Jacqueline A French, Mike Glynn, et al. Ilae official report: a practical clinical definition of epilepsy. Epilepsia, 55(4):475-482, 2014. Ary L Goldberger, Luis AN Amaral, Leon Glass, Jef-frey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of a new Int'l Conf. Computer and Information Technology, pages 126-130. IEEE, 2014. Morteza Zabihi, Serkan Kiranyaz, Ali Bahrami Rad, Aggelos K Katsaggelos, Moncef Gabbouj, Darshana Priyasad, Tharindu Fernando, Simon Den-and Turker Ince. Analysis of high-dimensional man, Sridha Sridharan, and Clinton Fookes. In-phase space via poincar? section for patient-specific terpretable seizure classification using unprocessed eeg with multi-channel attentive feature fusion. IEEE Sensors Journal, 2021. seizure detection. IEEE Transactions on Neu-ral Systems and Rehabilitation Engineering, 24(3): 386-398, 2015. Mirco Ravanelli and Yoshua Bengio. Speaker recog-Saeedeh Ziyabari, Vinit Shah, Meysam Golmoham-nition from raw waveform with sincnet. In madi, Iyad Obeid, and Joseph Picone. Objective</cell><cell>and Clinton Fookes. Neural memory networks for seizure type classification. In 2020 42nd Annual In-ternational Conference of the IEEE Engineering in Ali Hossam Shoeb. Application of machine learning volutional networks. In Proceedings of the IEEE to epileptic seizure onset detection and treatment. conference on computer vision and pattern recogni-PhD thesis, Massachusetts Institute of Technology, tion, pages 4700-4708, 2017. 2009. Medicine &amp; Biology Society (EMBC), pages 569-575. IEEE, 2020. Behnaz Akbarian and Abbas Erfanian. A frame-work for seizure detection using effective connec-tivity, graph theory, and multi-level modular net-work. Biomedical Signal Processing and Control, 59:101878, 2020. Ralph G Andrzejak, Klaus Lehnertz, Florian Mor-mann, Christoph Rieke, Peter David, and Chris-tian E Elger. Indications of nonlinear deterministic Guangyu Jia, Hak-Keung Lam, and Kaspar Althoe-David B Springer, Lionel Tarassenko, and Gari D fer. Variable weight algorithm for convolutional Clifford. Logistic regression-hsmm-based heart neural networks and its applications to classifica-sound segmentation. IEEE Transactions on tion of seizure phases and types. Pattern Recogni-Biomedical Engineering, 63(4):822-832, 2015. tion, 121:108226, 2022. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-ton. Imagenet classification with deep convolu-Natarajan Sriraam, Yasin Temel, Shyam Vasudeva Rao, Pieter L Kubben, et al. A convolutional neu-ral network based framework for classification of tional neural networks. Advances in neural infor-seizure types. In 2019 41st Annual International mation processing systems, 25:1097-1105, 2012. Conference of the IEEE Engineering in Medicine</cell></row><row><cell>research resource for complex physiologic signals. 2018 IEEE Spoken Language Technology Workshop evaluation metrics for automatic classification of</cell><cell>and finite-dimensional structures in time series of Mari Ganesh Kumar, MS Saranya, Shrikanth and Biology Society (EMBC), pages 2547-2550.</cell></row><row><cell>circulation, 101(23):e215-e220, 2000. (SLT), pages 1021-1028. IEEE, 2018. eeg events. arXiv preprint arXiv:1712.10107, 2017.</cell><cell>brain electrical activity: Dependence on recording Narayanan, Mriganka Sur, and Hema A Murthy. IEEE, 2019.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Construction of Temple University Hospital v 1.5.2 Dataset</figDesc><table><row><cell>Type</cell><cell cols="4">Patient No. Seizure Events Seizure Events (Train set) Seizure Events (Development set)</cell></row><row><cell>Focal Non-Specific</cell><cell>150</cell><cell>1,836</cell><cell>1,536</cell><cell>300</cell></row><row><cell>Generalized Non-Specific</cell><cell>81</cell><cell>583</cell><cell>409</cell><cell>174</cell></row><row><cell>Simple Partial Seizure</cell><cell>3</cell><cell>52</cell><cell>49</cell><cell>3</cell></row><row><cell>Complex Partial Seizure</cell><cell>41</cell><cell>367</cell><cell>283</cell><cell>84</cell></row><row><cell>Absence Seizure</cell><cell>12</cell><cell>99</cell><cell>50</cell><cell>49</cell></row><row><cell>Tonic Seizure</cell><cell>3</cell><cell>62</cell><cell>18</cell><cell>44</cell></row><row><cell>Tonic Clonic Seizure</cell><cell>14</cell><cell>48</cell><cell>30</cell><cell>18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Average length of seizure events within EEG signals of each seizure types.</figDesc><table><row><cell>Type</cell><cell>Seizure Event (sec)</cell></row><row><cell>Background</cell><cell>357.36 ? 384.29</cell></row><row><cell>Focal Non-Specific</cell><cell>65.98 ? 143.55</cell></row><row><cell>Generalized Non-Specific</cell><cell>102.43 ? 265.18</cell></row><row><cell>Simple Partial Seizure</cell><cell>41.27 ? 19.81</cell></row><row><cell>Complex Partial Seizure</cell><cell>98.97 ? 112.39</cell></row><row><cell>Absence Seizure</cell><cell>8.61 ? 5.34</cell></row><row><cell>Tonic Seizure</cell><cell>19.41 ? 9.30</cell></row><row><cell>Tonic Clonic Seizure</cell><cell>115.59 ? 327.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Model Configurations. Model parameter size measured with torchinfo API.</figDesc><table><row><cell>Methods</cell><cell>Parameter Size (MB)</cell></row><row><cell>CNN2D + LSTM</cell><cell>6.16</cell></row><row><cell>CNN2D + BLSTM</cell><cell>6.16</cell></row><row><cell>ResNet-short + LSTM</cell><cell>15.75</cell></row><row><cell>ResNet-short + Dilation + LSTM</cell><cell>15.75</cell></row><row><cell>MobileNetV3-short + LSTM</cell><cell>5.44</cell></row><row><cell>CNN1D + LSTM</cell><cell>6.41</cell></row><row><cell>CNN1D + BLSTM</cell><cell>7.99</cell></row><row><cell>ResNet18</cell><cell>20.82</cell></row><row><cell>MobileNetV3</cell><cell>16.54</cell></row><row><cell>AlexNet</cell><cell>127.86</cell></row><row><cell>DenseNet</cell><cell>4.84</cell></row><row><cell>ChronoNet</cell><cell>0.5</cell></row><row><cell>TDNN + LSTM</cell><cell>2.67</cell></row><row><cell>Feature Transformer + LSTM</cell><cell>5.6</cell></row><row><cell>Guided Transformer + LSTM</cell><cell>5.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">: Real-time seizure detection with different</cell></row><row><cell cols="4">window size on bipolar TUH EEG dataset</cell></row><row><cell cols="4">trained with CNN2D+LSTM, averaged over</cell></row><row><cell cols="4">5 runs.The CPU speed setting and meaning</cell></row><row><cell cols="3">are explained in A.4</cell><cell></cell></row><row><cell>Window</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>Process</cell></row><row><cell>(sec)</cell><cell></cell><cell></cell><cell>(sec)</cell></row><row><cell>1</cell><cell cols="3">0.87 ? 0.02 0.86 ? 0.02 0.0364</cell></row><row><cell>2</cell><cell cols="3">0.88 ? 0.02 0.87 ? 0.02 0.0665</cell></row><row><cell>4</cell><cell cols="2">0.89 ? 0.01 0.88 ? 0.01</cell><cell>0.079</cell></row><row><cell>6</cell><cell cols="2">0.89 ? 0.02 0.88 ? 0.02</cell><cell>0.1243</cell></row><row><cell>7</cell><cell cols="2">0.88 ? 0.02 0.87 ? 0.02</cell><cell>0.1305</cell></row><row><cell>8</cell><cell cols="2">0.88 ? 0.02 0.87 ? 0.02</cell><cell>0.1222</cell></row><row><cell>10</cell><cell cols="2">0.89 ? 0.01 0.87 ? 0.01</cell><cell>0.2441</cell></row><row><cell>12</cell><cell>0.90 ? 0.01</cell><cell>0.88? 0.01</cell><cell>0.2616</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Real-time seizure detection with different window shift length on bipolar TUH EEG dataset trained with CNN2D+LSTM with fixed 4 second sliding window, averaged over 5 runs.</figDesc><table><row><cell>Shift Length</cell><cell>AUROC</cell><cell>AUPRC</cell></row><row><cell>(sec)</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="2">0.89 ? 0.01 0.88 ? 0.01</cell></row><row><cell>2</cell><cell>0.87 ? 0.05</cell><cell>0.86 ? 0.04</cell></row><row><cell>3</cell><cell>0.86 ? 0.04</cell><cell>0.86 ? 0.04</cell></row><row><cell>4</cell><cell>0.85 ? 0.04</cell><cell>0.84 ? 0.04</cell></row><row><cell>5</cell><cell>0.84 ? 0.05</cell><cell>0.84 ? 0.05</cell></row><row><cell cols="3">nal feature extraction increases linearly to the num-</cell></row><row><cell cols="3">ber of EEG channels as it cannot be done in parallel</cell></row><row><cell cols="3">within GPU processors. Thus, although CPU based</cell></row><row><cell cols="3">on-device (1 batch size) speed doesn't have huge dif-</cell></row><row><cell cols="3">ference between raw and STFT feature based mod-</cell></row><row><cell cols="3">els, the STFT feature based model's training speed</cell></row><row><cell cols="3">is much lower than the raw feature based neural net-</cell></row><row><cell cols="3">work. We compared the processing speed of CNN2D</cell></row><row><cell cols="3">+ LSTM (Raw) as base model with Resnet-short +</cell></row><row><cell cols="3">LSTM (RAW) as deeeper neural network model and</cell></row><row><cell cols="3">CNN2D + LSTM (Frequency band) as complex sig-</cell></row><row><cell cols="3">nal feature extraction model according to batch size</cell></row><row><cell>(Table 10).</cell><cell></cell><cell></cell></row><row><cell cols="3">B.4. Real-time Seizure Detection on</cell></row><row><cell cols="2">Unipolar EEG lead</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Computation speed (sec) per one 4 seconds size window of CNN2D + LSTM (RAW) as base model, Resnet-short + LSTM (RAW) as deeper nerual network model and CNN2D + LSTM (Frequency Based) as more complex signal feature extraction method model according to input batch size.</figDesc><table><row><cell>Model (Feature)</cell><cell>GPU/CPU</cell><cell></cell><cell></cell><cell>Batch Size</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell></row><row><cell>CNN2D + LSTM</cell><cell>1 CPU</cell><cell cols="2">0.0792 0.3031</cell><cell>0.571</cell><cell>1.1527</cell><cell>2.27</cell><cell>4.4862</cell></row><row><cell>(Raw)</cell><cell>1 GPU + 5 CPUs</cell><cell cols="2">0.0036 0.0028</cell><cell>0.0026</cell><cell>0.0034</cell><cell>0.0043</cell><cell>0.0069</cell></row><row><cell>ResNet-short + LSTM</cell><cell>1 CPU</cell><cell>0.941</cell><cell>4.0771</cell><cell>7.741</cell><cell cols="3">16.4739 33.2041 67.0107</cell></row><row><cell>(Raw)</cell><cell>1 GPU + 5 CPUs</cell><cell>0.013</cell><cell>0.0049</cell><cell>0.0056</cell><cell>0.0077</cell><cell>0.013</cell><cell>0.0213</cell></row><row><cell>CNN2D + LSTM</cell><cell>1 CPU</cell><cell>0.131</cell><cell>0.3403</cell><cell>0.6433</cell><cell>1.2804</cell><cell>2.4279</cell><cell>4.9421</cell></row><row><cell>(Frequency Bands)</cell><cell>1 GPU + 5 CPUs</cell><cell cols="2">0.0106 0.0344</cell><cell>0.0687</cell><cell>0.1354</cell><cell>0.254</cell><cell>0.6481</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Result of real-time seizure detection on raw Unipolar TUH EEG dataset trained with each architecture, averaged over 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Seizure type-wise performance of our real-time seizure detectors on raw bipolar TUH EEG V1.5.2 dataset. The results were averaged over 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Exploration on four evaluation methods on real-time seizure detection with various models. We evaluate TPR, TNR and FAs/24hours that maximizes TPR + TNR, and measured MARGIN and Latency when TNR is above 0.95.</figDesc><table><row><cell></cell><cell>Metrics</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>TPR</cell><cell>TNR</cell><cell>FAs / 24 hours</cell><cell>Acc(Onset / Offset)</cell><cell>Time(Sec)</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.7</cell><cell>0.86</cell><cell>47.18</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.3</cell><cell>1.0</cell><cell>2.06</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN2D+BLSTM</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.87 -</cell><cell>0.86 -</cell><cell>0.76 -</cell><cell>0.85 -</cell><cell>--</cell><cell>-0.44 / 0.58</cell><cell>-</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.57 / 0.64</cell><cell></cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.69</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.75</cell><cell>0.84</cell><cell>64.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.4</cell><cell>1.0</cell><cell>1.68</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-short + LSTM</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.92 -</cell><cell>0.91 -</cell><cell>0.83 -</cell><cell>0.85 -</cell><cell>--</cell><cell>-0.49 / 0.58</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.62 / 0.65</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.23</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.62</cell><cell>0.89</cell><cell>35.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.37</cell><cell>1.0</cell><cell>1.67</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-short + Dilation</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.91 -</cell><cell>0.9 -</cell><cell>0.84 -</cell><cell>0.83 -</cell><cell>--</cell><cell>-0.43 / 0.5</cell><cell>--</cell></row><row><cell>+ LSTM</cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.58 / 0.61</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7.76</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.55</cell><cell>0.94</cell><cell>15.73</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.34</cell><cell>0.99</cell><cell>2.61</cell><cell>-</cell><cell>-</cell></row><row><cell>MobileNetV3 + LSTM</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.89 -</cell><cell>0.87 -</cell><cell>0.83 -</cell><cell>0.78 -</cell><cell>--</cell><cell>-0.44 / 0.51</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.53 / 0.59</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.83</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.52</cell><cell>72.13</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.21</cell><cell>0.99</cell><cell>6.6</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN1D + LSTM</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.8 -</cell><cell>0.79 -</cell><cell>0.69 -</cell><cell>0.75 -</cell><cell>--</cell><cell>-0.24 / 0.35</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.33 / 0.4</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.84</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.48</cell><cell>82.44</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.22</cell><cell>0.99</cell><cell>4.71</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN1D + BLSTM</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.8 -</cell><cell>0.78 -</cell><cell>0.73 -</cell><cell>0.71 -</cell><cell>--</cell><cell>-0.33 / 0.42</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.56 / 0.51</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.64</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.51</cell><cell>76.15</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.18</cell><cell>0.99</cell><cell>7.47</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet18</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.81 -</cell><cell>0.78 -</cell><cell>0.75 -</cell><cell>0.75 -</cell><cell>--</cell><cell>-0.24 / 0.28</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.30 / 0.34</cell><cell></cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.99</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.86</cell><cell>0.43</cell><cell>739.24</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.23 0.01</cell><cell>1.0 0.0</cell><cell>2.1</cell><cell>-</cell><cell>-</cell></row><row><cell>MobileNetV3</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.85 -</cell><cell>0.83 -</cell><cell>0.77 0.04 -</cell><cell>0.76 -</cell><cell>--</cell><cell>-0.38 / 0.46</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.47 / 0.55</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.66</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.86 0.2</cell><cell>0.62</cell><cell>93.54</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.26</cell><cell>1.0</cell><cell>2.09</cell><cell>-</cell><cell>-</cell></row><row><cell>AlexNet</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.83 -</cell><cell>0.81 -</cell><cell>0.70 -</cell><cell>0.81 -</cell><cell>--</cell><cell>-0.46 / 0.55</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.56 / 0.64</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12.07</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.49</cell><cell>85.19</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.19</cell><cell>0.99</cell><cell>4.51</cell><cell>-</cell><cell>-</cell></row><row><cell>DenseNet</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.82 -</cell><cell>0.79 -</cell><cell>0.78 0.04 -</cell><cell>0.73 -</cell><cell>--</cell><cell>-0.29 / 0.36</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.37 / 0.45</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.74</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.52</cell><cell>74.34</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.13</cell><cell>0.93</cell><cell>85.50</cell><cell>-</cell><cell>-</cell></row><row><cell>ChronoNet</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.59 -</cell><cell>0.57 -</cell><cell>0.58 -</cell><cell>0.56 -</cell><cell>--</cell><cell>-0.20 / 0.25</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.25 / 0.34</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.78</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.52</cell><cell>80.55</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.20</cell><cell>0.98</cell><cell>18.66</cell><cell>-</cell><cell>-</cell></row><row><cell>TDNN+LSTM</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.80 -</cell><cell>0.78 -</cell><cell>0.72 -</cell><cell>0.73 -</cell><cell>--</cell><cell>-0.28 / 0.33</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.36 / 0.41</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.72</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.53</cell><cell>73.22</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.16</cell><cell>0.89</cell><cell>110.96</cell><cell>-</cell><cell>-</cell></row><row><cell>Feature Transformer</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.6 -</cell><cell>0.6 -</cell><cell>0.46 -</cell><cell>0.72 -</cell><cell>--</cell><cell>-0.05 / 0.11</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.08 / 0.13</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.78</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.52</cell><cell>72.13</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.28</cell><cell>0.99</cell><cell>2.89</cell><cell>-</cell><cell>-</cell></row><row><cell>Guided Feature Transformer</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.82 -</cell><cell>0.81 -</cell><cell>0.7 -</cell><cell>0.8 -</cell><cell>--</cell><cell>-0.43 / 0.48</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.49 / 0.55</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>19.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Exploration on four evaluation methods on real-time seizure detection trained with each different signal process feature extractor on CNN2D + LSTM. We evaluate TPR, TNR and FAs/24hours that maximizes TPR + TNR, and measured MARGIN and Latency when TNR is above 0.95.</figDesc><table><row><cell>Feature Extraction</cell><cell>Metrics</cell><cell cols="7">AUROC AUPRC TPR TNR FAs / 24 hrs Acc(Onset, Offset) Time(Sec)</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.75</cell><cell>0.82</cell><cell>47.06</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.33</cell><cell>1.0</cell><cell>1.03</cell><cell>-</cell><cell>-</cell></row><row><cell>Raw</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.89 -</cell><cell>0.88 -</cell><cell>0.81 -</cell><cell>0.83 -</cell><cell>--</cell><cell>-0.41, 0.5</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.56, 0.51</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.55</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.52</cell><cell>79.33</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.2</cell><cell>0.99</cell><cell>3.83</cell><cell>-</cell><cell>-</cell></row><row><cell>Sincnet</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.83 -</cell><cell>0.81 -</cell><cell>0.72 -</cell><cell>0.78 -</cell><cell>--</cell><cell>-0.42, 0.45</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.5, 0.52</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.46</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.67</cell><cell>0.9</cell><cell>34.13</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.29</cell><cell>0.99</cell><cell>4.3</cell><cell>-</cell><cell>-</cell></row><row><cell>STFT</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.91 -</cell><cell>0.90 -</cell><cell>0.85 -</cell><cell>0.82 -</cell><cell>--</cell><cell>-0.39, 0.52</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.55, 0.59</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.39</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.7</cell><cell>0.88</cell><cell>41.28</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.3</cell><cell>1.0</cell><cell>0.87</cell><cell>-</cell><cell>-</cell></row><row><cell>Frequency Bands</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.92 -</cell><cell>0.91 -</cell><cell>0.85 -</cell><cell>0.83 -</cell><cell>--</cell><cell>-0.44, 0.48</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.56, 0.57</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.35</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.63</cell><cell>0.87</cell><cell>42.94</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.26</cell><cell>0.99</cell><cell>4.81</cell><cell>-</cell><cell>-</cell></row><row><cell>Downsampled</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.88 -</cell><cell>0.87 -</cell><cell>0.78 -</cell><cell>0.83 -</cell><cell>--</cell><cell>-0.45, 0.45</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.53, 0.55</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.52</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.66</cell><cell>0.85</cell><cell>62.75</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.26</cell><cell>0.99</cell><cell>4.81</cell><cell>-</cell><cell>-</cell></row><row><cell>LFCC</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.86 -</cell><cell>0.84 -</cell><cell>0.73 -</cell><cell>0.84 -</cell><cell>--</cell><cell>-0.41, 0.44</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.52, 0.51</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Real-time seizure detection with different window size on bipolar TUH EEG dataset trained with Resnet-short+LSTM, averaged over 5 runs.The CPU speed setting and meaning are explained in A.4</figDesc><table><row><cell>Window</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>Process</cell></row><row><cell>(sec)</cell><cell></cell><cell></cell><cell>(sec)</cell></row><row><cell>1</cell><cell cols="3">0.89 ? 0.01 0.88 ? 0.01 0.2423</cell></row><row><cell>2</cell><cell>0.9 ? 0.01</cell><cell cols="2">0.86 ? 0.06 0.5178</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Real-time seizure detection with different window shift length on bipolar TUH EEG dataset trained with Resnet-short+LSTM with fixed 4 second sliding window, averaged over 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 :</head><label>17</label><figDesc>Exploration on four evaluation methods on real-time seizure detection trained with each different signal process feature extractor on Resnet-Short + LSTM. We evaluate TPR, TNR and FAs/24hours that maximizes TPR + TNR, and measured MARGIN and Latency when TNR is above 0.95.</figDesc><table><row><cell>Feature Extraction</cell><cell>Metrics</cell><cell cols="3">AUROC AUPRC TPR</cell><cell>TNR</cell><cell cols="3">FAs / 24 hrs Acc(Onset, Offset) Time(Sec)</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.75</cell><cell>0.84</cell><cell>64.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.4</cell><cell>1.0</cell><cell>1.68</cell><cell>-</cell><cell>-</cell></row><row><cell>Raw</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.92 -</cell><cell>0.91 -</cell><cell>0.83 -</cell><cell>0.85 -</cell><cell>--</cell><cell>-0.49 / 0.58</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.62 / 0.65</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.23</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.52</cell><cell>81.14</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.33</cell><cell>0.99</cell><cell>4.87</cell><cell>-</cell><cell>-</cell></row><row><cell>Sincnet</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.86 -</cell><cell>0.76 -</cell><cell>0.74 -</cell><cell>0.81 -</cell><cell>--</cell><cell>-0.38, 0.45</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.51, 0.57</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.77</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>0.52</cell><cell>74.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.34</cell><cell>0.99</cell><cell>6.12</cell><cell>-</cell><cell>-</cell></row><row><cell>STFT</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.91 -</cell><cell>0.8 -</cell><cell>0.85 -</cell><cell>0.83 -</cell><cell>--</cell><cell>-0.35, 0.48</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.45, 0.52</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.19</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.85</cell><cell>0.713</cell><cell>161.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.37</cell><cell>1.0</cell><cell>1.39</cell><cell>-</cell><cell>-</cell></row><row><cell>Frequency Bands</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.92 -</cell><cell>0.86 -</cell><cell>0.87 -</cell><cell>0.87 -</cell><cell>--</cell><cell>-0.46, 0.56</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.59, 0.63</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.8</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.74</cell><cell>0.83</cell><cell>81.57</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.41</cell><cell>1.0</cell><cell>2.65</cell><cell>-</cell><cell>-</cell></row><row><cell>Downsampled</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.92 -</cell><cell>0.87 -</cell><cell>0.85 -</cell><cell>0.87 -</cell><cell>--</cell><cell>-0.38, 0.45</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.51, 0.57</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.96</cell></row><row><cell></cell><cell>OVLP</cell><cell>-</cell><cell>-</cell><cell>0.75</cell><cell>0.67</cell><cell>184.68</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TAES</cell><cell>-</cell><cell>-</cell><cell>0.3</cell><cell>0.99</cell><cell>4.74</cell><cell>-</cell><cell>-</cell></row><row><cell>LFCC</cell><cell>EPOCH MARGIN(3sec)</cell><cell>0.92 -</cell><cell>0.84 -</cell><cell>0.8 -</cell><cell>0.88 -</cell><cell>--</cell><cell>-0.43, 0.48</cell><cell>--</cell></row><row><cell></cell><cell>MARGIN(5sec)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.52, 0.55</cell><cell>-</cell></row><row><cell></cell><cell>Onset Latency</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0.92 ? 0.00 0.91 ? 0.00 2 0.9 ? 0.02 0.89 ? 0.02 3 0.9 ? 0.01 0.89 ? 0.01 4 0.89 ? 0.02 0.88 ? 0.02 5 0.9 ? 0.01 0.89 ? 0.01</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep convolutional neural network for the automated detection and diagnosis of seizure using eeg signals. Computers in biology and medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><forename type="middle">Lih</forename><surname>U Rajendra Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen</forename><forename type="middle">Hong</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adeli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
