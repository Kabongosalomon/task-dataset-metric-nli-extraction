<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Classification Equilibrium in Long-Tailed Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
							<email>feng.chengjian@intellif.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
							<email>zhongyujie@meituan.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Intellifusion Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Meituan Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Weilin huang Tao Technology, Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Classification Equilibrium in Long-Tailed Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The conventional detectors tend to make imbalanced classification and suffer performance drop, when the distribution of the training data is severely skewed. In this paper, we propose to use the mean classification score to indicate the classification accuracy for each category during training. Based on this indicator, we balance the classification via an Equilibrium Loss (EBL) and a Memoryaugmented Feature Sampling (MFS) method. Specifically, EBL increases the intensity of the adjustment of the decision boundary for the weak classes by a designed score-guided loss margin between any two classes. On the other hand, MFS improves the frequency and accuracy of the adjustment of the decision boundary for the weak classes through over-sampling the instance features of those classes. Therefore, EBL and MFS work collaboratively for finding the classification equilibrium in long-tailed detection, and dramatically improve the performance of tail classes while maintaining or even improving the performance of head classes. We conduct experiments on LVIS using Mask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to show the superiority of the proposed method. It improves the detection performance of tail classes by 15.6 AP, and outperforms the most recent longtailed object detectors by more than 1 AP. Code is available at https://github.com/fcjian/LOCE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection plays an important role in computer vision, and recent object detectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> have achieved promising performance on several common datasets with a few categories and balanced class distribution, such as PAS-CAL VOC (20 classes) <ref type="bibr" target="#b4">[5]</ref> and COCO (80 classes) <ref type="bibr" target="#b14">[15]</ref>. However, most real-world data contains a large number of categories and its distribution is long-tailed: a few head classes contain abundant instances while a great number of tail classes only have a few instances. * Corresponding author. Recently, LVIS <ref type="bibr" target="#b5">[6]</ref> is released for exploring long-tailed object detection. Not surprisingly, the performance of the state-of-the-art detectors designed for balanced data is significantly degraded <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> if they are directly applied to such datasets. The reason for the performance degradation mainly comes from two aspects: <ref type="bibr">(1)</ref> The long-tailed distribution of data. The number of the instances from tail classes (e.g., only 1 instance for one class) is insufficient for training a deep learning model, resulting in under-fitting of these classes. Moreover, the tail classes will be overwhelmed by the head classes during training, because the number of the instances of head classes is much larger than that of tail classes (e.g., thousands of times). As a result, the detectors cannot learn the tail classes well, and recognize those tail classes with very low confidence, as demonstrated in <ref type="figure" target="#fig_0">Figure 1.</ref> (2) The large number of categories. With the increase of the number of categories, it brings a higher chance of misclassification, especially for the tail classes with a very low classification score.</p><p>Several works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> attempted to cope with the problem of long-tail learning by re-sampling training data or re-weighting loss function. However, most of them assign the sampling rate and the loss weight according to the sampling frequency of each category, which is modelagnostic and sensitive to hyper-parameters <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. It may bring the following problems: (1) the model-agnostic data re-sampling is prone to over-fit the tail classes and underrepresent the head classes; (2) the dataset-based loss reweighting may cause excessive gradients and unstable training especially when the category distribution is extremely imbalanced. Recently, wang et al. <ref type="bibr" target="#b22">[23]</ref> introduce Seesaw loss to adaptively re-balance the gradients of positive and negative samples by dynamically accumulating the number of class instances during training. However, the number of the training samples cannot accurately reflect the learning quality of the classes, due to the diversity and complexity of instances and categories, e.g., training a classifier for the categories with visual similarity usually requires more training samples than the categories with very different visual appearance.</p><p>To address the above problems, we propose to use the mean classification score to monitor the learning status (i.e., classification accuracy) of each category during training. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the mean classification score has an approximate positive correlation with the classification accuracy. Thus, it can be used as an effective indicator to reflect the classification accuracy during training. Based on this indicator, we design an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method, to dynamically balance the classification. Equilibrium Loss: To balance the classification of different classes, EBL assigns different loss margins between any two classes based on the statistical mean classification score. It increases the loss margin between weak (with low mean score) positive classes and dominant (with high mean score) negative classes, and vice versa. Thus, the designed loss margin increases the intensity of the adjustment of the classification decision boundary for the weak classes, resulting in a more balanced classification. Memory-augmented Feature Sampling: In addition to increasing the intensity of the adjustment of the decision boundary, we design MFS to increase the frequency and accuracy of the adjustment of the decision boundary for the weak classes. Specifically, rich instance features are firstly extracted based on a set of dense bounding boxes generated by a model-agnostic bounding box generator, and then stored by a feature memory module for feature reuse across training iterations. Finally, a probabilistic sampler is used to access the feature memory module to sample more instance features of weak classes to improve the training.</p><p>We codename the proposed method Long-tailed Object detector with Classification Equilibrium (LOCE). In summary, our contributions are as follows: (1) we propose to use the mean classification score to monitor the classifica-tion accuracy of each category during training; (2) we develop a score-guided equilibrium loss that improves the intensity of the adjustment of the decision boundary for the weak classes. (3) we design a memory-augmented feature sampling to enhance the frequency and accuracy of the adjustment of the decision boundary for the weak classes. <ref type="bibr" target="#b3">(4)</ref> we conduct experiments on LVIS <ref type="bibr" target="#b5">[6]</ref> using Mask R-CNN <ref type="bibr" target="#b7">[8]</ref> with various backbones including ResNet-50-FPN and ResNet-101-FPN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. Extensive experiments show the superiority of LOCE. It improves the tail classes by 15.6 AP based on the Mask R-CNN with ResNet-50-FPN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> and outperforms the most recent long-tailed object detectors by more than 1 AP on LVIS v1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection. Modern object detection frameworks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr">28]</ref> can be divided into two-stage and one-stage ones. The two-stage detectors first generate a set of region proposals, and then classify and refine the proposals. In contrast, the one-stage detectors directly predict the category and bounding box at each location. Most of the detectors are designed for balanced data. When it comes to long-tailed data, the performance of such detectors is greatly degraded. Recently, extensive studies attempted to optimize the two-stage detectors such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> to cope with the long-tailed data, by designing balanced samplers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> or balanced loss functions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Some works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref> adopt decoupled training pipeline <ref type="bibr" target="#b9">[10]</ref>, which first learns the universal representations with unbalanced data and then fine-tunes the classifier with re-balanced data or balanced loss function. Inspired by them, we propose both adaptive feature sampling and adaptive loss function for long-tailed detection.</p><p>Sampler for long-tail learning. Data re-sampling is a common solution for long-tail learning. It typically oversamples the training data from tail classes while undersamples those from head classes. In long-tailed detection, the data samplers balance the training data on the imagelevel or instance-level. Gupta et al. <ref type="bibr" target="#b5">[6]</ref> use image-level Repeat Factor Sampling (RFS) to up-sample the data from minority classes based on the sampling frequency of each class. Wang et al. <ref type="bibr" target="#b23">[24]</ref> propose a class-based sampler to balance the data from the instance-level, by only considering the proposals of the selected classes. Wu et al. <ref type="bibr" target="#b25">[26]</ref> set a higher NMS threshold for tail classes to sample more proposals from tail classes. These methods design the balanced sampler depending on the frequency distribution of categories. In contrast, we design a memory-augmented feature sampling based on the mean classification score, which can adapt to the training process dynamically. Recently, ren et al. <ref type="bibr" target="#b17">[18]</ref> introduce Meta Sampler to estimate the optimal sample rate with meta-learning. Compared with Meta Sampler, our feature sampling is simpler and more versatile. Loss Function for long-tail learning. Balanced loss functions have received lots of attention in long-tailed classification. Most of them are achieved through loss weighting or margin modification related to the distribution of training data. For example, the works such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">27]</ref> reweight the loss functions by the inverse of the sampling frequency of each class, while those of <ref type="bibr">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> increase the loss margins of tail classes and decrease those of head classes for balanced classification. Recently, several works attempted to design balanced loss for long-tailed detection. Tan et al. <ref type="bibr" target="#b20">[21]</ref> propose Equalization Loss (EQL) to improve the performance of tail classes by ignoring the suppressing gradients for tail classes. Li et al. <ref type="bibr" target="#b11">[12]</ref> introduce Balanced Group Softmax that first groups the classes based on the instance numbers and then separately apply softmax within each group. Ren et al. <ref type="bibr" target="#b17">[18]</ref> design Balanced Softmax to accommodate the label distribution shifts of the long-tailed data according to the number of category samples. Tan et al. <ref type="bibr" target="#b20">[21]</ref> improve EQL by re-balancing the positive and negative gradients for each category independently and equally. Wang et al. <ref type="bibr" target="#b22">[23]</ref> develop Seesaw loss to re-balance the gradients of positive and negative samples by accumulating the number of the training samples. Different from the existing methods, we design the loss function according to the mean classification score calculated during training. It can track and adjust the learning status of the model dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As mentioned in Section 1, if the distribution of the training data is severely skewed, the mean classification score obtained by the conventional detectors is extremely imbalanced for each category. In this work, we propose LOCE, an object detector with classification equilibrium, to alleviate this problem. We first use the mean classification score to indicate the learning status (i.e., classification accuracy) of each category during training (Section 3.1). Then, based on this indicator, we balance the classification through a score-guided equilibrium loss (Section 3.2) and a memory-augmented feature sampling method (Section 3.3). The proposed loss function and the feature sampling method collaboratively adjust the classification decision boundary, as demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Similar to most methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref> for long-tailed object classification or detection, we adopt the decoupled training pipeline <ref type="bibr" target="#b9">[10]</ref>. The two methods are adopted in the fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mean Classification Score</head><p>We first analyze the classification problem when applying conventional detectors to long-tailed data, and then introduce the mean classification score to indicate the learning status of the detector.</p><p>Problem formulation. Most conventional detectors designed for balanced data are optimized with softmax crossentropy loss function and random image sampler. Formally, let F denotes a scorer that takes an image x as input and generates a category prediction z = F (x), where z ? R C+1 (C object classes plus a background class). During training, with the corresponding ground truth label y ? {1, 2, ..., C + 1}, the standard softmax cross-entropy can be written as: With a random image sampler, each image x is selected with equal probability during training. Considering the long-tail learning where the distribution P(y) is highly skewed, the images from tail classes have a very low probability of occurrence. Under the optimization of softmax cross-entropy and random image sampler, the classifier tends to predict unbalanced classification scores resulting in lots of misclassification for tail classes, as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Classification accuracy indicator. To alleviate the problem of classification imbalance, we attempt to find an effective indicator to reflect the learning status (i.e., classification accuracy) of the classifier for each category and dynamically adjust the learning process. Previous works <ref type="bibr">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref> proposed to balance the classification based on the number of training samples of each category. However, the number of the training samples cannot indicate the learning quality of the model accurately, because of the diversity and complexity of instances and categories. For example, training a classifier for the categories with high inter-class visual similarity usually requires more training samples than the categories with very different visual appearances. Instead, we seek a more effective indicator to reflect the classification status. From the statistics shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we found that the mean classification score has an approximate positive correlation with the classification performance. Namely, for LVIS, the head classes have higher mean classification scores and higher classification accuracy, while the tail classes have lower mean classification scores and lower classification accuracy. For the balanced dataset COCO, we also observe a similar pattern: high mean classification scores are usually associated with high classification accuracy. Therefore, we propose to use the mean classification score to indicate the learning status of the model for each category. However, computing the mean classification score for the whole dataset at each training iteration is infeasible. Instead, we approximate the mean classification score by a mean score vector s ? R C+1 during training. For an instance with positive label y at the ith iteration, we update the corresponding s i y in the mean score vector with the exponential moving average function:</p><formula xml:id="formula_0">s i y = ?s i?1 y + (1 ? ?)p i y ,<label>(2)</label></formula><p>where p i y is the predicted probability of the instance, and ? is a smoothing coefficient hyper-parameter.</p><p>Compared with the existing works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> that use the number of training instances to indicate the learning status of the model for each category, the proposed indicator has the following advantages: (1) it can monitor the classification accuracy of each category during training; (2) it can be applied when the distribution of the training data is not visible or the model is pre-trained with other datasets, e.g., the training samples are obtained from an online stream. The mean classification score affects the training by guiding the proposed loss function and the feature sampling method to balance the classifier, which is introduced in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Equilibrium Loss</head><p>In this section, we introduce the Equilibrium Loss (EBL) to balance the classification through shifting the decision boundary of the classifier. Recently, several works such as <ref type="bibr">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> also attempt to adjust the decision boundary to balance the classification, based on the prior distribution P(y) or the accumulated number of the training samples of each category. However, as mentioned in Section 3.1, the number of the training samples cannot indicate the learning quality of the model accurately. Different from the existing methods, EBL adjusts the decision boundary according to the mean classification score. Concretely, if a category (e.g., a tail class) has a low classification score, we expect EBL to push the decision boundary away from that category and towards other categories (e.g., head classes). This behavior, in turn, improves the mean classification score of the current category resulting in a more balanced classification. To achieve that, we add a score-relevant margin ? yy into the softmax cross-entropy loss, turning into EBL as:</p><formula xml:id="formula_1">L(z, y) = log[1 + y =y e z y ?zy+? yy ],<label>(3)</label></formula><p>where ? yy works as a tunable balancing margin between any two classes, based on the distribution of the mean classification score. To adjust the decision boundary accordingly, the design of ? yy should satisfy the following two properties: (1) it should reduce the suppression of dominant classes (i.e., having high mean classification score) over weak classes (i.e., having low mean classification score), which can be achieved by reducing the margin between dominant positive classes and weak negative classes; (2) it should enlarge the suppression of weak classes over dominant classes, which can be achieved by increasing the margin between weak positive classes and dominant negative classes. Therefore, we design the following adaptive loss margin between any two classes:</p><formula xml:id="formula_2">? yy = log( s y s y ).<label>(4)</label></formula><p>Background class. As defined above, the background is regarded as an auxiliary category in the classifier. In the experiments, we found that the classifier trained with EBL tends to predict false positive results, i.e., misclassifying the backgrounds as foregrounds. To reduce those false positive cases, we enlarge the corresponding punishment. In the margin loss, it can be achieved by increasing the margin between the positive background class and the negative foreground classes. Consequently, we decrease the mean classification score of background class s C+1 when computing Eq.(4). It is worth noting that most training samples (at least 75%) for classifier are negative samples (i.e., background). For simplicity and efficiency, instead of computing the statistical s C+1 , we use a small value (e.g., 0.01)  to replace s C+1 , namely? C+1 . Some works such as <ref type="bibr" target="#b22">[23]</ref> attempt to reduce the false positive cases by introducing an extra objectness branch. Comparatively, the proposed method is simpler and more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Memory-augmented Feature Sampling</head><p>Although EBL tends to move the decision boundary from the tail classes to the dominant ones, the decision boundary sometimes is still closer to the tail classes. This is because EBL only adjusts the intensity of the adjustment of the decision boundary, while the frequency of the adjustment is very low for tail classes during training, due to the overwhelming number of images in the head classes. Especially, the tail classes usually have very few training samples (e.g., &lt; 10 for LVIS). Therefore, we need to increase the number (or occurrence) of the training samples of tail classes to enhance the frequency of such boundary adjustment, in addition to EBL.</p><p>To increase the occurrence of the training samples from tail classes, a straightforward method is data re-sampling. As shown in <ref type="figure" target="#fig_4">Figure 3(a)</ref>, the widely used sampling methods for data balancing can be divided into two categories: (1) Dataset-based image sampling such as Repeat Factor Sampling (RFS) <ref type="bibr" target="#b5">[6]</ref> and Class-balanced Sampling (CBS) <ref type="bibr" target="#b23">[24]</ref> usually samples more training images of the tail classes based on the training set statistics. (2) Classbased proposal sampling such as NMS Resampling <ref type="bibr" target="#b25">[26]</ref> and Bi-level Sampling <ref type="bibr" target="#b23">[24]</ref> samples more proposals from Region Proposal Network (RPN) for the tail classes or the selected classes. Despite their success, these sampling methods have the following limitations: (1) the diversity of the training features from proposal sampling depends on the behavior of RPN; (2) image sampling usually requires more training iterations; (3) most existing sampling methods are model-agnostic and prone to over-fit the tail classes and under-represent the head classes. To overcome these limitations, we propose a more efficient Memory-augmented Feature Sampling (MFS) method. As shown in <ref type="figure" target="#fig_4">Figure 3(b)</ref>, MFS consists of a bounding box generator, a feature memory module, and a probabilistic sampler.</p><p>Bounding box generator. Recent two-stage detectors sample RoI features for the classifier based on the proposals from RPN and the sampling configuration. In such pipeline, the diversity of the classification training features is limited by the performance of RPN and the sampling configuration such as the number of positive samples at each iteration. It hinders the classifier from improving its generalization ability and accuracy, especially for the tail classes. Instead, we design a model-agnostic bounding box generator with aim to extract rich instance features for training the classifier. Moreover, we propose to reuse the extracted instance features across training iterations. With such design, the feature diversity for the classifier is no longer sensitive to the performance of RPN and the sampling configuration.</p><p>Concretely, given an object instance, we have its groundtruth class y and ground-truth box b = [x 1 , y 1 , x 2 , y 2 ], where (x 1 , y 1 ) and (x 2 , y 2 ) are the coordinates of the upper left corner and the lower right corner of the groundtruth box. The bounding box generator yields the following dense bounding boxes: To alleviate the above problems, we use a feature memory module to store the instance features, and reuse the instance features as needed during the following training. The rationale is that the parameters of the backbone (e.g., ResNet-50-FPN) are frozen during the fine-tuning stage. Thus, the features extracted from the backbone are stable and can be reused to fine-tune the classifier during different training iterations. Memory module is widely adopted in contrastive learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref>, but never used in longtailed object detection. Specifically, the feature memory module is maintained and updated with a class queue for each class y:</p><formula xml:id="formula_3">b = [x 1 ? ? 1 w 6 , y 1 ? ? 2 h 6 , x 2 ? ? 3 w 6 , y 2 ? ? 4 h 6 ],<label>(5)</label></formula><formula xml:id="formula_4">M y = [f 1 y , f 2 y , ..., f M y ],<label>(6)</label></formula><p>where f i y is the ith RoI features of class y in the memory module M y , and M is the memory size. At each iteration, we enqueue the instance features f y of current mini-batch into the corresponding class queue, and dequeue the instance features of the earliest mini-batch accordingly. Most two-stage detectors such as Mask R-CNN share Fullyconnected (FC) layers for classification and box regression before the prediction layers. We also store the regression target t i y corresponding to f i y to train the box branch, which can improve the accuracy of box regression for tail classes with negligible overhead.</p><p>Probabilistic Sampler. At each training iteration, we access the feature memory module by a sampler to augment the training features for balancing the classifier. In particular, we use the mean classification score to adaptively adjust the sampling process, similar to that for EBL. To improve the training effectiveness and classification score of the weak classes (e.g., tail classes), we sample more memory features of these classes. Specifically, we design a probabilistic sampler that samples the memory features according to the probability p with negative correlation with the mean classification score, namely:</p><formula xml:id="formula_5">p y = f (s y ) y f (s y ) ,<label>(7)</label></formula><p>where f (?) is a non-increasing transform and p y is the sampling probability of class y. For simplicity, we define:</p><formula xml:id="formula_6">f (s y ) = 1 s y .<label>(8)</label></formula><p>Finally, we randomly choose k classes according to p, and select m features from the feature memory for each selected class. Then, the k ? m selected features will be used together with the RoI features from RPN to train the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset and evaluation metric. We conduct experiments on the recent long-tail and large-scale dataset LVIS <ref type="bibr" target="#b5">[6]</ref>. The latest version v1.0 contains 1203 categories with both bounding box and instance mask annotations. We use the train set (100k images with 1.3M instances) for training and val set (19.8k images) for validation. We also perform experiments on LVIS v0.5, which contains 1230 and 830 categories respectively in its train set and val set. All the categories are divided into three groups based on the number of the images that each category appears in the train set: rare (1-10 images), common (11-100 images), and frequent (&gt;100 images). Apart from the official metrics Average Precision (AP), we also report AP r (for rare classes), AP c (for common classes) and AP f (for frequent classes) to measure the detection performance and segmentation performance. Unless specific, AP b denotes the detection performance, while AP denotes the segmentation performance.</p><p>Implementation details. We implement our method with MMDetection <ref type="bibr" target="#b1">[2]</ref> and conduct experiments using Mask R-CNN <ref type="bibr" target="#b7">[8]</ref> with various backbones including ResNet-50-FPN and ResNet-101-FPN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> pre-trained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. Following <ref type="bibr" target="#b17">[18]</ref>, we use the decoupled training pipeline. Namely, we first train the model with standard softmax cross-entropy and random image sampler for 24 epochs, then fine-tune the model with the proposed method for 6 epochs. Specifically, the initial learning rate is 0.02 and dropped by a factor of 10 at the 16th and 22th epoch for the first training stage and the 3th and 5th epoch for the fine-tuning stage. The models are trained using SGD optimizer with 0.9 momentum and 0.0001 weight decay and batch size of 16 on 8 GPUs. Following the convention, we train the detector with scale jitter (640-800) and horizontal flipping. At testing time, the model is evaluated without test time augmentation, and the maximum number of detections per image is 300 with the minimum score threshold of 0.0001. We set ? as 0.9 for updating the mean classification score. The substituted? C+1 is set to be 0.01. The memory size M is 80, and k and m in feature sampler are 8 and 4 on each GPU. As in <ref type="bibr" target="#b22">[23]</ref>, we adopt normalized linear activation to the mask prediction. More implementation and training details refer to the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We use Mask R-CNN with backbone ResNet-50-FPN for ablation study, and report the results on LVIS v1.0. <ref type="table">Table 1</ref> reports the detection and segmentation results of each proposed component. For a fair comparison, we train the baseline using standard softmax cross-entropy and random image sampler for 30 epochs. First, we evaluate the performance of EBL. EBL improves both the AP b and AP by 3.6 AP, comparing to the baseline. Specifically, it improves the performance of the classes of all groups, i.e., +5.4 AP for rare classes, +5.5 AP for common classes and +0.6 AP for frequent classes, respectively. These results show that the score-guided loss margin can also help to optimize the decision boundary of head classes, even though it is mainly designed for weak classes. We then examine the effectiveness of MFS. Compared with the baseline, MFS improves the performance by 5.6 AP for object detection and 5.3 AP for instance segmentation. To be more specific, most of the improvements are from rare classes and common classes, which yield +15.5 AP and +6.8 AP improvement for instance segmentation. We can see that using more instance features from weak  classes can bring large improvements, especially for rare classes. Next, we verify the effectiveness of the complete method (i.e., LOCE). EBL and MFS work collaboratively and improve the AP by 7.0 AP for object detection and 6.4 AP for instance segmentation, comparing to the baseline. Notably, MFS itself achieves a little lower performance on frequent classes than the baseline while LOCE achieves higher performance on frequent classes. This shows that EBL helps MFS to find a better equilibrium point for the frequent classes. Therefore, LOCE dramatically improves the performance of tail classes, while maintaining or even improving the performance of head classes. Lastly, we evaluate the performance of using the prior distribution of the dataset (i.e., P(y)) to guide EBL and MFS to balance the classification. Specifically, we use the number of the instances of each category in train set to replace the mean classification score in LOCE, and its results are shown in <ref type="table">Table 1</ref> (last row). We can see that the datasetguided LOCE achieves a lower performance than the scoreguided one (e.g., 23.4 AP vs. 27.4 AP for object detection). Specifically, the dataset-guided LOCE obtains a higher AP for rare classes, but it greatly hurts the performance of frequent classes. These results show that the dataset-guided method under-represents the head classes, as discussed in Section 1. In contrast, the proposed score-guided method can adaptively adjust the loss margin and the sampling rate for each class according to the learning status, improving the performance for the classes from all groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component analysis.</head><p>Hyper-parameters. We compare the performance with different smoothing coefficients ? for updating the mean classification score. From the experiment results shown in <ref type="table">Table 2</ref>, we observe that the performance is insensitive to the value of ?, and ? = 0.9 yields the best performance. The performance with different? C+1 is shown in <ref type="table">Table 3</ref>.</p><p>We can see that the detector achieves the best performance when? C+1 = 0.01. We then conduct several experiments to study the robustness with respect to k and m of feature sampler in <ref type="table">Table 4</ref>. Through a coarse search, we set k = 8 and m = 4 for the rest of the experiments.</p><p>Analysis of classification equilibrium. Here we analyze the mean classification score and the classification accuracy between different methods on LVIS val set. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, the distribution of the mean classification score predicted by the detector trained with softmax cross-entropy loss and random image sampling is severely skewed. Specifically, the mean classification score of tail classes is close to 0, and its classification accuracy is also close to 0. When using RFS instead of random image sampling to train the detector, both the mean classification score and the classification accuracy are improved marginally. In contrast, the mean classification score predicted by the proposed LOCE is more balanced than those predicted by the two methods mentioned above, and the classification accuracy of common classes and tail classes is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-Art</head><p>We compare LOCE with the state-of-the-art methods on LVIS v0.5 and LVIS v1.0 in <ref type="table">Table 5</ref>. On LVIS v0.5, the proposed method achieves the detection performance of 28.2 AP and segmentation performance of 28.4 AP, surpassing the most recent long-tailed object detectors such as BAGS <ref type="bibr" target="#b11">[12]</ref> (by 2.4 AP and 2.1 AP) and BALMS <ref type="bibr" target="#b17">[18]</ref> (by 0.6 AP and 1.4 AP). Specifically, it outperforms BAGS by 4.0 points on rare classes, which shows the superior performance of the proposed method for tail classes. Compared with most existing methods such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>, the proposed method gets a much higher performance on head classes, in addition to improving the detection performance on tail classes. On LVIS v1.0, the proposed method achieves a better result than all the methods shown in <ref type="table">Table 5</ref>, including the concurrent work such as Seesaw Loss <ref type="bibr" target="#b22">[23]</ref> and EQL v2 <ref type="bibr" target="#b19">[20]</ref>. With the framework of Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>, LOCE achieves the detection performance of 27.4 AP and 29.0 AP on R-50-FPN and R-101-FPN, outperforming recent works such as BAGS <ref type="bibr" target="#b11">[12]</ref>, Seesaw Loss <ref type="bibr" target="#b22">[23]</ref> and EQL v2 <ref type="bibr" target="#b19">[20]</ref> by more than 1 AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we explore the classification equilibrium in long-tailed object detection. We propose to use the mean classification score to indicate the learning status of the model for each category, and design an equilibrium loss and a memory-augmented feature sampling method to balance the classification. Extensive experiments show the superiority of the proposed method, which sets a new state-of-theart in long-tailed object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>category index (the number of instances from this category) 10 (11266) 20 (8631) 30 (6567) 40 (6062) 50 (5474) 60 (4192) 70 (2261) Sorted category index (the number of instances from this category) Statistics of mean classification score and classification accuracy for each category on LVIS v1.0 training set and COCO training set tested by Mask R-CNN with ResNet-50-FPN. The xaxis represents the sorted category index and the number of instances from the corresponding category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>direction and intensity from new features and : optimization direction and intensity from original features and direction and intensity from memory features and : optimization direction and intensity from RPN features and : direction and intensity from new features and : optimization direction and intensity from original features and direction and intensity from memory features and : optimization direction and intensity from RPN features and : direction and intensity from new features and : optimization direction and intensity from original features and : original features and direction and intensity from new features and : optimization direction and intensity from original features and : Demonstration of the adjustment of the decision boundary for classification balance. For simplicity, we only demonstrate the adjustment process between a head class and a tail class. The relative magnitudes of the mean classification score can approximately reflect the distances from the class feature center to decision boundary, i.e., s h and st. The region proposal features are obtained from RPN. The memory features are obtained from feature memory which stores rich instance features from the dense bounding boxes. The direction and size of the arrows mean the direction and intensity of the adjustment of the decision boundary. Specifically, EBL (Section 3.2) decides the size of the arrows while MFS (Section 3.3) decides the number of the arrows. With stronger and more adjustments from tail class, the decision boundary moves from tail class to head class until the equilibrium is reached.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>(z, y) = ?log e zy y ?{1,2,...,C+1} e z y = log[1 + y =y e z y ?zy ]. (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>RPN and :images containing head class and tail class and :ground-truth boxes of head class and tail class and :bounding boxes from RPN and pre-defined dense boxes containing head class and tail class; and :ground-truth boxes of head class and tail class; and :bounding boxes from RPN and mod containing head class and tail class; and : ground-truth boxes of head class and tail class; and : bounding boxes from RPN and model-agnostic bounding box generator. (a) Existing sampling methods (b) The proposed method Box generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Comparison between the existing balanced sampling methods and the proposed memory-augmented feature sampling method. (a): Dataset-based image sampling (e.g., RFS<ref type="bibr" target="#b5">[6]</ref> and CBS<ref type="bibr" target="#b23">[24]</ref>) over-samplings the images from tail classes or under-samplings the images from head classes. Class-based proposal sampling (e.g., NMS Resampling<ref type="bibr" target="#b25">[26]</ref> and Bi-level Sampling<ref type="bibr" target="#b23">[24]</ref>) samples more proposals from tail classes or the selected classes. (b): Memory-augmented feature sampling stores the instance features from the model-agnostic dense bounding boxes by the feature memory module, and samples the memory features based on the mean classification score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Analysis of classification equilibrium between different training methods on LVIS v1.0 val set. The x-axis represents the sorted category index obtained from the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where w, h are box width and height, i.e., w = x 2 ? x 1 , h = y 2 ? y 1 , and ? i ? [0, 1] is a random number. Such bounding box generator can obtain any potential positive Memory Module. Image re-sampling usually requires extra training iterations while proposal sampling does not provide enough samples to balance the classification, especially for tail classes. In contrast, the proposed bounding box generator can yield dense bounding boxes for extracting more instance features, without the requirement of extra training iterations. However, using all the instance features from the dense bounding boxes to train the classifier within an iteration can bring lots of computational overhead and memory consumption, especially when there are lots of instances in an image. Besides, for the tail classes that only occur a few times in a training epoch, taking all the instance features within an iteration may still be inadequate.</figDesc><table><row><cell>Feature</cell></row></table><note>bounding boxes (i.e., IoU &gt; 0.5). Based on the dense bounding boxesb, we extract the corresponding instance features f y by applying RoI-Align [8] to the features from Feature Pyramid Network (FPN).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 4 .</head><label>14</label><figDesc>25.7 18.4 25.0 29.8 27.4 26.6 18.5 26.2 30.7 Effectiveness of each proposed component. s denotes the mean classification score, and P(y) denotes the class distribution of the dataset. Analysis of different settings of k and m in sampler.</figDesc><table><row><cell cols="4">Indicator EBL MFS AP b</cell><cell>AP</cell><cell>APr APc AP f</cell></row><row><cell>s</cell><cell></cell><cell></cell><cell cols="2">20.4 20.2</cell><cell>2.9</cell><cell>18.2 30.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">24.0 23.8</cell><cell>8.3</cell><cell>23.7 30.7</cell></row><row><cell cols="5">26.0 P(y) 23.4 23.1 20.8 23.1 24.1</cell></row><row><cell>?</cell><cell></cell><cell>AP b</cell><cell>AP</cell><cell>APr APc AP f</cell></row><row><cell cols="5">0.8 0.9 0.95 27.3 26.6 18.1 26.4 30.7 27.2 26.4 17.2 26.3 30.7 27.4 26.6 18.5 26.2 30.7</cell></row><row><cell cols="5">Table 2. Analysis of different smoothing coefficients of ?.</cell></row><row><cell cols="3">sC+1 AP b</cell><cell>AP</cell><cell>APr APc AP f</cell></row><row><cell cols="5">0.1 0.01 0.001 27.4 26.5 18.4 26.0 30.7 26.7 26.0 17.8 25.5 30.3 27.4 26.6 18.5 26.2 30.7</cell></row><row><cell cols="5">Table 3. Analysis of different value of?C+1 in equilibrium loss.</cell></row><row><cell cols="3">k m AP b</cell><cell>AP</cell><cell>APr APc AP f</cell></row><row><cell>2</cell><cell>2</cell><cell cols="3">27.1 26.3 17.8 26.1 30.6</cell></row><row><cell>4 4 8 8</cell><cell>4 8 4 8</cell><cell cols="3">27.2 26.5 18.3 26.1 30.6 27.3 26.6 18.5 26.3 30.6 27.4 26.6 18.5 26.2 30.7 27.2 26.5 17.5 26.3 30.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 5. Comparison with the state-of-the-art on LVIS v0.5 and v1.0.</figDesc><table><row><cell>Method</cell><cell>Framework</cell><cell>Backbone</cell><cell>Dataset</cell><cell>AP b</cell><cell>AP</cell><cell cols="2">APr APc AP f</cell></row><row><cell>RFS [6]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">26.1 25.9 17.8 26.2 28.8</cell></row><row><cell>EQL [21]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">24.1 25.2 14.6 24.4 26.8</cell></row><row><cell>Forest R-CNN [26] BAGS [12]</cell><cell>Mask R-CNN</cell><cell>R-50-FPN</cell><cell>LVIS v0.5</cell><cell cols="4">25.9 25.6 18.3 26.4 27.6 25.8 26.3 18.0 26.9 28.7</cell></row><row><cell>BALMS [18]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">27.6 27.0 19.6 28.9 27.5</cell></row><row><cell>EQL v2 [20]  ? LOCE (ours)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">27.0 27.1 18.6 27.6 29.9 28.2 28.4 22.0 29.0 30.2</cell></row><row><cell>RFS [6]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">24.7 23.7 13.5 22.8 29.3</cell></row><row><cell>EQL [21]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">22.5 21.6</cell><cell>3.8</cell><cell>21.7 29.2</cell></row><row><cell>Seesaw Loss [23]  *   ?</cell><cell>Mask R-CNN</cell><cell>R-50-FPN</cell><cell>LVIS v1.0</cell><cell cols="4">24.3 23.3 13.0 22.9 28.2</cell></row><row><cell>EQL v2 [20]  ? LOCE (ours)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">26.1 25.5 17.7 24.3 30.2 27.4 26.6 18.5 26.2 30.7</cell></row><row><cell>RFS [6]</cell><cell></cell><cell></cell><cell></cell><cell cols="4">26.6 25.5 16.6 24.5 30.6</cell></row><row><cell>EQL [21]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">24.0 22.7</cell><cell>3.7</cell><cell>23.3 30.4</cell></row><row><cell>BAGS [12] Seesaw Loss [23]  ?</cell><cell cols="3">Mask R-CNN R-101-FPN LVIS v1.0</cell><cell cols="4">26.4 25.6 17.3 25.0 30.1 27.4 27.1 18.7 26.3 31.7</cell></row><row><cell>EQL v2 [20]  ? LOCE (ours)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">27.9 27.2 20.6 25.9 31.4 29.0 28.0 19.5 27.8 32.0</cell></row></table><note>* indicates 1x training schedule.? indicates the concurrent work.</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>-Supplementary material -Exploring Classification Equilibrium in Long-Tailed Object Detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Implementation details</head><p>Here we elaborate on the more detailed implementations for the experiments in ablation study.</p><p>Feature memory module. The memory size M is set to 80, which can store five latest instance features for each category. Thus, we initialize the memory features by randomly selecting 5 instances per class. In addition, an image may contain multiple instances of the same category (e.g., &gt; 20 instances) on LVIS. For simplicity and efficiency, we randomly select 5 instances to update the feature memory for each category that contains more than 5 instances in an image during the training.</p><p>Dataset-based EBL. For dataset-based EBL, we use the number of the training instances of each category to compute the loss margin as follow:</p><p>where n y is the number of the training instances of category y. As the score-based EBL, we use a small valuen C+1 to replace n C+1 (i.e., the number of the training instances of background) to reduce the false positive cases. In our experiments,n C+1 is set to 1.</p><p>Dataset-based MFS. The bounding box generator and the feature memory module of dataset-based MFS are the same as those of score-based MFS, while the probabilistic sampler of dataset-based MFS uses the sampling probability based on the number of the training instances of each category. Namely, the sampling probability is computed as follow:</p><p>where:</p><p>Repeat factor sampling (RFS). RFS over-samples the images containing the tail classes by increasing the sampling rate for these categories. As demonstrated in <ref type="bibr">[1]</ref>, we use the best setting of RFS that over-samples the categories that appear in less than 0.1% of the total images.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07413</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memory-based neighbourhood embedding for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suichan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6102" to="6111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10991" to="11000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07314</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Combining statistical learning with a knowledgebased approach: a case study in intensive care monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Morik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brockhausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Equalization loss v2: A new gradient balance approach for long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seesaw loss for longtailed instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The devil is in classification: A simple framework for long-tail instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="728" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-batch memory for embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6388" to="6397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Forest r-cnn: Large-vocabulary long-tailed object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1570" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
