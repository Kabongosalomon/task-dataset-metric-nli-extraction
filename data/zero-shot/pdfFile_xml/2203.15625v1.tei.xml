<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PoseTriplet: Co-evolving 3D Human Pose Estimation, Imitation, and Hallucination under Self-supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehong</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huawei International Pte Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei International Pte Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Bi</forename><surname>Mi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei International Pte Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PoseTriplet: Co-evolving 3D Human Pose Estimation, Imitation, and Hallucination under Self-supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://github.com/Garfield-kh/PoseTriplet.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing self-supervised 3D human pose estimation schemes have largely relied on weak supervisions like consistency loss to guide the learning, which, inevitably, leads to inferior results in real-world scenarios with unseen poses. In this paper, we propose a novel self-supervised approach that allows us to explicitly generate 2D-3D pose pairs for augmenting supervision, through a self-enhancing dual-loop learning framework. This is made possible via introducing a reinforcement-learning-based imitator, which is learned jointly with a pose estimator alongside a pose hallucinator; the three components form two loops during the training process, complementing and strengthening one another. Specifically, the pose estimator transforms an input 2D pose sequence to a low-fidelity 3D output, which is then enhanced by the imitator that enforces physical constraints. The refined 3D poses are subsequently fed to the hallucinator for producing even more diverse data, which are, in turn, strengthened by the imitator and further utilized to train the pose estimator. Such a co-evolution scheme, in practice, enables training a pose estimator on self-generated motion data without relying on any given 3D data. Extensive experiments across various benchmarks demonstrate that our approach yields encouraging results significantly outperforming the state of the art and, in some cases, even on par with results of fully-supervised methods. Notably, it achieves 89.1% 3D PCK on MPI-INF-3DHP under self-supervised cross-dataset evaluation setup, improving upon the previous best self-supervised method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref> by 8.6%. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video-based 3D human pose estimation aims to infer 3D pose sequences from videos, and therefore plays a crucial role in many applications such as action recognition <ref type="bibr" target="#b47">[47,</ref><ref type="bibr">Figure 1</ref>. Overview of our PoseTriplet framework. The pose estimator, imitator, hallucinator are trained jointly in a dual-loop strategy. In the first loop, the estimator provides physically implausible motion information, which is then enhanced by the imitator via enforcing physical constraints to generate physically plausible motion. In the second loop, the hallucinator generates more diverse motion patterns given motion sequence from previous loop, and sends them to the imitator again for further refinement. This dual-loop paradigm facilitates tight co-evolution of the three components and enables iterative self-improving training of the estimator with the generated diverse and plausible motion data. <ref type="bibr" target="#b58">58</ref>], virtual try-on <ref type="bibr" target="#b30">[31]</ref>, and mixed reality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>. Existing methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b48">48]</ref> mainly rely on the fullysupervised paradigms, in which the ground truth 3D data are given as input. However, capturing 3D pose data is cost-intensive and time-consuming, as it typically requires a multi-view setup or a motion capturing system <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref>, making it infeasible under in-the-wild scenarios.</p><p>To this end, two categories of methods have been introduced to alleviate the 3D data availability issue. The first category explores the semi-supervised settings, in which only a small amount of the 3D annotations are given <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b68">68]</ref>. The second category, on the other hand, assumes no 3D data are available at all and only 2D poses are provided. Under this setup, state-of-the-art methods have mainly focused on imposing weak supervision signals to guide the training, such as aligning the projection of an inferred 3D pose with a 2D pose <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61]</ref>. Due to the lack of 3D data and hence the missing of 2D-3D pairs, these methods are, by nature, brittle to the challenging scenarios such as unseen poses inherent to the in-the-wild tasks.</p><p>In this paper, we propose a novel self-supervised approach termed as PoseTriplet, which allows for explicitly generating physically-and semantically-plausible 2D-3D pose pairs, so that full supervisions can be imposed and further significantly strengthen the self-learning process. This is made possible through introducing a reinforcementlearning-based imitator, which is jointly optimized with the pose estimator alongside a pose hallucinator. Specifically, the imitator takes the form a of physics simulator with non-differentiable dynamics to ensure physically plausibility. The hallucinator helps generate more diverse motion with generative motion completion. These three key components are integrated into a self-contained framework and co-evolve via a dual-loop strategy as the training proceeds. With only 2D pose data as input, PoseTriplet progressively generates, refines and hallucinates 3D data, which in turn reinforces all components in the loop. Once trained, each component of PoseTriplet can be readily taken out and serves as an off-the-shelf tool for its dedicated task, such as pose estimation or imitation.</p><p>The key motivation behind co-evolving the pose estimator, imitator and hallucinator, lies in their complementary natures. In particular, pose estimator takes 2D poses as input and generates 3D poses with reasonable semantics (e.g., nature behaviors) but implausible dynamics; such derived 3D poses are then refined through the physics-based imitator that enforces physical constraints. Conversely, the reinforcement-learning-based imitator is possible to generate unnatural behaviors (e.g., overly energetic movements), which can be rectified through the pose estimator to ensure the semantic plausibility. Pose hallucinator, on the other hand, enhances the data diversity by producing realistic 3D pose sequences under both the semantic and physical guidance, which further strengthens data synthesizing and hence improves generalization performance.</p><p>We show the overall workflow of PoseTriplet in <ref type="figure">Fig. 1</ref>, which effectively aligns with aforementioned motivation. Unlike prior endeavors that rely on self-consistency-based supervisions or 3D sequences as input, PoseTriplet, through the dual-loop scheme, turns the input 2D poses into dependable 3D poses of realistic semantics and dynamics, thereby lending itself to much stronger supervisions and consequently the co-evolution of the pose estimator, imitator and hallucinator. Experimental results across H36M, 3DHP, and 3DPW datasets demonstrate that, PoseTriplet gives rises to pose estimation results significantly superior to the state-of-the-art self-supervised methods, and sometimes even on par with results from fully-supervised ones. Notably, it achieves 89.1% 3D PCK on MPI-INF-3DHP under self-supervised cross-dataset evaluation setup, improving upon the previous best self-supervised method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref> by 8.6%.</p><p>Our contribution is therefore a novel scheme dedicated for self-supervised 3D pose estimation, achieved by the co-evolution of a pose estimator, imitator, and hallucinator. The three components complement and benefit one another, together leading to a self-contained system that enables realist 3D pose sequences and further the 2D-3D augmented supervisions. By taking only 2D poses as input, PoseTriplet delivers truly encouraging results across various benchmarks, largely outperforming the state of the art and even approaching full-supervised results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>3D pose estimation 3D pose estimation have been wildly explored under fully supervised, semi-supervised, selfsupervised. Various approaches have been explored under fully supervised setting <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b66">66]</ref>. Through offering impressive results, those approaches highly rely on accurate motion capture data, which are hard to collect. To address high cost of data collection, semisupervised methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b68">68]</ref> are proposed to utilize the information from unlabeled data. Besides semi-supervised approach, augmentation based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref> are proposed to enlarge the data amount through evolution strategy <ref type="bibr" target="#b28">[29]</ref> or learnable approach <ref type="bibr" target="#b8">[9]</ref>. Different from the above schemes, self-supervised methods, with multi-view data, explore the intrinsic supervision for model training, without requiring ground truth 3D pose <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b51">51]</ref>. For instance, Kocabas et al. <ref type="bibr" target="#b22">[23]</ref> utilize the epipolar geometry to generate pseudo label, <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b51">51]</ref> utilize the 3D pose consistency across different views. Though being effective, those approaches require synchronized multiple cameras, which are not usual in real scenarios. Other methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61]</ref> explore the more challenge single view setting. For example, Drover et al. <ref type="bibr" target="#b6">[7]</ref> utilize the prior that a random projection of a plausible 3D pose estimation will be plausible in 2D pose distribution through adversary training. Chen et al. <ref type="bibr" target="#b3">[4]</ref> improves this idea by adding cycle consistency. Yu et al. <ref type="bibr" target="#b61">[61]</ref> further introduces the scale steps for 2D poses to resolve the ambiguity issue. Zhang et al. <ref type="bibr" target="#b65">[65]</ref> applies self-supervised learning on test data to adapt model to new scenarios.</p><p>Our method belongs to the self-supervised approaches under single view setting. Different from previous selfsupervised approaches which implement weak supervision signal through consistency <ref type="bibr" target="#b3">[4]</ref> or adversary <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b61">61]</ref>, our method directly uses the strong supervision signal from self-generated data, results in more accurate and stable model performance. The pseudo label strategy <ref type="bibr" target="#b29">[30]</ref> under semi-supervised category is close to our approach. However, our approach does not require ground truth data for model pretraining, and our method introduces physical plausibility refinement and diversity enhancement to achieve better performance, which are absent in <ref type="bibr" target="#b29">[30]</ref>. Physics-based pose estimation The above methods are all Kinematics based. Though providing impressive results, they do not consider physical constrains, thus suffering physical implausible artifacts (e.g., foot skating and ground penetration). To ensure physical plausibility, recent works explore physical constraints. Rempe et al. <ref type="bibr" target="#b43">[43]</ref> introduces physical law to the foot contact and human dynamic, while its iterative optimization is high time costly (e.g., 30 minutes for 2s clip). Later, <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b57">57]</ref> propose differentiable physical constrain to reduce the time cost. But they only consider foot contact, making them less effective in scenarios with other important contact (e.g., lay down, sitting with chair).</p><p>Different from optimization based approaches, physics simulation based methods use physics simulators to provide realistic physical constrains. DeepMimic <ref type="bibr" target="#b40">[40]</ref> tries to imitate various motion from reference mocap data in physics engine via reinforcement learning. SFV <ref type="bibr" target="#b41">[41]</ref> proposes to refine the low fidelity motion data from video-based pose estimation through imitation learning. However, their adopted imitation learning requires days of training for just one clip. Later, SimPoe <ref type="bibr" target="#b64">[64]</ref> addresses this issue by introducing RFC <ref type="bibr" target="#b63">[63]</ref> to effectively reduce the time consumption by training one policy for all motion clips. Our method is built upon SimPoe <ref type="bibr" target="#b64">[64]</ref> for better generalization and low time cost. However, different from those methods only using physical constraints for post processing, our method propose to involve it in the learning loop. As such, no mocap data for pose estimation training and imitation learning is required. Motion synthesis Motion synthesis includes non-learning based and learning based approaches. In non-learning based approaches, the motion graph method <ref type="bibr" target="#b24">[25]</ref> first builds transition edges between different motion points based on their similarity, and then generates new motion data through traversing the graphs. Motion matching <ref type="bibr" target="#b35">[35]</ref> searches proper future frames in motion data based on motion states in real time. In learning based approaches, motion perdition based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b67">67]</ref> aim to predict future poses conditioned on previous poses. Action generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b55">55]</ref> aims to generate pose sequence conditioned on action labels. Motion completion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b20">21]</ref> generates realistic transitions between key frames, which most relevant to our work in their aims. The pose hallucination in our framework also aims to generate novel motion sequence, where motion graph and motion match methods are not applicable due to tight restriction in their generated data. We therefore choose motion completion considering it can generate longer sequence with continuously input key frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given a 2D pose sequence x 1:T = (x 1 , ..., x T ) of length T , where x t ? R J?2 is the 2D spatial coordinate of J body joints at time t, our goal is to estimate the 3D pose sequence X 1:T = (X 1 , ..., X T ), where X t ? R J?3 is the corresponding 3D joint position under the camera coordinate system. Conventionally, a pose estimator P : x 1:T ? X 1:T with parameter ? is trained with a large set of paired 2D and 3D pose data {x 1:T , X 1:T } through fully-supervised learning approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b48">48]</ref>:</p><formula xml:id="formula_0">min ? L P (P ? (x 1:T ), X 1:T ).<label>(1)</label></formula><p>Here L P denotes the loss function which is typically defined as mean square errors (MSE) between predicted and ground truth 3D poses sequences. However, ground truth 3D pose data is expensive to capture, which limits the applicability of these approaches. To avoid using 3D data, previous self-supervised approaches typically apply weak 2D re-projection loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61]</ref> to learn the estimator:</p><formula xml:id="formula_1">min ? L P (?(P ? (x 1:T )), x 1:T ),<label>(2)</label></formula><p>where ? is the perspective projection function. The reprojection loss only provides weak supervision which tends to induce unstable or unnatural estimations. In this work, we aim to design a self-supervised learning framework of which the core is an iterative self-improving paradigm. Specifically, we propose to enhance the current estimation with some specifically designed transformation T (e.g., to produce more smooth and diverse motion):</p><formula xml:id="formula_2">X ? 1:T = T (P ?n (x 1:T ))<label>(3)</label></formula><p>The enhanced estimates are then projected to 2D pose to obtain paired training data {x ? 1:T , X ? 1:T }, which are used to improve the pose estimator:</p><formula xml:id="formula_3">? n+1 ? min ? L P (P ? (x ? 1:T ), X ? 1:T )<label>(4)</label></formula><p>Here ? n and ? n+1 denote the parameters of the current estimator and improved estimator. The improved estimator can then be utilized to start a new iteration of data enhancement and training. Build on this self-improving paradigm, we can train a superior pose estimator starting from only a set of 2D pose sequences {x 1:T }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">PoseTriplet</head><p>To construct an effective self-improving framework, we identify two challenging aspects for enhancing the 3D motion sequence: 1) the pose estimation from the estimator may not be physically plausible due to ignorance of force, mass and contact modeling; 2) existing 2D motion may be limited in diversity and thus the learned model cannot generalize well. To address these challenges, we introduce a pose imitator based on the reinforcement learning aided human motion modeling and a pose hallucinator based on generative motion interpolation accordingly to refine and diversify the 3D motion. The former helps correct the physical artifacts while the latter generates novel pose sequences based on the existing estimates. We find these two aspects in motion are complementary and thus combine them together. The resulting pipeline helps obtain 3D motion data {x ? 1:T , X ? 1:T } with significantly improved physical plausibility and motion diversity. Nevertheless, we find naive twostep combination of the two approaches generate inferiorquality 3D pose sequence. The reason is that performing motion diversification first could be ineffective due to implausible estimate while conducting motion diversification later could introduce physical artifacts. Therefore, we further introduce a dual-loop scheme and unify the two components with pose estimator into a novel self-supervised framework named PoseTriplet. Dual-loop architecture Concretely, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our PoseTriplet introduces a dual-loop architecture to integrate the three modules: a pose estimator P, a pose imitator I, and a pose hallucinator H. Given the set of available 2D pose sequence x 1:T , the pose estimator first transforms them to low-fidelity 3D pose sequence:</p><formula xml:id="formula_4">X 1:T = P(x 1:T )<label>(5)</label></formula><p>{X 1:T } is converted to low-fidelity reference motions and served as semantic guidance signal to the pose imitator, which imposes the physical human motion dynamic modeling and obtains physically plausible motion sequence:</p><formula xml:id="formula_5">X 1:T = I(X 1:T )<label>(6)</label></formula><p>By learning a generative motion completion model, the pose hallucinator then generates novel and diverse motion sequences {X 1:T } based on the improved plausible motion from the imitator:X 1:T = H(X 1:T )</p><p>Afterwords, instead of closing the loop by treating {X 1:T } as augmented data to the estimator, we introduce another loop. We feed {X 1:T } back into the imitator to correct the induced physical artifacts and obtain the final expected plausible and diverse motion sequences:</p><formula xml:id="formula_7">X 1:T = I(X 1:T )<label>(8)</label></formula><p>{X 1:T } is then projected to 2D to obtain paired data {x 1:T ,X 1:T } for training the pose estimator. By jointly optimizing this dual-loop architecture, the three components form a tight co-evolving paradigm: 1) the estimator benefits from the diverse and plausible augmented data to learn more accurate estimation. 2) the imitator learns more robust and physically natural motion based on the improved estimation and diverse data generated from the hallucinator. 3) the hallucinator generates diverse pose sequence of higher quality based on the improved data from the imitator. Loop starting Another challenging aspect of this selfimproving learning paradigm is the loop starting. Without access to 3D motion data, the whole framework cannot start learning. Recall our pose imitator employs physics-based human motion model, we thus develop a zero-data generating strategy that produces initial 3D pose sequence for starting the dual-loop learning. Specifically, we generate root trajectory signal in horizontal plane with random direction and proper velocity. This trajectory is then used for guidance signal for RL agent. By control the agent to follow the generated trajectory, we can generate motion sequences that are physically plausible. These motion sequences is then projected to obtain 2D-3D pose pairs and used to train a initial pose estimator. In this way, the whole dual-loop learning can be started.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Module detail</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pose estimator</head><p>The pose estimator estimates the 3D pose sequence X 1:T from the input sequence x 1:T . Specifically, we adopt a similar estimator architecture as VideoPose <ref type="bibr" target="#b38">[38]</ref>, which predicts both root trajectory and root-relative joint locations. The trajectory can be used as additional movement signal to pose imitator. Meanwhile, the noise in root movement can be corrected by the pose imitator and in turn help the pose estimator. We use Mean Square Error (MSE) loss for the root-related pose estimation and Weighted L1 loss for the trajectory estimation following <ref type="bibr" target="#b38">[38]</ref>. Projection for training estimator Given the generated motion sequence data {X 1:T }, we project them to 2D to obtain paired training data. We consider two strategies for the projection: 1) Heuristic random projection. We set the virtual camera with certain elevation, azimuth range, height and distance range to match the indoor capture environment. This is similar to the projection strategy for 3D pose data synthesis as Chen et al. <ref type="bibr" target="#b5">[6]</ref>; 2) Generative adversarial learning based projection <ref type="bibr" target="#b8">[9]</ref>. A generator is used to regress the camera orientation and position for each motion sequence. The regression is learned through a discriminator by distinguishing the real and the projected 2D pose sequences with the generated camera parameters. In this way, reasonable camera viewpoint distribution can be extracted from real 2D pose data, improving the plausibility of generated 2D-3D paired data. The two strategies are combined in our framework to ensure the diversity of camera viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pose imitator</head><p>The 3D pose sequences {X 1:T } predicted from pose estimator P, due to lack of physical constrain, would suffer unnatural artifacts such as foot skating, floating, floor penetration. Those artifacts prevent it from being used as training data directly for estimator P or hallucinator H. To address the issue, motivated by <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b64">64]</ref>, we introduce a reinforcement learning based pose imitator I to imitate the low fidelity 3D pose sequence {X 1:T } from pose estimator to generate more physically plausible motion sequence {X 1:T }. Background The imitation process can be seen as a Markov decision process. Given a reference motion and current state s t ? S, the agent interacts with the simulation environment with action a t ? A and receive reward r t . the action is determined by a policy ?(a t |s t ) conditioned on state s t ? S; the reward is determined based on how similar the agent behaves like the reference motion. When an action is taken, the current state s t changes to next state s t+1 through transition function T (s t+1 |s t , a t ). The goal is to learn a policy that maximizes the average cumulative rewards ? i=1 ? i r t (i.e., performing similar behavior in physics simulator as reference motion), where ? is the discounting factor. The state, action and rewards are detailed below. State includes current pose q t , current velocityq t , and target pose q t+1 from reference motion. To deal with the noisy reference motion from the pose estimator, we introduce an extra encoded feature ? by concatenating and fusing the past and future motion information. In this way, the control policy is aware of past and future reference motion, and is thus more robust to the noise. Action involves two kinds of forces: internal force and external force. The internal force is applied by actuator on the non-root joints (e.g., elbow, knee). Following previous work <ref type="bibr" target="#b42">[42]</ref>, we use PD (proportional-derivative) control for internal force control. The external force ? t is a virtual force applied on root joint(i.e., hip) <ref type="bibr" target="#b63">[63]</ref> for extra interaction (e.g., sitting on the chair) and is regressed by the policy network.</p><p>Rewards measure the motion differences between the agent and reference motion. These differences capture pose related (pose, velocity), root related (root height, root velocity) and body end factors (position, velocity). Besides, a regulation loss on virtual force is applied to avoid unnecessary external force following <ref type="bibr" target="#b63">[63]</ref>. As we find that the agent is hard to move with the above setting due to the noisy reference motion, we further introduce a feet relative position into the motion characteristics to enhance the feet motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Pose hallucinator</head><p>The pose hallucinator aims to generate novel and diverse motion sequence based on the refined data from pose imitator. In this work, we choose motion interpolation technique to generate novel pose motions. Specifically, we sample key-frames from the refined pose sequence, and interpolate the missing frames via neural networks to generate new motion data. In details, the pose hallucinator is constructed by a recurrent neural network (RNN) structure. The inputs are the sampled temporal key-frames (we sample key-frames with a certain frame interval). Conditioned on these sam-pled key-frames, the model predicts the intermediate frames in sequential manner. A reconstruction loss and an adversary loss is used to train this model. The reconstruction loss measures the L 2 distance between the ground truth and predicted poses. The adversary loss provides temporal supervision to avoid RNN collapse (i.e., predicting average motion). In the inference stage, we randomly select frames from different motion clips and generate novel motion sequences based on these sampled key-frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We study three questions in experiments.</p><p>1) Is PoseTriplet able to improve performance of video pose estimator for both intra-and cross-dataset scenarios? 2) How does the performance improves with the round of coevolving process? 3) How does the amount of training data affects model performance? We conduct experiments with H36M (source dataset) and 3DHP/3DPW (for cross-dataset evaluation). Throughout the experiments, we adopt Video-Pose <ref type="bibr" target="#b38">[38]</ref> (T=27) as our pose estimator. We report results from estimator for comparison. Please refer to supplementary for more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>H36M <ref type="bibr" target="#b16">[17]</ref> is the most popular 3D pose benchmark captured by marker-based motion capture system. It contains 3.6 million video frames for 11 subjects and 15 scenarios. Following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b61">61]</ref>, we use the 2D poses of subject S1, S5, S6, S7, S8 as our training set and evaluate the performance on S9 and S11. The two standard metrics Mean Per Joint Position Error (MPJPE) in millimeters and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE) are used for evaluation. 3DHP <ref type="bibr" target="#b33">[34]</ref> is a large 3D pose dataset. It contains both indoor and outdoor scenarios. Following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>, we report the metrics of MPJPE, Percentage of Correct Keypoints (PCK) and Area Under the Curve (AUC) after scale and rigid alignment for evaluation. We only use its test set to evaluate the model's generalization performance. 3DPW <ref type="bibr" target="#b50">[50]</ref> is a more challenging in-the-wild dataset. It contains more complicated activities and scenarios. Same as 3DHP, we only use its test set to evaluate model's generalization performance. Follow previous work <ref type="bibr" target="#b23">[24]</ref>, we report MPJPE and PA-MPJPE for 3DPW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative results</head><p>Results on H36M We compare our PoseTriplet with other state-of-the-art self-supervised methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b61">61]</ref> under GT (ground truth 2D poses) and Det (detected 2D poses) settings as shown in <ref type="table" target="#tab_0">Table 1</ref>. Among which, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">44]</ref> implement weak supervision (i.e., consistency supervision), <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b61">61]</ref> utilize temporal information through adversary learning <ref type="bibr" target="#b61">[61]</ref> and smoothness constrains <ref type="bibr" target="#b15">[16]</ref>.</p><p>Our method outperforms the best of them by a large margin in MPJPE for both GT (85.3 vs. 68.2) and Det (82.1 vs. 78.0) settings. The result verifies that our method with co-evolving strategy and augmented supervision performs better compared with previous approaches. Moreover, our method also outperforms some weakly-supervised approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b56">56]</ref> which involve ground truth data during training. Especially, comparing with Li et al. <ref type="bibr" target="#b29">[30]</ref> which implements low rank representation and temporal smoothing for pseudo 3D label generation, our approach, utilizing the advantage of physics simulator, provides better refinement and outperforms <ref type="bibr" target="#b29">[30]</ref> by a large margin in MPJPE (88.8 Vs 78.8) even it use ground truth data (i.e., subject 1). This verifies the effectiveness of our co-evolving strategy on reducing reliance on 3D data. </p><formula xml:id="formula_8">Mode Method GT Det P1 (?) P2 (?) P1 (?) P2 (?) Full</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on 3DHP</head><p>We then evaluate the generalization performance of our method on cross dataset 3DHP. We compare our PoseTriplet with state-of-the-art methods, including fully supervised, weakly supervised, and self supervised approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b61">61]</ref>. As shown in <ref type="table" target="#tab_2">Table 2</ref>, under cross-data evaluation, our method overruns previous self-supervised methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b61">61]</ref>   <ref type="bibr" target="#b25">[26]</ref>, which uses extra data and unpaired 3D poses for model training and thus achieves slightly better performance in AUC (56.3 vs. 53.1). Our method also outperforms self-supervised methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b61">61]</ref> trained on 3DHP dataset directly. In addition, our method achieves better performance than weakly supervised approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">49]</ref> in all metrics, even though they use un-paired images and 3D poses for supervision during the training process. Notably, our method even achieves comparable performance with fully supervised approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">49]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on 3DPW</head><p>We further evaluate the generalization performance of our method on in-the-wild 3DPW dataset. Note that there is few works evaluated on 3DPW under the self-supervised cross dataset setting. Therefore, we compare to the supervised approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b54">54]</ref> directly. From <ref type="table" target="#tab_3">Table 3</ref>, we can observe our method achieves comparable results with the fully supervised baseline without relying on any 3D data. This demonstrates that our method performs well on complicated and challenging in-the-wild scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative results</head><p>While previous self-supervised methods rely on weak supervision signal (e.g., consistency loss), our method trains the pose estimator with augmenting supervision from the self-generated data, resulting in more stable, plausible, and  Red skeleton is prediction, green skeleton is ground truth. accurate estimation 1 . As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, although Hu et al. <ref type="bibr" target="#b15">[16]</ref> implements temporal smoothness prior during the training process, jittering effect is still obvious. While our result, learned from co-evolving approach, is much smoother. Yu et al. <ref type="bibr" target="#b61">[61]</ref> introduce a scale estimation strategy for 2D pose to reduce the scale ambiguity. Through the weak supervision from the bone length consistency and scale distribution, his result still contains scale ambiguity (i.e., the body size varies) as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Ours result maintains stable and accurate in term of body size compared with it. We further demonstrate the result from 3DHP ( <ref type="figure" target="#fig_3">Fig. 5</ref>) and 3DPW <ref type="figure" target="#fig_4">(Fig. 6</ref>). These results demonstrate that our method perform well on unseen poses for in-the-wild scenarios. More in-the-wild examples can be viewed in the supplementary material in video format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study 4.4.1 Ablation on round of co-evolution</head><p>We then analysis how the co-evolving round improves the performance of each component (estimator P, imitator I, hallucinator H). To demonstrate the improvement, we select three evaluation metrics for each component. For estimator, we evaluate the trained model P on H36M test set and report the MPJPE as evaluation metric. For imitator, we evaluate the trained policy I on GT 3D reference motion (H36M) to measure the number of termination (e.g., fall down) as evaluation metric. For hallucinator, we evaluate the trained model H on GT 3D data (Walking scenario <ref type="bibr" target="#b11">[12]</ref> in H36M) for intermediate pose completion. We measure the MPJPE of pose and root position as evaluation metric.   We involve an oracle by training each model using the GT data directly as showed in the last row of <ref type="table">Table 4</ref>. Through iterative co-evolving, the performance of estimator P, imitator I, hallucinator H are improved and getting closer to the result which is trained with GT data. We further provide visualization result for imitator I <ref type="figure" target="#fig_5">(Fig. 7)</ref>, hallucinator H <ref type="figure" target="#fig_6">(Fig. 8</ref>). This result shows that the imitator I and hallucinator H co-evolved by our PoseTriplet without using 3D data achieve a comparable performance compared with the oracle trained with GT 3D data.  <ref type="table">Table 4</ref>. Results on co-evolving for estimator P, imitator I, hallucinator H. Note that round 0 is the Loop starting, and we involve hallucinator H after round one to ensure the quality of initial pose estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Ablation on amount of data usage</head><p>To study how the amount of data affects the performance, we construct an ablation experiment with limited 2D pose data. As shown in <ref type="table">Table 5</ref>, we gradually involve more data in our method, (i.e., S1, S1+S5, S1+S5+S6+S7+S8). Result shows that the performance of PoseTriplet can be improved gradually by adding more 2D pose data in both intra and cross-dataset scenarios.</p><p>Mode Sub H36M 3DHP 3DPW Self S1 89.2 94.0 135.8 Self S1,S5 81.9 83.5 128.6 Self S1,S5,S6,S7,S8 68.2 79.5 115.0 <ref type="table">Table 5</ref>. Results on ablation amount of data in terms of MPJPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we present a novel framework PoseTriplet for self-supervised 3D pose estimation, which is achieved by a co-evolution strategy of a pose estimator, imitator, and hallucinator. These three components, complement and strength one another through a dual-loop strategy as the training procedure. The framework enables generating diverse and plausible motion data, which help train superior pose estimator. Experiments on varies benchmarks demonstrate that PoseTriplet yields encouraging results. It outperforms the state of the art self-supervised approaches and even competes with fully-supervised approaches. Limitations The major limitation is that our pipeline suffers low training efficiency, e.g., it takes 7 days to train for 3 rounds on a machine with a Intel Xeon Gold 6278C CPU and a Tesla T4 GPU. The reason is that the imitator (I) is implemented with CPU-based reinforcement learning (RL) and the hallucinator (H) is instantiated with RNN architecture. In the future, we will explore GPU-based RL implementation and more efficient hallucinator architecture (e.g., transformer) to speed up the training process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Detail of our PoseTriplet framework. Given available 2D pose sequence x1:T , the pose estimator P transforms it to low-fidelity 3D pose sequenceX1:T .X1:T is then served as semantic guidance signal (i.e., reference motion) for imitator I to obtain physically plausible motionX1:T . The hallucinator H then generates novel and diverse motionX1:T fromX1:T , which is then refined by the imitator I to obtain the final enhanced diverse and plausible motionX1:T .X1:T is then projected to 2D-3D pairs to train the estimator. The improved estimator takes the available 2D pose sequence x1:T and start another round of dual-loop optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Result on UID<ref type="bibr" target="#b15">[16]</ref> comparison with Hu et al.<ref type="bibr" target="#b15">[16]</ref>. The figure includes: input (left), ours (middle), Hu et al.<ref type="bibr" target="#b15">[16]</ref> (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Result on H36M comparison with Yu et al. [61]. The figure includes: input (left), ours (middle), Yu et al. [61] (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Result on 3DHP compared with Ground Truth. The figure includes: input (left), ours (middle), ground truth (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Result on 3DPW compared with Ground Truth. The figure includes: input (left), ours (middle), ground truth (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Results on co-evolving for imitator I. The figure includes: video source (left), our co-evolving result (middle), oracle trained with ground truth data(right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Results on co-evolving for hallucinator H. The figure includes: ground truth (left), our co-evolving result (middle), oracle trained with ground truth data(right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Martinez et al. [33] 45.5 37.1 62.9 47.7 Full Pavllo et al. Results on H36M in terms of MPJPE (P1) and PA-MPJPE (P2).</figDesc><table><row><cell>[38]</cell><cell cols="4">37.2 27.2 46.8 36.5</cell></row><row><cell>Weak 3DInterpreter [56]</cell><cell>-</cell><cell>88.6</cell><cell>-</cell><cell>98.4</cell></row><row><cell>Weak AIGN [15]</cell><cell>-</cell><cell>79.0</cell><cell>-</cell><cell>97.4</cell></row><row><cell>Weak Drover et al. [7]</cell><cell>-</cell><cell>38.2</cell><cell>-</cell><cell>64.6</cell></row><row><cell>Weak Li et al. [30]</cell><cell>-</cell><cell>-</cell><cell cols="2">88.8 66.5</cell></row><row><cell>Weak Umar et al. [18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.9</cell></row><row><cell>Self Rhodin et al. [44]</cell><cell>-</cell><cell>-</cell><cell cols="2">131.7 98.2</cell></row><row><cell>Self Chen et al. [4]</cell><cell>-</cell><cell>51.0</cell><cell>-</cell><cell>68.0</cell></row><row><cell>Self Kundu et al. [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.4</cell></row><row><cell>Self Kundu et al. [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.8</cell></row><row><cell>Self Yu et al. [61]</cell><cell cols="4">85.3 42.0 92.4 52.3</cell></row><row><cell>Self Hu et al. [16]</cell><cell>-</cell><cell>-</cell><cell>82.1</cell><cell>-</cell></row><row><cell>Self Wandt et al. [51]  *</cell><cell>-</cell><cell>-</cell><cell cols="2">81.9 53.0</cell></row><row><cell>Self Ours</cell><cell cols="4">68.2 45.1 78.0 51.8</cell></row></table><note>* uses multi-view setting. Best results are shown in bold under self supervised setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>significantly in PCK (82.2 vs. 89.1) and MPJPE (103.8 vs. 79.5). The result indicates that the diverse and plausible motion generated by our PoseTriplet improves generalization. Exception is that Kundu et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on 3DHP in terms of PCK, AUC, and MPJPE. CE denotes cross-data evaluation.</figDesc><table><row><cell>).</cell></row></table><note>* uses extra unpaired 2D/3D dataset for training. Best results are shown in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results on 3DPW in terms of MPJPE and PA-MPJPE. CE denotes cross-data evaluation.</figDesc><table><row><cell>Mode</cell><cell>Method</cell><cell cols="3">CE MPJPE (?) P-MPJPE (?)</cell></row><row><cell cols="3">Full Wang et al. [54] ?</cell><cell>124.2</cell><cell>-</cell></row><row><cell cols="3">Full DSD-SATN [49] ?</cell><cell>-</cell><cell>69.5</cell></row><row><cell>Full</cell><cell>CRMH [19]</cell><cell>?</cell><cell>105.3</cell><cell>62.3</cell></row><row><cell>Full</cell><cell>BMP [66]</cell><cell>?</cell><cell>104.1</cell><cell>63.8</cell></row><row><cell>Full</cell><cell cols="2">VideoPose [38] ?</cell><cell>101.8</cell><cell>63.0</cell></row><row><cell>Self</cell><cell>Ours</cell><cell>?</cell><cell>115.0</cell><cell>69.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Fig.3-8are video figure that are best viewed in Adobe Reader (click and play), and videos are in supplementary materials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This project is supported by NUS Faculty Research Committee Grant (WBS: A-0009440-00-00) and NUS Advanced Research and Technology Innovation Centre (Project Reference ECT-RP2). Kehong would like to thank Ye Yuan for the discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPRw</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Glocalnet: Class-aware longterm human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Battan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudhik</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Sai Soorya Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharma</surname></persName>
		</author>
		<idno>WACV, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kjellstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Geometry-guided progressive nerf for generalizable and efficient neural human rendering. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Cong Phuoc Huynh. Can 3d pose be learned from 2d projections alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amit Agrawal</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>In ECCVw</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single-shot motion completion with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yenan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhehui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<idno>arXiv, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent transition networks for character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust motion in-betweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human motion prediction via spatio-temporal inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung Hsiao-Yu</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harley</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seto</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiadaki</forename><surname>Katerina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation for hierarchical dance video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human pose fitting towards in-thewild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolutional autoencoders for human motion infilling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Pece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remo</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Venkatesh Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kinematic-structure-preserved representation for unsupervised 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Mv Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Babu Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wee Sun Lee, and Gim Hee Lee. Convolutional sequence to sequence model for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On boosting single-frame 3d human pose estimation via monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial-aware texture transformer for high-fidelity garment transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Image Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Motion matching-the road to next-gen animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buttner</forename><surname>Michael</surname></persName>
		</author>
		<editor>Nucl. ai</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiview-consistent semi-supervised learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Quaternet: A quaternion-based recurrent model for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sfv: Reinforcement learning of physical skills from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning locomotion skills using deeprl: Does the choice of action space matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contact and human dynamics from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Neural monocular 3d human motion capture with physical awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Physcap: Physically plausible monocular 3d motion capture in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soshi</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Canonpose: Self-supervised monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrissa</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Not all parts are created equal: 3d pose estimation by modelling bi-directional dependencies of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Direct multi-view multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
		<editor>ECCVw</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning diverse stochastic human-action generators by learning smooth latent transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Physics-based human motion estimation and synthesis from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunrong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Shkurti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distilling knowledge from graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning dynamics via graph neural networks for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards alleviating the modeling ambiguity of unsupervised monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dlow: Diversifying latent flows for diverse human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Residual force control for agile human behavior imitation and extended motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Simpoe: Simulated character control for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Inference stage optimization for cross-scenario 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Body meshes as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">We are more than our joints: Predicting how 3d bodies move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
