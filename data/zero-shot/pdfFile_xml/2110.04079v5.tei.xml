<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A hybrid spatial-temporal deep learning architecture for lane detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqi</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Transport and Planning</orgName>
								<orgName type="department" key="dep2">Faculty of Civil Engineering and Geosciences</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Patil</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mechanical, Maritime and Materials Engineering</orgName>
								<orgName type="department" key="dep2">Department of Transport and Planning</orgName>
								<orgName type="department" key="dep3">Faculty of Civil Engineering and Geosciences</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<addrLine>The Netherlands Correspondence Haneen Farah</addrLine>
									<settlement>Delft</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Arem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Transport and Planning</orgName>
								<orgName type="department" key="dep2">Faculty of Civil Engineering and Geosciences</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haneen</forename><surname>Farah</surname></persName>
							<email>h.farah@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Transport and Planning</orgName>
								<orgName type="department" key="dep2">Faculty of Civil Engineering and Geosciences</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A hybrid spatial-temporal deep learning architecture for lane detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1111/mice.12829</idno>
					<note>R E S E A R C H A R T I C L E</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate and reliable lane detection is vital for the safe performance of lanekeeping assistance and lane departure warning systems. However, under certain challenging circumstances, it is difficult to get satisfactory performance in accurately detecting the lanes from one single image as mostly done in current literature. Since lane markings are continuous lines, the lanes that are difficult to be accurately detected in the current single image can potentially be better deduced if information from previous frames is incorporated. This study proposes a novel hybrid spatial-temporal (ST) sequence-to-one deep learning architecture. This architecture makes full use of the ST information in multiple continuous image frames to detect the lane markings in the very last frame. Specifically, the hybrid model integrates the following aspects: (a) the single image feature extraction module equipped with the spatial convolutional neural network; (b) the ST feature integration module constructed by ST recurrent neural network; (c) the encoder-decoder structure, which makes this image segmentation problem work in an end-to-end supervised learning format. Extensive experiments reveal that the proposed model architecture can effectively handle challenging driving scenes and outperforms available state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single image feature extraction module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-temporal feature integration module</head><p>ST <ref type="figure">-RNN(0,0)  ST -RNN(1,0)  ST -RNN(r,0)</ref> ST <ref type="figure">-RNN(0,1)  ST -RNN(1,1)  ST -RNN(r,1)</ref> ST <ref type="figure">-RNN(0,2)  ST -RNN(1,2)  ST -RNN(r,2)</ref> ST-RNN(0,n-1) ST-RNN(1,n-1) ST -RNN(r,n-1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST -RNN(0,n) ST -RNN(1,n) ST -RNN(r,n)</head><p>Abbreviations: ConvGRU, convolutional gated recurrent unit; ConvLSTM, convolutional long short-term memory; SCNN, spatial convolutional neural network; ST-RNN, spatial-temporal recurrent neural network; ReLU, Rectified Linear Unit.</p><p>* Similar to the SegNet-based network architecture, two types of ST-RNN, i.e., ConvLSTM and ConvGRU, are tested; ** ST-RNN blocks are tested with one hidden layer or two hidden layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The interest in developing automated driving functionalities, and in the end, fully automated vehicles, has been increasing vastly over the last decade. The safety of these automated functionalities is a crucial element and a priority for academic researchers, manufacturers, policymakers, and their potential future users. Automated driving requires a full understanding of the environment around the automated vehicle through its sensors. Vision-based methods have lately been boosted by advancements in computer vision and machine learning. Regarding envi-ronmental perception, camera-based lane detection is important, as it allows the vehicle to position itself within the lane. This is also the foundation of most lane-keeping assistance and lane departure warning systems <ref type="bibr" target="#b2">(Andrade et al., 2019;</ref><ref type="bibr" target="#b5">Bar Hillel et al., 2014;</ref><ref type="bibr" target="#b9">W. Chen et al., 2020;</ref><ref type="bibr" target="#b24">Liang et al., 2020;</ref><ref type="bibr" target="#b51">Xing et al., 2018)</ref>.</p><p>Traditional vision-based lane-detection methods rely on hand-crafted low-level features (e.g., color, gradient, and ridge features) and usually work in a four-step procedure, that is, image pre-processing, feature extraction, line detection and fitting, and post-processing <ref type="bibr" target="#b5">(Bar Hillel et al., 2014;</ref><ref type="bibr" target="#b16">Haris &amp; Glowacz, 2021)</ref>. Traditional computer vision techniques, for example, Inverse Perspective Mapping <ref type="bibr" target="#b0">(Aly, 2008;</ref><ref type="bibr" target="#b47">Wang et al., 2014)</ref>, Hough transform <ref type="bibr" target="#b6">(Berriel et al., 2017;</ref><ref type="bibr" target="#b19">Jiao et al., 2019;</ref><ref type="bibr" target="#b58">Zheng et al., 2018)</ref>, Gaussian filters <ref type="bibr" target="#b0">(Aly, 2008;</ref><ref type="bibr" target="#b42">Sivaraman and Trivedi, 2013;</ref><ref type="bibr" target="#b49">Wang et al., 2012)</ref>, and Random Sample Consensus (RANSAC) <ref type="bibr" target="#b0">(Aly, 2008;</ref><ref type="bibr" target="#b12">Choi et al., 2018;</ref><ref type="bibr" target="#b14">Du et al., 2018;</ref><ref type="bibr" target="#b15">Guo et al., 2015;</ref><ref type="bibr" target="#b32">Lu et al., 2019)</ref>, are usually adopted in the 4-step procedure. The problems of traditional methods are: (a) hand-crafted features are cumbersome to manage and not always useful, suitable, or powerful; and (b) the detection results are always based on one single image. Thus, the detection accuracies are relatively not high.</p><p>During the last decade, with the advancements in deep learning algorithms and computational power, many deep neural network based methods have been developed for lane detection with good performance. There are generally two dominant approaches <ref type="bibr" target="#b46">(Tabelini et al., 2020b)</ref>, i.e., (1) segmentation-based pipeline <ref type="bibr" target="#b20">(Kim and Park, 2017;</ref><ref type="bibr" target="#b21">Ko et al., 2020;</ref><ref type="bibr" target="#b35">Pan et al., 2018;</ref><ref type="bibr" target="#b57">Zhang et al., 2021;</ref><ref type="bibr" target="#b60">Zou et al., 2020)</ref>, in which predictions are made on the perpixel basis, classifying each pixel as either lane or not; (2) the pipeline using row-based prediction <ref type="bibr" target="#b38">Qin et al., 2020;</ref><ref type="bibr" target="#b55">Yoo et al., 2020)</ref>, in which the image is split into a (horizontal) grid and the model predicts the most probable location to contain a part of a lane marking in each row. Recently, <ref type="bibr" target="#b27">Liu et al. (2021)</ref> summarized two additional categories of deep learning based lane detection methods, i.e., the anchor-based approach <ref type="bibr" target="#b46">Tabelini et al., 2020b;</ref><ref type="bibr" target="#b52">Xu et al., 2020)</ref>, which focuses on optimizing the line shape by regressing the relative coordinates with the help of predefined anchors, and the parametric prediction based method which directly outputs parametric lines expressed by curve equation (R. <ref type="bibr" target="#b45">Tabelini et al., 2020a)</ref>. Apart from these dominant approaches, some other less common methods were proposed recently. For instance, <ref type="bibr" target="#b25">Lin et al. (2020)</ref> fused the adaptive anchor scheme (designed by formulating a bilinear interpolation algorithm) aided informative feature extraction and object detection into a single deep convolutional neural network for lane detection from a top-view perspective. <ref type="bibr" target="#b37">Philion (2019)</ref> developed a novel learning-based approach with a fully convolutional model to decode the lane structures directly rather than delegating structure inference to post-processing, plus an effective approach to adapt the model to new contexts by unsupervised transfer learning.</p><p>Similar to traditional vision-based lane-detection methods, most available deep learning models utilize only the current image frame to perform the detection. Until very recently, a few studies have explored the combination of convolutional neural network (CNN) and recurrent neural network (RNN) to detect lane markings or simulate autonomous driving using continuous driving scenes <ref type="bibr" target="#b57">Zhang et al., 2021;</ref><ref type="bibr" target="#b60">Zou et al., 2020)</ref>. However, the available methods do not take full advantage of the essential properties of the lane being long continuous solid or dashed line structures. Also, they do not yet make the utmost of the spatial-temporal information together with correlation and dependencies in the continuous driving frames. Thus, for certain extremely challenging driving scenes, their detection results are still unsatisfactory.</p><p>In this paper, lane detection is treated as a segmentation task, in which a novel hybrid spatial-temporal sequence-to-one deep learning architecture is developed for lane detection through a continuous sequence of images in an end-to-end approach. To cope with challenging driving situations, the hybrid model takes multiple continuous frames of an image sequence as inputs, and integrates the single image feature extraction module, the spatial-temporal feature integration module, together with the encoder-decoder structure to make full use of the spatial-temporal information in the image sequence. The single image feature extraction module utilizes modified common backbone networks with embedded spatial convolutional neural network (SCNN) <ref type="bibr" target="#b35">(Pan et al., 2018)</ref> layers to extract the features in every single image throughout the continuous driving scene. SCNN is powerful in extracting spatial features and relationships in one single image, especially for long continuous shape structures. Next, the extracted features are fed into spatial-temporal recurrent neural network (ST-RNN) layers to capture the spatial-temporal dependencies and correlations among the continuous frames. An encoder-decoder structure is adopted with the encoder consisting of SCNN and several fully-convolution layers to downsample the input image and abstract the features, while the decoder, constructed by CNNs, upsample the abstracted outputs of previous layers to the same size as the input image. With the labelled ground truth of the very last image in the continuous frames, the model training works in an end-to-end way as a supervised learning approach. To train and validate the proposed model on two large-scale open-sourced datasets, i.e., tvtLANE <ref type="bibr" target="#b60">(Zou et al., 2020)</ref> and TuSimple, a corresponding training strategy has been also developed. To summarize, the main contributions of this paper lie in:</p><p>? A hybrid spatial-temporal sequence-to-one deep neural network architecture integrating the advantages of the encoder-decoder structure, SCNN embedded single image feature extraction module, and ST-RNN module, is proposed; ? The proposed model architecture is the first attempt that tries to strengthen both spatial relation feature extraction in every single image frame and spatial-temporal correlation together with dependencies among continuous image frames for lane detection;</p><p>? The implementation utilized two widely used neural network backbones, i.e., UNet <ref type="bibr" target="#b39">(Ronneberger et al., 2015)</ref> and SegNet <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref> and included extensive evaluation experiments on commonly used datasets, demonstrating the effectiveness and strength of the proposed model architecture;</p><p>? The proposed model can tackle lane detection in challenging scenes such as curves, dirty roads, serious vehicle occlusions, etc., and outperforms all the available state-of-theart baseline models in most cases with a large margin. ? Under the proposed architecture, the light version model variant can achieve beyond state-of-the-art performance while using fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Images</head><formula xml:id="formula_0">? ? t 0 I ? ? t 1 I ? ? t 2 I ? ? t n-1 I ? ? t n I ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROPOSED METHOD</head><p>Although many sophisticated methods have been proposed for lane detection, most of the available methods use only one single image resulting in unsatisfactory performance under some extremely challenging scenarios, e.g., dazzle lighting, and serious occlusion. This study proposes a novel hybrid spatial-temporal sequence-to-one deep neural network architecture for lane detection. The architecture was inspired by: (a) the successful precedents of hybrid deep neural network architectures which fuse CNN and RNN to make use of by: (a) the successful precedents of hybrid deep neural network architectures which fuse CNN and RNN to make use of information in continuous multiple frames <ref type="bibr" target="#b57">(Zhang et al., 2021;</ref><ref type="bibr" target="#b60">Zou et al., 2020)</ref>; (b) the domain prior knowledge that traffic lanes are long continuous shape line structure with strong spatial relationship. The architecture integrates two modules utilizing two distinctive neural networks with complementary merits, i.e., SCNN and convolutional Long Short Term Memory (ConvLSTM) neural network, under an end-to-end encoder-decoder structure, to tackle lane detection in challenging driving scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the proposed model architecture</head><p>The proposed deep neural network architecture adopts a sequence-to-one end-to-end encoder-decoder structure as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Here "sequence-to-one" means that the model gets a sequence of multi images as input and outputs the detection result of the last image (please note that essentially the model is still utilizing sequence-to-sequence neural networks); "endto-end" means that the learning algorithm goes directly from the input to the desired output, which refers to the lane detection result in this paper, bypassing the intermediate states <ref type="bibr" target="#b22">(Levinson et al., 2011;</ref><ref type="bibr" target="#b34">Neven et al., 2017)</ref>; the encoderdecoder structure is a modular structure that consists of an encoder network and a decoder network, and is often employed in sequence-to-sequence tasks, such as language translation e.g., <ref type="bibr" target="#b44">(Sutskever et al., 2014)</ref>, and speech recognition e.g., <ref type="bibr" target="#b50">(Wu et al., 2017)</ref>. Here, the proposed model adopts encoder CNN with SCNN layers and decoder CNN using fully convolutional layers. The encoder takes a sequence of continuous image frames, i.e., time-series-images, as input and abstracts the feature map(s) in smaller sizes. To make use of the prior knowledge that traffic lanes are solid-or dashed-line structures with a continuous shape, one special kind of CNN, i.e., SCNN, is adopted after the first CNN hidden layer. With the help of SCNN, spatial features and relationships in every single image will be better extracted. Following this, the extracted feature maps of the continuous frames, constructed in a time-series manner, will be fed to ST-RNN blocks for sequential feature extraction and spatial-temporal information integration. Finally, the decoder network upsamples the abstracted feature maps obtained from the ST-RNN and decodes the content to the original input image size with the detection results. The proposed model architecture is implemented with two backbones, UNet <ref type="bibr" target="#b39">(Ronneberger et al., 2015)</ref> and SegNet <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>. Note, in the UNet based architecture, similar to <ref type="bibr" target="#b39">(Ronneberger et al., 2015)</ref>, the proposed model employs the skip connection between the encoder and decoder phase by concatenating operation to reuse features and retain information from previous encoder layers for more accurate predictions; while in the SegNet based networks, at the decoder stage, similar to <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>, the proposed model reuses the pooling indices to capture, store, and make use of the vital boundary information in the encoder feature maps. The detailed network implementation is elaborated in the remaining parts of Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network design</head><p>1) End-to-end encoder-decoder: Regarding lane detection as an image segmentation problem, the encoder-decoder structure based neural network can be implemented and trained in an end-to-end way. Inspired by the excellent performance of CNN-based encoder-decoder for image semanticsegmentation tasks in various domains <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b48">Wang et al., 2020;</ref><ref type="bibr" target="#b53">Yasrab et al., 2017)</ref>, this study also adopts the "symmetrical" encoder-decoder as the main backbone structure. Convolution and pooling operations are employed to extract and abstract the features in every image in the encoder stage; while in the decoder subset, the inverted convolution and upsampling operation are adopted to grasp the extracted high-order features and construct the outputs layer by layer with regards to the targets. By setting the output target size the same as the input image size, the whole network can work in an end-to-end approach. In the implementation, two widely used backbones, U-Net and Seg-Net, are adopted. To better extract and make use of the spatial relations in every image frame, the SCNN layer is introduced in the encoder part of the single image feature extraction module. Furthermore, to excavate and make use of the spatial-temporal correlations and dependencies among the input continuous image frames, ST-RNN blocks are embedded in the middle of the encoderdecoder networks.</p><p>2) SCNN: The Spatial Convolutional Neural Network (SCNN) was first proposed by <ref type="bibr" target="#b35">Pan et al. (2018)</ref>. The "spatial" here means that the specially designed CNN can propagate spatial information via slice-by-slice message passing. The detailed structure of SCNN is demonstrated in the bottom part of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>SCNN can propagate the spatial information in one image through four directions, as shown with the suffix "DOWN", "UP", "RIGHT", "LEFT" in <ref type="figure" target="#fig_0">Figure 1</ref>, which denotes downward, upward, rightward, and leftward, respectively. Take the "SCNN_DOWN" module for an example, considering that SCNN is adopted on a three dimensional tensor of size C ? W ? H, where in the lane detection task, C, W, and H denote the number of channels, image (or its feature map) width, and heights respectively. For SCNN_D, the input tensor would be split into H slices, and the first slice will then be sent into a convolution operation layer with C kernels of size C ? w, in which w is the kernel width. Different from the traditional CNN in which the output of one convolution layer is introduced into the next layer directly, in SCNN_D the output is added to the next adjacent slice to produce a new slice, and iteratively to the next convolution layer continuing until the last slice in the selected direction is updated. The convolution kernel weights are shared throughout all slices, and the same mechanism works for other directions of SCNNs.</p><p>With the above properties, SCNN has demonstrated its strengths in extracting spatial relationships in the image, which makes it suitable for detecting long continuous shape structures, e.g., traffic lanes, poles, and walls <ref type="bibr" target="#b35">(Pan et al., 2018)</ref>. However, using only one image to do the detection, SCNN still could not produce satisfying performance under extremely challenging conditions. And that is why a sequence-to-one architecture with continuous image frames as inputs and ST-RNN blocks to capture the spatial-temporal correlations in the continuous frames is proposed in this paper.</p><p>3) ST-RNN module: In this proposed framework, the multiple continuous frames of images are modelled as "imagetime-series" inputs. To capture the spatial-temporal dependencies and correlations among the image-time-series, the ST-RNN module is embedded in the middle of the encoderdecoder structure, which takes over the output extracted features of the encoder as its input and outputs the integrated spatial-temporal information to the decoder.</p><p>Various versions of RNNs have been proposed, e.g., Long Short Term Memory (LSTM) together with its multivariate version, i.e., fully connected LSTM (FC-LSTM), and Gated Recurrent Unit (GRU), to tackle time-series data in different application domains. In this paper, two state-of-the-art RNN networks, i.e., ConvLSTM <ref type="bibr" target="#b41">(Shi et al., 2015)</ref> and Convolutional Gated Recurrent Unit (ConvGRU) <ref type="bibr" target="#b4">(Ballas et al., 2016)</ref>, are employed. These models, considering their abilities in spatialtemporal feature extraction, generally outperform other traditional RNN models.</p><p>A general critical problem for the vanilla RNN model is the gradients vanishing <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b36">Pascanu et al., 2013;</ref><ref type="bibr" target="#b40">Ribeiro, 2020)</ref>. For this, LSTM introduces memory cells and gates to control the information flow to trap the gradient preventing it from vanishing during the back-propagation. In LSTM, the information of the new time-series inputs will be accumulated to the memory cell if the input gate is on. In contrast, if the information is not "important", the past cell status ?1 could be "forgotten" by activating the forget gate . Also, there is the output gate which decides whether the latest cell output will be propagated to the final state ? . The traditional FC-LSTM contains too much redundancy for spatial information, which makes it time-consuming and computational-expensive. To address this, the ConvLSTM <ref type="bibr" target="#b41">(Shi et al., 2015)</ref> is selected to build the ST-RNN block of the proposed framework. In ConvLSTM, the convolutional structures and operations are introduced in both the input-to-state and state-to-state transitions to do spatial information encoding, which also alleviates the problem of time-and computation-consuming.</p><p>The key formulation of the ConvLSTM is shown by equations (1)-(5), where ? denotes the Hadamard product, * denotes the convolution operation, (?) represents the sigmoid function, and tanh(?) represents the hyperbolic tangent function;</p><p>, , and ? are the input (i.e., the extracted features from the encoder in the proposed framework), memory cell status, and output at time ; , , and are the function values of the input gate, forget gate, and output gate, respectively; denotes the weight matrices, whose subscripts indicate the two corresponding variables are connected by this matrix. For instance, is the weight matrix between the input extracted features and the memory cell ; ? ?s are biases of the gates, e.g., is the input gate's bias. <ref type="bibr" target="#b4">(Ballas et al., 2016)</ref> further lightens the computational complexity by reducing a gate structure but could perform similarly or slightly better compared with the traditional RNNs or even ConvLSTM. The procedure of computing different gates and hidden states/outputs of ConvGRU is demonstrated with equations (6)-(9), in which the symbols have the same meaning as described before, while additional and mean the update gate and the reset gate, respectively, plus H represents the current candidate hidden representation.</p><formula xml:id="formula_1">= ( * + ? * ? ?1 + ? ?1 + ) (1) = ( * + ? * ? ?1 + ? ?1 + ) (2) = ? ?1 + ? tanh( * + ? * ? ?1 + )(3) = ( * + ? * ? ?1 + ? + ) (4) ? = ? tanh( ) (5) The ConvGRU</formula><formula xml:id="formula_2">= ( * + ? * ? ?1 + ) (6) = ( * + ? * ? ?1 + ) (7) H = tanh( * + ? * ( ? ? ?1 ) + ) (8) ? = H + (1-)? ?1<label>(9)</label></formula><p>In ConvGRU, there are only two gate structures, i.e., the update gate and the reset gate . It is the update gate that decides how to update the hidden representation when generating the ultimate result of ? at the current layer, as shown in equation <ref type="formula" target="#formula_2">(9)</ref>. While the reset gate is served to control to what extent the feature information captured in the previous hidden state is supposed to be forgotten through an element-wise multiplication operation when calculating current candidate hidden representation. From the equations, it is concluded that the information of ? mainly comes from H , while ? ?1 as the previous hidden-state representation also contributes to the process of computing the final representation of ? , thus the temporal dependencies are captured.</p><p>In practice, both ConvLSTM and ConvGRU with different numbers of hidden layers were employed to serve as the ST-RNN module in the proposed architecture, and the corresponding performances were evaluated, respectively. To be specific, in the proposed network, the input and the output sizes of the ST-RNN block are equivalent to the feature map size extracted through the encoder, which are 8 ? 16 and 4 ? 8 for the UNet based and SegNet based backbone, respectively. The convolutional kernel size in ConvLSTM and ConvGRU is 3 ? 3, and the dimension of each hidden layer is 512. The detailed implementations are described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Detailed implementation</head><p>1) Network Design Details: The proposed spatial-temporal sequence-to-one neural network was developed for the lane detection task with K (in this paper K=5 if not specified) continuous image frames as inputs. The image frames were firstly fed into the encoder for feature extraction and abstraction. Different from the normal CNN-based encoder, the SCNN layer was utilized to effectively extract the spatial relationships within every image. Different locations of the SCNN layer were tested, i.e., embedding the SCNN layer after the first hidden convolutional layer or at the very beginning. The outputs of the encoder network were modelled in a timeseries manner and fed into the ST-RNN blocks (i.e., ConvLSTM or ConvGRU layers) to further extract more useful and accurate features, especially the spatial-temporal dependencies and correlations among different image frames. In short, the encoder network is primarily responsible for spatial feature extraction and abstraction transforming input images into specified feature maps, while the ST-RNN blocks accept the extracted features from the continuous image frames in a time-series manner to capture the spatial-temporal dependencies.</p><p>The outputs of the ST-RNN blocks were then transferred into the decoder network that adopts deconvolution and upsampling operations to highlight and make full use of the features and rebuild the target to the original size of the input image. Note there is the skip concatenate connection (for UNet-based architecture) or pooling indices reusing (for SegNet-based architecture) between the encoder and decoder to reuse the retained features from previous encoder layers for more accurate predictions at the decoder phase. After the decoder phase, the lane detection result is obtained as an image in the equivalent size to the input image frame. With the labelled ground truth and the help of the encoder-decoder structure, the proposed model can be trained and implemented in an end-to-end way. The detailed input, output sizes, together with parameters of the layers in the entire neural network are listed in Appendix <ref type="table" target="#tab_1">Table A1 and Table A2</ref>.</p><p>For both SegNet-based and UNet-based implementations, two types of RNN layers, i.e., ConvLSTM and ConvGRU, were tested to serve as the ST-RNN block. Besides, the ST-RNN blocks were tested with 1 hidden layer and 2 hidden layers, respectively. So there are four variants of in the proposed SegNet-based models, i.e., SCNN_SegNet_ConvGRU1, SCNN_SegNet_ConvGRU2, SCNN_SegNet_ConvLSTM1, and SCNN_SegNet_ConvLSTM2. SCNN_SegNet_ConvGRU1 means the model is using SegNet as the backbone with SCNN layer embedded encoder, and 1 hidden layer of ConvGRU as the ST-RNN block. This naming rule applies to the other 3 variants. Also, there are four variants of the proposed UNetbased models, with a similar naming rule.</p><p>In the proposed models with U-Net as the backbone, the number of kernels used in the last convolutional block of the encoder part differs from the original U-Net's settings. Here, the number of output kernels (channels) of the last convolutional block in the proposed encoder does not double its input kernels, which applies to all the previous convolutional blocks. This is done, similar to <ref type="bibr" target="#b60">(Zou et al., 2020)</ref>, to better connect the output of the encoder with the ST-RNN block (ConvLSTM or ConvGRU layers). To do so, the parameters of the full-connection layer are designed to be quadrupled while the side lengths of the feature maps reduced to half, at the same time, the number of kernels remains unchanged. This strategy also somewhat contributes to reducing the parameter size of the whole network.</p><p>A modified light version of UNet (UNetLight) was also tested to serve as the network backbone to reduce the total parameter size, increase the model's ability to operate in realtime, and also further verify the proposed network architecture's effectiveness. The UNetLight has a similar network design to the demonstration in <ref type="table" target="#tab_4">Table A2</ref>. The only difference is that all the numbers of kernels in the ConvBlocks are reduced to half except for the Input in In_ConvBlock (with the input channel of 3 unchanged) and Output in Out_ConvBlock (with the output channel of 2 unchanged). To save space, the parameter settings of UNetLight based implementation will not be illustrated.</p><p>2) Loss function: Since the lane detection is modeled as a segmentation task and a pixel-wise binary classification problem, cross-entropy is a suitable candidate to serve as the loss function. However, because the pixels classified to be lanes are always quite less than those classified to be the background (meaning that it is an imbalanced binary classification and discriminative segmentation task), in the implementation, the loss was built upon the weighted crossentropy. The adopted loss function as the standard weighted binary cross-entropy function is given as in equation <ref type="formula" target="#formula_3">(10)</ref>,</p><formula xml:id="formula_3">= ? 1 ? [ * * (? ( )) + (1-) * (1 ? =1 ? ( ))]<label>(10)</label></formula><p>where is number of training examples, stands for the weight which is set according to the ratio between the total lane pixel quantities and none-lane pixel quantities throughout the whole training set, is the true target label for training example , is the input for training example , and ? stands for the model with neural network weights .</p><p>3) Training details: The proposed neural networks with different variants, together with the baseline models were trained on the Dutch high-performance supercomputer clusters, Cartesius and Lisa, using 4 Titan RTX GPUs with the data parallel mechanism in PyTorch. The input image size was set as 128 ? 256 to reduce the computational payload. The batch size was set to be as large as possible (e.g., 64 for UNetbased network architecture, 100 for SegNet based ones, and 136 for UNetLight based ones), and the learning rate was initially set to 0.03. The RAdam optimizer  was first used in this work for training the model at the beginning. At the later stage, when the training accuracy was beyond 95%, the optimizer was switched to the Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b7">(Bottou, 2010)</ref> optimizer with decay. With the labelled ground truth, the models were trained through iteratively updating the parameters in the weight matrixes and the losses on the basis of the deviation between outputs of the proposed neural network and the ground truth using the backpropagation mechanism. To speed up the training process, the pre-trained weights of SegNet and U-Net on ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009</ref>) were adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS AND RESULTS</head><p>Extensive experiments were carried out to inspect and verify the accuracy, effectiveness, and robustness of the proposed lane detection model using two large-scale opensourced datasets. The proposed models were evaluated on different driving scenes and were compared with several stateof-the-art baseline lane detection methods which also employ deep learning, e.g., U-Net <ref type="bibr" target="#b39">(Ronneberger et al., 2015)</ref>, Seg-Net <ref type="bibr" target="#b3">(Badrinarayanan et al., 2017)</ref>, SCNN <ref type="bibr" target="#b35">(Pan et al., 2018)</ref>, LaneNet <ref type="bibr" target="#b33">(Neven et al., 2018)</ref>, UNet_ConvLSTM <ref type="bibr" target="#b60">(Zou et al., 2020)</ref>, and SegNet_ConvLSTM <ref type="bibr" target="#b60">(Zou et al., 2020)</ref>. In each clip, there are 20 continuous frames saved in the same folder. In each clip, only the lane marking lines of the very last frame, i.e., the 20 th frame, are labelled with the ground truth officially. <ref type="bibr" target="#b60">Zou et al. (2020)</ref> additionally labelled every 13 th image in each clip and added their own collected lane dataset which includes 1,148 sequences of rural driving scenes collected in China. This immensely expanded the variety of the road and driving conditions since the original TuSimple dataset only covers the highway driving conditions. K continuous frames of each clip are used as the inputs with the ground truth of the labelled 13 th or 20 th frame to train the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>To further augment the training dataset, crop, flip, and rotation operations were employed, thus a total number of (3,626 + 1,148) ? 4 = 19,096 continuous sequences were produced, in which 38,192 images are labelled with ground truth. To adapt to different driving speeds, the input image sequences were sampled at 3 strides with a frame interval of 1, 2, or 3, respectively. Then, 3 sampling methods were employed to construct the training samples regarding the labelled 13 th and 20 th frames in each sequence, as demonstrated in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>2) tvtLANE testing set: Two different datasets were used for testing, i.e., Testset #1 (normal) and Testset #2 (challenging), which are also formatted with 5 continuous images as the input to detect the lane markings in the very last frame with the labelled ground truth. To be specific, Testset #1 is built upon the original TuSimple test set for normal driving scene testing; while Testset #2 is constructed with 12 challenging driving situations, especially used for robustness evaluation. The detailed descriptions of the trainset and testset in tvtLANE are illustrated in <ref type="table" target="#tab_1">Table 1</ref>, with examples shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Qualitative evaluation</head><p>Qualitative evaluation with the visualization of the lane detection results is the most intuitive approach to compare and evaluate the properties of different models, and it helps to find insights regarding their pros and cons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) tvtLANE Testset #1: normal situations</head><p>Samples of the lane detection results on tvtLANE testset #1 of the proposed models and other state-of-the-art models are demonstrated in <ref type="figure">Figure 3</ref>(1). All these results are without postprocessing.</p><p>In general, a good lane detection should include the following 5 properties:</p><p>? The number of lines need to be predicted correctly. A wrong detection or a misprediction might cause the automated vehicles to consider unsafe or unreachable areas as drivable areas resulting in potential accidents. As illustrated in the 1 st and 2 nd columns in <ref type="figure">Figure 3</ref>(1), the proposed models can identify the correct number of lane lines, while the baseline models, especially the ones using a single image, somewhat cannot detect the correct number of lines compared with ground truth.</p><p>? The positions of each lane marking line should be predicted precisely accords with the ground truth. As illustrated in <ref type="figure">Figure 3</ref>(1), the proposed models in row (j) with the model named by SCNN_SegNet_ConvLSTM2 and row (n) with the model named by SCNN_UNet_ConvLSTM2, could deliver better lane location predictions with thinner lines, compared with the baseline models. Superior to scattering  ? The predicted lane lines should not merge or be broken.</p><p>As illustrated in the 1 st , 2 nd , 6 th , 7 th , and 8 th columns of <ref type="figure">Figure  3</ref>(1), some baseline models' output lane lines either merge at the far end or break the continuity with dashed lines. The proposed models perform slightly better although in a few cases the lines are also discontinuous.</p><p>? The lanes should be predicted correctly even at the boundary of the image. As can be found in <ref type="figure">Figure 3</ref>(1), some baseline models, e.g., row (c), (d), and (e), run across difficulties at the top boundary of the image with merge lanes on the top. This also accords with the aforementioned property.</p><p>? The lane detection models should deliver accurate predictions under different driving scenes, even under some challenging situations. For example, in the 2 nd , 3 rd , 5 th , and 7 th columns of <ref type="figure">Figure 3</ref>(1), vehicles are occluding the lanes. A good lane detection model should be able to handle these. The proposed models perform well under these slightly challenging cases, more challenging situations are further discussed later.   2) tvtLANE Testset #2: 12 challenging driving cases <ref type="figure">Figure 3</ref>(2) shows the comparison of the proposed models with the baseline models under some extremely challenging driving scenes in the tvtLANE testset#2. All the results are not post-processed. These challenging scenes cover wide situations including serious vehicle occlusion, bad lighting conditions (e.g., shadow, dim), tunnel situations, and dirt road conditions. In some extremely challenging cases, the lanes are totally occluded by vehicles, other objects, and/or shadows, which could be very difficult even for humans to do the detection.</p><formula xml:id="formula_4">(b) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o) (p) (q)<label>(r)</label></formula><p>As can be observed in <ref type="figure">Figure 3</ref>(2), although all the baseline models fail in these challenging cases, the proposed models, especially the one named SCNN_SegNet_ConvLSTM2 illustrated in the row (k), could still deliver good predictions in almost every situation listed in <ref type="figure">Figure 3</ref>(2). The only flaw is that in the 3 rd column where vehicle occlusion and blur road conditions happen simultaneously, the proposed models also find it hard to predict precisely. With the results in the 4 th , 7 th , and 8 th columns, the robustness of SCNN_SegNet_ConvLSTM2's property in detecting the correct number of lane lines is further verified, especially, one can observe in the 4 th column, where almost all the other models are defeated, SCNN_SegNet_ConvLSTM2 can still predict the correct number of lanes.</p><p>Furthermore, it should be noticed that correct lane location predictions in these challenging situations are of vital importance for safe driving. For example, regarding the situation in the last column where a heavy vehicle totally shadows the field of vision on the left side, it will be very dangerous if the automated vehicle is driving according to the lane detection results demonstrated in the 3 rd to 5 th rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative evaluation</head><p>1) Evaluation metrics: This subsection examines the proposed models' properties regarding quantitative evaluations. When treated as a pixel-wise classification task, accuracy must be the most simple criterion for the performance evaluation of lane detection <ref type="bibr" target="#b62">(Zou et al., 2017)</ref>, which represents the overall classification performance in terms of correctly classified pixels, indicated in equation <ref type="formula" target="#formula_5">(11)</ref>. Accuracy =</p><p>However, since it is an imbalanced binary classification problem, where the lanes pixels are far less than the background pixels, using only accuracy to evaluate the model is not suitable. Thus, Precision, Recall, and F-measure, illustrated by equation <ref type="formula">(12)</ref> </p><p>In the above equation, true positive indicates the number of image pixels that are lane marking and are correctly identified; false positive means the number of image pixels that are background but are wrongly classified as lane markings; false negative stands for the number of image pixels which are lane marking but are wrongly classified as the background.</p><p>Specifically, this study chooses = 1, which corresponds to the F1-measure (harmonic mean) shown in equation <ref type="formula">(15)</ref>. The F1-Measure, which balances Precision and Recall, is always selected as the main benchmark for model evaluation, e.g., <ref type="bibr" target="#b35">Pan et al., 2018;</ref><ref type="bibr" target="#b52">Xu et al., 2020;</ref><ref type="bibr" target="#b57">Zhang et al., 2021;</ref><ref type="bibr" target="#b60">Zou et al., 2020)</ref>. Furthermore, the model parameter size, i.e., Params (M), together with the multiply-accumulate (MAC) operations, i.e., MACs (G), are provided as indicators of the model complexity. The two indicators are commonly used to estimate models' computational complexities and real-time capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Performance and comparisons on tvtLANE testset #1(normal situations)</head><p>As shown in <ref type="table" target="#tab_4">Table 2</ref>, the proposed model of SCNN_UNet_ConvLSTM2, performs the best when evaluating on tvtLANE Testset#1, with the highest Accuracy and F1-Measure, while the proposed model of SCNN_SegNet_ConvLSTM2 delivers the best Precision.   Incorporating the quantitative evaluation with the qualitative evaluation, it could be easily interpreted that the highest Precision, Accuracy, and F1-Measure are mainly derived from (i) the correct lane number, (ii) the accurate lane position, (iii) the sound continuity in the detected lanes, and (iv) the thinness of the predicted lanes with less blurriness, which accords with (ii). The correct prediction directly reduces the number of False Positives, and a good Precision contributes to better Accuracy and F1-Measure. Considering the structure of the proposed model architecture, a further explanation of the Models Models high F1-Measure, Accuracy, and Precision can be explained as follows:</p><p>Firstly, the SCNN layer embedded in the encoder equips the proposed model with better information extracting ability regarding the low-level features and spatial relations in each image.</p><p>Secondly, the ST-RNN blocks, i.e., ConvLSTM / ConvGRU layers, can effectively capture the temporal dependencies among the continuous image frames, which could be very helpful for challenging situations where the lanes are shadowed or covered by other objects in the current frame.</p><p>Finally, the proposed architecture could make the best of the spatial-temporal information among the processed K continuous frames by regulating the weights of the convolutional kernels within the SCNN and ConvLSTM / ConvGRU layers.</p><p>All in all, with the proposed architecture the proposed model tries to not only strengthen feature extraction regarding spatial relation in one image frame but also the spatialtemporal correlation and dependencies among image frames for lane detection.</p><p>Looking at the main metric, F1-Measure, it is demonstrated that increasing only Precision or only Recall will not improve the F1-Measure. Although the bassline models of U-Net, SegNet, and SegNet_ConvLSTM get better Recalls, they do not deliver good F1-Measure since their Precisions is much lower than the proposed model of SCNN_SegNet_ConvLSTM2 or SCNN_UNet_ConvLSTM2. Regarding the good Recall of U-Net and SegNet, it could be speculated from the qualitative evaluation, where one can find that U-Net and SegNet tend to produce thicker lane lines. With thicker lines and blurry areas, the two models can somehow reduce the False Negative, which will contribute to better Recall. This also demonstrates that Recall and Precision antagonize each other which further proves that F1-Measure should be a more reasonable evaluation measure compared with Precision and Recall.</p><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Performance and comparisons on tvtLANE testset #2 (challenging situations)</head><p>To further evaluate the proposed models' performance and verify the models' robustness, the models were evaluated on a brand-new dataset, i.e., the tvtLANE Testset #2. As introduced in 3.1 Datasets, tvtLANE Testset #2 includes 728 images in highway, urban, and rural driving scenes. These challenging driving scenes' data were obtained by data recorders at various locations, outside and inside the car front windshield under different road and weather conditions. Testset #2 is a challenging and comprehensive dataset for model evaluation, from which some cases would be difficult enough for humans to do the correct detection. <ref type="table">Table 3</ref> demonstrates the model performance comparison on the 12 types of challenging scenes in tvtLANE Testset #2. Following the results and discussions in 2) Performance and comparisons on tvtLANE testset #1(normal situations), here <ref type="table">Table 3</ref> provides the Precision and F1-Measure for the evaluation reference.</p><p>As indicated by the bold numbers, the proposed model, SCNN_SegNet_ConvLSTM2, results in the best F1-Measure at the overall level and in more situations, while the UNet_ConvLSTM results in the best Precision at the overall level and in more situations. Incorporating with the qualitative evaluation in <ref type="figure">Figure 3</ref>(2), it is shown that UNet_ConvLSTM tends to not classify pixels into lane lines for uncertain areas under some challenging situations (e.g., the 2 nd and 7 th columns in <ref type="figure" target="#fig_1">Figure 3(2)</ref>). This might be the reason for its obtaining better Precision. To further confirm this speculation, <ref type="figure" target="#fig_7">Figure 4</ref> compares the lane detection results of SCNN_SegNet_ConvLSTM2 and UNet_ConvLSTM under challenging situations 8-blur&amp;curve, and 10-shadow-dark, where UNet_ConvLSTM delivers very good Precisions.   <ref type="formula">(1)</ref> is for challenging situation 8-blur&amp;curve, while the down part (2) is for situation 10-shadow-dark. <ref type="figure" target="#fig_7">Figure 4</ref>, truly UNet_ConvLSTM tries not to classify pixels into lane lines under uncertain areas as much as possible. This leads to fewer False Negatives which helps for raising a better Precision. However, in real application scenarios, this is not wise and not acceptable. On the contrary, the proposed model SCNN_SegNet_ConvLSTM2 tries to make tough but valuable detections classifying candidate points into lane lines in the challenging uncertain areas with dirt, dark road conditions, and/or vehicle occlusions. This may lead to more False Negatives and a worse Precision but is praiseworthy. These analyses further demonstrate that F1-Measure is a better measure compared with Precision. Finally, it can be concluded that the proposed model, SCNN_SegNet_ConvLSTM2, delivers the best performance on the challenging tvtLANE Testset #2, which verified the proposed model architecture's robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As illustrated in</head><p>To sum up, the proposed model architecture demonstrates its effectiveness in both normal and challenging driving scenes, with the UNet based model, SCNN_UNet_ConvLSTM2, beats the baseline models with a large margin on normal situations, while the SegNet based model, SCNN_SegNet_ConvLSTM2 performs the best handling almost all the challenging driving scenes. The finding that, compared with UNet based models, SegNet based neural network models are more robust coping with challenging driving environments accords with results in <ref type="bibr" target="#b60">(Zou et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter analysis and ablation study</head><p>1) The added value of SCNN Regarding the neural network architecture, the effects of SCNN were investigated by evaluating performances of the model variants with and without SCNN layers. As demonstrated in <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_7">Figure 4</ref>, together with the quantitative results in <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table">Table 3</ref>, the proposed SegNet and UNet based models with SCNN embedded encoder, i.e., SCNN_SegNet_ConvLSTM, SCNN_SegNet_ConvGRU, SCNN_UNet_ConvLSTM, and SCNN_UNet_ConvGRU, outperform SegNet_ConvLSTM and UNet_ConvLSTM which are also SegNet or UNet based sequential model using multiple continuous image frames as inputs but without SCNN. Especially, SCNN_UNet_ConvLSTM2 obtains the best result in normal testing while SCNN_SegNet_ConvLSTM2 delivers the best performance in challenging situations.</p><p>For normal cases' testing on tvtLANE Testset#1, as shown in <ref type="table" target="#tab_4">Table 2</ref>, by adding SCNN layer in the encoder, almost all the proposed models with SCNN embedded encoder outperform the baseline models with better F1-Measure. To be specific, SCNN_SegNet_ConvLSTM2 improves the lane detection accuracy by around 0.3% and F1-measure by around 1%, and these improvements are from the already very good results obtained by SegNet_ConvLSTM. Similarly, SCNN_UNet_ConvLSTM2 overperforms UNet_ConvLSTM with even larger margins regarding both Accuracy, Precision, and F1-measure.</p><p>For challenging situations, adding the SCNN layer also helps the proposed model, SCNN_SegNet_ConvLSTM2, beat other baseline models, and deliver the best F1-Measure as indicated in <ref type="table">Table 3</ref>. <ref type="figure" target="#fig_8">Figure 5</ref> visualizes the extracted features at Down_ConvBlock_1 layer for UNet based models, with and without SCNN. Clearly, vast differences can be witnessed between the baseline model UNet_ConvLSTM and the proposed model SCNN_UNet_ConvLSTM2. In <ref type="figure" target="#fig_8">Figure 5 (b)</ref>, the CNN-based UNet layers identify the low-level features in the images regarding the target lane lines. However, the extracted features are not so clear, i.e., there are some interference signals, especially as visualized in the third image of row (b), which is supposed to affect the model training (i.e., updating weight parameters of the neural networks) and thus affect the model's performance regarding the marking detection results. It might further influence the final detection results. In contrast, with SCNN layers, the extracted features of the lanes are more inerratic, clear, and evident as shown in <ref type="figure" target="#fig_8">Figure 5</ref> (c). There are fewer interferences surrounding the detected lane features. This verifies SCNN's powerful strength in detecting the spatial relations in every single image with its message passing mechanism.</p><p>All the above results demonstrate the adding of the SCNN layer embedded in the encoder does contribute to the spatial feature extraction, with which the model could better make the utmost use of the spatial-temporal information among the continuous image frames.</p><p>2) Different locations of SCNN layer Results of testing different locations of the SCNN layer in the proposed model architecture are shown in <ref type="table" target="#tab_6">Table 4</ref>. The results reveal that: (a) Compared with baseline models without SCNN layers, the embedding of SCNN layers really help to improve the models' performance, which further verifies the added-value of SCNN and accords with the aforementioned results in 1); <ref type="bibr">(b)</ref> In terms of the main evaluation metric F1measure, embedding SCNN layer after the Conv1_1 (in SegNet based model) or In_Conv_1 (in UNet based model) layer delivers better results compared with embedding it at the very beginning or early layers of the encoder; (c) For UNet based model, embedding SCNN layer at the very beginning Models s delivers quite good Precision and Accuracy, but worse Recall, which means there are fewer False Positives but more False Negatives. This should be related to the properties of the UNet style neural network. These results further confirm the effectiveness of the proposed model architecture.</p><p>3) Type and number of ST-RNN layers As described in Section 3, in the proposed model architecture two types of RNNs, i.e., ConvLSTM and ConvGRU, are employed to serve in the ST-RNN block, to capture and make use of the spatial-temporal dependencies and correlations among the continuous image sequences. The number of hidden ConvLSTM and ConvGRU layers were also tested from 1 to 2. The quantitative results are demonstrated in <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table">Table 3</ref>, while some intuitive qualitative insights could be drawn from <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_7">Figure 4</ref>.</p><p>From <ref type="table" target="#tab_4">Table 2</ref>, it is illustrated that in general models adopting ConvLSTM layers in the ST-RNN block perform better than those adopting ConvGRU layers with improved F1measure, except for the UNetLight based. This could be explained by ConvLSTM's better properties in extracting spatial-temporal features and capturing time dependencies by more control gates and thus more parameters compared with ConvGRU. Furthermore, from <ref type="table" target="#tab_4">Table 2 and Table 3</ref>, it is observed that models with two hidden ST-RNN layers, for both ConvLSTM and ConvGRU, generally perform better than those with only one hidden ST-RNN layer. This could be speculated that with two hidden ST-RNN layers, one layer can serve for sequential feature extraction, and the other can achieve spatial-temporal feature integration. The improvements of two ST-RNN layers over one are not that significant which might be due to (a) models employing one ST-RNN layer already obtain good results; (b) since the length of the continuous image frames is only five, one ST-RNN layer might be already enough to do the spatial-temporal feature extraction, so when incorporating longer image sequences the superiorities of two ST-RNN layers could be promoted. However, longer image sequences require more computational resources and longer training time, which could not be afforded at the present stage in this study. This could be the future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Number of parameters and real-time capability</head><p>As shown in <ref type="table" target="#tab_4">Table 2</ref>, the two proposed candidate models, i.e., SCNN_SegNet_ConvLSTM2 and SCNN_UNet_ConvLSTM2, possess a bit more parameters compared with the baseline SegNet_ConvLSTM and UNet_ConvLSTM, respectively. However, almost all of the proposed model variants with different types and numbers of ST-RNN layers outperform the baselines, and some of them are even with low parameter sizes e.g., SCNN_SegNet_ConvGRU1, SCNN_SegNet_ConvLSTM1, SCNN_UNet_ConvGRU1, SCNN_UNet_ConvLSTM1. Generally speaking, lower numbers of model parameters mean better real-time capability.</p><p>In addition, four model variants were implemented with a modified light version of UNet, i.e., UNetLight, serving as the network backbone to reduce the total parameter size and improve the model's ability to operate in real-time. The UNetLight backbone has a similar network design with UNet whose parameter settings are demonstrated in <ref type="table" target="#tab_4">Table A2</ref>. The only difference is that all the numbers of kernels in the ConvBlocks are reduced to half except for the Input in In_ConvBlock (with the input channel of 3 unchanged) and Output in Out_ConvBlock (with the output channel of 2 unchanged). From the testing results in <ref type="table" target="#tab_4">Table 2</ref>, it is shown that the model named SCNN_UNetLight_ConvGRU2, with fewer parameters than all the baseline models, beat the baselines exhibiting better performance regarding both Accuracy and F1-Measure. To be specific, compared with the best baseline model, i.e., UNet_ConvLSTM, SCNN_UNetLight_ConvGRU2 only uses less than one-fifth of the parameter size but delivers better evaluation metrics in testing Accuracy, Precision, and F1-Measure.</p><p>Regarding UNetLight based models, models using ConvGRU layers in the ST-RNN block perform better than those adopting ConvLSTM. The reason could be that light version UNet cannot implement high-quality feature extraction which does not feed enough information for ConvLSTM, while ConvGRU, with fewer control gates, is more robust when low-level features are not that fully extracted.</p><p>All these results further verify the proposed network architecture's effectiveness and strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, a novel spatial-temporal sequence-to-one model framework with a hybrid neural network architecture is proposed for robust lane detection under various normal and challenging driving scenes. This architecture integrates single image feature extraction module with SCNN, spatial-temporal feature integration module with ST-RNN, together with the encoder-decoder structure. The proposed architecture achieved significantly better results in comparison to baseline models that use a single frame (e.g., U-Net, SegNet, and LaneNet), as well as the state-of-art models adopting "CNN+RNN" structures (e.g., UNet_ConvLSTM, SegNet_ConvLSTM), with the best testing Accuracy, Precision, F1-measure on the normal driving dataset (i.e., tvtLANE Testset #1) and the best F1-measure on 12 challenging driving scenarios dataset (tvtLANE Testset #2). The results demonstrate the effectiveness of strengthening spatial relation abstraction in every single image with SCNN layer, plus the employment of multiple continuous image sequences as inputs. The results also demonstrate the proposed model architecture's ability in making the best of the spatial-temporal information in continuous image frames. Extensive experimental results show the superiorities of the sequence-to-one "SCNN + ConvLSTM" over "SCNN + ConvGRU" and ordinary "CNN + ConvLSTM" regarding sequential spatial-temporal feature extracting and learning, together with target-information classification for robust lane detection. In addition, testing results of the model variants with the modified light version of UNet (i.e., UNetLight) as the backbone, demonstrate the proposed model architecture's potential regarding real-time capability.</p><p>To the best of the authors' knowledge, the proposed model is the first attempt that tries to strengthen both spatial relations regarding feature extraction in every image frame together with the spatial-temporal correlations and dependencies among image frames for lane detection, and the extensive evaluation experiments demonstrate the strength of this proposed architecture. Therefore, it is recommended in future research to incorporate both aspects to obtain better performance.</p><p>In this paper, the challenging cases do not include night driving, rainy or wet road conditions, neither do they include situations in which the input images are defective (e.g., partly masked or blurred). There are demands to build larger test sets with comprehensive challenging situations to further validate the model's robustness. Since a large amount of unlabeled driving scene data involving various challenging cases was collected within the research group, a future research direction might be to develop semi-supervised learning methods and employ domain adaption to label the collected data, and then open source them for boosting the research in the field of robust lane detection. Furthermore, to further enhance the lane detection model, customed loss function, pre-trained techniques adopted in image-inpainting task, e.g., masked autoencoders, plus sequential attention mechanism could be introduced and integrated into the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>See <ref type="table" target="#tab_1">Table A1 and Table A2</ref>. ST-RNN Layer1* 5 * ConvLSTMCell(input=(512?4?8), kernel=(3,3), stride=(1,1), padding=(1,1)) Or 5 * ConvGRUCell(input=(512?4?8), kernel=(3,3), stride=(1,1), padding=(1,1), dropout(0.5))</p><p>ST-RNN Layer2** 5 * ConvLSTMCell(input=(512?4?8), kernel=(3,3), stride=(1,1), padding=(1,1)) Or 5 * ConvGRUCell(input=(512?4?8), kernel=(3,3), stride=(1,1), padding=(1,1), dropout(0.5))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Up_ConvBlock_5</head><p>MaxUnpool1 <ref type="formula">512?4?8</ref>   ST-RNN Layer1* 5 * ConvLSTMCell(input=(512?8?16), kernel=(3,3), stride=(1,1), padding=(1,1)) Or 5 * ConvGRUCell(input=(512?8?16), kernel=(3,3), stride=(1,1), padding=(1,1), dropout(0.5))</p><p>ST-RNN Layer2** 5 * ConvLSTMCell(input=(512?8?16), kernel=(3,3), stride=(1,1), padding=(1,1)) Or 5 * ConvGRUCell(input=(512?8?16), kernel=(3,3), stride=(1,1), padding=(1,1), dropout(0.5))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Up_ConvBlock_4</head><p>UpsamplingBilinear2D_1 <ref type="formula">512?8?16</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>The architecture of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>Samples data in trainset and testset. (a) original TuSimple dataset (Highway), (b) Zou et al., (2020) added Rural Road situations, (c) Testset #1 Normal situations, and (d) Testset #2 Challenging situations. In each row, the first five images are the input image sequence the last image is the labelled ground truth. points around, thinner predicted lane lines indicate a more precise model prediction of the lane position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Visualization of the lane-detection results on tvtLANE Testset #1 (normal situations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Visualization of the lane-detection results on tvtLANE Testset #2 (challenging situations) FIGURE 3. Qualitative evaluation: visualization of the lane-detection results on (1) tvtLANE Testset #1 and (2) tvtLANE Testset #2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0.7873 0.8548 0.7654 0.8805 0.5319 0.4735 0.8064 0.8765 0.8431 0.7112 0.7388 0.7640 SCNN_UNet_ConvLSTM1 0.8602 0.7844 0.8119 0.7807 0.8871 0.4066 0.4652 0.7445 0.8321 0.8972 0.7507 0.7068 0.7531 SCNN_UNet_ConvLSTM2 0.8182 0.8362 0.8189 0.7359 0.8365 0.5872 0.5377 0.8046 0.8770 0.8722 0.7952 0.7817 0.7784 SCNN_UNetLight_ConvGRU1 0.8212 0.7454 0.7189 0.6996 0.8521 0.3499 0.3999 0.7851 0.7282 0.8686 0.6940 0.6289 0.7011 SCNN_UNetLight_ConvGRU2 0.8147 0.8349 0.7390 0.7004 0.8591 0.4039 0.3360 0.6811 0.8300 0.8533 0.8125 0.7996 00.8544 0.7688 0.6878 0.9069 0.4128 0.5317 0.7873 0.7575 0.8503 0.7865 0.7947 0.7609 SCNN_SegNet_ConvGRU1 0.8821 0.8626 0.7734 0.7185 0.9039 0.3027 0.5288 0.7229 0.7866 0.8658 0.7759 0.7763 0.7547 SCNN_SegNet_ConvGRU2 0.8710 0.8630 0.8094 0.6989 0.9005 0.3963 0.5497 0.7470 0.7637 0.8525 0.7798 0.7396 0.7591 SCNN_SegNet_ConvLSTM1 0.8768 0.8801 0.8185 0.7166 0.9083 0.3750 0.4516 0.7806 0.7320 0.8622 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIGURE 4 .</head><label>4</label><figDesc>Visual comparison of the lane-detection results on challenging driving situations for UNet_ConvLSTM and the proposed model SCNN_SegNet_ConvLSTM2. All the results are not postprocessed. (a) Input images. (b) Ground truth. (c) Detection results of UNet_ConvLSTM. (d) Detection results of UNet_ConvLSTM overlapping on the original images. (e) Detection results of SCNN_SegNet_ConvLSTM2the original images. The upper part</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIGURE 5 .</head><label>5</label><figDesc>Visualization of the extracted low-level features at Down_ConvBlock_1 for UNet based models. (a) Original image. (b) Results of UNet_ConvLSTM (without SCNN layers). (c) Results of the SCNN_UNet_ConvLSTM2 (with SCNN layers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1) tvtLANE training set:To verify the proposed model performance, the tvtLANE dataset<ref type="bibr" target="#b60">(Zou et al., 2020)</ref> based upon the TuSimple lane marking challenge dataset, was first utilized for training, validating, and testing. The original dataset of the TuSimple lane marking challenge includes 3,626 clips of training and 2,782 clips of testing which are collected under various weather conditions and during different periods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 .</head><label>1</label><figDesc></figDesc><table /><note>Trainset and testset in tvtLANE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 . Model performance comparison on tvtLANE testset #1 (normal situations)</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Test_Acc (%)</cell><cell>Precision Recall</cell><cell>F1-Measure</cell><cell>MACs (G)</cell><cell>Params (M)</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models using</cell><cell>U-Net</cell><cell>96.54</cell><cell>0.790 0.985</cell><cell>0.877</cell><cell>15.5</cell><cell>13.4</cell></row><row><cell>single</cell><cell>SegNet</cell><cell>96.93</cell><cell>0.796 0.962</cell><cell>0.871</cell><cell>50.2</cell><cell>29.4</cell></row><row><cell>image as input</cell><cell>SCNN*</cell><cell>96.79</cell><cell>0.654 0.808</cell><cell>0.722</cell><cell>77.7</cell><cell>19.2</cell></row><row><cell></cell><cell>LaneNet*</cell><cell>97.94</cell><cell>0.875 0.927</cell><cell>0.901</cell><cell>44.5</cell><cell>19.7</cell></row><row><cell></cell><cell>SegNet_ConvLSTM**</cell><cell>97.92</cell><cell>0.874 0.931</cell><cell cols="2">0.901 217.0</cell><cell>67.2</cell></row><row><cell></cell><cell>UNet_ConvLSTM**</cell><cell>98.00</cell><cell>0.857 0.958</cell><cell>0.904</cell><cell>69.0</cell><cell>51.1</cell></row><row><cell></cell><cell cols="4">Proposed Models (SegNet-Based)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SCNN_SegNet_ConvGRU1</cell><cell>98.00</cell><cell>0.878 0.935</cell><cell cols="2">0.905 219.2</cell><cell>43.7</cell></row><row><cell></cell><cell>SCNN_SegNet_ConvGRU2</cell><cell>98.05</cell><cell>0.888 0.918</cell><cell cols="2">0.903 221.5</cell><cell>57.9</cell></row><row><cell></cell><cell>SCNN_SegNet_ConvLSTM1</cell><cell>98.01</cell><cell>0.881 0.935</cell><cell cols="2">0.907 220.0</cell><cell>48.5</cell></row><row><cell>Models</cell><cell>SCNN_SegNet_ConvLSTM2</cell><cell>98.07</cell><cell>0.893 0.928</cell><cell cols="2">0.910 223.0</cell><cell>67.3</cell></row><row><cell>using</cell><cell cols="4">Proposed Models (UNet-Based)</cell><cell></cell><cell></cell></row><row><cell>continuous images</cell><cell>SCNN_UNet_ConvGRU1</cell><cell>98.13</cell><cell>0.878 0.957</cell><cell>0.916</cell><cell>77.9</cell><cell>27.7</cell></row><row><cell>sequence</cell><cell>SCNN_UNet_ConvGRU2</cell><cell>98.19</cell><cell>0.887 0.950</cell><cell>0.917</cell><cell>87.0</cell><cell>41.9</cell></row><row><cell>as inputs</cell><cell>SCNN_UNet_ConvLSTM1</cell><cell>98.18</cell><cell>0.886 0.948</cell><cell>0.916</cell><cell>81.0</cell><cell>32.4</cell></row><row><cell></cell><cell>SCNN_UNet_ConvLSTM2</cell><cell>98.19</cell><cell>0.889 0.950</cell><cell>0.918</cell><cell>93.0</cell><cell>51.3</cell></row><row><cell></cell><cell cols="5">Proposed Models (Light Version UNet-Based)</cell><cell></cell></row><row><cell></cell><cell>SCNN_UNetLight_ConvGRU1</cell><cell>97.83</cell><cell>0.850 0.960</cell><cell>0.902</cell><cell>19.6</cell><cell>6.9</cell></row><row><cell></cell><cell>SCNN_UNetLight_ConvGRU2</cell><cell>98.01</cell><cell>0.863 0.950</cell><cell>0.905</cell><cell>21.9</cell><cell>10.5</cell></row><row><cell></cell><cell>SCNN_UNetLight_ConvLSTM1</cell><cell>97.71</cell><cell>0.830 0.950</cell><cell>0.886</cell><cell>20.4</cell><cell>8.1</cell></row><row><cell></cell><cell>SCNN_UNetLight_ConvLSTM2</cell><cell>97.76</cell><cell>0.840 0.953</cell><cell>0.893</cell><cell>23.4</cell><cell>12.8</cell></row></table><note>* Results reported in (Zhang et al., 2021).** There are two hidden layers of ConvLSTM in SegNet_ConvLSTM and UNet_ConvLSTM.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 . Model performance comparison with different locations of SCNN layer on tvtLANE testset #1 and #2. Testing Datasets Testset #1 (Normal Situations) Testset #2 (Challenging Scenes)</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Location of SCNN</cell><cell>Test_ Acc (%)</cell><cell>Precisi on</cell><cell>Recall</cell><cell>F1-Measure</cell><cell>Test_ Acc (%)</cell><cell>Precisi on</cell><cell>Recall</cell><cell>F1-Measure</cell></row><row><cell>SegNet_Conv LSTM</cell><cell cols="9">Without 97.92 0.874 0.931 0.901 97.83 0.756 0.765 0.761</cell></row><row><cell>SCNN_SegNet_</cell><cell cols="9">Conv1_1 98.00 0.884 0.921 0.902 97.92 0.757 0.757 0.757</cell></row><row><cell>ConvLSTM2</cell><cell cols="9">Conv2_1 98.07 0.893 0.928 0.910 97.90 0.767 0.766 0.767</cell></row><row><cell>UNet_Conv LSTM</cell><cell cols="9">Without 98.00 0.857 0.957 0.904 97.93 0.778 0.660 0.714</cell></row><row><cell>SCNN_UNet_</cell><cell cols="9">In_Conv_1 98.28 0.896 0.939 0.917 98.08 0.776 0.593 0.672</cell></row><row><cell>ConvLSTM2</cell><cell cols="9">Conv1_1 98.19 0.889 0.950 0.918 97.95 0.778 0.640 0.702</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE A1 . Parameter settings for each layer of the SegNet-based neural network.</head><label>A1</label><figDesc></figDesc><table><row><cell>Layer</cell><cell></cell><cell>Input (channel?hight?width)</cell><cell>Output (channel?hight?width)</cell><cell>Kernel</cell><cell cols="2">Padding Stride</cell><cell>Activation</cell></row><row><cell></cell><cell>Conv_1_1</cell><cell>3?128?256</cell><cell>64?128?256</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell>Down_ConvBlock_1</cell><cell>Conv_1_2</cell><cell>64?128?256</cell><cell>64?128?256</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Maxpool1</cell><cell>64?128?256</cell><cell>64?64?128</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell></cell><cell>SCNN_Down</cell><cell>64?1?128</cell><cell>64?1?128</cell><cell>1?9</cell><cell>(0,4)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell>SCNN</cell><cell>SCNN_Up SCNN_Right</cell><cell>64?1?128 64?64?1</cell><cell>64?1?128 64?64?1</cell><cell>1?9 9?1</cell><cell>(0,4) (4,0)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>SCNN_Left</cell><cell>64?64?1</cell><cell>64?64?1</cell><cell>9?1</cell><cell>(4,0)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Conv_2_1</cell><cell>64?64?128</cell><cell>128?64?128</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell>Down_ConvBlock_2</cell><cell>Conv_2_2</cell><cell>128?64?128</cell><cell>128?64?128</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Maxpool2</cell><cell>128?64?128</cell><cell>128?32?64</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell></cell><cell>Conv_3_1</cell><cell>128?32?64</cell><cell>256?32?64</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell>Down_ConvBlock_3</cell><cell>Conv_3_2 Conv_3_3</cell><cell>256?32?64 256?32?64</cell><cell>256?32?64 256?32?64</cell><cell>3?3 3?3</cell><cell>(1,1) (1,1)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>Maxpool3</cell><cell>256?64?128</cell><cell>256?16?32</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell></cell><cell>Conv_4_1</cell><cell>256?16?32</cell><cell>512?16?32</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell>Down_ConvBlock_4</cell><cell>Conv_4_2 Conv_4_3</cell><cell>512?16?32 512?16?32</cell><cell>512?16?32 512?16?32</cell><cell>3?3 3?3</cell><cell>(1,1) (1,1)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>Maxpool4</cell><cell>512?16?32</cell><cell>512?8?16</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell></cell><cell>Conv_5_1</cell><cell>512?8?16</cell><cell>512?8?16</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell>Down_ConvBlock_5</cell><cell>Conv_5_2 Conv_5_3</cell><cell>512?8?16 512?8?16</cell><cell>512?8?16 512?8?16</cell><cell>3?3 3?3</cell><cell>(1,1) (1,1)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>Maxpool5</cell><cell>512?8?16</cell><cell>512?4?8</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>ConvGRU, convolutional gated recurrent unit; ConvLSTM, convolutional long short-term memory; SCNN, spatial convolutional neural network; ST-RNN, spatial-temporal recurrent neural network; ReLU, Rectified Linear Unit.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>512?8?16</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell></cell><cell>Up_Conv_5_1</cell><cell>512?8?16</cell><cell>512?8?16</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Up_Conv_5_2</cell><cell>512?8?16</cell><cell>512?8?16</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Up_Conv_5_3</cell><cell>512?8?16</cell><cell>512?8?16</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>MaxUnpool2</cell><cell>512?8?16</cell><cell>512?16?32</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Up_ConvBlock_4</cell><cell>Up_Conv_4_1 Up_Conv_4_2</cell><cell>512?16?32 512?16?32</cell><cell>512?16?32 512?16?32</cell><cell>3?3 3?3</cell><cell>(1,1) (1,1)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>Up_Conv_4_3</cell><cell>512?16?32</cell><cell>256?16?32</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>MaxUnpool3</cell><cell>256?16?32</cell><cell>256?32?64</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Up_ConvBlock_3</cell><cell>Up_Conv_3_1 Up_Conv_3_2</cell><cell>256?32?64 256?32?64</cell><cell>256?32?64 256?32?64</cell><cell>3?3 3?3</cell><cell>(1,1) (1,1)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>Up_Conv_3_3</cell><cell>256?32?64</cell><cell>128?32?64</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>MaxUnpool4</cell><cell>128?32?64</cell><cell>128?64?128</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Up_ConvBlock_2</cell><cell>Up_Conv_2_1</cell><cell>128?64?128</cell><cell>128?64?128</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Up_Conv_2_2</cell><cell>128?64?128</cell><cell>64?64?128</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>MaxUnpool5</cell><cell>64?64?128</cell><cell>64?128?256</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Up_ConvBlock_1</cell><cell>Up_Conv_1_1</cell><cell>64?128?256</cell><cell>64?128?256</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Up_Conv_1_2</cell><cell>64?128?256</cell><cell>2?128?256</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>LogSoftmax</cell></row><row><cell>Abbreviations:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Two types of ST-RNN, i.e., ConvLSTM and ConvGRU are tested;** ST-RNN blocks are tested with 1 hidden layer or 2 hidden layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE A2 . Parameter settings for each layer of the UNet-based neural network.</head><label>A2</label><figDesc></figDesc><table><row><cell>Layer</cell><cell></cell><cell>Input (channel?hight?width)</cell><cell>Output (channel?hight?width)</cell><cell>Kernel</cell><cell cols="2">Padding Stride</cell><cell>Activation</cell></row><row><cell>In_ConvBlock</cell><cell>In_Conv_1 In_Conv_2</cell><cell>3?128?256 64?128?256</cell><cell>64?128?256 64?128?256</cell><cell>3?3 3?3</cell><cell>(1,1) (1,1)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>SCNN_Down</cell><cell>64?1?256</cell><cell>64?1?256</cell><cell>1?9</cell><cell>(0,4)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell>SCNN</cell><cell>SCNN_Up SCNN_Right</cell><cell>64?1?256 64?128?1</cell><cell>64?1?256 64?128?1</cell><cell>1?9 9?1</cell><cell>(0,4) (4,0)</cell><cell>1 1</cell><cell>ReLU ReLU</cell></row><row><cell></cell><cell>SCNN_Left</cell><cell>64?128?1</cell><cell>64?128?1</cell><cell>9?1</cell><cell>(4,0)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Maxpool1</cell><cell>64?128?256</cell><cell>64?64?128</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Down_ConvBlock_1</cell><cell>Conv_1_1</cell><cell>64?64?128</cell><cell>128?64?128</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Conv_1_2</cell><cell>128?64?128</cell><cell>128?64?128</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Maxpool2</cell><cell>128?64?128</cell><cell>128?32?64</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Down_ConvBlock_2</cell><cell>Conv_2_1</cell><cell>128?32?64</cell><cell>256?32?64</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Conv_2_2</cell><cell>256?32?64</cell><cell>256?32?64</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Maxpool3</cell><cell>256?32?64</cell><cell>256?16?32</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Down_ConvBlock_3</cell><cell>Conv_3_1</cell><cell>256?16?32</cell><cell>512?16?32</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Conv_3_2</cell><cell>512?16?32</cell><cell>512?16?32</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Maxpool4</cell><cell>512?16?32</cell><cell>512?8?16</cell><cell>2?2</cell><cell>(0,0)</cell><cell>2</cell><cell>---</cell></row><row><cell>Down_ConvBlock_4</cell><cell>Conv_4_1</cell><cell>512?8?16</cell><cell>512?8?16</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row><row><cell></cell><cell>Conv_4_2</cell><cell>512?8?16</cell><cell>512?8?16</cell><cell>3?3</cell><cell>(1,1)</cell><cell>1</cell><cell>ReLU</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real time detection of lane markers in urban streets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Veh. Symp. Proc</title>
		<imprint>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/IVS.2008.4621152</idno>
		<ptr target="https://doi.org/10.1109/IVS.2008.4621152" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Novel Strategy for Road Lane Detection and Tracking Based on a Vehicle&apos;s Forward Monocular Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H Z</forename><surname>Neme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Omoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Farinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tusset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Okida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D S</forename><surname>Amaral</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2018.2856361</idno>
		<ptr target="https://doi.org/10.1109/TITS.2018.2856361" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1497" to="1507" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2644615</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2644615" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00138-011-0404-2</idno>
		<ptr target="https://doi.org/10.1007/s00138-011-0404-2" />
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ego-Lane Analysis System (ELAS): Dataset and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2017.07.005</idno>
		<ptr target="https://doi.org/10.1016/j.imavis.2017.07.005" />
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-7908-2604-3_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-7908-2604-3_16" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT 2010 -19th International Conference on Computational Statistics</title>
		<meeting>COMPSTAT 2010 -19th International Conference on Computational Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep learning algorithm for simulating autonomous driving considering prior knowledge and temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Labi</surname></persName>
		</author>
		<idno type="DOI">10.1111/mice.12495</idno>
		<ptr target="https://doi.org/10.1111/mice.12495" />
	</analytic>
	<monogr>
		<title level="j">Comput. Civ. Infrastruct. Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="305" to="321" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lane departure warning systems and lane line detection methods based on image processing and semantic segmentation-a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jtte.2020.10.002</idno>
		<ptr target="https://doi.org/10.1016/j.jtte.2020.10.002" />
	</analytic>
	<monogr>
		<title level="j">J. Traffic Transp. Eng</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PointLaneNet: Efficient end-to-end CNNs for accurate real-time lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Veh. Symp. Proc</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/IVS.2019.8813778</idno>
		<ptr target="https://doi.org/10.1109/IVS.2019.8813778" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lane Detection Using Labeling Based RANSAC Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer and Information Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="248" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/cvprw.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The fast lane detection of road using RANSAC algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67071-3_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67071-3_1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Systems and Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lane Detection Method Based on Improved RANSAC Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISADS.2015.24</idno>
		<ptr target="https://doi.org/10.1109/ISADS.2015.24" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -2015 IEEE 12th International Symposium on Autonomous Decentralized Systems, ISADS 2015</title>
		<meeting>-2015 IEEE 12th International Symposium on Autonomous Decentralized Systems, ISADS 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lane line detection based on object feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glowacz</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics10091102</idno>
		<ptr target="https://doi.org/10.3390/electronics10091102" />
	</analytic>
	<monogr>
		<title level="j">Electron</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1102</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long Short Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation. Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inter-Region Affinity Distillation for Road Marking Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01250</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01250" />
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="page" from="12483" to="125492" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time lane detection and tracking for autonomous vehicle applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1177/0954407019866989</idno>
		<ptr target="https://doi.org/10.1177/0954407019866989" />
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Mech. Eng. Part D J. Automob. Eng</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-To-End Ego Lane Estimation Based on Sequential Transfer Learning for Self-Driving Cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.158</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2017.158" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Key Points Estimation and Point Instance Segmentation Approach for Lane Detection 1-10</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<idno type="DOI">10.1109/tits.2021.3088488</idno>
		<ptr target="https://doi.org/10.1109/tits.2021.3088488" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards fully autonomous driving: Systems and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Askeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sokolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<idno type="DOI">10.1109/IVS.2011.5940562</idno>
		<ptr target="https://doi.org/10.1109/IVS.2011.5940562" />
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Line-CNN: End-to-End Traffic Line Detection with Line Proposal Unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2019.2890870</idno>
		<ptr target="https://doi.org/10.1109/TITS.2019.2890870" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lane Detection: A Survey with New Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11390-020-0476-4</idno>
		<ptr target="https://doi.org/10.1007/s11390-020-0476-4" />
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="493" to="505" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">Deep Learning-Based Lane Marking Detection using A2-LMDet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<idno type="DOI">10.1177/0361198120948508</idno>
		<ptr target="https://doi.org/10.1177/0361198120948508" />
	</analytic>
	<monogr>
		<title level="j">Transp. Res. Rec</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05003</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end Lane Shape Prediction with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/wacv48630.2021.00374</idno>
		<ptr target="https://doi.org/10.1109/wacv48630.2021.00374" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lane Detection in Lowlight Conditions Using an Efficient Data Enhancement : Light Conditions Style Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV47402.2020.9304613</idno>
		<ptr target="https://doi.org/10.1109/IV47402.2020.9304613" />
	</analytic>
	<monogr>
		<title level="m">IEEE Intell. Veh. Symp. Proc. 2020-May</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1394" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A lane detection method based on a ridge detector and regional G-RANSAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19184028</idno>
		<ptr target="https://doi.org/10.3390/s19184028" />
	</analytic>
	<monogr>
		<title level="j">Sensors (Switzerland</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards End-to-End Lane Detection: An Instance Segmentation Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/IVS.2018.8500547</idno>
		<ptr target="https://doi.org/10.1109/IVS.2018.8500547" />
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Veh. Symp. Proc</title>
		<imprint>
			<date type="published" when="2018-06-286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fast Scene Understanding for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial CNN for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence, AAAI 2018</title>
		<imprint>
			<publisher>AAAI press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7276" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">FastDraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01185</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.01185" />
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ultra Fast Structure-aware Deep Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="276" to="291" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIV 16</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="j">Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond exploding and vanishing gradients: analysing RNN training using attractors and smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tiels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sch?n</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2370" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Integrated lane and vehicle detection, localization, and tracking: A synergistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TITS.2013.2246835</idno>
		<ptr target="https://doi.org/10.1109/TITS.2013.2246835" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">PolyLaneNet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paix? O</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10924</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paix? O</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Olivera-Santos</surname></persName>
		</author>
		<idno>arXiv-2010</idno>
		<title level="m">Keep your Eyes on the Lane: Attentionguided Lane Detection. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust lane recognition for structured road based on monocular vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Beijing Inst. Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="345" to="351" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic Building Extraction from High-Resolution Aerial Imagery via Fully Convolutional Encoder-Decoder Network with Non-Local Block</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2964043</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.2964043" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A novel system for robust lane detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dahnoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sigpro.2011.07.019</idno>
		<ptr target="https://doi.org/10.1016/j.sigpro.2011.07.019" />
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning approach to simultaneous speech dereverberation and acoustic modeling for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H L</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTSP.2017.2756439</idno>
		<ptr target="https://doi.org/10.1109/JSTSP.2017.2756439" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Advances in Vision-Based Lane Detection: Algorithms, Integration, Assessment, and Perspectives on ACP-Based Parallel Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Velenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JAS.2018.7511063</idno>
		<ptr target="https://doi.org/10.1109/JAS.2018.7511063" />
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Autom. Sin</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="645" to="661" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12147</idno>
		<title level="m">CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">An encoder-decoder based Convolution Neural Network (CNN) for future Advanced Driver Assistance System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yasrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ADAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Appl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sci</surname></persName>
		</author>
		<idno type="DOI">10.3390/app7040312</idno>
		<ptr target="https://doi.org/10.3390/app7040312" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-to-end lane marker detection via row-wise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seok Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoon Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recognit. Work. 2020-June</title>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4335" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPRW50498.2020.00511</idno>
		<ptr target="https://doi.org/10.1109/CVPRW50498.2020.00511" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Lane Detection Model Based on Spatio-Temporal Network With Double Convolutional Gated Recurrent Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2021.3060258</idno>
		<ptr target="https://doi.org/10.1109/TITS.2021.3060258" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improved Lane Line Detection Algorithm Based on Hough Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="254" to="260" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<idno type="DOI">10.1134/S1054661818020049</idno>
		<ptr target="https://doi.org/10.1134/S1054661818020049" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust lane detection from continuous driving scenes using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TVT.2019.2949603</idno>
		<ptr target="https://doi.org/10.1109/TVT.2019.2949603" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust Gait Recognition by Integrating Inertial and RGBD Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2017.2682280</idno>
		<ptr target="https://doi.org/10.1109/TCYB.2017.2682280" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
