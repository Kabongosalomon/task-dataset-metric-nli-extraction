<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-to-text Generation with Entity Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
							<email>r.puduppully@sms.ed.ac.ukli.dong@ed.ac.ukmlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-to-text Generation with Entity Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. These models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. In this work we propose an entity-centric neural architecture for data-to-text generation. Our model creates entity-specific representations which are dynamically updated. Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. We present experiments on the ROTOWIRE benchmark and a (five times larger) new dataset on the baseball domain which we create. Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-to-text generation is the task of generating textual output from non-linguistic input <ref type="bibr" target="#b32">(Reiter and Dale, 1997;</ref><ref type="bibr" target="#b9">Gatt and Krahmer, 2018)</ref>. The input may take on several guises including tables of records, simulations of physical systems, spreadsheets, and so on. As an example, <ref type="figure" target="#fig_0">Figure 1</ref> shows (in a table format) the scoring summary of a major league baseball (MLB) game, a play-by-play summary with details of the most important events in the game recorded chronologically (i.e., in which play), and a human-written summary.</p><p>Modern approaches to data-to-text generation have shown great promise <ref type="bibr" target="#b21">(Lebret et al., 2016;</ref><ref type="bibr" target="#b26">Mei et al., 2016;</ref><ref type="bibr" target="#b29">Perez-Beltrachini and Lapata, 2018;</ref><ref type="bibr" target="#b31">Puduppully et al., 2019;</ref><ref type="bibr" target="#b41">Wiseman et al., 2017)</ref> thanks to the use of large-scale datasets and neural network models which are trained end-toend based on the very successful encoder-decoder architecture <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. In contrast to traditional methods which typically implement pipeline-style architectures <ref type="bibr" target="#b33">(Reiter and Dale, 2000)</ref> with modules devoted to individual generation components (e.g., content selection or lexical choice), neural models have no special-purpose mechanisms for ensuring how to best generate a text. They simply rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically.</p><p>In this paper we are interested in the generation of descriptive texts such as the game summary shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Descriptive texts are often characterized as "entity coherent" which means that their coherence is based on the way entities (also known as domain objects or concepts) are introduced and discussed in the discourse <ref type="bibr" target="#b15">(Karamanis et al., 2004)</ref>. Without knowing anything about baseball or how game summaries are typically written, a glance at the text in <ref type="figure" target="#fig_0">Figure 1</ref> reveals that it is about a few entities, namely players who had an important part in the game (e.g., Brad Keller, Hunter Dozier) and their respective teams (e.g., Orioles, Royals). The prominent role of entities in achieving discourse coherence has been long recognized within the linguistic and cognitive science literature <ref type="bibr" target="#b20">(Kuno, 1972;</ref><ref type="bibr" target="#b4">Chafe, 1976;</ref><ref type="bibr" target="#b12">Halliday and Hasan, 1976;</ref><ref type="bibr" target="#b16">Karttunen, 1976;</ref><ref type="bibr" target="#b6">Clark and Haviland, 1977;</ref><ref type="bibr" target="#b30">Prince, 1981)</ref>, with Centering Theory <ref type="bibr" target="#b10">(Grosz et al., 1995)</ref> being most prominent at formalizing how entities are linguistically realized and distributed in texts.</p><p>In this work we propose an entity-centric neural architecture for data-to-text generation. Instead of treating entities as ordinary tokens, we create entity-specific representations (i.e., for players and teams) which are dynamically updated as text is  being generated. Our model generates descriptive texts with a decoder augmented with a memory cell and a processor for each entity. At each time step in the decoder, the processor computes an updated representation of the entity as an interpolation between a candidate entity memory and its previous value. Processors are each a gated recurrent neural network and parameters among them are shared. The model generates text by hierarchically attending over memory cells and the records corresponding to them.</p><p>We report experiments on the benchmark RO-TOWIRE dataset <ref type="bibr" target="#b41">(Wiseman et al., 2017)</ref> which contains statistics of NBA basketball games paired with human-written summaries. In addition, we create a new dataset for MLB (see <ref type="figure" target="#fig_0">Figure 1</ref>). Compared to ROTOWIRE, MLB summaries are longer (approximately by 50%) and the input records are richer and more structured (with the addition of play-by-play). Moreover, the MLB dataset is five times larger in terms of data size (i.e., pairs of tables and game summaries). We compare our entity model against a range of recently proposed neural architectures including an encoder-decoder model with conditional copy <ref type="bibr" target="#b41">(Wiseman et al., 2017)</ref> and a variant thereof which generates texts while taking content plans into account <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref>. Our results show that modeling entities explicitly is beneficial and leads to output which is not only more coherent but also more concise and grammatical across both datasets.</p><p>Our contributions in this work are three-fold: a novel entity-aware model for data-to-text generation which is linguistically motivated, yet resource lean (no preprocessing is required, e.g., to extract document plans); a new dataset for data-to-text generation which we hope will encourage further work in this area; a comprehensive evaluation and comparison study which highlights the merits and shortcomings of various recently proposed datato-text generation models on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The sports domain has attracted considerable attention since the early days of generation systems <ref type="bibr" target="#b34">(Robin, 1994;</ref><ref type="bibr" target="#b36">Tanaka-Ishii et al., 1998)</ref>. Likewise, a variety of coherence theories have been developed over the years (e.g., <ref type="bibr" target="#b25">Mann and Thomson 1988;</ref><ref type="bibr" target="#b10">Grosz et al. 1995)</ref> and their principles have found application in many symbolic text generation systems (e.g., <ref type="bibr" target="#b35">Scott and de Souza 1990;</ref><ref type="bibr" target="#b17">Kibble and Power 2004)</ref>. Modeling entities and their communicative actions has also been shown to improve system output in interactive storytelling <ref type="bibr" target="#b3">(Cavazza et al., 2002;</ref><ref type="bibr" target="#b2">Cavazza and Charles, 2005)</ref> and dialogue generation <ref type="bibr" target="#b39">(Walker et al., 2011)</ref>.</p><p>More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and neural network models. <ref type="bibr" target="#b14">Ji et al. (2017)</ref> make use of dynamic entity representations for language modeling. And <ref type="bibr" target="#b5">Clark et al. (2018)</ref> extend this work by adding entity context as input to the decoder. Both approaches condition on a single entity at a time, while we dynamically represent and condition on multiple entities in parallel. <ref type="bibr" target="#b18">Kiddon et al. (2016)</ref> make use of fixed entity representations to improve the coverage and coherence of the output for recipe generation. <ref type="bibr" target="#b1">Bosselut et al. (2018)</ref> model actions and their effects on entities for the same task. However, in contrast to our work, they keep entity representations fixed during generation. <ref type="bibr" target="#b13">Henaff et al. (2017)</ref> make use of dynamic entity representations in machine reading. Entity representations are scored against a query vector to directly predict an output class or combined as a weighted sum followed by softmax over the vocabulary. We make use of a similar entity representation model, extend it with hierarchical attention and apply it to data-to text generation. The hierarchical attention mechanism was first introduced in <ref type="bibr" target="#b42">Yang et al. (2016)</ref> as a way of learning document-level representations. We apply attention over records and subsequently over entity memories.</p><p>Several models have been proposed in the last few years for data-to-text generation <ref type="bibr" target="#b26">(Mei et al. 2016;</ref><ref type="bibr" target="#b21">Lebret et al. 2016;</ref><ref type="bibr" target="#b41">Wiseman et al. 2017</ref>, inter alia) based on the very successful encoderdecoder architecture <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. Various attempts have also been made to improve these models, e.g., by adding content selection <ref type="bibr" target="#b29">(Perez-Beltrachini and Lapata, 2018)</ref> and content planning <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref> mechanisms. However, we are not aware of any prior work in this area which explicitly handles entities and their generation in discourse context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: Encoder-Decoder with Conditional Copy</head><p>The input to our model is a table of records (see <ref type="figure" target="#fig_0">Figure 1</ref>). Records in turn have features, represented as {r j,l } L l=1 where L is the number of features in each record. Examples of features are values (r j,1 ; e.g., 8.0, Baltimore) or entities (r j,2 ; e.g., Orioles, C. Mullins). The model output y is a document containing words y = y 1 ? ? ? y |y| where |y| is the document length. Following previous work <ref type="bibr" target="#b41">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b31">Puduppully et al., 2019)</ref>, we construct embeddings of features, and then obtain a vector representation r j of each record by using a multilayer perceptron:</p><formula xml:id="formula_0">r j = ReLU(W r [r j,1 ; r j,2 ; ...; r j,L ] + b r ) (1)</formula><p>where ReLU is the rectifier activation function, W r ? R n?nL , b r ? R n are parameters and [; ] indicates vector concatenation.</p><p>Let {e j } |r| j=1 denote the output of the encoder. We use an LSTM decoder to compute the probability of each target word, conditioned on previously generated words, and on e j . In the case of ROTOWIRE, we follow previous work <ref type="bibr" target="#b41">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b31">Puduppully et al., 2019)</ref> and consider e j = r j . The first hidden state of the decoder is initialized by the average of the record vectors,</p><formula xml:id="formula_1">avg({e j } |r| j=1 ).</formula><p>In the case of MLB, information encoded in play-by-play is sequential. Recall, that it documents the most important events in a game in chronological order. To account for this, we encode MLB records into {e j } |r| j=1 with a bidirectional LSTM. We impose an ordering on records in the box score (i.e., home team followed by away team) which is in turn followed by play-by-play where records are naturally ordered by time. The decoder is initialized with the concatenation of the hidden states of the final step of the encoder.</p><p>At time step t, the input to the decoder LSTM is the embedding of the previously predicted word y t?1 . Let d t denote the hidden state of the t-th LSTM unit. We compute attention scores ? t,j over the encoder output e j and obtain dynamic context vector q t as the weighted sum of the hidden states of the input:</p><formula xml:id="formula_2">? t,j ? exp(d t W a e j ) q t = j ? t,j e j d att t = tanh(W c [d t ; q t ])<label>(2)</label></formula><p>where W a ? R n?n , j ? t,j = 1, W c ? R n?2n , and d att t is the attention vector. The probability of output text y conditioned on the input table r is modeled as:</p><formula xml:id="formula_3">p gen (y t |y &lt;t , r)=softmax yt (W y d att t + b y ) (3) Gate 2,1 2,2 2, ,2 ,2,1 ,2,2 ,2, ,1 , u ,1 u ,2 u , ? 1 ? 2 ? ? ? ,1 ? ,1</formula><p>Entity Memory  <ref type="formula" target="#formula_7">(6)-(8)</ref> where ? is the set of trainable parameters. The gate represents the entity memory update (Equation <ref type="formula">(9)</ref>). Block B covers Equations <ref type="formula" target="#formula_11">(10)</ref> and <ref type="formula" target="#formula_12">(11)</ref>, and block C Equations <ref type="formula" target="#formula_2">(12)</ref> and <ref type="formula" target="#formula_14">(13).</ref> where W y ? R |Vy|?n , b y ? R |Vy| are parameters and |V y | is the output vocabulary size. We further augment the decoder with a copy mechanism i.e., the ability to copy values from the input; copy implies y t = r j,1 for some t and j (e.g., Royals, Orioles, 9, 2 in the summary in Figure 1 are copied from r). We use the conditional copy method proposed in <ref type="bibr" target="#b11">Gulcehre et al. (2016)</ref> where a binary variable is introduced as a switch gate to indicate whether y t is copied or not.</p><formula xml:id="formula_4">u ?1,1 ? ? , ? , Gate u ?1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entity Memory and Hierarchical Attention</head><p>We extend the basic model from Section 3 with entity memory and hierarchical attention. <ref type="figure" target="#fig_1">Figure 2</ref> provides a schematic overview of our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity Memory</head><p>In order to render the model entity-aware, we compute x k as an average of record representation for each unique entity k (i.e., one of r j,2 values):</p><formula xml:id="formula_5">x k = j (1[r j,2 = k]r j )/ j 1[r j,2 = k] (4) where 1[x] = 1 if x is true, and 0 otherwise.</formula><p>We initialize u t=?1,k , the memory representation of an entity at time t = ?1, as:</p><formula xml:id="formula_6">u t=?1,k = W i x k<label>(5)</label></formula><p>where u t=?1,k ? R p and W i ? R p?n .</p><p>To capture the fact that discourse in descriptive texts may shift from one entity to the next, e.g., some entities may be salient in the beginning of the game summary (see Brad Kelly in the text in <ref type="figure" target="#fig_0">Figure 1</ref>), others only towards the end (see Dozier in <ref type="figure" target="#fig_0">Figure 1</ref>), and a few throughout (e.g., references to teams), we update entity representations at each time step during decoding. We use gate ? t to indicate whether there should be an update in the entity representation:</p><formula xml:id="formula_7">? t = ?(W d d t + b d )<label>(6)</label></formula><p>where t &gt;= 0, ? is the sigmoid function, W d ? R p?p , and b d ? R p . We also compute ? t,k , the extent to which the entity representation should change, and? t,k , the memory of the candidate entity:</p><formula xml:id="formula_8">? t,k =? t ?(W e d t +b e +W f u t?1,k +b f ) (7) u t,k =W g d t<label>(8)</label></formula><p>where denotes element-wise multiplication, <ref type="figure" target="#fig_1">Figure 2</ref>).</p><formula xml:id="formula_9">W e , ? R p?n , W f ? R p?p , b e , b f ? R p , and ? t , ? t,k ? [0, 1] p (see block A in</formula><p>An element in gate ? t will have value approaching 1 if an update in any u t?1,k is required. The value of an element in gate ? t,k will approach 1 if the corresponding value of the element in u t?1,k changes. Equation <ref type="formula">(9)</ref> computes the update in entity memory as an interpolation over the gated representation of the previous value of the entity memory and the candidate entity memory:</p><formula xml:id="formula_10">u t,k = (1 ? ? t,k ) u t?1,k + ? t,k ? t,k (9)</formula><p>where u t,k represents entity k at time t.</p><p>Previous work <ref type="bibr" target="#b13">(Henaff et al., 2017;</ref><ref type="bibr" target="#b14">Ji et al., 2017;</ref><ref type="bibr" target="#b5">Clark et al., 2018</ref>) employs a normalization term over u t,k . We empirically found that normalization hurts performance and hence did not include it in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Attention</head><p>We hypothesize that our generator should first focus on entities (e.g., the main players and their teams) and then on the records corresponding to theses entities (e.g, player performance in the game). Our model implements this view of text generation via a hierarchical attention mechanism which we explain below. We also expect that focusing on entities first should improve the precision of the texts we generate as the entity distribution will constrain the probability distribution of records corresponding to each entity.</p><p>To better understand the hierarchical attention mechanism, we can view the encoder output e j as a 2-dimensional array g k,z where k ? [1, K] represents entities and z ? [1, Z] represents records of entities and there is a one-to-one correspondence between positions j and k, z. We compute attention over g k,z , the encoder output, as:</p><formula xml:id="formula_11">? t,k,z ? exp(d t W a g k,z )<label>(10)</label></formula><p>where W a ? R n?n , z ? t,k,z = 1 (see block B in <ref type="figure" target="#fig_1">Figure 2</ref>). We compute the entity context as:</p><formula xml:id="formula_12">s t,k = z ? t,k,z g k,z<label>(11)</label></formula><p>while attention over entity vectors u t,k is:</p><formula xml:id="formula_13">? t,k ? exp(d t W h u t,k )<label>(12)</label></formula><p>with W h ? R n?p , k ? t,k = 1. And the encoder context q t (see block C in <ref type="figure" target="#fig_1">Figure 2</ref>) is computed as follows:</p><formula xml:id="formula_14">q t = k ? t,k s t,k<label>(13)</label></formula><p>We feed q t into Equation <ref type="formula" target="#formula_2">(2)</ref> and compute p gen (y t |y &lt;t , r), the probability of generating output text y conditioned on records r, as shown in Equation <ref type="formula">(3)</ref>.  We experimented with feeding k ? t,k u t,k as input context along the lines of <ref type="bibr" target="#b5">Clark et al. (2018)</ref>; however, results on the development dataset degraded performance, and we did not pursue this approach further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training and Inference</head><p>Our training objective maximizes the log likelihood of output text given an input table of records:</p><formula xml:id="formula_15">max (r,y)?D log p (y|r)</formula><p>where D is the training set consisting of pairs of record tables and output game summaries. During inference, we make use of beam search to approximately obtain the best output? among candidate outputs y :? = arg max y p(y |r)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>Data We performed experiments on two datasets. The first one is ROTOWIRE <ref type="bibr" target="#b41">(Wiseman et al., 2017)</ref> which contains NBA basketball game statistics matched with human-written summaries. In addition, we created MLB, a new dataset which contains baseball statistics and corresponding human-authored summaries obtained from the ESPN website. 2 Basic statistics on the two datasets are given in <ref type="table" target="#tab_2">Table 1</ref>. As can be seen, MLB is approximately five times larger than ROTOWIRE, with richer vocabulary and longer summaries. For ROTOWIRE, we used the official training, development, and test splits of 3,398/727/728 instances. Analogously, for MLB we created a split of 22,821/1,739/1,744 instances. Game summaries in MLB were tokenized using nltk and hyphenated words were separated. Sentences containing quotes were removed as they included opinions and non-factual statements unrelated to the input tables. Sometimes MLB summaries contain a "Game notes" section with incidental information which was also removed.</p><p>For MLB, the value of L in Equation <ref type="formula">(1)</ref> is 6, and for ROTOWIRE it is 4. The first four features are similar in both datasets and include value (r j,1 ; e.g., 8.0, Baltimore), entity (r j,2 ; e.g., Orioles, C. Mullins), record type (r j,3 ; e.g., <ref type="bibr">RBI, R,H)</ref> and whether a player is on the home-or away-team (r j,4 ). MLB has two additional features which include the inning of play (r j,5 ; e.g., 9, 7, and -1 for records in the box score), and play index, a unique play identifier for a set of records in a play (r j,6 ; e.g., 0, 10, and -1 for records in the box score).</p><p>Information Extraction For automatic evaluation, we make use of the Information Extraction (IE) approach proposed in <ref type="bibr" target="#b41">Wiseman et al. (2017)</ref>. The idea is to use a fairly accurate IE tool to extract relations from gold summaries and model summaries and then quantify the extent to which the extracted relations align or diverge (see Section 7 for the specific metrics we use).</p><p>The IE system first identifies candidate entities (i.e., players, teams) and values (i.e., numbers), and given an "entity, value" pair it predicts the type of relation. For example, in ROTOWIRE, the relation for the pair "Kobe Bryant, 40" is PTS. Training data for the IE system is obtained automatically by matching entity-value pairs from summary sentences against record types. The IE system has an ensemble architecture which combines convolutional and bidirectional LSTM models.</p><p>We reused the updated IE models from Puduppully et al. <ref type="formula" target="#formula_2">(2019)</ref> for ROTOWIRE 3 and trained our own IE system for MLB. Box and line scores in MLB are identical in format to ROTOWIRE and pose no particular problems to the IE system. However, it is difficult to extract information from play-by-play and match it against the input tables. Consider the sentences Ryan O'Hearn homered or Keller gave up a home run from <ref type="figure" target="#fig_0">Figure 1</ref> where we can identify entities (Ryan O'Hearn, Keller) and record types (home-run-batter, home-run-pitcher) but no specific values. We created a dummy value of -1 for such cases and the IE system was trained to predict the record type of entity value pairs such as (Ryan O'Hearn, -1) or (Keller, -1). Moreover, 3 https://github.com/ratishsp/data2text-1/ the IE system does not capture attributes such as inning and team scores in play-by-play as it is difficult to deterministically match these against corresponding spans in text. The IE system thus would not be able to identify any records in the snippet tied 1-1 in the fourth. On MLB, the system achieved 83.4% precision and 66.7% recall (on held out data). We note that designing a highly accurate IE module for MLB is in itself a research challenge and outside the scope of this paper.</p><p>In order to compare our model against Puduppully et al. <ref type="formula" target="#formula_2">(2019)</ref>, we must have access to content plans which we extracted from ROTOWIRE and MLB by running the IE tool on gold summaries (training set). We expect the relatively low IE recall on MLB to disadvantage their model which relies on accurate content plans.</p><p>Training Configuration Model hyperparameters were tuned on the development set. We used the Adagrad optimizer <ref type="bibr" target="#b8">(Duchi et al., 2011)</ref> with an initial learning rate of 0.15, decayed by 0.97 for every epoch after the 4th epoch. We used truncated BPTT <ref type="bibr" target="#b40">(Williams and Peng, 1990)</ref> of length 100 and made use of input feeding <ref type="bibr" target="#b24">(Luong et al., 2015)</ref>. We summarize the hyperparameters of the ROTOWIRE and MLB models in the Appendix. All models were implemented on a fork of OpenNMT-py <ref type="bibr" target="#b19">(Klein et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Comparison</head><p>We compared our entity model against the following systems:</p><p>TEMPL is a template-based generator; we reused TEMPL from <ref type="bibr" target="#b41">Wiseman et al. (2017)</ref> for RO-TOWIRE and created a new system for MLB. The latter consists of an opening sentence about the two teams playing the game. It then describes statistics of pitchers (innings pitched, runs and hits given etc.) followed by a description of play-by-play (home run, single, double, triple etc.).</p><p>ED+CC is the encoder-decoder model with conditional copy from Section 3 and the best performing system in <ref type="bibr" target="#b41">Wiseman et al. (2017)</ref>.</p><p>NCP+CC is the best performing system in <ref type="bibr" target="#b31">Puduppully et al. (2019)</ref>; it generates content plans by making use of pointer networks <ref type="bibr" target="#b37">(Vinyals et al., 2015)</ref> to point to the input e j ; the resultant content plans are then encoded using a BiLSTM followed by an LSTM decoder with an attention and copy mechanism.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Automatic Evaluation We first discuss the results of automatic evaluation using the metrics defined in <ref type="bibr" target="#b41">Wiseman et al. (2017)</ref>. Let? be the gold output and y the model output. Relation Generation measures how factual y is compared to input r. Specifically, it measures the precision and number of relations extracted from y which are also found in r. Content Selection measures the precision and recall of relations between? and y. Content Ordering measures the Damerau-Levenshtein distance between relations in y and relations in?. In addition, we also report BLEU <ref type="bibr" target="#b28">(Papineni et al., 2002)</ref> with the gold summaries as reference. <ref type="table" target="#tab_4">Table 2</ref> (top) summarizes our results on the RO-TOWIRE test set (results on the development set are available in the Appendix). We report results for our dynamic entity memory model (ENT), the best system of <ref type="bibr" target="#b41">Wiseman et al. (2017</ref><ref type="bibr">) (WS-2017</ref> which is an encoder-decoder model with conditional copy, and NCP+CC <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref>. We see that ENT achieves scores comparable to NCP+CC, but performs better on the metrics of RG precision, CS precision, and CO. ENT achieves substantially higher scores in CS precision compared to WS-2017 and NCP+CC, without any planning component; CS recall is worse for ENT compared to NCP+CC mainly because the latter model is trained to first create a content plan with good coverage of what to say.  decoder model (with conditional copy) on MLB. We see that ENT achieves highest BLEU amongst all models and highest CS recall and RG count amongst neural models. The RG precision of ENT is lower than ED+CC. Inspection of model output revealed that on MLB, ED+CC tends to focus on one or two players getting most of the facts about them right, whereas ENT sometimes gets the coreference wrong, and thus lower RG precision. The TEMPL system scores highest on RG precision and count, and CS recall on both datasets. This is because TEMPL can make use of domain knowledge which is not available to the neural models. TEMPL performs poorly on MLB in terms of BLEU, in fact it is considerably worse compared to the similar template system on RO-TOWIRE (see <ref type="table" target="#tab_4">Table 2</ref>). This suggests that the task of creating MLB game summaries is hard, even for a template system which does not perform any sophisticated generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Experiments</head><p>We further examined how individual model components contribute to the quality of the generated summaries. To assess the impact of hierarchical attention (Section 4.2) over ED+CC, we report the performance of a stripped-down variant of our model without dynamic entity memory. Specifically, the entity memory was kept static and set to u t=?1,k (see Equation <ref type="formula" target="#formula_6">(5)</ref>). In this model, attention over entity vectors is:</p><formula xml:id="formula_16">? t,k ? exp(d t W h u t=?1,k )<label>(14)</label></formula><p>We next examined the contribution of dynamic memory, by adding it to this model without the gate ? t (i.e., we set ? t to one) and Equation <ref type="formula">(7)</ref> then becomes:</p><formula xml:id="formula_17">? t,k = ?(W e d t + b e + W f u t?1,k + b f ) (15)</formula><p>Finally, we obtain our final ENT model, by incorporating the update gate mechanism.</p><p>The results of the ablation study are shown in <ref type="table" target="#tab_6">Table 3</ref>. We compare ED+CC against variants "+Hier", "+Dyn" and "+Gate" corresponding to successively adding hierarchical attention, dynamic memory, and the update gate mechanism. On both datasets, hierarchical attention, improves relation generation, content selection, and BLEU. Dynamic memory and the update gate brings further improvements to content selection and BLEU.</p><p>Because it conditions on entities, ENT is able to produce text displaying nominal coreference which is absent from the outputs of ED+CC and WS-2017. We present an example in <ref type="table" target="#tab_7">Table 4</ref> (and in the Appendix) where entities Dwight Howard and James Harden are introduced and then later referred to as Howard and Harden. We also see that while generating the last sentence about the next game, ENT is able to switch the focus of attention from one team (Rockets) to the other (Nuggets), while NCP+CC verbalises Nuggets twice.</p><p>Human-Based Evaluation Following earlier work <ref type="bibr" target="#b41">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b31">Puduppully et al., 2019)</ref>, we also evaluated our model by asking humans to rate its output in terms of relation generation, coherence, grammaticality, and conciseness. Our studies were conducted on the Amazon Mechanical Turk platform. For ROTOWIRE, we compared ENT against NCP+CC, Gold, and TEMPL. We did not compare against WS-2017 or ED+CC, since prior work <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref> has shown that NCP+CC is superior to these models in terms of automatic and human-based evaluation. For MLB, we compared ENT against NCP+CC, ED+CC, Gold, and TEMPL.</p><p>In the first study, participants were presented with sentences randomly selected from the game summary (test set) together with corresponding box and line score tables and were asked to count supporting and contradicting facts in these sentences. We evaluated 30 summaries and 4 sentences per summary for each of ROTOWIRE and MLB. We elicited 5 responses per summary.</p><p>As shown in <ref type="table" target="#tab_9">Table 5</ref>, on ROTOWIRE ENT yields a comparable number of supporting and contradicting facts to NCP+CC (the difference is The Houston Rockets (18-5) defeated the Denver Nuggets (10-13) 108-96 on Tuesday at the Toyota Center in Houston. The Rockets had a strong first half where they outscored . . . The Rockets were led by Donatas Motiejunas, who scored a game-high of 25 points . . . James Harden also played a factor in the win, as he went 7-for . . . Coming off the bench, Donatas Motiejunas had a big game and finished with 25 points . . . The only other player to reach double figures in points was Arron Afflalo, who came off the bench for 12 points . . . Coming off the bench, Arron Afflalo chipped in with 12 points . . . The Nuggets' next game will be on the road against the Boston Celtics on Friday, while the Nuggets will travel to Boston to play the Celtics on Wednesday. The Houston Rockets (18-5) defeated the Denver Nuggets (10-13) 108-96 on Monday at the Toyota Center in Houston. The Rockets were the superior shooters in this game, going . . . The Rockets were led by the duo of Dwight Howard and James Harden. Howard shot 9-for-11 from the field and . . . Harden on the other hand recorded 24 points (7-20 FG, 2-5 3Pt, 8-9 FT), 10 rebounds and 10 assists, The only other Nugget to reach double figures in points was Arron Afflalo, who finished with 12 points (4-17 FG,. . . The Rockets' next game will be on the road against the New Orleans Pelicans on Wednesday, while the Nuggets will travel to Los Angeles to play the Clippers on Friday. not statistically significant). TEMPL has the highest number of supporting facts, even relative to gold summaries, and very few contradicting facts. This is expected as TEMPL output is mostly factual, it essentially parrots statistics from the tables. On MLB, ENT yields a number of supporting facts comparable to Gold and NCP+CC, but significantly lower than ED+CC and TEMPL. Contradicting facts are significantly lower for ENT compared to NCP+CC, but comparable to ED+CC and higher than TEMPL and Gold.</p><p>We also evaluated the quality of the generated summaries. Following earlier work <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref>, we presented participants with two summaries at a time and asked them to choose which one is better in terms of Grammaticality (is the summary written in well-formed English?), Coherence (do the sentences in summary follow a coherent discourse?), and Conciseness (does the summary tend to repeat the same content?) We divided the four competing systems (Gold, TEMPL, NCP+CC, and ENT) into six pairs of summaries for ROTOWIRE and the five competing systems (Gold, TEMPL, ED+CC, NCP+CC, and ENT) into ten pairs for MLB. We used Best-Worst scaling <ref type="bibr" target="#b23">(Louviere and Woodworth, 1991</ref>   <ref type="bibr">, 2015)</ref>, a more reliable alternative to rating scales. The score of a system is computed as the number of times it was rated best minus the number of times is rated worst <ref type="bibr" target="#b27">(Orme, 2009</ref>). Scores range from ?100 (worst) to 100 ( best). We elicited judgments for 30 test summaries for RO-TOWIRE and MLB; each summary was rated by 3 participants. As shown in <ref type="table" target="#tab_9">Table 5</ref>, on ROTOWIRE Gold receives highest scores in terms of Grammaticality, which is not unexpected. ENT comes close, achieving better scores than NCP+CC and TEMPL, even though our model only enhances the coherence of the output. Participants find ENT on par with Gold on Coherence and better than NCP+CC and TEMPL whose output is stilted and exhibits no variability. In terms of Conciseness, TEMPL is rated best, which is expected since it does not contain any duplication, the presented facts are mutually exclusive; ENT is comparable to NCP+CC and better than Gold.</p><p>As far as MLB is concerned, ENT achieves highest scores on Grammaticality and Coherence. It is rated high on Conciseness also, second only to TEMPL whose scores are lowest on Grammaticality and Coherence. Perhaps surprisingly, Gold is rated lower than ENT on all three metrics; we hypothesize that participants find Gold's output too verbose compared to the other systems. Recall that MLB gold summaries are relative long, the average length is 542 tokens compared to ROTOWIRE whose summaries are almost half as long (see <ref type="table" target="#tab_2">Table 1</ref>). The average length of output summaries for ENT is 327 tokens.</p><p>Taken together, our results show that ENT performs better than comparison systems on both RO-TOWIRE and MLB. Compared to NCP+CC, it is conceptually simpler and more portable, as it does not rely on content plans which have to be extracted via an IE system which must be reconfigured for new datasets and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this work we presented a neural model for datato-text generation which creates entity-specific representations (that are dynamically updated) and generates text using hierarchical attention over the input table and entity memory. Extensive automatic and human evaluation on two benchmarks, ROTOWIRE and the newly created MLB, show that our model outperforms competitive baselines and manages to generate plausible output which humans find coherent, concise, and factually correct. However, we have only scratched the surface; future improvements involve integrating content planning with entity modeling, placing more emphasis on play-by-play, and exploiting dependencies across input tables.  <ref type="bibr">-2017)</ref> which is an encoder-decoder model with conditional copy, the template generator (TEMPL), our implementation of encoder-decoder model with conditional copy (ED+CC), and NCP+CC <ref type="bibr" target="#b31">(Puduppully et al., 2019)</ref>. We see that ENT achieves scores comparable to NCP+CC, but performs better on the metrics of RG precision, CS precision, and CO.   (8-13 FG, 3-4 3PT, 4-5 FT) to go with 9 rebounds. Tobias Harris scored 21 points (10-20 FG, 1-3 3PT, 0-0 FT) to go with 10 rebounds. Andre Drummond scored 19 points (7-11 FG, 0-0 3PT, 5-9 FT) to go with 17 rebounds. Kent Bazemore scored 17 points (7-9 FG, 3-5 3PT, 0-0 FT) to go with 4 rebounds. Aron Baynes scored 15 points (5-6 FG, 0-0 3PT, 5-6 FT) to go with 7 rebounds. Al Horford scored 13 points (6-15 FG, 1-5 3PT, 0-0 FT) to go with 5 rebounds. The Atlanta Hawks' next game will be at home against the Dallas Mavericks, while the Detroit Pistons will travel to play the Bulls. NCP+CC The Atlanta Hawks (44-30) defeated the Detroit Pistons (39-35) 112-95 on Wednesday at the Palace of Auburn Hills. The Hawks came into this game riding a three-game losing streak and it was clear they did n't have it all on the floor. Paul Millsap led the way for the Hawks with 23 points (8-13 FG, 3-4 3Pt, 4-5 FT), along with nine rebounds, five assists and four steals, in 33 minutes. Al Horford chipped in 13 points (6-15 FG, 1-5 3Pt), and Kent Bazemore chipped in 17 points on 7-9 shooting, including 3-5 from deep. Kyle Korver chipped in 12 points (5-10 FG, 1-5 3Pt, 1-1 FT) and three rebounds in 29 minutes, while Tobias Harris chipped in 21 points (10-20 FG, 1-3 3Pt), 10 rebounds and five assists. Tobias Drummond had a double-double of his own with 19 points (7-11 FG, 5-9 FT) and 17 rebounds, along with two blocked shots, in 33 minutes. Andre Drummond had a double-double of his own, with 19 points (7-11 FG, 5-9 FT) and 17 rebounds, along with two blocked shots. The only other player to score in double digits for Detroit was Andre Drummond, who finished with 19 points (7-11 FG, 5-9 FT) and 17 rebounds, along with two blocked shots. The Pistons' next game will be on the road against the Cleveland Cavaliers on Friday, while the Pistons will travel to Minnesota to play the Timberwolves on Wednesday.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on the Development Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ENT</head><p>The Atlanta Hawks (44-30) defeated the Detroit Pistons (39-35) 112-95 on Monday at the Palace of Auburn Hills. The Hawks got off to a quick start in this one, out-scoring the Pistons 27-15 in the first quarter alone. The Hawks were the superior shooters in this game, going 45 percent from the field and 38 percent from the three-point line, while the Pistons went 39 percent from the floor and just 24 percent from beyond the arc. The Hawks were led by the duo of Paul Millsap and Andre Drummond. Millsap finished with 23 points (8-13 FG, 3-4 3Pt, 4-5 FT), nine rebounds and four blocked shots, while Drummond had 19 points (7-11 FG, 5-9 FT), 17 rebounds and two blocked shots. It was his second double-double in a row, as he's combined for 45 points and 19 rebounds over his last two games. He's now averaging 15 points and 7 rebounds on the season. Jeff Teague was the other starter to reach double figures in points, as he finished with 12 points (3-13 FG, 2-3 3Pt, 4-4 FT) and 12 assists. The Hawks' next game will be at home against the Cleveland Cavaliers on Friday, while the Pistons will travel to Los Angeles to play the Clippers on Friday. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>MLB statistics tables and game summary. The tables summarize the performance of the two teams and of individual team members who played as batters and pitchers as well as the most important events (and their actors) in each play. Recurring entities in the summary are boldfaced and colorcoded, singletons are shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Diagram of entity memory network (block A) and hierarchical attention (blocks B and C). Module f ? represents update equations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TEAM Inn1 Inn2 Inn3 Inn4 . . . R H E . . . . . . 2 4 0 . . . . . . 9 14 1 . . . BATTER H/V AB R H RBI TEAM . . . Royals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inn1: innings, R: runs, H: hits, E: errors, AB: at-bats, RBI: runs-batted-in, H/V: home or visiting, W: wins, L: losses, IP: innings pitched, ER: earned runs, BB: walks, K: strike outs.KANSAS CITY, Mo. -Brad Keller kept up his recent pitching surge with another strong outing. Keller gave up a home run to the first batter of the game -Cedric Mullins -but quickly settled in to pitch eight strong innings in the Kansas City Royals' 9-2 win over the Baltimore Orioles in a matchup of the teams with the worst records in the majors. Keller (7-5) gave up two runs and four hits with two walks and four strikeouts to improve to 3-0 with a 2.16 ERA in his last four starts. Ryan O'Hearn homered among his three hits and drove in four runs, Whit Merrifield scored three runs, and Hunter Dozier and Cam Gallagher also went deep to help the Royals win for the fifth time in six games on their current homestand. With the scored tied 1-1 in the fourth, Andrew Cashner (4-13) gave up a sacrifice fly to Merrifield after loading the bases on two walks and a single. Dozier led off the fifth inning with a 423-foot home run to left field to make it 3-1. The Orioles pulled within a run in the sixth when Mullins led off with a double just beyond the reach of Dozier at third, advanced to third on a fly ball and scored on Trey Mancini's sacrifice fly to the wall in right.The Royals answered in the bottom of the inning as Gallagher hit his first home run of the season. . .</figDesc><table><row><cell cols="5">Orioles 0 0 Royals 1 0 1 0 0 3 C. Mullins H 4 2 2</cell><cell cols="3">1 Orioles . . .</cell><cell></cell></row><row><cell>J. Villar</cell><cell></cell><cell>H</cell><cell>4</cell><cell>0 0</cell><cell cols="3">0 Orioles . . .</cell><cell></cell></row><row><cell cols="3">W. Merrifield V</cell><cell>2</cell><cell>3 2</cell><cell cols="3">1 Royals . . .</cell><cell></cell></row><row><cell cols="8">R. O'Hearn 4 PITCHER H/V W V 5 1 3 L IP H R ER BB K . . .</cell><cell></cell></row><row><cell cols="2">A. Cashner H</cell><cell cols="3">4 13 5.1 9</cell><cell>4</cell><cell cols="2">4 3 1 . . .</cell><cell></cell></row><row><cell>B. Keller</cell><cell>V</cell><cell>7</cell><cell></cell><cell>5 8.0 4</cell><cell cols="3">2 2 2 4 BATTER PITCHER</cell><cell>SCORER</cell><cell>EVENT</cell><cell>TEAM</cell><cell>INN RUNS . . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">C. Mullins</cell><cell>B. Keller</cell><cell>-</cell><cell>Home run</cell><cell>Orioles</cell><cell>1</cell><cell>1</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">H. Dozier</cell><cell cols="3">A. Cashner W. Merrifield Grounded into DP Royals</cell><cell>1</cell><cell>1</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">W. Merrifield A. Cashner B. Goodwin</cell><cell>Sac fly</cell><cell>Royals</cell><cell>4</cell><cell>2</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">H. Dozier</cell><cell>A. Cashner</cell><cell>-</cell><cell>Home run</cell><cell>Royals</cell><cell>4</cell><cell>3</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell></cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row></table><note>. . . . . . . . .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Vocabulary size, number of tokens, number of instances (i.e., record-summary pairs), average summary length, number of record types and average number of records in ROTOWIRE and MLB datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Evaluation on ROTOWIRE (RW) and MLB</cell></row><row><cell>test sets using relation generation (RG) count and</cell></row><row><cell>precision, content selection (CS) precision and re-</cell></row><row><cell>call, content ordering (CO) in normalized Damerau-</cell></row><row><cell>Levenshtein distance, and BLEU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>(bottom) also presents our results on</cell></row><row><cell>MLB (test set). Note that ED+CC is a reim-</cell></row><row><cell>plementation of Wiseman et al.'s (2017) encoder-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Ablation results on ROTOWIRE (RW) and</cell></row><row><cell>MLB development set using relation generation (RG)</cell></row><row><cell>count and precision, content selection (CS) preci-</cell></row><row><cell>sion and recall, content ordering (CO) in normalized</cell></row><row><cell>Damerau-Levenshtein distance, and BLEU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Examples of model output for NCP+CC (top) and ENT (bottom) on ROTOWIRE. Recurring entities in the summaries are boldfaced and colorcoded, singletons are shown in black.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Average number of supporting and contra-</cell></row><row><cell>dicting facts in game summaries and best-worst scaling</cell></row><row><cell>evaluation (higher is better) on ROTOWIRE and MLB</cell></row><row><cell>datasets. Systems significantly different from ENT are</cell></row><row><cell>marked with an asterisk * (using a one-way ANOVA</cell></row><row><cell>with posthoc Tukey HSD tests; p ? 0.05).</cell></row></table><note>. et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>(top) shows results on the ROTOWIRE development set for our dynamic entity memory model (ENT), the best system of Wiseman et al. (2017) (WS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc>(bottom) also presents our results on MLB. ENT achieves highest BLEU amongst all models and highest CS recall and RG count amongst neural models.Qualitative ExamplesTables 8 and 9contain examples of model output for ROTOWIRE and MLB, respectively. Because it conditions on entities, ENT is able to produce text displaying nominal coreference compared to other models.</figDesc><table><row><cell></cell><cell>ROTOWIRE</cell><cell>MLB</cell></row><row><cell>Word Embeddings</cell><cell>600</cell><cell>300</cell></row><row><cell>Hidden state size</cell><cell>600</cell><cell>600</cell></row><row><cell>Entity memory size</cell><cell>300</cell><cell>300</cell></row><row><cell>LSTM Layers</cell><cell>2</cell><cell>1</cell></row><row><cell>Input Feeding</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Dropout</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>Optimizer</cell><cell>Adagrad</cell><cell>Adagrad</cell></row><row><cell>Initial learning rate</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell>Learning rate decay</cell><cell>0.97</cell><cell>0.97</cell></row><row><cell>Epochs</cell><cell>25</cell><cell>25</cell></row><row><cell>BPTT size</cell><cell>100</cell><cell>100</cell></row><row><cell>Batch size</cell><cell>5</cell><cell>12</cell></row><row><cell>Inference beam size</cell><cell>5</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for ROTOWIRE and MLB.</figDesc><table><row><cell>RW</cell><cell>#</cell><cell>RG P%</cell><cell>P%</cell><cell>CS</cell><cell>R%</cell><cell>CO DLD%</cell><cell>BLEU</cell></row><row><cell cols="6">TEMPL 54.29 99.92 26.61 59.16</cell><cell>14.42</cell><cell>8.51</cell></row><row><cell cols="6">WS-2017 23.95 75.10 28.11 35.86</cell><cell cols="2">15.33 14.57</cell></row><row><cell cols="6">ED+CC 22.68 79.40 29.96 34.11</cell><cell cols="2">16.00 14.00</cell></row><row><cell cols="6">NCP+CC 33.88 87.51 33.52 51.21</cell><cell cols="2">18.57 16.19</cell></row><row><cell>ENT</cell><cell cols="5">31.84 91.97 36.65 48.18</cell><cell cols="2">19.68 15.97</cell></row><row><cell>MLB</cell><cell>#</cell><cell>RG P%</cell><cell>P%</cell><cell>CS</cell><cell>R%</cell><cell>CO DLD%</cell><cell>BLEU</cell></row><row><cell cols="6">TEMPL 59.93 97.96 22.82 68.46</cell><cell>10.64</cell><cell>3.81</cell></row><row><cell cols="6">ED+CC 18.69 92.65 62.29 51.36</cell><cell>25.93</cell><cell>9.55</cell></row><row><cell cols="6">NCP+CC 17.70 88.01 59.76 55.23</cell><cell>26.87</cell><cell>9.43</cell></row><row><cell>ENT</cell><cell cols="5">21.32 88.16 57.36 61.50</cell><cell cols="2">24.87 11.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7</head><label>7</label><figDesc>Atlanta Hawks (44-30) defeated the Detroit Pistons (39-35) 112-95. Paul Millsap scored 23 points</figDesc><table><row><cell>: Results on ROTOWIRE (RW) and MLB de-</cell></row><row><cell>velopment sets using relation generation (RG) count</cell></row><row><cell>and precision, content selection (CS) precision and re-</cell></row><row><cell>call, content ordering (CO) in normalized Damerau-</cell></row><row><cell>Levenshtein distance, and BLEU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Example output from the template-based system, NCP+CC<ref type="bibr" target="#b31">(Puduppully et al., 2019</ref>) and our ENT model for ROTOWIRE. Recurring entities in the summaries are boldfaced and colorcoded, singletons are shown in black.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code and dataset can be found at https:// github.com/ratishsp/data2text-entity-py.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.espn.com/mlb/recap?gameId={gameid}</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Adam Lopez for helpful discussions. We acknowledge the financial support of the European Research Council (Lapata; award number 681760).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Hyperparameters <ref type="table">Table 6</ref> contains the hyperparameters used for our ENT model on the RO-TOWIRE and MLB datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simulating action dynamics with neural process networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corin</forename><surname>Ennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dialogue generation in character-based interactive storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIIDE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Character-based interactive storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven J</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Givenness, contrastiveness, definiteness, subjects, topics, a nd point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallace</forename><forename type="middle">L</forename><surname>Chafe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Subject and topic</title>
		<editor>Charles N. Li</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1976" />
			<biblScope unit="page" from="25" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural text generation in stories using entity representations as context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2250" to="2260" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Comprehension and the given-new conract</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haviland</surname></persName>
		</author>
		<editor>Roy O</editor>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Discourse production and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedle</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="39" />
			<pubPlace>Ablex, Norwood, NewJersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.5477</idno>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centering: A framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A K</forename><surname>Halliday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruqaiya</forename><surname>Hasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<pubPlace>London</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cohesion in English. Longman</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic entity representations in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating centeringbased metrics of coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikiforos</forename><surname>Karamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mellish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Oberlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discourse referents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauri</forename><surname>Karttunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax and Semantics: Notes from the Linguistic Underground</title>
		<editor>James D. McCawley</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1976" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="363" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing referential coherence in text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodger</forename><surname>Kibble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Power</surname></persName>
		</author>
		<idno type="DOI">10.1162/0891201042544893</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="416" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1032</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Functional sentence perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susumu</forename><surname>Kuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Inquiry</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="269" to="320" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Best-worst scaling: Theory, methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony Alfred John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Best-worst scaling: A model for the largest difference judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George G</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodworth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>University of Alberta: Working Paper</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomson</surname></persName>
		</author>
		<title level="m">Rhetorical structure theory. Text</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1086</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Maxdiff analysis: Simple counting, individual-level logit, and hb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Orme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Sawtooth Software</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bootstrapping generators from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Beltrachini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1137</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1516" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward a taxonomy of givennew information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">Prince</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Radical pragmatics</title>
		<editor>Peter Cole</editor>
		<meeting><address><addrLine>New York/London</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1981" />
			<biblScope unit="page" from="223" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324997001502</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Revision-based generation of Natural Language Summaries providing historical Background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Robin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Getting the message across in RST-based text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donia</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clarisse Sieckenius De</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current Research in Natural Language Generation</title>
		<editor>Robert Dale, Chris Mellish, and Michael Zock</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="47" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reactive content selection in the generation of real-time soccer commentary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koiti</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsuki</forename><surname>Noda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<editor>C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Perceived or not perceived: Film character models for expressive nlg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wardrip-Fruin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Interactive Digital Storytelling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An efficient gradient-based algorithm for on-line training of recurrent network trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1990.2.4.490</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics. System Summary Template The Tampa Bay Rays defeated the Oakland Athletics 13-4. Jason Isringhausen (0-0) allowed 0 runs, 1 hits and 0 walks in 1 innings. Jeff Niemann (4-3) allowed 4 runs, 8 hits and 0 walks in 8 innings. Sean Gallagher (1-2) allowed 9 runs, 3 hits and 5 walks in 2 1/3 innings. Kevin Cameron (0-0) allowed 0 runs, 0 hits and 1 walks in 2 innings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Hierarchical attention networks for document classification. Gio Gonzalez (0-0) allowed 4 runs, 6 hits and 3 walks in 3 2/3 innings</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Jason Bartlett hit 2 RBI single in the first. Orlando Cabrera hit 1 RBI homer in the third Athletics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Akinori Iwamura hit 2 RBI single in the third. Jason Bartlett hit 1 RBI triple in the fifth. Akinori Iwamura hit 1 RBI single in the fifth. Jason Bartlett hit 2 RBI single in the sixth</title>
	</analytic>
	<monogr>
		<title level="j">Jack Cust hit</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>RBI homer in the eighth Athletics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Bartlett had three hits and drove in five runs, and the Tampa Bay Rays beat the Oakland Athletics 13-4 on Tuesday night. Bartlett hit a two-run single during a six-run first inning off Gio Gonzalez (1-2), who gave up nine runs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed+cc</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fla.-Jason</forename><surname>Petersburg</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>three hits and three walks in 2 1/3 innings</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Orlando Cabrera hit a solo homer for the A&apos;s, who have lost four of five. Orlando Cabrera hit a solo homer for the A&apos;s, who have lost four of five. Jack Cust hit a two-run homer in the eighth for the A&apos;s, who have lost four of five. Orlando Cabrera hit a solo homer for the A&apos;s, who have lost four of five. Orlando Cabrera hit a solo homer for the A&apos;s, who have lost three of four. Orlando Cabrera hit a solo homer in the third for the A&apos;s, who have lost four of five. Orlando Cabrera hit a solo homer in the third for the A&apos;s, who have lost three straight and five of six. Gallagher (1-2) gave up nine runs, four earned, and six hits in 2 1/3 innings. He walked three and struck out six. Gallagher (1-2) gave up nine runs</title>
	</analytic>
	<monogr>
		<title level="m">the third for the A&apos;s, who have lost four of five</title>
		<imprint/>
	</monogr>
	<note>and six hits in 2 1/3 innings. He walked three and struck out six. Cust hit a two-run homer in the eighth</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sean Gallagher (1-2) gave up a career-high nine runs in 2 1/3 innings. Jack Cust hit a two-run homer in the seventh and Jack Cust added a two-run homer for the A&apos;s, who have lost four of five. Tampa Bay has lost four of five. The Rays have lost four of five. Tampa Bay has lost four of five. The Rays have lost four of five. Oakland has lost four of five. The Rays have lost four of five. Tampa Bay has lost four of five. The Rays have lost four of five. Tampa Bay has lost five of six. The Rays have lost four of five. Tampa Bay has lost five of six. The Rays have lost four of five. Tampa Bay has lost five of six overall. Oakland has lost five of six. The Rays have lost four of five. Tampa Bay has lost four of five. Oakland has lost four of five. Oakland has lost five of six. The Rays have lost four of five</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ncp+cc</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fla</forename><surname>Petersburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tampa Bay Rays got a big boost from Jason Bartlett&apos;s grand slam. Bartlett drove in five runs, Jason Bartlett had five RBIs, Jason Bartlett had five RBIs and the Rays beat the Oakland Athletics 13-4 on Friday night</title>
		<imprint/>
	</monogr>
	<note>Jeff Niemann (4-3) allowed four runs, three earned, and eight hits in eight innings. Tampa Bay has lost four of five. Oakland has lost four of five. Oakland has lost five of six. Oakland has lost five of six</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bartlett had a two-run single in the third and a two-run single in the sixth. Jack Cust hit a two-run homer in the eighth for the A&apos;s, who have won five of six. The A&apos;s scored six runs off Sean Gallagher (1-2), who gave up a career-high nine runsseven earned-and three hits in 2 1/3 innings. Niemann (4-3) gave up four runs, three earned, and eight hits in eight innings. The right-hander struck out three and did not walk a batter for the second time this season. The right-hander is 4-0 in six career starts against the A&apos;s. Orlando Cabrera hit a solo homer in the third for the A&apos;s, who have lost four of five. Oakland starter Gio Gonzalez gave up four runs and six hits in 3 2/3 innings. The right-hander struck out six and walked three. The right-hander was coming off a 1-0 loss to the A&apos;s in his previous start</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ent</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-Jason</forename><surname>Fla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tampa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rays finally found a way to beat the Oakland Athletics. Bartlett had a career-high five RBIs, Jeff Niemann pitched eight strong innings and the Rays beat the Oakland Athletics 13-4 on Tuesday night</title>
		<imprint/>
	</monogr>
	<note>Bartlett had a two-run single in the first and added a two-run single in the third to help the Rays take a 6-1 lead. when he gave up six runs in 4 1/3 innings of a 10-0 loss to the A&apos;s. The A&apos;s took a 1-0 lead in the first when Ben Zobrist drew a bases-loaded walk and Bartlett had a two-run single</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Example output from the template-based system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ncp+cc</forename><surname>Ed+cc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puduppully</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>2019) and our ENT model for MLB. Recurring entities are boldfaced and colorcoded, singletons are shown in black</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
