<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Jeong</roleName><forename type="first">Minsu</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Systems Lab</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hun</forename><surname>Yeo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Systems Lab</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Ro</surname></persName>
							<email>ymro@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Systems Lab</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing speech from silent lip movement, which is called lip reading, is a challenging task due to 1) the inherent information insufficiency of lip movement to fully represent the speech, and 2) the existence of homophenes that have similar lip movement with different pronunciations. In this paper, we try to alleviate the aforementioned two challenges in lip reading by proposing a Multi-head Visual-audio Memory (MVM). Firstly, MVM is trained with audio-visual datasets and remembers audio representations by modelling the interrelationships of paired audio-visual representations. At the inference stage, visual input alone can extract the saved audio representation from the memory by examining the learned inter-relationships. Therefore, the lip reading model can complement the insufficient visual information with the extracted audio representations. Secondly, MVM is composed of multihead key memories for saving visual features and one value memory for saving audio knowledge, which is designed to distinguish the homophenes. With the multi-head key memories, MVM extracts possible candidate audio features from the memory, which allows the lip reading model to consider the possibility of which pronunciations can be represented from the input lip movement. This also can be viewed as an explicit implementation of the one-to-many mapping of viseme-to-phoneme. Moreover, MVM is employed in multitemporal levels to consider the context when retrieving the memory and distinguish the homophenes. Extensive experimental results verify the effectiveness of the proposed method in lip reading and in distinguishing the homophenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Lip reading, also known as Visual Speech Recognition (VSR), is a task that recognizes speech by watching lip movements only. It has a wide range of positive applications such as conversation with people who cannot make a voice (e.g., aphonia), an auxiliary technology for audiobased speech recognition in a noisy environment, and video conference in a crowded or silent environment. With the diverse applications, it has drawn big attention for a long time. However, compared to audio-based Automatic Speech Recognition (ASR), it is still regarded as a challenging task due to the following two reasons. First is the inherent information insufficiency of lip movements. The speech is jointly produced with various human organs such as vocal folds, larynx, tongue, and lips <ref type="bibr" target="#b29">(Sataloff 1992)</ref>. Therefore, watching lip movements individually might be insufficient to fully represent the speech. The second is the existence of homophenes. The homophenes refer to some different pronunciations that show the same lip movements. Therefore, the same lip movements can be observed from different pronunciations. In other words, the viseme-to-phoneme mapping is one-to-many which places a challenge on lip reading.</p><p>Recently, deep memory network <ref type="bibr" target="#b36">(Weston, Chopra, and Bordes 2014;</ref><ref type="bibr" target="#b31">Sukhbaatar et al. 2015a</ref>) that remembers valuable information learned during training and exploits the stored knowledge on a given task shows powerful performances on diverse applications such as question answering <ref type="bibr" target="#b27">(Miller et al. 2016)</ref>, video prediction <ref type="bibr" target="#b20">(Lee et al. 2021)</ref>, and retrieval <ref type="bibr" target="#b30">(Song, Wang, and Tan 2018;</ref><ref type="bibr" target="#b13">Huang and Wang 2019)</ref>. Especially, a cross-modal memory network <ref type="bibr" target="#b41">(Zhang et al. 2020a</ref>) that saves different modal information in distinct memories shows promising results on informationlimited situation such as few-shot learning scenario. By using a cross-modal memory network, it is possible to read one modal feature from another modal feature as being inputs, which is a highly attractive characteristic when applied to a task where only a single modal input is available. Therefore, the cross-modal memory seems valuable to be extended to the lip reading task using the audio-visual modalities <ref type="bibr" target="#b16">(Kim et al. 2021b)</ref>, and to fulfill insufficient information of lip movement with the saved audio information read from the memory. However, directly applying the cross-modal memory into lip reading is not trivial due to the existence of homophenes. Since the visual features of homophenes are similar, they might point to the same value memory slots when addressing the memory. In other words, the same audio features could be obtained by different lip movement videos of homophenes, preventing the lip reading system from fully utilizing the audio information. Therefore, in order to take full advantage of the cross-modal memory, a method of considering the homophenes while bringing the audio information through memory is necessary.</p><p>In this paper, we propose Multi-head Visual-audio Memory (MVM) for lip reading, to mitigate the two aforementioned challenges, homophenes and information insuf-ficiency of lip movements. The proposed visual-audio memory consists of multi-head key memories and one value memory. The value memory saves representative audio features and the key memory saves visual features utilized to retrieve proper audio representations from the value memory. Similar to the multi-head attention proposed in <ref type="bibr" target="#b33">(Vaswani et al. 2017</ref>) that attends to information at different positions, MVM allows the model to jointly consider the information from different possible audio representations, even if the visual features of homophenes are given. Moreover, we employ the proposed MVM in multi-temporal levels so that the context can be considered, when querying the key memories to find out proper audio features from homophenes. With the proposed MVM, the lip reading network can fully utilize audio information stored in the value memory with an enhanced ability to distinguish the homophenes.</p><p>The effectiveness of the proposed MVM is validated on popular benchmark databases. Moreover, we analyze and demonstrate the effectiveness of the proposed MVM in distinguishing homophenes by examining the pairs of words that belong to homophenes and visualizing the addressing scores of different head key memories.</p><p>The contributions of this work are summarized as follows:</p><p>? In order to consider the homophenes while bringing the saved audio information through a memory network, we propose Multi-head Visual-audio Memory (MVM) network that can extract possible candidate audio representations from one visual lip movement. ? To capture the context while querying the key memory and retrieving the saved audio features in the value memory, we employ the memory in multi-temporal levels so that the ambiguous mapping of viseme-to-phoneme can be refined. ? The proposed MVM achieves state-of-the-art performances on word-level lip reading in both English and Mandarin. Moreover, we analyze and validate the effectiveness of MVM in distinguishing homophenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Lip Reading</head><p>Lip reading <ref type="bibr" target="#b8">(Chung and Zisserman 2016;</ref><ref type="bibr" target="#b25">Ma et al. 2021c;</ref><ref type="bibr" target="#b2">Akbari et al. 2018;</ref><ref type="bibr" target="#b17">Kim, Hong, and Ro 2021)</ref> is a task that recognizes speech from lip movements. Many research contributions in lip reading have focused on finding better spatio-temporal features for modelling visemes, the counterpart of phonemes in speech audio, from lip video. (Petridis, Li, and Pantic 2017) successfully recognized words by using front-end architecture of one 3D convolution layer and 2D ResNet <ref type="bibr" target="#b10">(He et al. 2016</ref>) and back-end composed of LSTM. To capture the lip movement better, some studies <ref type="bibr" target="#b35">(Weng and Kitani 2019;</ref><ref type="bibr" target="#b37">Xiao et al. 2020)</ref> proposed twostream networks that jointly model the raw video and the optical flow. <ref type="bibr" target="#b26">(Martinez et al. 2020)</ref> improved the temporal encoding by proposing Multi-Scale Temporal Convolutional Network (MS-TCN) and boosted the word-level lip reading performance. Along with the advances in wordlevel lip reading, sentence-level VSR has also made great progress. <ref type="bibr" target="#b3">(Assael et al. 2016)</ref> proposed the first deep learning based end-to-end sentence-level lip reading model using Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b9">(Graves et al. 2006</ref>) loss function. <ref type="bibr" target="#b7">(Chung et al. 2017</ref>) extended it to unconstrained sentence-level lip reading, proposing an English audio-visual corpus dataset and a Seq2Seq-based architecture <ref type="bibr" target="#b6">(Cho et al. 2014)</ref>. <ref type="bibr" target="#b0">(Afouras et al. 2018</ref>) further improved the sentence-level lip reading performance by using Transformer <ref type="bibr" target="#b33">(Vaswani et al. 2017</ref>) that has shown superior performances in language processing. Apart from the architectural improvement, some studies have focused on utilizing audio information to complement the visual information for lip reading. <ref type="bibr" target="#b1">(Afouras, Chung, and Zisserman 2020)</ref> tackled a problem that the labeled audiovisual dataset is much smaller than that of unlabeled. They proposed a method that utilizes the unlabeled data using a pre-trained ASR model. They trained a lip reading model with large-scale unlabeled data by guiding to follow the prediction of a pre-trained ASR model. On a similar line, <ref type="bibr" target="#b45">(Zhao et al. 2020b;</ref><ref type="bibr" target="#b28">Ren et al. 2021)</ref> proposed to train the lip reading network using teacher-student framework. They utilized a pre-trained ASR model as a teacher and trained the lip reading model as a student using knowledge distillation <ref type="bibr" target="#b11">(Hinton, Vinyals, and Dean 2015)</ref>. By distilling the knowledge of the teacher model into the student model, the lip reading model is expected to learn better representations for speech recognition. <ref type="bibr" target="#b24">(Ma et al. 2021b</ref>) proposed a selfsupervised learning method to learn powerful visual speech representations. They pre-trained the visual front-end to predict corresponding acoustic features from the lip video, then fine-tuned with lip reading task loss. They showed its effectiveness in learning discriminative visual representations by achieving impressive lip reading performances. <ref type="bibr" target="#b16">(Kim et al. 2021b;</ref><ref type="bibr" target="#b12">Hong et al. 2021)</ref> proposed to utilize a cross-modal memory network to save and extract the audio representations, and <ref type="bibr" target="#b15">(Kim et al. 2021a</ref>) described a limitation of crossmodal memory on handling the homophenes in lip reading.</p><p>In this paper, we also try to complement the visual information of lip movement using audio information. We employ a cross-modal memory network that memorizes visual and audio representations in the different memories (i.e., key and value), namely visual-audio memory. Apart from the previous methods, to handle the homophenes when reading the lips, we extend the cross-modal memory network to consist of multi-head key memories, and consider the context by applying it in multi-temporal levels. With the enriched context and candidate audio representations, the proposed method can effectively distinguish the homophene words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Network</head><p>Memory network is originally designed to alleviate the problem of forgetting information mainly in sequential data modelling. By augmenting the neural network with an external memory <ref type="bibr" target="#b36">(Weston, Chopra, and Bordes 2014;</ref><ref type="bibr" target="#b32">Sukhbaatar et al. 2015b)</ref>, it can flexibly utilize the important information learned during training. Due to its effectiveness, it has been widely applied not only in sequence modelling but also in question-answering <ref type="bibr" target="#b27">(Miller et al. 2016</ref>  2018), pedestrian detection , and video prediction <ref type="bibr" target="#b20">(Lee et al. 2021)</ref>. The memory network is also used for cross-modal data. <ref type="bibr" target="#b30">(Song, Wang, and Tan 2018)</ref> proposed a memory network that saves multi-modal representations for cross-modal retrieval. It pre-learns and stores the features with semantic concepts of each modal. When a query is given, it searches supporting clues in different modalities and aggregates the clues for retrieval. <ref type="bibr" target="#b41">(Zhang et al. 2020a</ref>) applied multi-modal memory network for fewshot activity recognition. Their cross-modal memory is read and written using other modal features, which guarantees each memory slot to correspond with each other. Distinct from the previous methods, we propose Multihead Visual-audio Memory (MVM) based on cross-modal memory that saves visual and audio modalities to alleviate the ambiguity mapping of viseme-to-phoneme in lip reading. In the proposed MVM, multi-head key memories enable the lip reading network to consider the possible candidate audio representations for a given visual lip movement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>Let x v ? R T ?H?W ?C be a lip video with frames of T , height of H, width of W , and color channel size of C, x a ? R F ?S be a mel-spectrogram converted from speech audio corresponded to the lip video, where F represents mel spectral size and S is frame length, and y be the corresponding ground-truth labels. The input video x v and audio x a are embedded through each respective front-end module as follows:</p><formula xml:id="formula_0">f v = E v (x v ) ? R T ?D and f a = E a (x a ) ? R T ?D ,</formula><p>where f v and f a represent embedded visual and audio features, respectively, E v (?) and E a (?) are the visual and audio front-end modules, and D is the dimension of embedding. Note since the paired audio and video are supposed to be aligned in time with the same duration, the front-end modules can be designed to output having the same frame numbers (i.e., T ). Then, our objective is to save the visual features f v and the audio features f a in distinct memory networks, while learning the inter-relationships of the two modalities in training procedure. At the inference stage where input lip video is available only, we can extract the saved audio features from the memory by examining the learned inter-relationships using the input visual features. The overview of the proposed lip reading framework is illustrated in <ref type="figure" target="#fig_0">Fig. 1a</ref>. Since there exist homophenes that have the same lip movement with different pronunciations, the mapping of viseme-to-phoneme is not one-to-one mapping but one-to-many. Thus, we explicitly design the cross-modal memory to represent the inter-relationships of one-to-many mapping to correctly extract the saved audio features by proposing MVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-audio Memory Network</head><p>Visual-audio memory network, based on a cross-modal memory, stores visual features in key memory M v ? R N ?D and the audio features in value memory M a ? R N ?D , where N is the number of memory slots. Since the key memory and value memory are trained to save and to read the features of paired audio-visual data, it is possible to obtain the saved audio features by using visual inputs only. Our visualaudio memory operates with similarity-based reading (i.e., addressing) and writing (i.e., saving) <ref type="bibr" target="#b16">(Kim et al. 2021b;</ref><ref type="bibr" target="#b20">Lee et al. 2021)</ref>. When a visual feature f v is given, the addressing score of i-th memory slot for j-th frame is obtained as follows,</p><formula xml:id="formula_1">A i,j v = exp(? ? d(M i v , W q f j v )) N m=1 exp(? ? d(M m v , W q f j v )) ,<label>(1)</label></formula><p>where d(?) is a cosine similarity metric, W q ? R D?D represents projection weight for querying, and ? is a scaling factor. The addressing score represents which memory slot contains the most relevant features to the given query visual feature. With the addressing score A i,j v obtained by using key memory and visual features, the saved audio features are extracted from the value memory as follows,</p><formula xml:id="formula_2">a j = N i=1 A i,j v ? M i a ,<label>(2)</label></formula><formula xml:id="formula_3">f j a = W o a j ,<label>(3)</label></formula><p>where a j is the extracted audio features from the value memory and W o ? R D?D represents projection weights. Therefore, besides the input visual features f v , the lip reading model can additionally utilize the audio knowledge features f a = {f j a } T j=1 . By jointly modelling the features of two modalities, the insufficient information of lip movements can be complemented with the audio information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Visual-audio Memory Network</head><p>Even though the visual-audio memory can bring the audio information without using the audio inputs during inference, it could fail on accessing proper memory slots if lip videos of homophenes are presented. This is natural since the visualaudio memory finds the saved audio using a fixed key memory and a given visual feature, and the visual features of homophene lip videos are similar to each other. In order to mitigate the ambiguity of the viseme-to-phoneme mapping, we strengthen the visual-audio memory to Multi-head Visual-audio Memory (MVM) with a multi-head structure. Specifically, the number of key memories (i.e., heads) is increased to h (i.e., number of heads) while that of the value memory stays one <ref type="bibr" target="#b19">(Lample et al. 2019</ref>) as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>. With the h heads, the memory network can extract possible candidate audio representations. Similar to the multi-head attention <ref type="bibr" target="#b33">(Vaswani et al. 2017)</ref>, it can be interpreted that the h different audio representations obtained from the value memory allow the lip reading network to jointly consider the possible audio information for a given lip movement. Moreover, this can also be viewed as a realization of one-to-many mapping that of viseme-to-phoneme mapping, since MVM outputs h saved audio features with a given visual feature. The multi-head key memory is defined with</p><formula xml:id="formula_4">M v = {M v 1 , ..., M v h }, where M v l ? R N ? D</formula><p>h represents l-th head key memory. Then, the addressing score of i-th memory slot for j-th frame is obtained for each head key memory as follows,</p><formula xml:id="formula_5">A i,j v l = exp(? ? d(M i v l , W q l f j v )) N m=1 exp(? ? d(M m v l , W q l f j v )) ,<label>(4)</label></formula><p>where W q l ? R D? D h represents projection weight for l-th head. With the addressing score, the h different audio features are extracted from the value memory and aggregated.</p><formula xml:id="formula_6">a j l = N i=1 A i,j v l ? M i a ,<label>(5)</label></formula><formula xml:id="formula_7">f j a = W o Concat(a j 1 , ..., a j h ),<label>(6)</label></formula><p>where a j l represents extracted audio features from the value memory using l-th head key memory and W o ? R Dh?D is embedding weight that aggregates the h different extracted audio features. The audio knowledge featuresf a are fused with the visual features f v by addition, followed by a layer normalization (Ba, Kiros, and Hinton 2016) that we omit in the equations for simple notations. Then, the back-end module models the context for prediction using the fused representations of visual and audio knowledge features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference</head><p>MVM is trained along with the whole lip reading network in an end-to-end manner. In order to save the representative audio representations in the value memory, we employ reconstruction-based learning and contrastive learning to train the value memory. The reconstruction-based learning guarantees the saved representations in the value memory correctly contain the audio information. It is defined with a cosine similarity-based reconstruction loss as follows,</p><formula xml:id="formula_8">L rec = ||1 ? d(f a , f a )|| 1 ,<label>(7)</label></formula><p>wheref j a = N i=1 A i,j a ? M i a represents reconstructed audio features from the value memory M a by using the addressing score A i,j a . Note that A i,j a is obtained similar to Eq. 1 by substituting key memory M v and visual feature f v with value memory M a and audio feature f a . In addition, to save the representative audio features that are distinct as possible from each other, we propose to use contrastive learning on the value memory as follows,</p><formula xml:id="formula_9">L cont = i =j ||d(M i a , M j a )|| 1 .<label>(8)</label></formula><p>The contrastive loss, L cont , guides the different memory slots to have less similar audio features, which leads the value memory can contain discriminative audio representations. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the training process of the value memory.</p><p>During training, the key memory M v is trained to attend and extract proper audio features saved in the value memory with lip reading loss such as cross-entropy loss and CTC loss <ref type="bibr" target="#b9">(Graves et al. 2006)</ref>. The task loss is applied as follows, <ref type="formula">(9)</ref> where F represents the lip reading loss and g represents the back-end module. The first term of the task loss guarantees the classification performance using both visual features f v and audio knowledge featuresf a obtained from the value memory, which is straightly the inference form of the proposed method. The second term makes possible the endto-end training by guiding the audio front-end module to extract useful audio features (which will be saved into the value memory) for speech recognition. The total loss function is the sum of the pre-defined loss functions,</p><formula xml:id="formula_10">L task = F(g(f v +f a ), y) + F(g(f v +f a ), y),</formula><formula xml:id="formula_11">L tot = L task + L rec + L cont .<label>(10)</label></formula><p>During inference, as the value memory is trained to save the representative audio features and the key memory is trained to extract proper saved audio features from the value memory, we do not require the input audio. Thus, the audio front-end is detached and only the input video is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Considering Context in Multi-temporal Levels</head><p>By implementing the one-to-many mapping through the proposed MVM, we can obtain the possible audio representations corresponding to an input lip visual representation. However, there is still room for improvement in terms of the clarity of the mapping. Since the visual features f v are embedded using a visual front-end module that usually has fixed temporal receptive field, f v also has limited temporal information. For example, the popular front-end module <ref type="bibr" target="#b27">(Petridis, Li, and Pantic 2017)</ref> in lip reading has a temporal receptive field of 5 consecutive frames. Therefore, when querying the key memory by using the visual features, it might fail to consider the context beyond 5 frames which is another key for distinguishing the homophenes. In order to improve the memory addressing stage, we employ MVM in multi-temporal levels, so that the context at different temporal ranges can be considered when querying the key memory. To this end, MVM is applied not only between the front-end and back-end but also in the back-end that models temporal  <ref type="figure" target="#fig_0">Fig. 1a</ref>. Thus, the mapping of viseme-to-phoneme can be elaborated as the layers go deeper by considering the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset</head><p>We conduct the experiments on both word-and sentencelevel lip reading databases. For the word-level lip reading, we use LRW <ref type="bibr" target="#b8">(Chung and Zisserman 2016)</ref> and LRW-1000 <ref type="bibr" target="#b39">(Yang et al. 2019</ref>) datasets, which are in English and Mandarin, respectively. For the sentence-level lip reading, we use LRS2 <ref type="bibr" target="#b7">(Chung et al. 2017)</ref> dataset. The LRW is an English word-level lip reading dataset which includes 500 words with a maximum of 1,000 training videos each. We crop the video into 136?136 centered at the lip without aligning the face and resize it into 112?112. The audio is transformed into a mel-spectrogram using a window size of 400, hop size of 160, and 80 mel-filters.</p><p>The LRW-1000 is a word-level dataset in Mandarin. It contains a total of 718,018 videos with 1,000 words. The dataset provided is already cropped, thus we resize the video into 112?112 without cropping. Since the audio provided is longer than the video, we use 0.2 sec longer video back and forth to match the duration of video and audio.</p><p>The LRS2 is a sentence-level audio-visual dataset. It is collected from British news programs to form 224 hours videos, containing large variations in head pose and illumination. The video is preprocessed similar to LRW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For the visual front-end, we use a popular architecture in lip reading that consists of one 3D convolution layer and ResNet-18 (Petridis, Li, and Pantic 2017). The audio frontend is designed with 2 convolution layers with stride 2 and one Residual block. For the back-end, we use MS-TCN <ref type="bibr" target="#b26">(Martinez et al. 2020</ref>), a state-of-the-art architecture, for word-level lip reading and Transformer <ref type="bibr" target="#b33">(Vaswani et al. 2017)</ref> architecture for sentence-level lip reading following <ref type="bibr" target="#b0">(Afouras et al. 2018)</ref>. MVM is empirically designed with 8 heads (h) and 112 slots (N ). For the word-level lip reading, it is applied in 4 different levels, before the back-end and after every MS-TCN layer except the last layer. For the sentence-level lip reading, MVM is applied in 4 different levels, after the front-end, and before the 2nd, 4th, and 6th transformer encoder layer.</p><p>For training the word-level lip reading, data augmentation including random horizontal flipping, random erasing, and  mixup <ref type="bibr" target="#b40">(Zhang et al. 2017;</ref><ref type="bibr" target="#b23">Ma et al. 2021a</ref>) is applied. The cross-entropy loss is utilized for the lip reading loss function. For sentence-level lip reading training, we follow the training schemes of <ref type="bibr" target="#b42">(Zhang, Cheng, and Wang 2019;</ref><ref type="bibr" target="#b7">Chung et al. 2017;</ref><ref type="bibr" target="#b0">Afouras et al. 2018</ref>) that 1) pre-train the visual front-end on the LRW dataset, 2) pre-train the whole network using pre-train sets of the LRS2 and the LRS3 with curriculum setting, and 3) finally train and test on the LRS2 dataset. The hybrid CTC/Attention <ref type="bibr" target="#b34">(Watanabe et al. 2017)</ref> loss function is utilized. The external language model is not utilized for decoding stage. We use AdamW optimizer <ref type="bibr" target="#b21">(Loshchilov and Hutter 2017)</ref>, batch size of 200, 64, and 40 for LRW, LRW-1000, and LRS2, respectively, with initial learning rate of 0.0001, and ? is set to 16. We use four Titan RTX GPUs (24GB) and Intel Xeon Gold 6130 CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Ablation study In order to examine the contributions of each proposed component on lip reading, we build 4 variants of the model, 1) baseline lip reading model without crossmodal memory, 2) lip reading model with cross-modal memory based on <ref type="bibr" target="#b16">(Kim et al. 2021b)</ref>, 3) lip reading model with MVM and without applying in multi-temporal levels, and 4) the final proposed model with MVM in multi-temporal levels. <ref type="table">Table 1</ref> shows the word-level accuracy of each model on the LRW dataset. The baseline model which does not contain the visual-audio memory achieves 86.1% word accuracy. By applying the visual-audio memory to the baseline, the accuracy is improved to 86.9%, and this result confirms that bringing the audio information can complement the insufficient information of visual lip movement in speech recognition. By considering the homophenes with extracting possible audio features using the proposed MVM, we can improve the performance to 87.2%. It is valuable to note that the key and value memory parameters remain similar before and after the multi-head key is applied, as we reduce the dimension of key memory according to the head size. Finally, by applying MVM in multi-temporal levels, the word-accuracy achieves 88.5% with additional 1.3% improvement. The result clearly shows that applying MVM in multi-temporal levels is beneficial by considering the context when retrieving the saved audio feature. In order to examine the performance changes according to the hyperparameter of MVM, we experiment by differing the memory slots (N ), the number of heads (h), and the  . The experimental result shows that too small number of memory slots are less effective, which means the audio representations cannot be fully covered with a small number of memory slots. Moreover, assigning an excessive number of memory slots does not lead to an increase in performance. By differing the number of head key memories, we observe that more heads tend to be beneficial to lip reading by providing possible audio information for the homophenes. Finally, we can further improve the mapping of viseme-to-phoneme by applying MVM at multi-temporal levels in the back-end module. We use memory slots (N ) of 112, the number of heads (h) of 8, and 4 multi-temporal levels in the other experiments.</p><p>Comparison with state-of-the-art methods In order to verify the effectiveness of the proposed method, we compare the word-level lip reading performances with the stateof-the-art methods. <ref type="table" target="#tab_5">Table 3</ref> shows the comparison of wordlevel lip reading performances on the LRW dataset. The proposed method achieves 88.5% word accuracy and sets a new state-of-the-art performance. The result shows the effectiveness of the proposed method on modelling the visual representations and complementing the insufficient information with the audio information. In addition, the word-level lip reading comparison on the LRW-1000 dataset is shown in <ref type="table" target="#tab_7">Table 4</ref>. The LRW-1000 dataset is a relatively challenging dataset than the LRW due to its unbalanced training samples. Even in the challenging environment, the proposed method outperforms the previous state-of-the-art method <ref type="bibr" target="#b16">(Kim et al. 2021b</ref>) by 3% accuracy, achieving 53.82% word accuracy. The two word-level lip reading results confirm that the proposed method is beneficial for lip reading by 1) fulfilling the insufficient visual information with the saved audio information during training and 2) considering the homophenes using multi-head key memory and context in multi-levels.    <ref type="table">Table 6</ref>: Homophene word accuracy and its change compared to the baseline that contains one visual-audio memory.</p><p>Moreover, we compare the proposed method on sentencelevel lip reading using the LRS2 dataset with the baseline model that has the same architecture as the proposed method except for MVM. <ref type="table" target="#tab_8">Table 5</ref> shows the Word Error Rate (WER) performance of each method. We can clearly observe the effectiveness of MVM as it improves the performance by 5.3% WER from the baseline <ref type="bibr" target="#b0">(Afouras et al. 2018)</ref>. Consistent with the two word-level lip reading results, we can confirm the benefits of the proposed method in lip reading by improving the performance with a large gap.</p><p>Effectiveness on distinguishing homophenes In order to analyze the effectiveness of the proposed method in distinguishing homophenes, we compare the word accuracy of the proposed method with a visual-audio memory model based on <ref type="bibr" target="#b16">(Kim et al. 2021b</ref>) as a baseline. To this end, we extract the words list of the LRW dataset and find the paired words of homophenes based on the viseme-to-phoneme mapping table of <ref type="bibr" target="#b5">(Cappelletta and Harte 2012)</ref>. <ref type="table">Table 6</ref> shows six pairs of words belonging to the homophenes and their predicted accuracy. Moreover, we represent the performance improvement by adopting the proposed method. For example, word accuracy for Giving of the proposed method <ref type="table" target="#tab_3">1  21  41  61  81  101 112  1  21  41  61  81  101 112   1  21  41  61  81  101 112  1  21  41  61  81</ref>  achieves 76% and that of the baseline is 64%. Therefore, the proposed method boosts the performance for Giving by 12%, which is denoted bold in the table. From the table, we can analyze the following two facts: 1) homophenes cause difficulty in lip reading, showing relatively less accuracy compared to the mean model accuracy (i.e., 88.5%) and 2) by employing the proposed MVM within the multitemporal levels, we can enhance the performance with a large gap from the model that utilizes visual-audio memory only, which mainly contributes to the improvement of mean model accuracy (i.e., 1.6% from the baseline).</p><p>To verify whether MVM 1) extracts different audio representations and 2) distinguishes the homophene words, we visualize the addressing scores of different head key memories at the third level MVM for homophene words. <ref type="figure" target="#fig_2">Fig. 3a</ref> shows addressing scores of head 5 and head 7 for homophene words END and ENT of SPEND and SPENT, respectively. It shows that the different value memory slots are accessed by different words, even if their lip movements are visually similar. This is because the context modeled at the backend helps the memory retrieving stage to distinguish the homophenes. Moreover, we can find that the different heads attend different memory slots which means MVM provides possible candidate audio representations. Moreover, <ref type="figure" target="#fig_2">Fig. 3b</ref> shows that the same word yield similar value memory addressing. From the visualization results, we can confirm the effectiveness of the proposed MVM on distinguishing the homophenes which induce challenges in lip reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have proposed Multi-head Visual-audio Memory (MVM) for lip reading. The proposed MVM can complement insufficient information of lip movement with audio knowledge when the audio input is not provided. Moreover, to distinguish the homophenes during memory accessing, MVM is designed with a one-to-many mapping fashion, which allows the lip reading network to jointly model the different possible audio representations that correspond to a given lip visual feature. Through extensive experiments, we have verified the effectiveness of the proposed framework in distinguishing homophenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed method. (a) Overview of the proposed lip reading framework on word-level lip reading. During training, audio information is saved into the proposed memory and the learned knowledge is utilized at the inference stage without using the audio input. (b) Multi-head Visual-audio Memory (MVM). With the h head key memories and a single value memory, it can extract different possible saved audio representations from the value memory with one visual feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Learning to save audio representations into value memory with reconstruction and contrastive losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Addressing scores of head 5 and head 7 for (a) homophene words, and (b) the ones of the same words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Multi-head Visual-audio Memory ...</figDesc><table><row><cell>Linear</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Back-end</cell><cell></cell><cell>Add &amp; norm</cell><cell></cell></row><row><cell>MS-TCN layer # 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>&amp; norm</cell><cell></cell></row><row><cell></cell><cell></cell><cell>cat</cell><cell></cell></row><row><cell>MS-TCN layer # 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Multi-head Visual-audio Memory</cell><cell>Weighted Sum</cell><cell></cell></row><row><cell>MS-TCN layer # 1</cell><cell></cell><cell>Value Memory</cell><cell></cell></row><row><cell cols="2">Multi-head Visual-audio Memory</cell><cell>Addressing Score</cell><cell></cell></row><row><cell>Visual</cell><cell>Audio</cell><cell>Key Memory</cell><cell>h</cell></row><row><cell>Front-end</cell><cell>Front-end</cell><cell>Cosine Similarity</cell><cell></cell></row><row><cell></cell><cell></cell><cell>&amp; Softmax</cell><cell></cell></row><row><cell></cell><cell></cell><cell>_</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>? ? ?</cell><cell>Input audio</cell><cell></cell><cell></cell></row><row><cell>Input video</cell><cell></cell><cell></cell><cell></cell></row></table><note>), visual explana- tion (Kim and Ro 2021), few-shot learning (Zhu and Yang</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Effect of different hyperparameters of MVM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Word accuracy comparison on LRW.</figDesc><table><row><cell>number of multi-temporal levels on the LRW dataset. Ta-</cell></row><row><cell>ble 2 shows the experimental results obtained under differ-</cell></row><row><cell>ent hyperparameters. To examine the effect of each hyper-</cell></row><row><cell>parameter, we fix the other factors when changing one fac-</cell></row><row><cell>tor and the fixed number is represented in the table with a</cell></row><row><cell>star mark (*)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Word accuracy comparison on LRW-1000.</figDesc><table><row><cell>Method</cell><cell>WER(%)</cell></row><row><cell>Baseline (Afouras et al. 2018)</cell><cell>49.8</cell></row><row><cell>Proposed Method</cell><cell>44.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>WER comparison with a baseline model on LRS2.</figDesc><table><row><cell cols="4">Word accuracy (%) / Performance change (%)</cell></row><row><cell>Living</cell><cell>Giving</cell><cell>Better</cell><cell>Matter</cell></row><row><cell>82 / +6</cell><cell>76 / +12</cell><cell>82 / +2</cell><cell>70 / +0</cell></row><row><cell>Million</cell><cell>Billion</cell><cell>Heard</cell><cell>Heart</cell></row><row><cell>84 / +2</cell><cell>88 / +4</cell><cell>72 / +8</cell><cell>90 / +4</cell></row><row><cell>Words</cell><cell>World</cell><cell>Black</cell><cell>Plans</cell></row><row><cell>66 / +0</cell><cell>76 / +4</cell><cell>88 / +4</cell><cell>88 / +2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ASR is all you need: Cross-modal distillation for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2143" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lip2audspec: Speech reconstruction from silent lip movements video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2516" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: End-to-end sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Phoneme-to-viseme mapping for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cappelletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM (2)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="322" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech Reconstruction with Reminiscent Sound via Visual Voice Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Acmm: Aligned crossmodal memory for few-shot image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5774" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust Small-Scale Pedestrian Detection With Cued Recall via Memory Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3050" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crommvsr: Cross-modal memory augmented visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodality associative bridging through memory: Speech sound recollected from face video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="296" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lip to Speech Synthesis with Visual Context Attentional GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">M-CAM: Visual Explanation of Challenging Conditioned Dataset with Bias-reducing Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 32nd British Machine Vision Conference, BMVC 2021. British Machine Vision Association (BMVA)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05242</idno>
		<title level="m">Large memory layers with product keys</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3054" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pseudoconvolutional policy gradient for sequence-to-sequence lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards practical lipreading with distilled and efficient models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7608" to="7612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">2021b</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09171</idno>
		<title level="m">Learning Visual Speech Representations from Audio through Self-supervision</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lip-reading with densely connected temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2857" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lipreading using temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6319" to="6323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2592" to="2596" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>End-to-end visual speech recognition with LSTMs</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning From the Master: Distilling Cross-Modal Advanced Knowledge for Lip Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13325" to="13333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The human voice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Sataloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="108" to="115" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep memory network for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1261" to="1275" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hybrid CTC/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal features with two-stream deep 3d cnns for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02540</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deformation flow based two-stream network for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="364" to="370" />
		</imprint>
	</monogr>
	<note>FG 2020</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative multi-modality speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14433" to="14442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Few-shot activity recognition with cross-modal memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">107348</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spatio-temporal fusion based convolutional sequence learning for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="713" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Can we read speech beyond the lips? rethinking roi selection for deep visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="356" to="363" />
		</imprint>
	</monogr>
	<note>FG 2020</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mutual information maximization for effective lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="420" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hearing lips: Improving lip reading by distilling speech recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6917" to="6924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="751" to="766" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
