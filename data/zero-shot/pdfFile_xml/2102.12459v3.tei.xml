<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<email>taoleics@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as ENWIK8, WIKI-103 and BIL-LION WORD datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to topperforming Transformer models. For instance, our model achieves a state-of-the-art result on the ENWIK8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many recent advances in language modeling have come from leveraging ever larger datasets and model architectures. As a result, the associated computation cost for developing such models have grown enormously, requiring hundreds of GPU hours or days per experiment, and raising concerns about the environmental sustainability of current research <ref type="bibr">(Schwartz et al., 2020)</ref>. As a consequence, it has become imperative to build computationally efficient models that retain top modeling power while reducing computational costs.</p><p>The Transformer architecture <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> was proposed to accelerate model training and has become the predominant architecture in NLP. Specifically, it is built entirely upon selfattention and avoids the use of recurrence to enable strong parallelization. While this change has 1 Our code, experimental setup and models are available at https://github.com/asappresearch/sru.  <ref type="figure">Figure 1</ref>: Bits-per-character on ENWIK8 dev set vs. GPU hours used for training. SRU++ obtains better BPC by using 1/8 of the resources. We compare with Transformer-XL as it is one of the strongest models on the datasets tested. Models are trained with single precision and comparable training settings. led to many empirical success and improved computational efficiency, we are interested in revisiting the architectural question: Is attention all we need for modeling?</p><p>The attention mechanism permits learning dependencies between any parts of the input, making it an extremely powerful neural component in many machine learning applications <ref type="bibr" target="#b3">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b32">Lin et al., 2017)</ref>. We hypothesize that this advantage can still be complemented with other computation that is directly designed for sequential modeling. Indeed, several recent works have studied and confirmed the same hypothesis by leveraging recurrence in conjunction with attention. For example, <ref type="bibr" target="#b35">Merity (2019)</ref> demonstrates that single-headed attention LSTMs can produce results competitive to Transformer models in language modeling. Other work have incorporated RNNs into Transformer, and obtain better results in machine translation <ref type="bibr" target="#b30">(Lei et al., 2018;</ref><ref type="bibr" target="#b17">Hao et al., 2019)</ref> and language understanding benchmarks <ref type="bibr">(Huang et al., 2020)</ref>. These results highlight one possibility -we could build more efficient models by combining attention and fast recurrent networks <ref type="bibr" target="#b57">Zhang and Sennrich, 2019)</ref>.</p><p>In this work, we validate this idea and present a self-attentive recurrent unit that achieves strong computational efficiency. Our work builds upon the SRU <ref type="bibr" target="#b30">(Lei et al., 2018</ref>), a highly parallelizable RNN implementation that has been shown effective in language and speech applications <ref type="bibr" target="#b37">(Park et al., 2018;</ref><ref type="bibr" target="#b25">Kim et al., 2019;</ref><ref type="bibr" target="#b20">Hsu et al., 2020;</ref><ref type="bibr" target="#b43">Shangguan et al., 2019)</ref>. We incorporate attention into the SRU by simply replacing the linear transformation of input with a self-attention component. The proposed architecture, called SRU++, enjoys enhanced modeling capacity and remains equally parallelizable. <ref type="figure">Figure 1</ref> compares its performance with the Transformer-XL model <ref type="bibr" target="#b10">(Dai et al., 2019)</ref> on the ENWIK8 dataset. SRU++ achieves better results while using a fraction of the training resources needed by the baseline.</p><p>We evaluate SRU++ on standard language modeling benchmarks including the ENWIK8, WIKI-103 and BILLION WORD datasets. SRU++ consistently outperforms various Transformer models on these datasets, delivering better or on par results while using 3x-10x less computation. Our model do not use positional encoding, multi-head attention and other techniques useful to Transformer models. Furthermore, we demonstrate that a couple of attention layers are sufficient for SRU++ to obtain near state-of-the-art performance. These changes not only highlight the effectiveness of recurrence but also enable strong computation reduction in training and inference. Finally, we also showcase the effectiveness of SRU++ on the IWSLT'14 De?En translation task, and open source our implementation in Pytorch to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: SRU</head><p>We first describe the Simple Recurrent Unit (SRU) in this section. A single layer of SRU involves the following computation:</p><formula xml:id="formula_0">f [t] = ? (Wx[t] + v c[t-1] + b) r[t] = ? W x[t] + v c[t-1] + b c[t] = f [t] c[t-1] + (1 ? f [t]) (W x[t]) h[t] = r[t] c[t] + (1 ? r[t]) x[t]</formula><p>where is the element-wise multiplication, W, W and W are parameter matrices and v, v , b and b are parameter vectors to be learnt during training. The SRU architecture consists of a light recurrence component which successively computes the hidden states c[t] by reading the input vector x[t] for each step t. The computation resembles other gated recurrent networks such as LSTM <ref type="bibr" target="#b18">(Hochreiter and Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b9">(Cho et al., 2014)</ref>. Specifically, the state vector c[t] is a weighted average between the previous state c[t-1] and a linear transformation of the input W x <ref type="bibr">[t]</ref>. The weighted aggregation is controlled by a forget gate f [t] which is a sigmoid function over the current input and hidden state. Once the internal state c[t] is produced, SRU uses a highway network to introduce a skip connection and compute the final output state h <ref type="bibr">[t]</ref>. Similarly, the information flow in the highway network is controlled by a reset gate r[t].</p><p>Two important code-level optimizations are performed to enhance the parallelism and speed of SRU. First, given the input sequence</p><formula xml:id="formula_1">X = {x[1], ? ? ? , x[L]} where each x[t] ? R d is a d- dimensional</formula><p>vector, SRU combines the three matrix multiplications across all time steps as a single multiplication. This significantly improves the computation intensity (e.g. GPU utilization). Specifically, the batched multiplication is a linear projection of the input tensor X ? R L?d :</p><formula xml:id="formula_2">U = ? ? W W W ? ? X ,<label>(1)</label></formula><p>where U ? R L?3?d is the output tensor, L is the sequence length and d is the hidden state size.</p><p>The second optimization performs all elementwise operations in an efficient way. This involves</p><formula xml:id="formula_3">f [t] = ?(U[t, 0] + v c[t-1] + b) (2) r[t] = ?(U[t, 1] + v c[t-1] + b ) (3) c[t] = f [t] c[t-1] + (1 ? f [t]) U[t, 2] (4) h[t] = r[t] c[t] + (1 ? r[t]) x[t].<label>(5)</label></formula><p>Similar to other built-in operations such as attention and cuDNN LSTM <ref type="bibr" target="#b0">(Appleyard et al., 2016)</ref>, SRU implements all these operations as a single CUDA kernel to accelerate computation. Note that each dimension of the hidden vectors is independent once U is computed. The computation can run in parallel across each hidden dimension (and each input sequence given a mini-batch of multiple sequences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SRU++</head><p>The key modification of SRU++ is to incorporate more expressive non-linear operations into the re- current network. Note that the computation of U (Equation 1) is a linear transformation of the input sequence X. We can replace this linear transformation with self-attention operation to enhance modeling capacity. Specifically, given the input sequence represented as a matrix X ? R L?d , the attention component computes the query, key and value representations using the following multiplications,</p><formula xml:id="formula_4">Q = W q X K = W k Q V = W v Q where W q ? R d ?d , W k , W v ? R d ?d are model parameters.</formula><p>d is the attention dimension that is typically much smaller than d. Note that the keys K and values V are computed using Q instead of X such that the weight matrices W k and W v are significantly smaller. We also tested another variant in which we first project X = WX into the lower dimension d , and then apply three independent d -by-d matrix multiplications over X to obtain the query, key and value representations. This variant achieves similar results.</p><p>Next, we compute a weighted average output A ? R d ?L using the scaled dot-product attention introduced in <ref type="bibr" target="#b51">Vaswani et al. (2017)</ref>,</p><formula xml:id="formula_5">A = softmax Q K ? d V .</formula><p>The final output U required by the elementwise recurrence is obtained by another linear projection,</p><formula xml:id="formula_6">U = W o (Q + ? ? A) .</formula><p>where ? ? R is a learned scalar and W o ? R 3d?d is a parameter matrix. Q + ? ? A is a residual connection which improves gradient propagation and stabilizes training. We initialize ? to zero and as a result,</p><formula xml:id="formula_7">U = W o Q = (W o W q ) X</formula><p>initially falls back to a linear transformation of the input X skipping the attention transformation. Intuitively, skipping attention encourages leveraging recurrence to capture sequential patterns during early stage of training. As |?| grows, the attention mechanism can learn long-range dependencies for the model. In addition, W o W q can be interpreted as applying a matrix factorization trick with a small inner dimension d &lt; d, reducing the total number of parameters. <ref type="figure" target="#fig_0">Figure 2</ref> (a)-(c) compares the differences of SRU, SRU with this factorization trick (but without attention), and SRU++ proposed in this section. The last modification is adding layer normalization <ref type="bibr">(Ba et al., 2016)</ref> to each SRU++ layer. In our implementation, we apply normalization after the attention operation and before the matrix multiplication with W o ,</p><formula xml:id="formula_8">U = W o layernorm(Q + ? ? A).</formula><p>This implementation is post-layer normalization in which the normalization is added after the residual connection. Alternatively, pre-layer normalization <ref type="bibr" target="#b54">(Xiong et al., 2020)</ref> only applies to the nonlinear transformation. While pre-normalization tends to be less sensitive to different learning rates, we use post-normalization for better results following the observations in <ref type="bibr" target="#b34">Liu et al. (2020b)</ref>. We analyze the effectiveness of layer normalization in Appendix A.2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>Datasets We evaluate our model on four standard NLP benchmarks.</p><p>? ENWIK8 <ref type="bibr" target="#b22">(Hutter, 2006)</ref> is a character-level language modeling dataset consisting of 100M tokens taken from Wikipedia. The vocabulary size of this dataset about 200. We use the standard 90M/5M/5M splits as the training, dev and test sets, and report bits-percharacter (BPC) as the evaluation metric.</p><p>? WIKI-103  is a wordlevel language modeling dataset. The training data contains 100M tokens extracted from Wikipedia articles. Following prior work, we use a vocabulary of 260K tokens, and adaptive embedding and softmax layers <ref type="bibr" target="#b14">(Grave et al., 2017;</ref><ref type="bibr" target="#b2">Baevski and Auli, 2019</ref>).</p><p>? BILLION WORD <ref type="bibr" target="#b8">(Chelba et al., 2013)</ref> is one of the largest language modeling datasets containing 768M tokens for training. Unlike WIKI-103 in which sentences in the same article are treated as consecutive inputs to model long context, the sentences in BIL-LION WORD are randomly shuffled. Following Baevski and Auli (2019), we use a vocabulary of 800K tokens, adaptive embedding and softmax layers.</p><p>? IWSLT'14 De?En is a low-resource machine translation dataset consists of 170K translation pairs. We showcase SRU++ can be applied to other tasks such as translation. We follow the same setup of <ref type="bibr" target="#b31">Lin et al. (2020)</ref> and other previous work. The dataset uses a shared vocabulary of 14K BPE tokens.</p><p>Models All our language models are constructed with a word embedding layer, multiple layers of  SRU++ and an output linear layer followed by softmax operation. We use single-head attention in each layer and 10 SRU++ layers for all our models. We use the same dropout probability for all layers and tune this value according to the model size and the results on the dev set. By default, we set the hidden dimension d : d = 4 : 1. We report additional analysis and tune this ratio for best results in Section 5 and Appendix A. For simplicity, SRU++ does not use recent techniques that are shown useful to Transformer such as multi-head attention, compressed memory <ref type="bibr" target="#b40">(Rae et al., 2020)</ref>, relative position <ref type="bibr" target="#b44">(Shaw et al., 2018;</ref><ref type="bibr" target="#b39">Press et al., 2021)</ref>, nearest-neighbor interpolation <ref type="bibr" target="#b24">(Khandelwal et al., 2020)</ref> and attention variants to handle very long context <ref type="bibr" target="#b46">(Sukhbaatar et al., 2019a;</ref>.</p><p>We compare with previous Transformer models that incorporate one or several these techniques. However, we do not compare with results that use additional data or dynamic evaluation <ref type="bibr" target="#b15">(Graves, 2013;</ref><ref type="bibr" target="#b28">Krause et al., 2018)</ref>, for a fair comparison between all models.</p><p>Optimization We use RAdam <ref type="bibr" target="#b33">(Liu et al., 2020a)</ref> with the default ? values as our optimizer. RAdam is a variant of Adam optimizer <ref type="bibr" target="#b26">(Kingma and Ba, 2014)</ref> that is reported less sensitive to the choice of learning rate and warmup steps while achieving similar results at the end. We use a fixed weight decay of 0.1 and an initial learning rate of 0.0003 in our experiments. These values are selected based on ENWIK8 dev set and used for other tasks. See Appendix A.3 for more details. We use a cosine learning rate schedule following <ref type="bibr" target="#b10">Dai et al. (2019)</ref>. We do not change the initial learning rate unless otherwise specified. See Appendix B for the detailed training configuration of each model. Each training batch contains B sequences (i.e. the batch size) and M consecutive tokens for each sequence (i.e. the unroll size), which gives an effective size of B ? M tokens per batch. Following standard practice, the previous training batch is provided as additional context for attention, which results in a maximum attention length of 2 ? M . For ENWIK8 and WIKI-103 datasets, the training data is partitioned into B chunks by concatenating articles and ignoring the boundaries between articles. For BILLION WORD dataset, we follow <ref type="bibr" target="#b10">Dai et al. (2019)</ref> and concatenate sentences to create the training batches. Sentences are randomly shuffled and separated by a special token &lt;s&gt; indicating sentence boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Does recurrence improve upon attention-only model? We first conduct a comparison with the Transformer-XL model <ref type="bibr" target="#b10">(Dai et al., 2019)</ref>   <ref type="figure">Figure 4</ref>: Analyzing where to apply attention. We enable only one attention layer (top <ref type="figure">figure)</ref> or two (bottom <ref type="figure">figure)</ref> in the SRU++ model. For the latter, we always apply attention in the last layer and move the location of the other. X-axis is the layer index. The layer closest to the input embedding layer has index 1.</p><p>tention context length to 2048 for testing, similarly to the Transformer-XL baseline. <ref type="table" target="#tab_0">Table 1</ref> presents the results. Our model achieves a test BPC of 1.03, outperforming the baseline by a large margin. This result suggests that combining recurrence and attention can greatly outperform an attention-only model. We obtain a BPC of 1.02 by extending the attention context length from 512 to 768, while keeping the number of tokens per batch the same.</p><p>How much attention is needed? <ref type="bibr" target="#b35">Merity (2019)</ref> demonstrated that using a single attention layer with LSTM retains most of the modeling capacity compared to using multiple attention layers. We conduct a similar analysis to understand how much attention is needed in SRU++. To do so, we only enable attention every k layers. The layers without attention become the variant with dimension projection illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (b). Note that k = 1 gives the default SRU++ model with attention in every layer, and k = 10 means only the last layer has attention in a 10-layer model. <ref type="table" target="#tab_4">Table 2</ref> presents the results by varying k. Our base model is the same 10-layer SRU++ model in <ref type="table" target="#tab_0">Table 1</ref>. We see that using 50% less attention (k = 2) achieves almost no increase in test BPC. Moreover, using only a single attention module (k = 10) leads to a marginal loss of 0.01 BPC but reduces the training time by 40%. Our results still outperform Transformer-XL model and single-headed attention LSTM <ref type="bibr" target="#b35">(Merity, 2019)</ref> greatly by 0.03 BPC. <ref type="figure">Figure 3</ref>    5x faster to reach the dev BPC obtained by the Transformer-XL model. Furthermore, using automatic mixed precision training and a single attention layer (k = 10) achieves 16x reduction on training cost.</p><p>Where to use attention? Next, we analyze if the location of attention in SRU++ makes a non-trivial difference. <ref type="figure">Figure 4</ref> (top) compares the results by enabling attention in only one of the SRU++ layers. Applying attention in the first bottom layer achieves significantly worse result. We believe this is due to the lack of positional information for attention, since SRU++ does not use positional encoding. Enabling attention in subsequent layers gives much better and comparable results because recurrence can encode positional information. Moreover, SRU++ consistently achieves worse results by moving the attention to lower layer closer to the input embedding. We also enable a second attention layer while fixing the first one in the 10th layer. The corresponding results are shown in <ref type="figure">Figure 4 (bottom)</ref>. Similarly, SRU++ achieves worse results if the attention is added to one of the lower layers. In contrast, results are comparable once the attention is placed in a highenough layer. These observations suggest that the model should first learn local features before attention plays a most effective role at capturing longrange dependencies. More analyses can be found in Appendix A.</p><p>Does the ratio d : d matter? Transformer models by default use a FFN dimension that is 4 times larger than the attention dimension <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref>. We analyze the ratio of recurrence dimension d to attention dimension d for SRU++. A small value of d can reduce the amount of computation and the number of parameters used in attention layers but may limit the modeling capacity. <ref type="table" target="#tab_9">Table 4</ref> compares the results of using different d : d ratio given a similar amount of model parameters. We fix the model size to around 108M and use 10 SRU++ layers. Changing this ratio from 4 to a higher value gives better result. The best dev result is obtained with a ratio of 8.</p><p>Given this observation, we report SRU++ result using a default ratio of 4 as well as a ratio of 8 in the subsequent result sections. This ensures we conduct a comparison that uses a setup similarly to the default of Transformer models, but also showcases stronger results SRU++ can achieve.</p><p>ENWIK8 <ref type="table" target="#tab_8">Table 3</ref>      Inference speed <ref type="table" target="#tab_13">Table 7</ref> compares the inference speed of SRU++ with other top-performing models on WIKI-103 test set. We use a single V100 GPU for inference. Our large model runs at least 4.5x faster than all baseline models except Shortformer <ref type="bibr" target="#b39">(Press et al., 2021)</ref>. In addition, our model achieves 0.9-1.1 perplexity lower than Shortformer and runs 50% faster when using 2 attention layers (k = 5).</p><p>IWSLT Does SRU++ work well for other tasks? We study this question by evaluating SRU++ on the IWSLT'14 De?En translation task.</p><p>We use the open-sourced training and evaluation code of <ref type="bibr" target="#b31">Lin et al. (2020)</ref>. The base model is an 8-layer Transformer model containing 20M parameters. We train SRU++ models using 6 layers and d = 1024, resulting in similar number of parameters. We use the original settings such as learning rate and batch size, except that we use RAdam optimizer for consistency and increase the number of training epochs to 50. Both architectures achieve much higher BLEU scores given more training epochs. 3 <ref type="table" target="#tab_14">Table 8</ref> presents the test results. Without additional hyperparameter tuning, SRU++ achieves 0.4 BLEU score higher and less training time compared to the Transformer model tuned in <ref type="bibr" target="#b31">Lin et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why does SRU++ reduce training cost in our experiments?</head><p>Several factors contribute to the computation reduction observed in our experiments. First, combining attention and recurrence gives stronger modeling capacity. As shown in our experiments, SRU++ often achieves comparable results using fewer layers and/or fewer parameters. The required computation are much lower for shallower and smaller models. We also observe higher training efficiency, requiring fewer training steps and smaller training batch compared to several Transformer models. <ref type="bibr">3</ref>  <ref type="bibr" target="#b31">Lin et al. (2020)</ref> reports a test BLEU of 35.2. We obtain 35.9 for the same Transformer model by training longer.</p><p>For example, SRU++ uses a maximum effective batch size of 98K tokens and 800K training steps on the BILLION WORD dataset, while the Transformer model in comparison <ref type="bibr" target="#b2">(Baevski and Auli, 2019)</ref> uses 128K tokens and near 1000K steps. The reduced batch size and gradient updates cut down the training cost.</p><p>Finally, model implementation is an important factor for computation saving. Our implementation is highly efficient for two reasons. First, the fast recurrence operation of SRU is a reusable module that is already optimized for speed <ref type="bibr" target="#b30">(Lei et al., 2018)</ref>. Second, since recurrence encodes positional information, we can use simple singlehead attention and remove positional encoding.</p><p>On the contrary, advanced attention and positional encoding mechanism can generate nontrivial computation overhead. To see this, we measure the running time of SRU++ and Transformer-XL using Pytorch Profiler. <ref type="figure" target="#fig_1">Figure 5 (a)</ref> shows the average model forward time of a single batch. SRU++ runs 4-5x times faster compared to the Transformer-XL implementation. <ref type="figure" target="#fig_1">Figure 5 (b)</ref> breaks down the computation and highlights the most time-consuming operations in both models. The matrix multiplications are one of the most expensive operations for both models. Surprisingly, many operations in the relative attention of Transformer-XL are computationally expensive. For example, the relative attention requires shifting the attention scores and adding up different attention score matrices. Both require a lot of time but they are not needed in non-relative attention. In addition, the last column shows the running time of tensor transpose operators needed by batch matrix-matrix multiplications in attention. Again, the relative attention uses an order of magnitude more time compared to the simple single-head attention used in our model implementation. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Accelerating common architectures for NLP has become an increasingly important research topic recently <ref type="bibr" target="#b49">(Tay et al., 2020;</ref><ref type="bibr" target="#b48">Sun et al., 2020;</ref>. Our work is closely related to two lines of research under this topic. <ref type="bibr">4</ref> Note that this high latency of tensor transpose might be caused by sub-optimal implementation choices such as a poor arrangement of tensor axes in the open-sourced model. There is room for improvement. Nevertheless, relative attention and positional encoding are reported to be non-trivially slower in other works <ref type="bibr" target="#b44">(Shaw et al., 2018;</ref><ref type="bibr" target="#b50">Tian et al., 2021)</ref>.   We use a single GPU for profiling to avoid extra overhead such as data synchronization between GPUs. We use an unroll size / context length M = 512 and 1024 respectively for small and large models. All models use a batch size B = 16 for profiling.</p><p>First, previous works have tackled the speed problem of recurrent neural networks (RNNs) and have proposed various fast RNN implementations <ref type="bibr" target="#b11">(Diamos et al., 2016;</ref><ref type="bibr" target="#b7">Campos et al., 2018;</ref><ref type="bibr" target="#b57">Zhang and Sennrich, 2019)</ref>. Notably, the Quasi-RNN  and SRU <ref type="bibr" target="#b30">(Lei et al., 2018)</ref> have invented highly-parallelizable recurrence and combined them with convolutions or highway networks respectively. The resulting architectures achieve equivalent parallelism as convolutional and attention models. This advancement eliminates the need of avoiding recurrence computation to trade model training efficiency, a design choice made by the Transformer architecture. Our model builds on top of SRU.</p><p>Second, several recent works have argued that using attention alone is not the best architecture in terms of model expressiveness. For example, <ref type="bibr" target="#b12">Dong et al. (2021)</ref> demonstrate theoretically and empirically that using pure attention results in performance degeneration. <ref type="bibr" target="#b16">Gulati et al. (2020)</ref> have combined convolution and attention and obtained new state-of-the-art results for speech recognition. Moreover, RNNs have been incorporated into Transformer architectures, resulting in improved results in machine translation and language understanding tasks <ref type="bibr" target="#b30">(Lei et al., 2018;</ref><ref type="bibr">Huang et al., 2020)</ref>. Our work is built upon a similar hypothesis that recurrence and attention are complementary at sequence modeling. We demonstrate that jointly leveraging fast recurrence and attention not only achieves state-of-the-art modeling results but also obtain significant computation reduction.</p><p>Being orthogonal to our work, many recent works improve the efficiency of Transformer mod-els by accelerating attention computation <ref type="bibr" target="#b56">(Zaheer et al., 2020;</ref><ref type="bibr">Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b52">Vyas et al., 2020;</ref><ref type="bibr" target="#b38">Peng et al., 2021)</ref>. Examples include Longformer <ref type="bibr" target="#b4">(Beltagy et al., 2020)</ref>, Reformer <ref type="bibr" target="#b27">(Kitaev et al., 2020)</ref>, Linformer  and Routing Transformer . In contrast, our work optimizes computational efficiency using recurrence combined with minimal attention and our model can incorporate these attention variants for additional speed improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a highly-efficient architecture combining fast recurrence and attention, and evaluate its effectiveness on various language modeling datasets. We demonstrate fast RNNs with little attention not only achieve top results but also reduce training cost significantly. Our work shares a different idea to accelerating attention, therefore providing an orthogonal direction to advancing stateof-the-art model architecture. As future work, we believe the model can be improved using stronger attention or recurrent implementations, better normalization or optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional results</head><p>A.1 Detailed analysis of attention <ref type="table" target="#tab_0">Table 10</ref> presents a more comprehensive analysis of attention in SRU++ models. First, we change the number of attention layers and their locations in the model. As shown in the top block of Table 10, using attention in 50% of the layers leads to no (or negligible) loss in model performance. This is consistent with the results in <ref type="table" target="#tab_4">Table 2</ref> using a smaller model. Enabling attention in higher layers performs slightly better than evenly distributing attention from the bottom to top layers.</p><p>We also experiment with using more than one attention head in each of the attention layer, as shown in the middle block of the table. Unlike Transformer models however, we do not observe a significant improvement using multiple heads. We hypothesize that the recurrence states can already carry different features or information that are present in different input positions, making redundant heads unnecessary.</p><p>Finally, changing the ratio d : d from 4 to 8 gives similar improvements regardless of using 2 attention layers or 10 attention layers. This suggests that the amount of attention and the hidden size ratio can be tuned independently for best model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The effectiveness of layer normalization</head><p>In our experiments, we have always used layer normalization to stabilize training. However, we also found layer normalization to achieve worse generalization for larger models that are more prone to over-fitting. <ref type="figure">Figure 6</ref> showcases our empirical observation on the ENWIK8 dataset. Using layer normalization achieves more rapid training progress and lower training loss, but results in higher dev loss in the case of training a 108M model. This generalization gap remains even if we tune the dropout rate carefully. In addition, although using layer normalization in the smaller model with 41M parameters gives slightly better dev results, we still observe a larger generalization gap (indicated by the difference between training loss and dev loss) compared to the run without layer normalization. Similar over-fitting patterns are observed on Wiki-103 dataset, and also in previous work <ref type="bibr" target="#b55">(Xu et al., 2019)</ref>.</p><p>On the other hand, turning off layer normalization can achieve better generalization but makes training sensitive to learning rate and parameter initialization. For example, we have to use a smaller learning rate of 0.00025 or lower to avoid sudden gradient explosion during training. These results suggest possible future work by improving the normalization method <ref type="bibr" target="#b45">(Shen et al., 2020;</ref><ref type="bibr" target="#b6">Brock et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Tuning weight decay and learning rate</head><p>We find that tuning the weight decay and learning rate critical to the success of training SRU++ and achieving best results. <ref type="table" target="#tab_17">Table 9</ref> provides a sensitivity analysis by testing different learning rates and weight decay values. Increasing the weight decay consistently gives better results for all learning rates tested. Tuning the learning rate is also needed to reach the best result. The non-trivial effect of weight decay seems to be unique for SRU++.</p><p>On the other hand, the performance of SRU++ remains robust once the appropriate weight decay and learning rate are set. As shown in previous results and analyses, SRU++ achieves strong and relatively stable results to various hidden sizes, number of attention layers and datasets. In particular, using the same weight decay value generalize well for all datasets (including language modeling and translation tasks) and model configurations tested. 0.10 0.01 0.00 3 ? 10 ?4 1.014 --2 ? 10 ?4 1.022 1.035 1.047 1.5 ? 10 ?4 1.030 1.038 1.040  <ref type="table" target="#tab_0">Table 11</ref> shows the detailed training configuration of SRU++ models on ENWIK8 dataset. Most training options are kept the same for all models. We tune the dropout probability more carefully as we found training is more prone to over-fitting and under-fitting for this dataset. The large model is trained with 2x batch size. As a result, we increase the learning rate proportionally by a factor of ? 2 <ref type="bibr" target="#b19">(Hoffer et al., 2017)</ref>, which results in a rounded learning rate of 0.0004. <ref type="table" target="#tab_0">Table 12</ref> presents the detailed training configuration on WIKI-103 dataset. Similarly we use d = 3072 and d = 4096 for the base and large model respectively for a hidden size ratio d : d = 4 : 1. Following <ref type="bibr" target="#b2">(Baevski and Auli, 2019)</ref>, we use an adaptive word embedding layer and an adaptive softmax layer for our models, and we tie the weight matrices of the two layers. We keep the total number of parameters comparable when we use a different hidden size ratio d : d = 8 : 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine translation</head><p>We use the open-sourced code from <ref type="bibr" target="#b31">Lin et al. (2020)</ref> for the IWSLT'14 De?En translation task. The Transformer model tuned by the original work uses 8 layers for both the encoder and decoder and a total of 20M parameters. Most of the training configuration remains the same as the original work 6 , except for a couple of changes. First, we use RAdam optimizer and the same ? values for consistency with the language model task. We use the same weight decay value of 0.1 for SRU++. The Transformer model uses a weight decay of 0 that is tuned based on dev set performance. Second, we increase the number of training epochs to 50 (or equivalently 64K training steps) since all models achieve better BLEU scores by training longer. This ensures we compare models when they reach the maximum performance.</p><p>Our SRU++ model uses a hidden size d = 1024, an attention size d = 256 and 6 layers for the encoder and decoder, resulting in a similar number of parameters as the Transformer model in comparison. Let X src be the output representation of the SRU++ encoder. Each SRU++ decoder layer make uses of X src by simplying treating it as extra attention context. That is, the query, key and value 6 https://github.com/asappresearch/ imitkd/blob/master/configs/iwslt/ teacher.yaml representations are computed by concatenating the input of the current layer X tgt with X src ,</p><formula xml:id="formula_9">Q = [Q src , Q tgt ] = W q [X src , X tgt ] K = W k Q V = W v Q</formula><p>The resulting representations Q tgt , K and V are used for the rest of the attention computation. The attention mask is set such that each target token can only attend to all source tokens and preceding target tokens.</p><p>Layers that has attention Num of heads  108M parameters <ref type="figure">Figure 6</ref>: Understanding the empirical effect of layer normalization. We show the training and dev loss of SRU++ models using 41M parameters and 108M parameters on ENWIK8 dataset. The model with layer normalization fits the training data better, but achieves worse generalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of SRU and SRU++ networks: (a) the original SRU, (b) the SRU variant with projection to reduce the number of parameters, experimented in Lei et al. (2018) and (c) SRU++ proposed in this work. Numbers indicate the dimension of intermediate inputs/outputs given hidden size d = 2048 and attention size d = 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Profiling of SRU++ and Transformer-XL: (a) forward time (in milliseconds) of small and large models and (b) forward time used in various types of time-consuming operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Effective GPU</cell><cell>Transformer-XL</cell><cell>SRU++</cell><cell>SRU++ (single</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hour</cell><cell></cell><cell></cell><cell>attention)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1.520</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>1.407</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>1.363</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11</cell><cell>1.324</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>14</cell><cell>1.308</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>18</cell><cell>1.284</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>22</cell><cell>1.278</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell>1.260</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>29</cell><cell>1.250</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>32</cell><cell>1.240</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>36</cell><cell>1.239</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell>1.226</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>43 47</cell><cell>1.222 1.214</cell><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell></row><row><cell>50 54 58 61 65 68 72 76 79 83 86</cell><cell>1.209 1.201 1.196 1.197 1.194 1.190 1.188 1.181 1.181 1.175 1.170</cell><cell></cell><cell cols="2">Bits Per Character (BPC)</cell><cell>1.2 1.3</cell><cell></cell><cell></cell><cell>Transformer-XL SRU++ SRU++ (single attention)</cell></row><row><cell>90 94</cell><cell>1.174 1.167</cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>97</cell><cell>1.166</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell>101</cell><cell>1.162</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>104</cell><cell>1.158</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Effective training hours</cell></row><row><cell>108</cell><cell>1.157</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>112</cell><cell>1.155</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>115</cell><cell>1.151</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>119</cell><cell>1.150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>122</cell><cell>1.148</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>126</cell><cell>1.144</cell><cell></cell><cell></cell><cell cols="2">1.50</cell><cell></cell><cell></cell></row><row><cell>130</cell><cell>1.147</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>133 137 140 144 148 151 155 158 162 166</cell><cell>1.142 1.140 1.139 1.138 1.133 1.129 1.134 1.127 1.130 1.128</cell><cell></cell><cell>Bits Per Character (BPC)</cell><cell cols="2">1.17 1.33</cell><cell></cell><cell>5.1x efficiency</cell><cell>Transformer-XL SRU++ (single attention)</cell><cell>8.7x efficiency</cell></row><row><cell>169</cell><cell>1.122</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>173</cell><cell>1.121</cell><cell></cell><cell></cell><cell cols="2">1.00</cell><cell></cell><cell></cell></row><row><cell>176</cell><cell>1.125</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell>180 184</cell><cell>1.121 1.120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Effective training hours</cell></row><row><cell>187</cell><cell>1.120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>191</cell><cell>1.118</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>194</cell><cell>1.114</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>198 202</cell><cell>1.112 1.114</cell><cell></cell><cell></cell><cell></cell><cell>1.50</cell><cell></cell><cell></cell></row><row><cell>205 209 212 216 220 223 227 230 234 237 241</cell><cell>1.113 1.110 1.111 1.107 1.109 1.108 1.108 1.106 1.104 1.102 1.104</cell><cell></cell><cell>Bits Per Character (BPC)</cell><cell></cell><cell>1.17 1.33</cell><cell></cell><cell>5.1x efficiency</cell><cell>Transformer-XL SRU++ (single attention) 8.7x efficiency</cell><cell>1.09 1.17</cell></row><row><cell>245 248</cell><cell>1.098 1.102</cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell>252</cell><cell>1.099</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>255</cell><cell>1.099</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Effective training hours</cell></row><row><cell>259</cell><cell>1.099</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>263</cell><cell>1.100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>266</cell><cell>1.098</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>270</cell><cell>1.100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>273</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>277</cell><cell>1.096</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>281</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>284</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>288</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>291</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>295</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>299</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>302</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>306</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>309</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>313</cell><cell>1.093</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>317</cell><cell>1.093</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>320</cell><cell>1.094</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>324</cell><cell>1.094</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>327</cell><cell>1.093</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Batch size B ? M BPC ?</figDesc><table><row><cell>Trans-XL</cell><cell>24?512</cell><cell>1.06</cell></row><row><cell>SRU++</cell><cell>24?512</cell><cell>1.03</cell></row><row><cell>SRU++</cell><cell>16?768</cell><cell>1.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Test BPC of SRU++ and Transformer-XL on ENWIK8 dataset. We train SRU++ using the same setting as Transformer-XL base model. Numbers are smaller the better. B is the number of sequence. M is the unroll size (and additional context size).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of SRU++ on ENWIK8 by enabling attention every k layers. We adjust the hidden size so the number of parameters are comparable. ? indicates mixed precision training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Dev BPC vs. total GPU hours used on EN-WIK8 for each model. Using automatic mixed precision (amp) and only one attention sub-layer achieves 16x reduction. To compute the dev BPC, the maximum attention length is the same as the unroll size M during training.</figDesc><table><row><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bits Per Character (BPC)</cell><cell>1.2 1.3</cell><cell></cell><cell cols="3">Transformer-XL SRU++ SRU++ (k=10, mixed precision)</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell cols="2">Figure 3:</cell><cell></cell><cell>Effective training hours</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>on EN-WIK8 dataset 2 . Their base model consists of 41M parameters and 12 Transformer layers. Following the official instructions, we reproduced the reported test BPC of 1.06 by training with 4 Nvidia 2080 Ti GPUs. The training took about 4 days or a total of 360 GPU hours equivalently.We train a 10-layer SRU++ model with 42M parameters. For a fair comparison, we use the same hyperparameter setting including the effective batch size, attention context length, learning rate and the number of training iterations as the Transformer-XL base model. Notably, our base model can be trained using 2 GPUs due to less GPU memory usage. After training, we set the at-</figDesc><table><row><cell></cell><cell>42M model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dev BPC</cell><cell>Test BPC</cell><cell>1.173</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dev BPC</cell></row><row><cell>10</cell><cell>dev_bpc=1.072 test_bpc=1.053</cell><cell>dev_bpc=1.053 test_bpc=1.032</cell><cell>1.053</cell><cell>1.032</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>dev_bpc=1.074 test_bpc=1.055</cell><cell>dev_bpc=1.056 test_bpc=1.034</cell><cell>1.056</cell><cell>1.034</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>dev_bpc=1.073 test_bpc=1.053</cell><cell>dev_bpc=1.056 test_bpc=1.033</cell><cell>1.056</cell><cell>1.033</cell><cell></cell><cell cols="10">1.074 1.067 1.064 1.061 1.058 1.057 1.056 1.056 1.053</cell></row><row><cell>7</cell><cell>dev_bpc=1.074 test_bpc=1.054</cell><cell>dev_bpc=1.057 test_bpc=1.035</cell><cell>1.057</cell><cell>1.035</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>dev_bpc=1.074 test_bpc=1.055</cell><cell>dev_bpc=1.058 test_bpc=1.036</cell><cell>1.058</cell><cell>1.036</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>dev_bpc=1.077 test_bpc=1.057</cell><cell>dev_bpc=1.061 test_bpc=1.040</cell><cell>1.061</cell><cell>1.040</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>dev_bpc=1.079 test_bpc=1.059</cell><cell>dev_bpc=1.064 test_bpc=1.043</cell><cell>1.064</cell><cell>1.043</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell></cell><cell>6</cell><cell>7</cell><cell></cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>3</cell><cell>dev_bpc=1.081 test_bpc=1.062</cell><cell>dev_bpc=1.067 test_bpc=1.048</cell><cell>1.067</cell><cell>1.048</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>dev_bpc=1.085 test_bpc=1.068</cell><cell>dev_bpc=1.074 test_bpc=1.056</cell><cell>1.074</cell><cell>1.056</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>dev_bpc=1.174 test_bpc=1.177</cell><cell>dev_bpc=1.173 test_bpc=1.176</cell><cell>1.173</cell><cell>1.176</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>dev_bpc=1.176 test_bpc=1.184</cell><cell>dev_bpc=1.176 test_bpc=1.184</cell><cell></cell><cell></cell><cell>1.056</cell><cell></cell><cell>1.054</cell><cell>1.054</cell><cell cols="2">1.053</cell><cell></cell><cell>1.050</cell><cell></cell><cell>1.049</cell><cell>1.050</cell><cell>1.050</cell></row><row><cell>9</cell><cell>dev_bpc=1.069 test_bpc=1.048</cell><cell>dev_bpc=1.050 test_bpc=1.026</cell><cell>1.050</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>dev_bpc=1.070 test_bpc=1.048</cell><cell>dev_bpc=1.050 test_bpc=1.025</cell><cell>1.050</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>dev_bpc=1.069 test_bpc=1.048</cell><cell>dev_bpc=1.049 test_bpc=1.026</cell><cell>1.049</cell><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell><cell>5</cell><cell></cell><cell>6</cell><cell></cell><cell>7</cell><cell></cell><cell>8</cell><cell>9</cell></row><row><cell>6 5</cell><cell>dev_bpc=1.070 test_bpc=1.049 dev_bpc=1.073 test_bpc=1.053</cell><cell>dev_bpc=1.050 test_bpc=1.026 dev_bpc=1.053 test_bpc=1.032</cell><cell>1.050 1.053</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Location of the attention</cell><cell></cell></row><row><cell>4</cell><cell>dev_bpc=1.073 test_bpc=1.053</cell><cell>dev_bpc=1.054 test_bpc=1.032</cell><cell>1.054</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>dev_bpc=1.073 test_bpc=1.054</cell><cell>dev_bpc=1.054 test_bpc=1.033</cell><cell>1.054</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>dev_bpc=1.075 test_bpc=1.055</cell><cell>dev_bpc=1.056 test_bpc=1.035</cell><cell>1.056</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Which layer to put the 1 attention</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dev BPC Test BPC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell cols="4">2 https://github.com/kimiyoung/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">transformer-xl/tree/master/pytorch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>showcases the training efficiency of our model. SRU++ is Model Parameters ? Test BPC ? GPU days ? Longformer 30L (Beltagy et al.</figDesc><table><row><cell>, 2020)</cell><cell>102M</cell><cell>0.99</cell><cell>104  ?</cell></row><row><cell>All-attention network 36L (Sukhbaatar et al., 2019b)</cell><cell>114M</cell><cell>0.98</cell><cell>64</cell></row><row><cell>Transformer-XL 24L (Dai et al., 2019)</cell><cell>277M</cell><cell>0.99</cell><cell>-</cell></row><row><cell>? Compressive memory (Rae et al., 2020)</cell><cell>-</cell><cell>0.97</cell><cell>-</cell></row><row><cell>Feedback Transformer (Fan et al., 2020)</cell><cell>77M</cell><cell>0.96</cell><cell>-</cell></row><row><cell>SRU++ Base</cell><cell>108M</cell><cell>0.97</cell><cell>6  ?</cell></row><row><cell>? only 2 attention layers (k = 5)</cell><cell>98M</cell><cell>0.98</cell><cell>4  ?</cell></row><row><cell>SRU++ Large</cell><cell>191M</cell><cell>0.96</cell><cell>12  ?</cell></row><row><cell>? d = 8 d</cell><cell>195M</cell><cell>0.95</cell><cell>13  ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Comparison with top-performing models on ENWIK8 dataset. We include the training cost (measured by the number of GPUs used ? the number of days) if it is reported in the previous work. Our results are obtained using an AWS p3dn instance with 8 V100 GPUs. The reported training time of all-attention network is based on V100 GPUs while the training time of Longformer is based on RTX8000 GPUs (which is about 90% speed of V100). ? indicates mixed precision training.</figDesc><table><row><cell cols="3">Ratio Dimensions d, d</cell><cell>Dev BPC ?</cell></row><row><cell>4</cell><cell>3072</cell><cell>768</cell><cell>0.997</cell></row><row><cell>6</cell><cell>3840</cell><cell>640</cell><cell>0.992</cell></row><row><cell>8</cell><cell>4480</cell><cell>560</cell><cell>0.991</cell></row><row><cell>10</cell><cell>5040</cell><cell>504</cell><cell>0.992</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Dev BPC on ENWIK8 by changing the ratio d : d in the SRU++ model while fixing the number of parameters to 108M.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Comparison with top-performing models on WIKI-103 dataset. We include the training cost (measured by the number of GPUs used ? the number of days) if it is reported in the previous work. The reported training costs are based on V100 GPUs. Our results are similarly obtained using an AWS p3dn instance with 8 V100 GPUs. ? indicates mixed precision training.</figDesc><table><row><cell>Model</cell><cell>Param</cell><cell cols="2">PPL ? Days ?</cell></row><row><cell>Transformer</cell><cell>331M</cell><cell>25.6 25.2</cell><cell>57  ? 147  ?</cell></row><row><cell></cell><cell>465M</cell><cell>23.9</cell><cell>192  ?</cell></row><row><cell>SRU++</cell><cell>328M</cell><cell>25.1</cell><cell>36  ?</cell></row><row><cell>SRU++ (k = 5)</cell><cell>465M</cell><cell>23.5</cell><cell>63  ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Test perplexity and effective GPU days for training of SRU++ models and the Transformer models of Baevski and Auli (2019) on BILLION WORD dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Speed? PPL?</cell></row><row><cell>kNNLM (Khandelwal et al.)</cell><cell>145</cell><cell>15.8</cell></row><row><cell>Trans (Baevski and Auli)</cell><cell>2.5k</cell><cell>18.7</cell></row><row><cell>Trans-XL (Dai et al.)</cell><cell>3.2k</cell><cell>18.3</cell></row><row><cell>Shortformer (Press et al.)</cell><cell>15k</cell><cell>18.2</cell></row><row><cell>SRU++ Large</cell><cell>15k</cell><cell>17.1</cell></row><row><cell>SRU++ Large (k = 5)</cell><cell>22k</cell><cell>17.3</cell></row><row><cell>tion. To compare the computation efficiency we</cell><cell></cell><cell></cell></row><row><cell>report the effective GPU days -the number of</cell><cell></cell><cell></cell></row><row><cell>GPUs multiplied by the number of days needed</cell><cell></cell><cell></cell></row><row><cell>to finish training. Our base model achieves bet-</cell><cell></cell><cell></cell></row><row><cell>ter BPC and uses a fraction of the training cost</cell><cell></cell><cell></cell></row><row><cell>reported in previous work. Furthermore, our large</cell><cell></cell><cell></cell></row><row><cell>models achieve a new state-of-the-art result on this</cell><cell></cell><cell></cell></row><row><cell>dataset, reaching a test BPC of 0.96 when d = 4 d</cell><cell></cell><cell></cell></row><row><cell>and 0.95 when d = 8 d .</cell><cell></cell><cell></cell></row><row><cell>WIKI-103 Table 5 presents the result of SRU++</cell><cell></cell><cell></cell></row><row><cell>models and other top results on the WIKI-103</cell><cell></cell><cell></cell></row><row><cell>dataset. We train one base model with 148M pa-</cell><cell></cell><cell></cell></row><row><cell>rameters and a few large models which contain</cell><cell></cell><cell></cell></row><row><cell>about 230M parameters. As shown in the table,</cell><cell></cell><cell></cell></row><row><cell>our base model obtains a test perplexity of 18.3</cell><cell></cell><cell></cell></row><row><cell>using 8 GPU days of training, about 3x reduction</cell><cell></cell><cell></cell></row><row><cell>compared to the Transformer model in Baevski</cell><cell></cell><cell></cell></row><row><cell>and Auli (2019) and over 10x reduction com-</cell><cell></cell><cell></cell></row><row><cell>pared to Feedback Transformer (Fan et al., 2020).</cell><cell></cell><cell></cell></row><row><cell>Again, changing the hidden size ratio to d = 8 d</cell><cell></cell><cell></cell></row><row><cell>improves the modeling capacity. Our big model</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Inference speed (tokens/second) on WIKI-103</cell></row><row><cell>test set. Results of baselines are taken from Press et al.</cell></row><row><cell>(2021). We use a single V100 GPU, a batch size of 1</cell></row><row><cell>and maximum attention length 2560 for consistency.</cell></row><row><cell>achieves a test perplexity of 17.1. The required</cell></row><row><cell>training cost remains significantly lower.</cell></row><row><cell>BILLION WORD We double our training itera-</cell></row><row><cell>tions to 800K and use a learning rate of 0.0002 for</cell></row><row><cell>the BILLION WORD dataset. We train a base model</cell></row><row><cell>using d = 4096, d = 1024 and an effective batch</cell></row><row><cell>size of 65K tokens per gradient update. We also</cell></row><row><cell>train a large model by increasing the hidden size</cell></row><row><cell>d to 7616 and the batch size to 98K. In addition,</cell></row><row><cell>we use only 2 attention layers (k = 5) for the large</cell></row><row><cell>model. Table 6 reports the test perplexity and asso-</cell></row><row><cell>ciated training cost. Our base and large model ob-</cell></row><row><cell>tain a test perplexity of 25.1 and 23.5 respectively,</cell></row><row><cell>outperforming the Transformer model of Baevski</cell></row><row><cell>and Auli (2019) given similar model size. More-</cell></row><row><cell>over, SRU++ achieves 3-4x training cost reduction</cell></row><row><cell>and is trained using 8 GPUs. In comparison, the</cell></row><row><cell>Transformer model uses 32 or 64 V100 GPUs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Results on IWSLT'14 De?En test set. We use a beam size of 5. BLEU scores and training time are averaged over 4 independent runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>42M parameters</cell><cell>139M parameters</cell></row><row><cell>SRU++</cell><cell>69.8</cell><cell>223.3</cell></row><row><cell>Transformer-XL</cell><cell>284.6</cell><cell>1175.6</cell></row><row><cell>1,200</cell><cell></cell><cell></cell><cell>1175.6</cell></row><row><cell>900</cell><cell cols="2">SRU++ Transformer-XL</cell></row><row><cell>600</cell><cell></cell><cell></cell></row><row><cell>300</cell><cell></cell><cell>284.6</cell><cell>223.3</cell></row><row><cell></cell><cell>69.8</cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">41M parameters</cell><cell>139M parameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>MatMul</cell><cell>Mem concat</cell><cell>Layernorm</cell><cell>Recurrence</cell><cell>Rel. position:</cell><cell>Rel. position:</cell><cell>Transpose</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>score shift</cell><cell>score addition</cell><cell></cell></row><row><cell>SRU++</cell><cell>130.7</cell><cell>15.9</cell><cell>3.3</cell><cell>23.2</cell><cell></cell><cell>9.3</cell></row><row><cell>Transformer-XL</cell><cell>160.4</cell><cell>5</cell><cell>15</cell><cell>64.6</cell><cell>269.1</cell><cell>255.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Dev BPC of SRU++ given a learning rate ? {1.5, 2, 3} ? 10 ?4 and a weight decay ? {0.1, 0.01, 0}. '-' means the training run diverged or got gradient explosion.</figDesc><table><row><cell>B Training details</cell></row><row><cell>Language modeling We use the RAdam opti-</cell></row><row><cell>mizer 5 with the default hyperparameters ? 1 = 0.9</cell></row><row><cell>and ? 2 = 0.999 for all our experiments. We use a</cell></row><row><cell>cosine learning rate schedule with only 1 cycle for</cell></row><row><cell>simplicity. For faster training, we also leverage the</cell></row><row><cell>native automatic mixed precision (AMP) training</cell></row><row><cell>and distributed data parallel (DDP) of Pytorch in</cell></row><row><cell>all experiments, except those in Table 1 and Fig-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Results of 10-layer SRU++ models by varying the attention setting. We report the dev BPC on the EN-WIK8 dataset. The first column indicates layers where the attention are located. Smaller index numbers represent layers that are closer to the input of the model.</figDesc><table><row><cell></cell><cell cols="2">w/o layernorm (train)</cell></row><row><cell>Wall time</cell><cell>Step</cell><cell>Value</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/LiyuanLucasLiu/ RAdam</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank ASAPP Inc. for making this work possible. We thank Hugh Perkins, Joshua Shapiro, Sam Bowman, Danqi Chen and Yu Zhang for providing invaluable feedback for this work. Finally, we thank Jeremy Wohlwend, Jing Pan, Prashant Sridhar and Kyu Han for helpful discussions, and ASAPP Language Technology and Infra teams for the compute cluster setup for our research experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Optimizing performance of recurrent neural networks on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01946</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quasi-Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?ctor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gir? I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Persistent rnns: Stashing recurrent weights on-chip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>The 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is not all you need: pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09402</idno>
		<title level="m">Accessing higher-level representations in sequential transformers with feedback memory</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-3015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference of the International Speech (INTERSPEECH)</title>
		<meeting>the 21st Annual Conference of the International Speech (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling recurrence for transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient inference for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Te</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsiu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chatsviorkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</title>
		<meeting>SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07000</idno>
		<title level="m">Ajay Mishra, and Bing Xiang. 2020. Trans-blstm: Transformer with bidirectional lstm for language understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The human knowledge compression contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://prize.hutter1.net/" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nikolaos Pappas, and Fran?ois Fleuret. 2020. Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From research to production and back: Ludicrously fast neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autoregressive knowledge distillation through imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?cero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Single headed attention rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11423</idno>
	</analytic>
	<monogr>
		<title level="m">Stop thinking with your head</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully neural network based speech recognition on mobile and embedded devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhwan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Boo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iksoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonyong</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shortformer: Better language modeling using shorter inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">2020. Green AI. Communications of the ACM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Optimizing speech recognition for the edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcgraw</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12408</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PowerNorm: Rethinking batch normalization in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Augmenting self-attention with persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01470</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mobile-BERT: a compact task-agnostic BERT for resourcelimited devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Shatter: An efficient transformer encoder with single-headed self-attention and relative sequence partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13032</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast transformers with clustered attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Angelos Katharopoulos, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Understanding and improving layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A lightweight recurrent network for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
