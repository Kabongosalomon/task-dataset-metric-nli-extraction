<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complementary Patch for Weakly Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochen</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyue</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
							<email>daiyuchao@nwpu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Complementary Patch for Weakly Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels has been greatly advanced by exploiting the outputs of Class Activation Map (CAM) to generate the pseudo labels for semantic segmentation. However, CAM merely discovers seeds from a small number of regions, which may be insufficient to serve as pseudo masks for semantic segmentation. In this paper, we formulate the expansion of object regions in CAM as an increase in information. From the perspective of information theory, we propose a novel Complementary Patch (CP) Representation and prove that the information of the sum of the CAMs by a pair of input images with complementary hidden (patched) parts, namely CP Pair, is greater than or equal to the information of the baseline CAM. Therefore, a CAM with more information related to object seeds can be obtained by narrowing down the gap between the sum of CAMs generated by the CP Pair and the original CAM. We propose a CP Network (CPN) implemented by a triplet network and three regularization functions. To further improve the quality of the CAMs, we propose a Pixel-Region Correlation Module (PRCM) to augment the contextual information by using object-region relations between the feature maps and the CAMs. Experimental results on the PAS-CAL VOC 2012 datasets show that our proposed method achieves a new state-of-the-art in WSSS, validating the effectiveness of our CP Representation and CPN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Thanks to the booming of deep learning methods, recent years have witnessed extraordinary progress in semantic segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. However, the prerequisite of a successful neural network for semantic segmentation is pixel-level segmentation ground-truth, which requires massive investments in manual annotation. Numerous efforts have been devoted to developing Weakly Supervised Semantic Segmentation (WSSS) to ease the pressure, which * Corresponding author ? = <ref type="figure">Figure 1</ref>. Illustration of our proposed method. The original CAM simply finds object seeds in most discriminative regions. To enlarge the seed areas, our Complementary Patch Network (CPN) uses a pair of images with CP regions (CP Pair) to generate two CAMs, the sum of which are supposed to incorporate more information of the foreground than the original CAM. aims to train a semantic segmentation network by using weaker supervision, such as image-level classification labels <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, bounding boxes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>, scribbles <ref type="bibr" target="#b26">[27]</ref> and points <ref type="bibr" target="#b3">[4]</ref>. Image-level labels, as the most convenientlyacquired annotation format, have been extensively studied in WSSS. In this work, we particularly focus on WSSS using image-level labels.</p><p>Most WSSS approaches generating initial seeds through image-level labels heavily rely on an efficient method-Class Activation Map (CAM) <ref type="bibr" target="#b44">[45]</ref>. Nevertheless, this architecture appears to be barely sensitive to the most discriminative regions, resulting in many incomplete foreground areas. To address the issue, a promising way is to erase or ignore some high response regions to help CAM 'see' more seeds in an image <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9]</ref>, i.e., region erasing or mining methods. However, these methods are more or less losing part of the regions of an image in each training epoch due to the randomness of the hiding process. It seems to be effective to intentionally cover the high response areas identified from the CAM in each training epoch, while such iterative operation introduces much computational complexity, and it is difficult to properly determine the number of iterations for each image as well.</p><p>In this paper, we show that CAM could explore more high response areas by taking full advantage of the information of an image, specifically including both uncov-ered and hidden parts. Based on the motivation, we treat the task of expanding object seeds in CAM as an increase in information, and develop a simple yet efficient concept-Complementary Patch (CP) Representation: the self-information of the CAM of an image is less than or equal to the sum of the self-information of the CAMs, which are obtained by CP Pair, namely two images with CP regions. Therefore, an improved CAM could be obtained by adding up the CAMs generated by the CP Pair (shown in <ref type="figure">Fig. 1</ref>). In addition, we show that the equality holds under two extreme cases. One is that if the patch size is too large, one of the CP Pair equals the original image, and the other is that two images in the CP Pair are almost the same for the network if the patch size is too small. Under these extreme conditions, the CP Pair is unable to seek out new seed areas compared to the original image. Thus the degree of the increase in information (object seeds) is subject to the patch size for the CP Pair.</p><p>Building upon the CP Representation, we propose a CP Network (CPN) to narrow down the gap between the improved CAM mentioned above and the one by the original image. CPN is formed by a triplet network with Triplet CP (TCP) loss and CP Cross Regularization (CPCR) loss, serving as minimizing the above discrepancy. For the generation of the CP Pair, we propose to use grid (Grid Patch) or superpixel (Super-pixel Patch) as the patch template. Furthermore, CPN introduces a Pixel-Region Correlation Module (PRCM), which aims to capture the relationship between the pixels and regions, and incorporates it with Pixel Correlation Module (PCM) <ref type="bibr" target="#b34">[35]</ref> to further improve the consistency of the predicted CAM.</p><p>Extensive experiments conducted on PASCAL VOC 2012 <ref type="bibr" target="#b12">[13]</ref> demonstrate the effectiveness of our CPN. As a result, our model yields a new state-of-the-art performance by 67.8% and 68.5% on the val set and test set. Furthermore, we notice that the performance of our CPN is influenced by the patch size, which is in accordance with our analysis of the CP Representation in extreme cases.</p><p>Our main contributions are summarized as three-fold:</p><p>? We propose a simple yet effective Complementary Patch (CP) representation to enlarge the seed regions in CAM, which narrows down the gap between the original CAM and the CAMs by summing up the CAMs of the CP pair.</p><p>? Building upon the CP representation, we present a triplet network (CPN) with Triplet CP (TCP) loss and CP Cross Regularization (CPCR). Moreover, a Pixel-Region Correlation Module (PRCM) is proposed to further refine the CAM.</p><p>? Experimental results on the PASCAL VOC 2012 show that our proposed framework achieves state-of-the-art performance in WSSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly supervised semantic segmentation As the most economic form in WSSS, image-level supervision has gained increasing attention from academia and industry. Recent advanced methods focus on modifying the seed areas produced by Class Activation Map (CAM) <ref type="bibr" target="#b44">[45]</ref>. The first category <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> is dedicated to pooling-based methods to overcome the drawbacks led by Global Max-Pooling (GMP) and Global Average-Pooling (GAP), which are used to aggregate score maps into a classification score. SPN <ref type="bibr" target="#b29">[30]</ref> proposes to regard super-pixel segmentation of the input image as the pooling module. The second category <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38]</ref> investigates the inter-pixel or semantic relationship to expand the seed areas or remove the wrong seeds <ref type="bibr" target="#b41">[42]</ref>. AffinityNet <ref type="bibr" target="#b1">[2]</ref> proposes to learn the similarity between pixels and applies Random Walk (RW) to further refine the seed areas. The third category concentrates on making efficient use of extra easily-obtained resources, including web images <ref type="bibr" target="#b17">[18]</ref>,videos <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> and saliency maps <ref type="bibr" target="#b36">[37]</ref>. The fourth category turns to region erasing or mining methods, aiming to mark out more object regions in CAM by erasing or mining some high response regions. Adversarial Erasing <ref type="bibr" target="#b35">[36]</ref> aims to explore more object seeds by iteratively erasing the discriminative regions detected by the original CAM from the image. However, it is hard to decide the exact number of iterations for each image. Attention-based Dropout Layer <ref type="bibr" target="#b8">[9]</ref> is a tool that highlights the potential points by thresholding the attention maps obtained from the feature maps. To expand the seed areas, FickleNet <ref type="bibr" target="#b23">[24]</ref> calculates the final score maps by randomly selecting the hidden units in the feature maps. As a data augmentation, Hide-and-Seek (HAS) <ref type="bibr" target="#b22">[23]</ref> enlarges the seed areas by randomly hiding grid patches in each image. Nevertheless, these hiding methods above are incapable of using the entire information in an image during each training epoch. To excavate full information in an image as much as possible, we propose Complementary Patch (CP) Representation and design the CPN to support CAM mine out more foreground seeds.</p><p>Self-attention model For the sake of improving the quality of segmentation masks, models based on selfattention <ref type="bibr" target="#b31">[32]</ref>, refining the feature maps by use of context feature, is widely employed in various segmentation networks. Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposes non-local block to produce an attention map by taking account of the correlation between each spatial point in the feature maps.</p><p>To further enrich the contextual information, DANet <ref type="bibr" target="#b16">[17]</ref> combines two self-attention modules, namely channel attention and spatial attention. Yuan et al. <ref type="bibr" target="#b39">[40]</ref> proposes the object-contextual representations to identify a pixel by using its corresponding object class, reinforcing the object contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CPN</head><formula xml:id="formula_0">CPN CPN ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework</head><p>Share weights <ref type="figure">Figure 2</ref>. The overall framework of our method. The whole structure of CPN is a triplet network with three branches, jointly feeding the original image (the black flow) and the CP Pair (the red and blue flows). PCM and the proposed PRCM collectively improve the quality of the original CAM to the refined CAM (RCAM). Finally, all outputs are constrained with three losses, which are L cls , Ltcp and Lcpcr. The dot (the red, blue, or black one) means that both outputs connected to it are leveraged in the following loss. During inference, the RCAM from the original image ( Y o) is used to predict the mask for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Complementary Patch Representation</head><p>Let us denote the CAM of an image I of size 3 ? H ? W as Y ? R C?H?W , where C refers to the number of objects (including background). The generation of Y typically begins with training a multi-label classification network, comprising a feature extractor layer, a Global Average-Pooling (GAP) layer, and a classification layer. Thus Y related to the c-th object, denoted as Y c , can be gained by:</p><formula xml:id="formula_1">Y c = (? c ) T f ,<label>(1)</label></formula><p>where f ? R C f ?H?W with C f channels is the feature maps from the final layer, and ? c ? R C f ?1 is the corresponding classifier weight of c-th class in the classification layer. Denote I with some hidden units as I h ? R 3?H?W and its counterpart with complementary hidden regions as I h ? R 3?H?W . Intuitively, we have I h +I h = I. For convenience, we name (I h , I h ) as the Complementary Patch Pair (CP Pair) of I. According to the previous experiments of the region and erasing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref>, both I h and I h may individually help CAM dig out more potential areas. However, some parts in I are apparently ignored if simply using I h or I h in each training epoch since each image could only be used once during one epoch. To make full use of the information in I, here we propose to use the CP Pair to find more seeds in Y c . For a partitioned image in <ref type="figure" target="#fig_0">Fig. 3</ref>, suppose I is split into N patches and there are two kinds of patch regions for the c-th class, it. We denote p c (x), x ? {???} as the probability function of finding c-th seeds in x. Higher p c (x) refers to more seeds in patch x of I. Therefore, we have</p><formula xml:id="formula_2">? = {A c i } Na i=1 and ? = {D c j } N d j=1 , where N a + N d = N .</formula><formula xml:id="formula_3">Na i=1 p c (x = A c i ) = 1 and p c (x = D c j ) = 0, j ? {1, 2, ..., N d }.</formula><p>Under the definition above, the self information of Y c is denoted as H(Y c ) and expressed as:</p><formula xml:id="formula_4">H(Y c ) = ? x??y log(p c (x)),<label>(2)</label></formula><p>where ? y ? ? refers to the set of A c in Y c . Note that the ground truth of the c-th class contains all patch A c , so it is said to have the maximum information. Our aim is to increase</p><formula xml:id="formula_5">H(Y c ) by increasing |? y |. Meanwhile, let H(Y c h ) and H(Y c h )</formula><p>be the information of the CAMs generated by the CP Pair, which can be jointly calculated as follows:</p><formula xml:id="formula_6">H(Y c q ) = ? x??q log(p c (x)), q ? {h, h},<label>(3)</label></formula><p>where</p><formula xml:id="formula_7">? h (? h ) ? ? is the set of A c in Y c h (Y c h )</formula><p>. Randomly covering parts of the object c in I leads to an increase of N ew A c . Unfortunately, it is undecidable to straightly</p><formula xml:id="formula_8">compare |? y | with |? h | or |? h |, because some discrimina- tive parts in ? y are possibly hidden in Y c h or Y c h . Due to the complementary attribution (? h ? ? h = ?), however, the sum of A c in Y c</formula><p>h and Y c h contains the original high response regions from the baseline CAM, and the N ew A c regions sought by the CP Pair. Therefore, we have:</p><formula xml:id="formula_9">? h ? ? h = ? y ? ? ? ,<label>(4)</label></formula><p>where ? ? ? ? refers to the set of N ew A c and ? ? ? ? y = ?. Note that ? ? = ? if one of the following extreme conditions holds: 1) I = I h or I = I h . One of the CP Pair is equal to the original image when the patch size equals the image size;</p><p>2</p><formula xml:id="formula_10">) Y c h ? Y c h .</formula><p>It is difficult for the classification net to discriminate the CP Pair if the patch size is too small, resulting in ? h = ? h = ? y .</p><p>Based on (4), we have that:</p><formula xml:id="formula_11">H(Y c h ) + H(Y c h ) = ? x?? h log(p c (x)) ? x?? h log(p c (x)) = ? x??y log(p c (x)) ? x?? ? log(p c (x)) ? H(Y c ).<label>(5)</label></formula><p>According to <ref type="bibr" target="#b4">(5)</ref>, it is concluded that, except for two extreme cases, the sum of CAMs by the CP Pair is able to find more foreground seeds than Y c . To achieve an improved CAM, we propose the CP regularization with a pair of chosen parameters ? ? [0, 1], ? = 1 ? ? as follows:</p><formula xml:id="formula_12">||(?Y c h + ?Y c h ) ? Y c || 1 .<label>(6)</label></formula><p>Denote the number of patches hidden in I h as N h . Then ? can be obtained by ? = 1 ? N h /N , meaning that the weight is decided by the quantity of uncovered pixels in I h .</p><p>To corporate (6) on the original classification net, we turn to a shared-weights triplet network as shown in <ref type="figure">Fig. 2</ref>, namely CP Network (CPN). One branch deals with input I and outputs Y c , while the other two branches respectively generate</p><formula xml:id="formula_13">(Y c h , Y c h ) made by the CP Pair. Note that we stop the gra- dient update for (Y c h , Y c h )</formula><p>to push Y c to approximate the better one. In this way, these three outputs are supposed to be regularized by (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Complementary Patch Strategies</head><p>Grid Patch <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44]</ref> is a common method that can be applied to generate a bunch of I h for I. Specifically, a grid patch with a fixed size of S ? S ? 3 can partition I into H ? W/(S ? S) patches. Then each patch hidden with a probability p h = 0.5 is fed into the classification net.</p><p>Following <ref type="bibr" target="#b22">[23]</ref>, S is evenly chosen from a set K of fixed numbers to fit the size of different objects. To guarantee the same data distribution between the training and the testing sets, the value of the hidden pixels is set to equal the mean RGB values of the images among the whole training sets.</p><p>On the other hand, the super-pixel region contains rich information about an image. Therefore, we also propose a Super-pixel Patch strategy, which uses the super-pixels generated by SLIC <ref type="bibr" target="#b15">[16]</ref> as the patches. Here the number of the super-pixels depends on a predefined segment number, denoted as S N . Note that we test these two patch strategies respectively in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Modules in CPN</head><p>We propose the CP Representation to help CAM find more foreground seeds. However, the regularization in <ref type="formula" target="#formula_12">(6)</ref> is incapable of sufficiently improving the CAM merely by using the output of the typical network. To further refine the original CAMs, <ref type="bibr" target="#b34">[35]</ref> proposes a modified self-attention module named Pixel Correlation Module (PCM), capturing contextual information by taking advantage of pixel relationships in feature maps. Here we take a brief introduction to the self-attention module <ref type="bibr" target="#b32">[33]</ref>, which can be normally expressed as:</p><formula xml:id="formula_14">Y out = ?(X in )J (X in ) HW i=1 HW j=1 J (X in ) ij + X in ,<label>(7)</label></formula><p>J (X in ) = e g(Xin) T ?(Xin) ,</p><p>where X in and Y out are respectively the input and output feature. J is used for measuring the relationship between the adjacent pixels, and ? provides a representation of each pixel in X in . Specially, ?, ? and g are implemented by is implemented by a 1 ? 1 convolution layer. Based on <ref type="formula" target="#formula_14">(7)</ref> and <ref type="formula" target="#formula_15">(8)</ref>, the PCM refines the CAM Y ? R C?HW (flattened into matrix formats) as:</p><formula xml:id="formula_16">Y pcm = Y J (X) HW i=1 HW j=1 J (X) ij ,<label>(9)</label></formula><formula xml:id="formula_17">J (X) = ReLU( g(X) T g(X) ||g(X)|| 1 2 ),<label>(10)</label></formula><p>where X ? R C1?HW is the aggregation of some features in the classification net, and J : R C1 ? R HW refers to the cosine distance to measure the inter-pixel feature similarity. Then we can obtain a refined CAM, denoted as Y pcm ? R C?H?W (reshaped from Y pcm ? R C?HW ). Object Contextual Representation (OCR) <ref type="bibr" target="#b39">[40]</ref> is an effective approach to augment the contextual information based on exploring object-pixel relation. Therefore, we propose a Pixel-Region Correlation Module (PRCM) to help further improve the CAMs. Firstly, an Object-Region  Relation matrix Z ? R C?C1 is represented by Z = SoftMax(Y )X T . Here we directly treat Y as soft object regions, which are supposed to be the coarse segmentation maps corresponding to C objects <ref type="bibr" target="#b39">[40]</ref>. Then we can obtain a Pixel-Region Relation P R ? R C?H?W (reshaped from P R ? R C?HW ) as:</p><formula xml:id="formula_18">P R = ?(Z)g(X),<label>(11)</label></formula><formula xml:id="formula_19">Y prcm = Y ? SoftMax(P R ),<label>(12)</label></formula><p>where ? : R C1 ? R C2 is also an embedding function similar to g. Since P R represents the relation between regions in X and pixels in Y <ref type="bibr" target="#b39">[40]</ref>, we strengthen Y by <ref type="bibr" target="#b11">(12)</ref> to gain the refined CAM, denoted as Y prcm ? R C?H?W . In order to combine the contextual information collected by PCM and PRCM, the final smoothed CAM, denoted as Y ? R C?H?W , is the sum of Y prcm and Y pcm . <ref type="figure" target="#fig_2">Fig. 4</ref> illustrates the structures of PCM and PRCM. Thus, a pair of output CAMs ( Y , Y ) can be readily generated for each branch in CPN. For convenience, we denote the CAMs in branch with I as ( Y o , Y o ), and the CAMs in branch with the CP Pair are respectively denoted as</p><formula xml:id="formula_20">( Y h , Y h ) and ( Y h , Y h ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss in CPN</head><p>Following conventions, an additional GAP layer is applied on the CAM, aggregating it into image-level prediction scores s ? R (C?1)?1 . Note that s simply contains C ? 1 foreground objects since the image-level supervision lacks the background label. Therefore, we can obtain the score maps generated from Y o , Y h and Y h ,which are respectively denoted as s o ,s h and s h . Then we employ Multi-Label Soft Margin Loss l cls for supervision:</p><formula xml:id="formula_21">L cls = 1 3 (l cls (s o ) + l cls (s h ) + l cls (s h )).<label>(13)</label></formula><p>Meanwhile, the CP Representation is adopted into the CPN for mining out more seeds, which is represented as:</p><formula xml:id="formula_22">L tcp =||(?Y h + ?Y h ) ? Y o || 1 + ||(? Y h + ? Y h ) ? Y o || 1 ,<label>(14)</label></formula><p>Here the Triplet CP (TCP) loss, denoted as L tcp , is proposed based on the regularization in <ref type="bibr" target="#b5">(6)</ref>. Note that there are six output CAMs in the CPN since each branch posses two of them. Consequently, the TCP loss builds a connection among these six CAMs. Similarly with <ref type="bibr" target="#b34">[35]</ref>, to address the problem that Y predicts all the pixels as the same class (mostly background), we introduce the CP Cross Regularization (CPCR) loss as:</p><formula xml:id="formula_23">L cpcr =||(Y o ? ?Y h ) ? ? Y h || 1 + ||(Y o ? ?Y h ) ? ? Y h || 1 ,<label>(15)</label></formula><p>Here we jointly regularize the refined CAMs by the CP Pair to make indirect effects on Y o . As for the example of regularization of ? Y h , it might be intuitive to regularize Y h by Y h . However, such direct regularization leads to a further degradation in our early experiments. Therefore, we use the gap between Y o and ?Y h to regularize Y h in light of <ref type="bibr" target="#b5">(6)</ref>. During the training, the background activation map is assessed by:</p><formula xml:id="formula_24">Y c=0 (x, y) = (1 ? max 1?c?C?1 Y c (x, y)) ? ,<label>(16)</label></formula><p>Here In all, the CPN is optimized by the final loss function L all <ref type="bibr" target="#b16">(17)</ref>, and empirically we set w 1 = w 2 = w 3 = 1. <ref type="figure">Fig.  2</ref> demonstrates the overall CPN framework.</p><formula xml:id="formula_25">L all = w 1 L cls + w 2 L tcp + w 3 L cpcr .<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Dataset and evaluated metric: The proposed approach is evaluated on the PASCAL VOC 2012 segmentation benchmark <ref type="bibr" target="#b12">[13]</ref>. There are 20 foreground object categories and 1 background annotated in the dataset. Following conventions, the number of training images is 10,582. The validation dataset contains 1,449 images and the test one has 1,456 samples. During the whole training process, only image-level annotation is provided. To measure the performance of all experiments, the mean Intersection-over-Union (mIoU) is used as the evaluation metric. Network settings: We adopt the ResNet38 <ref type="bibr" target="#b38">[39]</ref>, as one of the prevailing models in most WSSS frameworks, as the backbone of the CPN. The parameters trained on the Ima-geNet <ref type="bibr" target="#b10">[11]</ref> are used for the initialization of the CPN. Following the previous work, we remove the final GAP layer and fully-connected layer, and replace the last three convolution layers with the atrous convolutions with the adapted dilation rates, so that the output stride of the net is 8. According to <ref type="bibr" target="#b34">[35]</ref>, for the aggregated features X in PCM and PRCM, we firstly extract feature maps from stage 3 and 4, and then jointly decrease their channels into 64 and 128 by use of 1?1 convolution layers. Lastly, we concatenate these features and input images to form X. Training settings: Typical data augmentation on the training set: randomly scaling, color jittering, randomly cropping the images by 448 ? 448, and horizontal flip. The whole model implemented by Pytorch is trained on 1 RTX 3090 GPU with 24 GB memory. We take a mini-batch size of 4 images to train the CPN for 8 epochs. The initial learning rate is 0.01 and decreases by the poly policy with a decay power of 0.9. We leverage SGD Optimizer using weight decay 0.0005 with momentum 0.9. For each mini-batch, we sort the losses in L cpcr in descending order, and select the top 20% losses as the hard examples for training (Online Hard Example Mining (OHEM)) to further improve the performance. Similarly with the settings of <ref type="bibr" target="#b34">[35]</ref>, we block the gradients backpropagation stream from the PCM and PRCM to the network, to avoid the mutual interference of CAMs and the refined CAMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>In this section, we aim to certify the effectiveness of CPN. All experimental results are generated from VOC 2012 train set. For a fair comparison, the background score ? is the value that results in the best mIoU of the pseudo labels. Note that the patch strategy in Tab. 1-4 is Super-pixel Patch with S N =200. Improvements on CAM: To improve the performance of the final masks, it is a common way to aggregate prediction maps with different scales. Tab. 1 shows the mIoU of the segments using the baseline CAM, SEAM <ref type="bibr" target="#b34">[35]</ref> and our CPN under single-and multi-scale cases. The results show that our CPN presents superior mining ability than the baseline in all different scaling cases. In the multi-scale test, the CPN improves the mIoU over the baseline by nearly 10%. For SEAM, which is implemented by a siamese network with equivariant regularization, we adopt the hyperparameters that achieve the best performance in <ref type="bibr" target="#b34">[35]</ref>. By adding the PRCM, the new SEAM* outperforms the original one in all scale tests. Compared to SEAM, our framework achieves higher performance (57.43%) in the multi-scale test. <ref type="figure" target="#fig_4">Fig. 5</ref> shows several samples of the visualized CAM made by the baseline, SEAM and CPN. Compared to the baseline and SEAM, our CPN can help CAM seek more seeds in low response areas to generate a complete CAM for the foreground. However, for small objects (the last column in <ref type="figure" target="#fig_4">Fig. 5</ref>), it can be seen that the foreground seeds by CPN are over-segmented since it is indeed difficult to mine out accurate seeds for small objects without boundary. Effectiveness of regularization and PRCM: Tab. 2 illustrates the effect of every single module in our approach. Note that for the baseline method only l cls (s o ) is included in L cls since it lacks the CP Pair. Compared with the baseline, the L tcp and PCM improves the mIoU up to 51.08%. Benefit by L cpcr , the model further achieves a 4.63% improvement. By further applying OHEM to the L cpcr , the results achieves 56.58% mIoU on the VOC12 train set. Finally, the model achieves a 0.85% improvement after adopting the PRCM. Improvements on foreground localization: The CPN aims to improve the CAM by capturing more seeds related to the foreground objects. To verify the idea, we collect the mIoU of background and 20 foreground objects from the baseline, SEAM, and our CPN in Tab. 1. As shown in Tab. 3, compared to the baseline, SEAM achieves compelling mIoU elevation by 5.14% in the background and 7.68% in the foreground. In addition, the CPN improves the mIoU of the foreground up to 56.13%, which outperforms the baseline by 9.75% and the SEAM by 2.07%. The result validates that our CPN can find more foreground object regions than the baseline and the SEAM.   <ref type="table">Table 3</ref>. The mIoU (%) of the foreground objects (f.) and background (bg.) in different methods.</p><p>Patch size: Recall from that two patch strategies, namely Grid Patch and Super-pixel Patch, can both be applied on our CPN (Sec. 3.2). Note that both patch strategies are closely related to the patch size, and the total number of patches increases with the reduction of it.</p><p>For Super-pixel Patch, we explore the effect by simply changing S N , ranging from 5 to 8000. <ref type="figure">Fig. 6</ref> reports the mIoU of the results by the CPN with different S N . It shows that the mIoU firstly shows a rough increasing trend with the increase of S N , and reaches the peak point (57.43%) with S N = 200. Then the quality of CAM declines with higher S N , and arrives at the lowest point (55.79%) with S N = 8000. Note that lower S N causes the larger patch size, thus both too high and low patch size in Super-pixel Patch suppress the improvements on the CAM.</p><p>For Grid Patch, patch size S is evenly chosen from the set K of some fixed numbers. Thus we implement the experiment by changing the elements of K. We keep |K| = 2 and gradually increase the smaller element. Note that the input size of the image is 448. <ref type="figure" target="#fig_5">Fig. 7</ref> summaries the results. We notice that with the increase of the S, the mIoU follows a similar tendency to the one in the Super-pixel Patch.  1) and 2). Therefore, it is concluded that proper hidden patch size is crucial for significantly improving the performance of the CAM. Grid Patch vs Super-pixel Patch: To contrast the performance between the two strategies, we respectively calculate the average mIoU of the results in <ref type="figure" target="#fig_5">Fig. 7</ref> and <ref type="figure">Fig. 6</ref>. It is observed that CPN with the Super-pixel Patch strategy globally achieves better results (+0.43%) than the Grid Patch strategy. It is reasonable since the former one benefits from the pre-classification by the super-pixel. Besides, we test the average time consumption. The Super-pixel Patch (1.45 sec/per image) apparently consumes much more time than the Grid Patch (0.006 sec/per image), so the latter one could better meet some real-time operations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state of the art</head><p>To further improve our CAM, we use a common approach, namely Random Walk (RW) <ref type="bibr" target="#b1">[2]</ref>, to improve the mIoU of our pseudo labels generated by the CPN up to 67.79%. Following common practice, we then evaluate the quality of the final masks by using DeepLab <ref type="bibr" target="#b25">[26]</ref> with ResNet38 backbone. Note that post CRF refinement is used for the output maps. Tab. 5 gives a comparative overview concerning the previous methods. For all the approaches using ResNet38 backbone, our approach presents the state of the art performance on both PASCAL VOC 2012 val and test set, respectively scoring 67.8% and 68.5%. We also note that our results without applying CRF achieve better performance than MCIS <ref type="bibr" target="#b30">[31]</ref>. In addition, our method achieves better performance on test set than the ICD <ref type="bibr" target="#b13">[14]</ref>, which uses extra supervision labels. <ref type="figure" target="#fig_6">Fig. 8</ref> shows some samples of the final segmentation results, validating the effectiveness of our CPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a simple yet effective pipeline for weakly supervised semantic segmentation with Methods</p><p>Pub. Sup. Val Test *MCOF <ref type="bibr" target="#b33">[34]</ref> CVPR18 I + S 60.3 61.2 *SeeNet <ref type="bibr" target="#b18">[19]</ref> NIPS18 I + S 63.1 62.8 *DSRG <ref type="bibr" target="#b19">[20]</ref> CVPR18 only image-level labels provided. First, from the information theory perspective, we showed that the sum of CAMs generated by a pair of images with Complementary Patch regions (CP Pair) is able to mine out more foreground seeds. Then, based on this observation, we presented a CP Network (CPN) with a bunch of regularization to achieve an improved CAM. To further refine the results, we designed a Pixel-Region Correlation Module (PRCM) to bring more contextual information for the CAM. Extensive experiments on the PASCAL VOC 2012 dataset show that our proposed CPN achieves new state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>A c represents the patch region that contains the seeds of the object c, while D c covers no seeds related to ? ? ? Two kinds of patch regions in an image. A c refers to the region that covers the seeds related to object c, and D c contains no seeds of it. The baseline CAM (left) can merely recognize part of A c regions, while the new CAM (right) by the CP Pair can find N ew A c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The structures of PCM (the red stream) and proposed PRCM (the blue stream). The final refined CAM Y is the sum of Y pcm and Y prcm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Y c (x, y) is the activation value of category c at the position (x, y) in Y ? {Y o , Y h , Y h }), and ? is a hyperparameter for adjusting the confidence of background score, which empirically is set to 1. Y is firstly normalized by Y c (x, y) = Y c (x, y)/max x,y Y c (x, y), c ? [1, C ?1], and all scores irrelevant to ground truth are thresholding to 0. Finally we concatenate Y c=0 into Y c . During inference, Y o is used for segmentation, and Y c=0 o (x, y) is set to a fixed value ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Sample original images (a) and the corresponding visual results by different methods, which are the baseline (b), SEAM (c) and CPN (d). Our method can find more seeds that both the baseline and the SEAM are unable to dig out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The performance of the Grid Patch strategy with various patch sets K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results on PASCAL VOC 2012 val set. a) Input images. b) Ground-truth labels. c) Our segmentation results (w/ CRF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Experiments with single-and multi-scale tests on several methods. * marks the method uses our PRCM module. It is shown that the CPN boosts the overall performance of CAM in various scales, and achieves better CAM than SEAM and the baseline. In addition, our PRCM is effective for improving the results on different scales. Ablation studies on every part of our method. L * cpcr refers to Lcpcr with Online Hard Example Mining (OHEM)</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>1.0</cell><cell>Image scale 1.5</cell><cell>2.0</cell><cell>all</cell></row><row><cell>baseline</cell><cell cols="5">41.15 48.29 49.51 47.44 47.84</cell></row><row><cell cols="6">SEAM [35] 49.35 51.57 52.25 49.79 55.41</cell></row><row><cell>SEAM*</cell><cell cols="5">49.64 52.15 53.14 50.55 55.71</cell></row><row><cell>CPN</cell><cell cols="2">48.91 54.51</cell><cell cols="3">55.44 54.01 57.43</cell></row><row><cell>model</cell><cell></cell><cell></cell><cell></cell><cell cols="2">mIoU (%)</cell></row><row><cell cols="2">baseline (L cls )</cell><cell></cell><cell></cell><cell>47.84</cell><cell></cell></row><row><cell cols="3">baseline + Ltcp + PCM</cell><cell></cell><cell>51.08</cell><cell></cell></row><row><cell cols="4">baseline+Ltcp+ Lcpcr+PCM</cell><cell>55.71</cell><cell></cell></row><row><cell cols="2">baseline+Ltcp+ L  *</cell><cell cols="2">cpcr +PCM</cell><cell>56.58</cell><cell></cell></row><row><cell cols="2">baseline+Ltcp+ L  *</cell><cell cols="2">cpcr + PCM+PRCM</cell><cell>57.43</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The result achieves the best performance (57.07%) with K = {56, 112}, and reaches the bottom with K = {4, 7} and K = {224, 448}, respectively achieving 55.67% and 55.80% mIoU.Recall that two extreme conditions hold the equality of the CP representation (Sec. 3.1). The results in large and small patch size are exactly corresponding to the conditionFigure 6. The performance of the Super-pixel Patch strategy with different SN .</figDesc><table><row><cell cols="2">mIoU (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average mIoU</cell></row><row><cell>58.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>57.50 57.00</cell><cell cols="2">56.90</cell><cell cols="2">57.27 57.22</cell><cell cols="4">57.43 57.24 57.01</cell><cell>56.85</cell></row><row><cell>56.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">56.31</cell></row><row><cell>56.00</cell><cell>56.52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>55.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">55.79</cell></row><row><cell></cell><cell>5</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>1000</cell><cell>5000</cell><cell>8000</cell></row><row><cell cols="2">mIoU (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average mIoU</cell></row><row><cell>57.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>57.00</cell><cell></cell><cell></cell><cell></cell><cell>56.67</cell><cell cols="3">56.96 57.07</cell><cell>56.67</cell></row><row><cell>56.50</cell><cell></cell><cell cols="2">56.13</cell><cell></cell><cell></cell><cell>56.42</cell><cell></cell><cell></cell></row><row><cell>56.00</cell><cell>55.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.80</cell></row><row><cell>55.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>{4,7}</cell><cell cols="2">{7,14}</cell><cell cols="6">{14,28} {28,56} {56,112} {112,224} {224,448}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Hidden probability: Recall that the hidden regions in one of the CP Pair are randomly selected with p h = 0.5 (Sec. 3.2). Therefore, the number of the hidden patches in the CP Pair are expectedly equivalent. Here we aim to explore the relationship between the p h and our CPN. Due to the complementary attribution, we change p h from 0.1 to 0.5. Tab. 4 shows the CPN with p h = 0.5 achieves the best performance (57.43%) , and reaches the bottom (55.52%) with p h = 0.1. The result also validates the effect of the extreme condition 1) on our model since I h or I h is close to I as p h decreases. ) 55.52 56.87 56.29 57.05 57.<ref type="bibr" target="#b42">43</ref> The performance of our CPN with different hidden probabilities p h .</figDesc><table><row><cell>p h</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>mIoU(%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison with SOTA on VOC 2012 val and test in terms of mIoU (%). Methods marked by * use ResNet101 backbone, the others marked by ? use ResNet38. The supervision (Sup.) contains image-level label (I) and saliency maps (S).</figDesc><table><row><cell>I + S 61.4 63.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4253" to="4262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8991" to="9000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with boundary exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="347" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attentionbased dropout layer for weakly supervised single object localization and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4283" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10762" to="10769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghun</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7322" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frame-to-frame aggregation of active regions in web videos for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6808" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5208" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using superpixel pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwak</forename><surname>Suha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Seunghoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Bohyung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4111" to="4117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="347" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12275" to="12284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12765" to="12772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Causal intervention for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="655" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
