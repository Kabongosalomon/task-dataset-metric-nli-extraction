<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial Information Guided Convolution for Real-Time RGBD Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Zhuo</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<title level="a" type="main">Spatial Information Guided Convolution for Real-Time RGBD Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Spatial information</term>
					<term>Receptive field</term>
					<term>RGBD semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D spatial information is known to be beneficial to the semantic segmentation task. Most existing methods take 3D spatial data as an additional input, leading to a two-stream segmentation network that processes RGB and 3D spatial information separately. This solution greatly increases the inference time and severely limits its scope for real-time applications. To solve this problem, we propose Spatial information guided Convolution (S-Conv), which allows efficient RGB feature and 3D spatial information integration. S-Conv is competent to infer the sampling offset of the convolution kernel guided by the 3D spatial information, helping the convolutional layer adjust the receptive field and adapt to geometric transformations. S-Conv also incorporates geometric information into the feature learning process by generating spatially adaptive convolutional weights. The capability of perceiving geometry is largely enhanced without much affecting the amount of parameters and computational cost. Based on S-Conv, we further design a semantic segmentation network, called Spatial information Guided convolutional Network (SGNet), resulting in real-time inference and state-ofthe-art performance on NYUDv2 and SUNRGBD datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the development of 3D sensing technologies, RGBD data with spatial information (depth, 3D coordinates) is easily accessible. As a result, RGBD semantic segmentation for high-level scene understanding becomes extremely important, benefiting a wide range of applications such as automatic driving <ref type="bibr" target="#b0">[1]</ref>, SLAM <ref type="bibr" target="#b1">[2]</ref>, and robotics. Due to the effectiveness of Convolutional Neural Network (CNN) and additional spatial information, recent advances demonstrate enhanced performance on indoor scene segmentation tasks <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Nevertheless, there remains a significant challenge caused by the complexity of the environment and the extra efforts for considering spatial data, especially for applications that require real-time inference.</p><p>A common approach treats 3D spatial information as an additional input, followed by combining the features of RGB images to fuse multi-modal information <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref> (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). This approach achieves promising results at the cost of significantly increasing the parameter number and computational time, thus being unsuitable for real-time tasks. Meanwhile, several works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> encode raw spatial information into three channels (HHA) composed of horizontal <ref type="bibr">LZ</ref>    <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>. (b) The proposed SGNet. It can be seen that the approach in (a) largely increases parameter number and inference time due to processing spatial information, thus less suitable for real-time applications. We replace the convolution with our S-Conv in (b) where the kernel distribution and weights of the convolution are adaptive to the spatial information. S-Conv greatly enhances the spatial awareness of the network with few additional parameters and computations, thus can efficiently utilize spatial information. Best viewed in color. disparity, height above ground, and norm angle. However, the conversion from raw data to HHA is also time-consuming <ref type="bibr" target="#b8">[9]</ref>.</p><p>It is worth noting that indoor scenes have more complex spatial relations than outdoor scenes. This requires a stronger adaptive ability of the network to deal with geometric transformations. However, due to the fixed structure of the convolution kernel, the 2D convolution in the aforementioned methods cannot well adapt to spatial transformation and adjust the receptive field inherently, limiting the accuracy of semantic segmentation. Although alleviation can be made by revised pooling operation and prior data augmentation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, a better spatially adaptive sampling mechanism for conducting convolution is still desirable.</p><p>Moreover, the color and texture of objects in indoor scenes are not always representative <ref type="bibr" target="#b14">[15]</ref>. Instead, the geometry structure often plays a vital role in semantic segmentation. For example, to recognize the fridge and wall, the geometric structure is the primary cue due to the similar texture. However, such spatial information is ignored by 2D convolution on RGB data. The depth-aware convolution <ref type="bibr" target="#b15">[16]</ref> is proposed to address this problem. It forces pixels with similar depths as the center of the kernel to have higher weight than others.</p><p>Nevertheless, this prior is handcrafted and may lead to suboptimal results.</p><p>It can be seen that there is a contradiction between the fixed structure of 2D convolution and the varying spatial transformation, along with the efficiency bottleneck of separately processing RGB and spatial data. To overcome the limitations mentioned above, we propose a novel operation, called Spatial information guided Convolution(S-Conv), which adaptively changes according to the spatial information (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Specifically, this operation can generate convolution kernels with different sampling distributions adapting to spatial information, boosting the spatial adaptability and the receptive field regulation of the network. Furthermore, S-Conv establishes a link between the convolution weights and the underlying spatial relationship with their corresponding pixel, incorporating the geometric information into the convolution weights to better capture the spatial structure of the scene. Due to the input of spatial information in S-Conv, the scale and spatial transformation of objects can be directly analyzed to generate spatially adaptive offsets and weight.</p><p>The proposed S-Conv is light yet flexible and achieves significant performance improvements with only few additional parameters and computation costs, making it suitable for realtime applications. It can be seen as a novel and efficient method for multi-modal fusion task. Concretely, compared with other two-stream methods, we guide the convolution process by utilizing spatial information to achieve the purpose of multi-modal fusion. It performs better than other methods relying on two-stream network, and greatly reduces the amount of parameters and calculation compared with twostream methods, enabling real-time application. We conduct extensive experiments to demonstrate the effectiveness and efficiency of S-Conv. We first design the ablation study and compare S-Conv with two-stream methods, deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and depth-aware convolution <ref type="bibr" target="#b15">[16]</ref>, exhibiting the advantages of S-Conv. We also verify the applicability of S-Conv to spatial transformations by testing its influence on different types of spatial data with depth, HHA and 3D coordinates. We demonstrate that spatial information is more suitable to generate offset than RGB feature which is used by deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Finally, benefiting from the adaptability to spatial transformation and the effectiveness of perceiving spatial structure, our network equipped with S-Conv, named Spatial information Guided convolutional Network (SGNet), achieves high-quality results with realtime inference on NYUDv2 <ref type="bibr" target="#b16">[17]</ref> and SUNRGBD <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> datasets.</p><p>We highlight our contributions as follows:</p><p>? We propose a novel S-Conv operator that can adaptively adjust receptive field while effectively adapting to spatial transformation, and can perceive intricate geometric patterns with low cost. ? Based on S-Conv, we propose a new SGNet that achieves competitive RGBD segmentation performance in realtime on NYUDv2 <ref type="bibr" target="#b16">[17]</ref> and SUNRGBD <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Semantic Segmentation</head><p>The recent advances of semantic segmentation benefit a lot from the development of convolutional neural network (CNN) <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. FCN <ref type="bibr" target="#b2">[3]</ref> is the pioneer of leveraging CNN for semantic segmentation. It leads to convincing results and serves as the basic framework for many tasks. With the research efforts in the field, the recent methods can be classified into two categories according to the network architecture, including atrous convolution based methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, and encoderdecoder based methods <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Atrous convolution: The standard approach relies on stride convolutions or poolings to reduce the output stride of the CNN backbone and enables a large receptive field. However, the resolution of the resulting feature map is reduced <ref type="bibr" target="#b3">[4]</ref>, and many details are lost. One approach exploits atrous convolution to alleviate the conflict by enhancing the receptive field while keeping the resolution of the feature map <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>. We use atrous convolution based backbone in the proposed SGNet. Encoder-decoder architecture: The other approach utilizes the encoder-decoder structure <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, which learns a decoder to recover the prediction details gradually. De-convNet <ref type="bibr" target="#b27">[28]</ref> employs a series of deconvolutional layers to produce a high-resolution prediction. SegNet <ref type="bibr" target="#b26">[27]</ref> achieves better results by using pooling indices in the encoder to guide the recovery process in the decoder. RefineNet <ref type="bibr" target="#b24">[25]</ref> fuses low-level features in the encoder with the decoder to refine the prediction. <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> propose a scheme of gated sum, which can control the information flow of different scale in the encoder-decoder architecture. While this method can achieve more precise results, it requires longer inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGBD Semantic Segmentation</head><p>How to effectively use the extra geometry information (depth, 3D coordinates) is the key of RGBD semantic segmentation. A number of works focus on how to extract more information from geometry, which is treated as additional input in <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Two-stream network is used in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> to process RGB image and geometry information separately, and combines the two results in the last layer. These methods achieve promising results at the expense of doubling the parameters and computational cost. 3D CNNs or 3D KNN graph networks are also used to take geometry information into account <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. Besides, various deep learning methods on 3D point cloud <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b41">[42]</ref> are also explored. However, these methods cost a lot of memory and are computationally expensive. Another stream incorporates geometric information into explicit operations. <ref type="bibr" target="#b42">[43]</ref> proposes to perform 3D object detection based on depth-guided convolution, whose weights are location-variant and depth-adaptive. Cheng et al. <ref type="bibr" target="#b43">[44]</ref> use geometry information to build a feature affinity matrix acting in average pooling and up-pooling. Lin et al. <ref type="bibr" target="#b44">[45]</ref> splits the image into different branches based on geometry information. Wang et al. <ref type="bibr" target="#b15">[16]</ref> propose Depth-aware CNN, which adds depth prior to the convolutional weights. Although it improves feature extraction by convolution, the prior is handcrafted but not learned from data. Other approaches, such as multi-task learning <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b49">[50]</ref> or spatial-temporal analysis <ref type="bibr" target="#b50">[51]</ref>, are further used to improve segmentation accuracy. The proposed S-Conv aims to efficiently utilize spatial information to improve the feature extraction ability. It can significantly enhance the performance with high efficiency due to using only a small amount of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamic structure in CNN</head><p>Using dynamic structure to deal with varying input of CNN has also been explored. Dilation Convolution is used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref> to increase the receptive field size without reducing feature map resolution. Spatial transformer network <ref type="bibr" target="#b51">[52]</ref> adapts spatial transformation by warping feature map. Dynamic filter <ref type="bibr" target="#b52">[53]</ref> adaptively changes its weights according to the input. Besides, self-attention based methods <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b56">[57]</ref> generate attention maps from the intermediate feature map to adjust response at each location or capture long-range contextual information adaptively. Focusing on the understanding of contextual semantics, shape-variant convolution <ref type="bibr" target="#b56">[57]</ref> confines its contextual region by location-variant convolution based on semantic-correlated region. Some generalizations of convolution from 2D image to 3D point cloud are also presented. PointCNN <ref type="bibr" target="#b41">[42]</ref> is a seminal work that enables CNN on a set of unordered 3D points. There are other improvements <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref> on utilizing neural networks to effectively extract deep features from 3D point sets. Deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> can generate different distribution with adaptive weights. Nevertheless, their input is an intermediate feature map rather than spatial information. Our work experimentally verifies that better results can be obtained based on spatial information in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. S-CONV AND SGNET</head><p>In this section, we first elaborate on the details of Spatial information guided Convolution (S-Conv), which is a generalization of conventional RGB-based convolution by involving spatial information in the RGBD scenario. Then, we discuss the relation between our S-Conv and other approaches. Finally, we describe the network architecture of Spatial information Guided convolutional Network (SGNet), which is equipped with S-Conv for RGBD semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatial information guided Convolution</head><p>For completeness, we first review the conventional convolution operation. We use A i (j), A ? R c?h?w to denote a tensor, where i is the index corresponding to the first dimension, and j ? R 2 indicates the two indices for the second and third dimensions. Non-scalar values are highlighted in bold for convenience.</p><p>For an input feature map F ? R c?h?w . We describe it in 2D for simplicity, thus we note X as input feature map. X ? R 1?h?w . Note that it is straightforward to extend to the 3D case. The conventional convolution applied on X to get Y can be formulated as the following:</p><formula xml:id="formula_0">Y(p) = K i=1 W i ? X(p + d i ),<label>(1)</label></formula><p>where W ? R K represents the weight of convolution kernel with kernel size k h ? k w , and K = k h ? k w . p ? R 2 is the 2D convolution center, d ? R K?2 denotes the kernel distribution around p. For 3 ? 3 convolution, d is defined as Equ. <ref type="formula" target="#formula_1">(2)</ref>:</p><formula xml:id="formula_1">d = {[?1, ?1], [?1, 0], ..., [0, 1], [1, 1]}.<label>(2)</label></formula><p>From the above equation, we can see that the convolution kernel is constant over X. In other words, W and d are fixed, meaning the convolution is location-invariant and spatiallyagnostic.</p><p>In the RGBD context, we want to involve 3D spatial information efficiently by using adaptive convolution kernels. We first generate the offset according to the spatial information, then use the spatial information corresponding to the given offset to generate new spatially adaptive weights. Our S-Conv requires two inputs. One is the feature map X which is the same as conventional convolution. The other is the spatial information S ? R c ?h?w . In practice, S can be HHA (c = 3), 3D coordinates (c = 3), or depth (c = 1). The method of encoding depth into 3D coordinates and HHA is the same as <ref type="bibr" target="#b35">[36]</ref>. Note that the input spatial information is not included in the feature map.</p><p>As the first step of S-Conv, we project the input spatial information into a high-dimensional feature space, which can be expressed as:</p><formula xml:id="formula_2">S = ?(S),<label>(3)</label></formula><p>where ? is a spatial transformation function, and S ? R 64?h?w , which has a higher dimension than S. Then, we take the transformed spatial information S into consideration, perceive its geometric structure, and generate the distribution (offset of pixel coordinate in x? and y?axis) of convolution kernels at different p. This processes can be expressed as:</p><formula xml:id="formula_3">?d = ?(S ),<label>(4)</label></formula><p>where ?d ? R K?h ?w ?2 , For the sake of simplicity, we do not show the reshaping process of ?d in Equ. <ref type="bibr" target="#b3">(4)</ref>. ?d ? R 2K?h ?w before reshaping. h , w represent the feature map size after convolution. K = k h ? k w , which k h and k h are the kernel size. For 3 ? 3 convolution, ?d ? R 9?h ?w ?2 . ? is a non-linear function which can be implemented by a series of convolutions.</p><p>After generating the distribution of kernel for each possible p using ?d(p), we boost its feature extraction ability by establishing the link between the geometric structure and the convolution weight. Due to the shifting of convolution kernel in Equ. <ref type="bibr" target="#b3">(4)</ref>, the corresponding depth information of the convolution kernel has also changed. We want to collect the depth information corresponding to the convolution kernel after shifting for generating spatially adaptive weight. More specifically, we sample the geometric information of the pixels corresponding to the convolution kernel after shifting:</p><formula xml:id="formula_4">S * (p) = {S (p + d i + ?d i (p))| i=1,2,...,K },<label>(5)</label></formula><p>where ?d(p) is the spatial distribution of convolution kernels at p. S * (p) ? R 64K is the spatial information corresponding </p><formula xml:id="formula_5">? f .3 Equ .4 Equ .6 Equ Fig. 2.</formula><p>The illustration of the Spatial information guided Convolution (S-Conv). Firstly, the input 3D spatial information is projected by the spatial projector to match the input feature map. Secondly, the adaptive convolution kernel distribution is generated by the offset generator. Finally, the projected spatial information is sampled according to the kernel distribution and fed into the weight generator to generate adaptive convolution weights.  The conventional 2D convolution operation orderly places local points in a regular grid with fixed weights, while ignoring the spatial information. We can see that the spatial position variation of the yellow point can not be reflected in the weight. Our S-Conv can be regarded as placing a local patch into a weight space, which is generated by the spatial guidance of that patch. Hence the weight of each point establishes a link with its spatial location, effectively capturing the spatial variation of the local patch. The spatial relationship between the yellow point and other points can be reflected in the adaptive weights.</p><p>to the feature map of the convolution kernel centered on p after transformation.</p><p>Finally, we generate convolution weights according to the final spatial information as the following:</p><formula xml:id="formula_6">W * (p) = ?(f (S * (p))) ? W,<label>(6)</label></formula><p>where f is a non-linear function that can be implemented as a series of fully connected layers with non-linear activation function, ? is sigmoid function, ? is element-wise product, W ? R K indicates the convolution weights, which can be updated by the gradient descent algorithm. W * (p) ? R K denotes the spatially adaptive weights for convolution after shifting centered at p. Overall, our generalized S-Conv is formulated as:</p><formula xml:id="formula_7">Y(p) = K i=1 W * i (p) ? X(p + d i + ?d i (p)).<label>(7)</label></formula><p>We can see that W * i (p) establishes the correlation between spatial information and convolution weights. Moreover, con-volution kernel distribution is also relevant to the spatial information through ?d. Note that W * i (p) and ?d i (p) are not constant, meaning the generalized convolution is adaptive to different p. Also, as ?d is typically fractional, we use bilinear interpolation to compute X(p + d i + ?d i (p)) as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b51">[52]</ref>. The main formulae discussed above are labeled in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relation to other approaches</head><p>2D convolution is the special case of the proposed S-Conv without geometry information. Specifically, without geometry information, if we remove the W * i (p) and ?d i (p) which are generated by geometry information in Equ. <ref type="bibr" target="#b6">(7)</ref>, this process degenerates to 2D convolution. While for the RGBD case, our S-Conv can extract feature at the point level and is not limited to the discrete grid by introducing spatially adaptive weights as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> also alleviates this problem by generating different distribution weights. Nevertheless, their distributions are inferred from 2D feature maps instead of 3D spatial information as in our case. We will verify through experiments that our method achieves better results than deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Compared with shape-variant (SV) convolution <ref type="bibr" target="#b56">[57]</ref>, SV convolution confines its contextual region by location-variant convolution based on semantic-correlated region. It implements a location-variant convolution operator whose weights are location-variant and generated by feature map, focusing on the understanding of contextual semantics. Our S-Conv utilizes depth map rather than feature map to generate spatially adaptive offsets and weights. The weights and offset of S-Conv are defined by spatial information (depth map). This helps the convolutional layer to adjust the receptive field and adapt to geometric transformation according to the spatial information. Compared with the 3D KNN graph-based method, our S-Conv selects neighboring pixels adaptively instead of using the KNN graph, which is not flexible and computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SGNet architecture</head><p>Our semantic segmentation network, called SGNet, is equipped with S-Conv and consists of a backbone and decoder. The structure of SGNet is illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. We use ResNet101 <ref type="bibr" target="#b57">[58]</ref> as our backbone, and replace the first and the last two conventional convolutions (3 ? 3 filter) of each layer with our S-Conv. We add a series of convolutions to extract the feature further and then use bilinear up-sampling to produce the final segmentation probability map, which corresponds to the decoder part of the SGNet. The ? in Equ. (3) is implemented as three 3 ? 3 convolution layers, i.e. Conv(3, 64) -Conv(64, 64) -Conv(64, 64) with nonlinear activation function. The ? in Equ. (4) and the f in Equ. <ref type="bibr" target="#b5">(6)</ref> are implemented as single convolution layer and two fully connected layers separately. The S-Conv implementation is modified from deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. We add deep supervision between layer 3 and layer 4 to improve the network optimization capability, which is the same as PSPNet <ref type="bibr" target="#b58">[59]</ref>.</p><formula xml:id="formula_8">U |---------------------------Backbone-----------------------------------| ----------Decoder--------| Max-Pooling Convolution S-Convolution U Upsample |------Conv------|--Layer1--|--Layer2--|--Layer3--|--Layer4--| ... ----Final Result-----| |------RGB-Image------</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first validate the performance of S-Conv by analyzing its usage in different layers; conducting ablation study/comparison with its alternatives; evaluating results of using different input information to generate offset; and testing inference speed. Then we compare our SGNet equipped with S-Conv with other state-of-the-art semantic segmentation methods on NYUDV2 and SUNRGBD datasets. Finally, we visualize the depth adaptive receptive field in each layer and the segmentation results, demonstrating that the proposed S-Conv can well exploit spatial information. Datasets and metrics: We evaluate the performance of S-Conv operator and SGNet segmentation method on public datasets:</p><p>? NYUDv2 <ref type="bibr" target="#b16">[17]</ref>: This dataset has 1449 RGB images with corresponding depth maps and pixel-wise labels. 795 images are used for training, while 654 images are used for testing as in <ref type="bibr" target="#b59">[60]</ref>. The 40-class settings are used for experiments. ? SUN-RGBD <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>: This dataset contains 10335 RGBD images with semantic labels organized in 37 categories. 5285 images are used for training, and 5050 images are used for testing.</p><p>We use three common metrics for evaluation, including pixel accuracy (Acc), mean accuracy (mAcc), and mean intersection over union (mIoU). The three metrics are defined as the following:</p><formula xml:id="formula_9">Acc = i p ii g , mAcc = 1 p c i p ii g i , mIoU = 1 p c i p ii g i + j p ji ? p ii ,<label>(8)</label></formula><p>where p ij is the amount of pixels which are predicted as class j with ground truth i, p c is the number of classes, and g i is the number of pixels whose ground truth class is i. g = i g i is the number of pixels. The depth map is used as the default format of spatial information unless specified otherwise. Implementation details: We use dilated ResNet101 <ref type="bibr" target="#b57">[58]</ref> pretrained on ImageNet <ref type="bibr" target="#b60">[61]</ref> as the backbone network for feature extraction following <ref type="bibr" target="#b3">[4]</ref>, and the output stride is 16 by default. The whole system is implemented based on PyTorch. The SGD optimizer is adopted for training with the same learning rate schedule ("poly" policy) as <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b25">[26]</ref>, where the initial learning rate is 5e-3 for ablation study, 8e-3 for NYUDv2 and 1e-3 for SUNRGBD, and the weight decay is 5e-4. This learning policy updates the learning rate for every 40 epochs for NYUDv2 and ablation study and 10 epochs for SUNRGBD. We use ReLU activation function, and the batch size is 8. Following <ref type="bibr" target="#b5">[6]</ref>, we employ general data augmentation strategies, including random scaling, random cropping, and random flipping. The crop size is 480 ? 640.</p><p>During testing, we down-sample the image to the training crop size (480 ? 640), and its prediction map is upsampled to the original size. We use cross-entropy loss in both datasets, and reweight <ref type="bibr" target="#b61">[62]</ref> training loss of each class in SUNRGBD due to its extremely unbalanced label distribution. We train the network by 480 epochs for the NYUDv2 dataset and 200 epochs for the SUNRGBD dataset on two NVIDIA 1080Ti GPUs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis of S-Conv</head><p>We design ablation studies on NYUDv2 <ref type="bibr" target="#b16">[17]</ref> dataset. The ResNet101 with a simple decoder and deep supervision is used as the baseline. Replace convolution with S-Conv: We evaluate the effectiveness of S-Conv by replacing the conventional convolution (of 3 ? 3 filter) in different layers. We first replace convolution in layer 3, then extend the explored rules to other layers. The FPS (Frames per Second) is tested on NVIDIA 1080Ti with input image size 480 ? 640. The results are shown in Tab. I.</p><p>We can draw the following two conclusions from the results in the Tab. I. 1) The inference speed of the baseline network is fast, but its performance is poor. Replacing convolution with S-Conv can improve the results of the baseline network with a little bit more parameters and computational time. 2) In addition to the first convolution in layer 3 whose stride is 2, the effect of replacing the later convolution is better. The main reason would be that spatial information can better guide down-sampling operation in the first convolution. Thus  we choose to replace the first convolution and the last two convolutions of each layer with S-Conv. We generalize the rules found in layer 3 to other layers and achieve better results. The above experiments show that our S-Conv can significantly improve network performance with only a few parameters. It is worth noting that our network has no spatial information stream. The spatial information only affects the distribution and weight of convolution kernel. We also explore the performance of S-Conv embedded into different layers.</p><p>The results are shown in Tab. II. We can observe that the performance enhances with the number of layers equipped with S-Conv. We also show the IoU improvement of S-Conv on most categories in <ref type="figure">Fig. 5</ref>. It's obvious that our S-Conv improves IoU in most categories, especially for objects lacking representative texture information such as mirror, board and bathtub. There are also clear improvements for objects with rich spatial transformation, such as chairs and tables. This shows that our S-Conv can make good use of spatial information during the inference process. Architecture ablation: To evaluate the effectiveness of each component in our proposed S-Conv, we design ablation studies. The results are shown in Tab. III. By default, we replace the first convolution and the last two convolutions of each layer according to Tab. I. We can see that the offset generator, spatial projection module, and weight generator of S-Conv all contribute to the improvement of the results. Comparison with alternatives: Most methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b61">[62]</ref> use a two-stream network to extract features from two different modalities and then combine them. Our S-Conv focuses on advancing the feature extraction process of the network by utilizing spatial information. Here we compare our S-Conv with two-stream network, deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and depth-aware convolution <ref type="bibr" target="#b15">[16]</ref>. We use a simple baseline which consists of a ResNet101 network with deep supervision and a simple decoder. We add an additional ResNet101 network, called HHANet, to extract HHA features and fuse it with our baseline features at the final layer of a two-  <ref type="figure">Fig. 6</ref>. FPS, mIoU, and the number of parameters of different methods on NYUDv2. The input image size for all single-scale speed comparisons is 425 ? 560 following <ref type="bibr" target="#b15">[16]</ref>. The radius of the circle corresponds to the number of parameters of the model. The results of DCNet <ref type="bibr" target="#b15">[16]</ref> and 3DGNN <ref type="bibr" target="#b35">[36]</ref> are from <ref type="bibr" target="#b15">[16]</ref>. Our SGNet can achieve fastest inference time and state-of-the-art performance. stream network. To compare with depth-aware convolution and deformable convolution, similar to SGNet, we replace the first convolution and the last two convolutions of each layer. For "Baseline + DAC + DCV2", we replace convolution with depth-aware convolution <ref type="bibr" target="#b15">[16]</ref> (DAC) in first two layers and replace convolution with deformable convolution <ref type="bibr" target="#b12">[13]</ref> (DCV2) in last two layers, because DCV2 does not work for the lower layers <ref type="bibr" target="#b12">[13]</ref>. The results are shown in Tab. IV. We find that our S-Conv achieves better results than two-stream networks, deformable convolution <ref type="bibr" target="#b12">[13]</ref>, depth-aware convolution <ref type="bibr" target="#b15">[16]</ref>, and their combination. This demonstrates that our S-Conv can effectively utilizes spatial information. The baseline equipped with weight generator can also achieve better results than depth-aware convolution, indicating that learning weights from spatial information is necessary. Spatial information comparison: We also evaluate the impact of different formats of spatial information on S-Conv. The results are shown in Tab. V. We can see that depth information leads to comparable results with HHA and 3D coordinates, and better results than intermediate RGB features which are used by deformable convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. This shows the advantage of using spatial information for offset and weight generation over RGB features. However, converting depth to HHA is time-consuming <ref type="bibr" target="#b8">[9]</ref>. Hence 3D coordinates and depth map are more suitable for real-time segmentation using SGNet. It can be seen that even without spatial information input (with RGB features), our S-Conv has more than 3.4% improvement than the baseline. Inference speed test: To demonstrate the light weight of S-Conv, we test the inference speed of SGNet in this part. We also compare our S-Conv with two-stream methods. The input size of image is 480 ? 640. Results are shown in Tab. VI. We can observe that S-Conv only requires a small amount of additional computation compared with two-stream methods. Our SGNet can also achieve real-time inference speed using ResNet101 and ResNet50 <ref type="bibr" target="#b57">[58]</ref> backbone.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with state-of-the-art</head><p>We compare our SGNet with other state-of-the-art methods on NYUDv2 <ref type="bibr" target="#b16">[17]</ref> and SUNRGBD <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> datasets. The architecture of SGNet is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. NYUDv2 dataset: The comparison results can be found in Tab. VII and <ref type="figure">Fig. 6</ref>. We change the learning rate from 5e-3 to 8e-3. We down-sample the input image to 480 ? 640 and upsample its predict map to get final results during test.</p><p>To compare inference speed with other methods, the input image size for all speed comparisons in Tab. VII is 425 ? 560 following <ref type="bibr" target="#b15">[16]</ref>. The inference speed results of DCNet <ref type="bibr" target="#b15">[16]</ref> and 3DGNN <ref type="bibr" target="#b35">[36]</ref> are from <ref type="bibr" target="#b15">[16]</ref>, and we test the singlescale speed of other methods under the same conditions using NVIDIA 1080Ti in Tab. VII. Furthermore, inference speed test of SGNet with input size 480 ? 640 is shown in Tab. VI.</p><p>Note that some methods in Tab. VII do not report parameter quantities or open source. So we just listed the mIoU of these methods. We can draw the following conclusions from Tab. VI and Tab. VII. Instead of using additional networks to extract spatial features, our SGNet (ResNet50) can achieve competitive performance and fastest inference with minimum number of parameters. Our SGNet (ResNet101) can achieve more competitive performance and real-time inference. This benefits from S-Conv which can make use of spatial information efficiently with only a small amount of extra parameters and computation cost. Moreover, our S-Conv can achieve good results without using HHA information, making it suitable for real-time tasks. This verifies the efficiency of our S-Conv in utilizing spatial information. At the expense of a little bit more reasoning time by adding ASPP module <ref type="bibr" target="#b3">[4]</ref>   worse than RGB based methods. We can observe that our network benefiting from S-Conv can achieve better results than baseline and achieve competitive results on Cityscapes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Performance</head><p>Visualization of receptive field in S-Conv: Appropriate receptive field is very important for scene recognition. We visualize the input adaptive receptive field of SGNet in different layers generated by S-Conv. Specifically, we get the receptive field of each pixel by summing up the norm of their offsets during the S-Conv operation, then we normalize each value to [0, 255] and visualize the result using a gray-scale image.</p><p>The results are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. The brighter the pixel, the larger the relative receptive field. We also use the radius of circle to represent the size of the relative receptive field. We observe that the receptive fields of different convolutions vary adaptively with the depth of the input image. For example, in layer1 1, the receptive field is inversely proportional to the depth. The combination of the adaptive receptive field learned at each layer can help the network better resolve indoor scenes with complex spatial relations.</p><p>Qualitative comparison results: We show qualitative comparison results on NYUDv2 test dataset in <ref type="figure">Fig. 8</ref>. For the visual results in <ref type="figure">Fig. 8(a)</ref>, the bathtub and the wall have insufficient texture, which cannot be easily distinguished by the baseline method. Some objects may have reflections such as the table in <ref type="figure">Fig. 8(b)</ref>, which is also challenging for the baseline. SGNet, however, can recognize it well by incorporating spatial information with the help of S-Conv. The chairs in <ref type="figure">Fig. 8(c, d)</ref> are hard to be recognized by RGB data due to the low contrast and confused texture, while they can be easily recovered by SGNet benefiting from the equipped S-Conv. In the meantime, SGNet can recover the object's geometric shape nicely, as demonstrated by the chairs of <ref type="figure">Fig. 8(e)</ref>. We also show qualitative results on SUNRGBD test dataset in <ref type="figure">Fig. 9</ref>. It can be seen that our SGNet can also achieve precise segmentation on SUNRGBD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel Spatial information guided Convolution (S-Conv) operator. Compared with conventional 2D convolution, it can adaptively adjust the convolution weights and distributions according to the input spatial information, resulting in better awareness of the geometric structure with only a few additional parameters and computation cost. We also propose Spatial information Guided convolutional Network (SGNet) equipped with S-Conv that yields real-time inference speed and achieves competitive results on NYUDv2 and SUNRGBD datasets for RGBD semantic segmentation. We also compare the performance of using different inputs to generate offset, demonstrating the advantage of using spatial information over RGB feature. Furthermore, we visualize the depth-adaptive receptive field in each layer to show effectiveness. In the future, we will investigate the fusion of different modal information and the adaptive change of S-Conv structure simultaneously, making these two approaches benefit each other. We will also explore the application of S-Conv in different fields, such as pose estimation and 3D object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The network architecture of different multi-modal fusion approaches. (a) The conventional two-stream structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The illustration of weights W in 2D convolution and W * in S-Conv. The yellow dot indicates the point whose spatial position changes along the arrow. Illustration of 2D convolution is on the top, and S-Conv is on the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The network architecture of SGNet equipped with S-Conv for RGBD semantic segmentation. The SGNet consists of a backbone network and a decoder. The deep supervision is added between layer 3 and layer 4 to improve network optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>w a l l f l o o r c a b i n e t b e d c h a i r s o f a t a b l e d o o r w i n d o w b o o k s h e l f p i c t u r e c o u n t e r b l i n d s d e s k s h e l v e s c u r t a i n d r e s s e r p i l l o w m i r r o r f l o o r m a t c l o t h e s c e i l i n g b o o k s f r i d g e t v p a p e r t o w e l s h o w e r b o x b o a r d p e r s o n s t a n d t o i l e t s i n k l a m p b a t h t u b b a g o t h e r 1 Fig. 5 .</head><label>15</label><figDesc>Per-category IoU improvement of S-Conv on NYUDv2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>The visualization of relative receptive field in S-Conv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Chen(linzhuochen@mail.nankai.edu.cn), Z Lin, MM Cheng (corresponding author, cmm@nankai.edu.cn) are with TKLNDST, College of Computer Science, Nankai University, China.</figDesc><table /><note>Ziqin Wang is with University of Sydney. YL Yang is with University of Bath.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I TABLE II</head><label>III</label><figDesc>THE RESULTS OF REPLACING CONVOLUTION (OF 3 ? 3 FILTER) OF DIFFERENT LAYERS WITH S-CONV ON NYUDV2 DATASET. "LAYERX Y" MEANS THE 3 ? 3 CONVOLUTION OF Y-TH RESIDUAL BLOCK IN X-TH LAYER. THE RESULTS OF REPLACING CONVOLUTION (OF 3 ? 3 FILTER) OF DIFFERENT LAYERS WITH S-CONV ON NYUDV2 DATASET.</figDesc><table><row><cell>S-Conv</cell><cell cols="4">layer3 0 layer3 1 layer3 2 layer3 20 layer3 21 layer3 22 other layers mIoU(%) param(M) FPS</cell></row><row><cell></cell><cell></cell><cell>43.0</cell><cell>56.8</cell><cell>34</cell></row><row><cell></cell><cell></cell><cell>47.0</cell><cell>56.9</cell><cell>34</cell></row><row><cell>Baseline</cell><cell></cell><cell>46.6</cell><cell>57.2</cell><cell>33</cell></row><row><cell>(ResNet101)</cell><cell></cell><cell>46.5</cell><cell>57.2</cell><cell>33</cell></row><row><cell></cell><cell></cell><cell>47.8</cell><cell>57.2</cell><cell>33</cell></row><row><cell></cell><cell></cell><cell>49.0</cell><cell>58.3</cell><cell>26</cell></row><row><cell cols="3">layer1 layer2 layer3 layer4 mIoU(%) param(M)</cell><cell></cell></row><row><cell></cell><cell>43.0</cell><cell>56.8</cell><cell></cell></row><row><cell></cell><cell>46.5</cell><cell>57.2</cell><cell></cell></row><row><cell></cell><cell>47.0</cell><cell>57.5</cell><cell></cell></row><row><cell></cell><cell>48.8</cell><cell>57.9</cell><cell></cell></row><row><cell></cell><cell>49.0</cell><cell>58.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY OF SGNET ON NYUDV2 [17] DATASET. OG: OFFSET GENERATOR OF S-CONV, WG: WEIGHT GENERATOR OF S-CONV, SP: SPATIAL PROJECTION OF S-CONV.</figDesc><table><row><cell>Model</cell><cell>Acc</cell><cell>mAcc</cell><cell>mIoU</cell></row><row><cell>Baseline</cell><cell>72.1</cell><cell>54.6</cell><cell>43.0</cell></row><row><cell>Baseline+OG</cell><cell>73.9</cell><cell>58.2</cell><cell>46.3</cell></row><row><cell>Baseline+SP+OG</cell><cell>75.2</cell><cell>60.0</cell><cell>48.4</cell></row><row><cell>Baseline+SP+WG</cell><cell>74.5</cell><cell>58.4</cell><cell>46.8</cell></row><row><cell>Baseline+SP+OG+WG</cell><cell>75.5</cell><cell>60.9</cell><cell>49.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV THE</head><label>IV</label><figDesc>COMPARISON RESULTS ON NYUDV2 TEST DATASET. DCV2: DEFORMABLE CONVOLUTION V2 [14], DAC: DEPTH-AWARE CONVOLUTION [16], SP: SPATIAL PROJECTOR IN S-CONV, WG: WEIGHT GENERATOR IN S-CONV.</figDesc><table><row><cell>Model</cell><cell>Acc</cell><cell>mAcc</cell><cell>mIoU</cell></row><row><cell>Baseline</cell><cell>72.1</cell><cell>54.6</cell><cell>43.0</cell></row><row><cell>Baseline+DCV2</cell><cell>73.0</cell><cell>56.1</cell><cell>44.5</cell></row><row><cell>Baseline+HHANet</cell><cell>73.5</cell><cell>56.8</cell><cell>45.4</cell></row><row><cell>Baseline+DAC</cell><cell>73.8</cell><cell>57.1</cell><cell>45.4</cell></row><row><cell>Baseline+HHANet+DCV2</cell><cell>74.3</cell><cell>58.4</cell><cell>47.0</cell></row><row><cell>Baseline+DAC+DCV2</cell><cell>74.5</cell><cell>58.3</cell><cell>46.5</cell></row><row><cell>Baseline+SP+WG</cell><cell>74.5</cell><cell>58.4</cell><cell>46.8</cell></row><row><cell>Baseline+S-Conv(SGNet)</cell><cell>75.5</cell><cell>60.9</cell><cell>49.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>COMPARISON OF USING DIFFERENT TYPES OF SPATIAL INFORMATION ON NYUDV2 DATASET.</figDesc><table><row><cell>Information</cell><cell>Acc</cell><cell>mAcc</cell><cell>mIoU</cell></row><row><cell>Depth</cell><cell>75.5</cell><cell>60.9</cell><cell>49.0</cell></row><row><cell>RGB Feature</cell><cell>73.9</cell><cell>58.5</cell><cell>46.4</cell></row><row><cell>HHA</cell><cell>75.7</cell><cell>60.8</cell><cell>48.9</cell></row><row><cell>Coordinates</cell><cell>75.3</cell><cell>61.2</cell><cell>48.5</cell></row><row><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="4">INFERENCE SPEED TEST OF SGNET WITH INPUT IMAGE SIZE</cell></row><row><cell cols="4">480 ? 640. OG: OFFSET GENERATOR OF S-CONV,  ?: WITHOUT APPLYING</cell></row><row><cell cols="4">GENERATED LOCATION-VARIANT WEIGHT AND OFFSET IN SGNET,</cell></row><row><cell cols="4">HHANET: ADDITIONAL STREAM BACKBONE (RESNET101) TO UTILIZE</cell></row><row><cell cols="3">SPATIAL INFORMATION.</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">time(s) FPS param(M)</cell></row><row><cell>Baseline</cell><cell>0.029</cell><cell>34</cell><cell>56.8</cell></row><row><cell>Baseline+OG</cell><cell>0.033</cell><cell>30</cell><cell>57.7</cell></row><row><cell>Baseline+HHANet</cell><cell>0.053</cell><cell>18</cell><cell>99.4</cell></row><row><cell>SGNet(ResNet50)</cell><cell>0.028</cell><cell>36</cell><cell>39.3</cell></row><row><cell>SGNet  ?</cell><cell>0.032</cell><cell>31</cell><cell>58.3</cell></row><row><cell>SGNet</cell><cell>0.037</cell><cell>26</cell><cell>58.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>RESULTS ON NYUDV2 TEST DATASET. MS: MULTI-SCALE TEST; SI: SPATIAL INFORMATION. THE INPUT IMAGE SIZE FOR FORWARD SPEED COMPARISON IS 425 ? 560 USING NVIDIA 1080TI FOLLOWING<ref type="bibr" target="#b15">[16]</ref>. WE ADD ASPP MODULE<ref type="bibr" target="#b3">[4]</ref> AFTER THE FINAL LAYER OF SGNET, NOTED AS"SGNET*".</figDesc><table><row><cell>Network</cell><cell>Backbone</cell><cell>MS</cell><cell>SI</cell><cell>Acc</cell><cell>mAcc</cell><cell>mIoU</cell><cell>FPS</cell><cell>param (M)</cell></row><row><cell>FCN [3]</cell><cell>2?VGG16</cell><cell></cell><cell>HHA</cell><cell>65.4</cell><cell>46.1</cell><cell>34.0</cell><cell>8</cell><cell>272.2</cell></row><row><cell>LSD-GF [44]</cell><cell>2?VGG16</cell><cell></cell><cell>HHA</cell><cell>71.9</cell><cell>60.7</cell><cell>45.9</cell><cell>-</cell><cell>-</cell></row><row><cell>3DGNN [36]</cell><cell>VGG16</cell><cell></cell><cell>HHA</cell><cell>-</cell><cell>55.2</cell><cell>42.0</cell><cell>5</cell><cell>47.2</cell></row><row><cell>D-CNN [16]</cell><cell>VGG16</cell><cell></cell><cell>Depth</cell><cell>-</cell><cell>53.6</cell><cell>41.0</cell><cell>26</cell><cell>47.0</cell></row><row><cell>D-CNN [16]</cell><cell>2?ResNet152</cell><cell></cell><cell>Depth</cell><cell>-</cell><cell>61.1</cell><cell>48.4</cell><cell>-</cell><cell>-</cell></row><row><cell>ACNet [33]</cell><cell>2?ResNet50</cell><cell></cell><cell>Depth</cell><cell>-</cell><cell>-</cell><cell>48.3</cell><cell>18</cell><cell>116.6</cell></row><row><cell>RefineNet [25]</cell><cell>ResNet152</cell><cell></cell><cell>-</cell><cell>73.6</cell><cell>58.9</cell><cell>46.5</cell><cell>16</cell><cell>129.5</cell></row><row><cell>RDFNet [6]</cell><cell>2?ResNet152</cell><cell></cell><cell>HHA</cell><cell>76.0</cell><cell>62.8</cell><cell>50.1</cell><cell>9</cell><cell>200.1</cell></row><row><cell>RDFNet [6]</cell><cell>2?ResNet101</cell><cell></cell><cell>HHA</cell><cell>75.6</cell><cell>62.2</cell><cell>49.1</cell><cell>11</cell><cell>169.1</cell></row><row><cell>CFNet [45]</cell><cell>2?ResNet152</cell><cell></cell><cell>HHA</cell><cell>-</cell><cell>-</cell><cell>47.7</cell><cell>-</cell><cell>-</cell></row><row><cell>SGNet</cell><cell>ResNet50</cell><cell></cell><cell>Depth</cell><cell>75.0</cell><cell>59.6</cell><cell>47.7</cell><cell>39</cell><cell>39.3</cell></row><row><cell>SGNet</cell><cell>ResNet101</cell><cell></cell><cell>Depth</cell><cell>75.6</cell><cell>61.9</cell><cell>49.6</cell><cell>28</cell><cell>58.3</cell></row><row><cell>SGNet*</cell><cell>ResNet101</cell><cell></cell><cell>Depth</cell><cell>76.1</cell><cell>62.7</cell><cell>50.2</cell><cell>26</cell><cell>64.7</cell></row><row><cell>SGNet*</cell><cell>ResNet101</cell><cell></cell><cell>Depth</cell><cell>76.8</cell><cell>63.3</cell><cell>51.1</cell><cell>26</cell><cell>64.7</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RGB</cell><cell>Depth</cell><cell></cell><cell cols="2">Layer1 1</cell><cell></cell><cell>Layer1 3</cell><cell></cell><cell>Layer2 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>RESULTS ON SUNRGBD TEST DATASET. MS: MULTI-SCALE TEST, SI: SPATIAL INFORMATION. WE ADD ASPP MODULE<ref type="bibr" target="#b3">[4]</ref> AFTER THE FINAL LAYER OF SGNET, NOTED AS "SGNET*". compared with models that do not have real-time performance. SGNet's performance can be further improved by using multi-scale test. Cityscapes dataset: We add ASPP<ref type="bibr" target="#b3">[4]</ref> module after SGNet and set output stride = 8. noted as "SGNet-8s*". We training with 2975 images on training set for validation. We also provide our test result 1 on Cityscapes server. The comparison results on the Cityscapes dataset are shown in Tab. IX. It is worth noting that due to the serious noise of depth map in Cityscapes, most of previous RGB-D based methods perform</figDesc><table><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(d)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RGB</cell><cell>GT</cell><cell>SGNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Fig. 9. The qualitative semantic segmentation comparison results on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SUNRGBD test dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell>Backbone</cell><cell>MS</cell><cell>SI</cell><cell>Acc</cell><cell>mAcc</cell><cell>mIoU</cell><cell>param (M)</cell></row><row><cell>LSD-GF [44]</cell><cell>2?VGG16</cell><cell cols="2">HHA</cell><cell>-</cell><cell>58.0</cell><cell>-</cell><cell>-</cell></row><row><cell>RefineNet [25]</cell><cell>ResNet152</cell><cell></cell><cell>-</cell><cell>80.6</cell><cell>58.5</cell><cell>45.9</cell><cell>129.5</cell></row><row><cell>CGBNet [30]</cell><cell>ResNet101</cell><cell></cell><cell>-</cell><cell>82.3</cell><cell>61.3</cell><cell>48.2</cell><cell>-</cell></row><row><cell>3DGNN [36]</cell><cell>VGG16</cell><cell cols="2">HHA</cell><cell>-</cell><cell>57.0</cell><cell>45.9</cell><cell>47.2</cell></row><row><cell>D-CNN [16]</cell><cell>2?VGG16</cell><cell cols="2">HHA</cell><cell>-</cell><cell>53.5</cell><cell>42.0</cell><cell>92.0</cell></row><row><cell>ACNet [33]</cell><cell>2?ResNet50</cell><cell cols="2">HHA</cell><cell>-</cell><cell>-</cell><cell>48.1</cell><cell>272.2</cell></row><row><cell>RDFNet [6]</cell><cell>2?ResNet152</cell><cell cols="2">HHA</cell><cell>81.5</cell><cell>60.1</cell><cell>47.7</cell><cell>200.1</cell></row><row><cell>CFNet [45]</cell><cell>2?ResNet152</cell><cell cols="2">HHA</cell><cell>-</cell><cell>-</cell><cell>48.1</cell><cell>-</cell></row><row><cell>SGNet</cell><cell>ResNet101</cell><cell cols="2">Depth</cell><cell>81.0</cell><cell>59.6</cell><cell>47.1</cell><cell>58.3</cell></row><row><cell>SGNet*</cell><cell>ResNet101</cell><cell cols="2">Depth</cell><cell>81.0</cell><cell>59.8</cell><cell>47.5</cell><cell>64.7</cell></row><row><cell>SGNet*</cell><cell>ResNet101</cell><cell cols="2">Depth</cell><cell>82.0</cell><cell>60.7</cell><cell>48.6</cell><cell>64.7</cell></row><row><cell cols="3">as SGNet*, the proposed SGNet can achieve better results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">than other methods and RDFNet which uses multi-scale test,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">HHA information and two ResNet152 backbones. After using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">multi-scale test which is used by other methods, SGNet's</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">performance can be further improved.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>SUNRGBD dataset: The comparison results on the SUN- RGBD dataset are shown in Tab. VIII. It is worth noting that some methods in Tab. VII did not report results on the SUNRGBD dataset. The inference time and parameter number of models in Tab. VIII are the same as those in Tab. VII. Our SGNet can achieve competitive results in real-time</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>RESULTS ON CITYSCAPES VALIDATION DATASET. ?: RESULTS ON TEST DATASET.</figDesc><table><row><cell>Network</cell><cell cols="3">Backbone iterations MS mIoU</cell></row><row><cell cols="2">Baseline ResNet101</cell><cell>40k</cell><cell>78.2</cell></row><row><cell cols="2">SGNet-8s* ResNet101</cell><cell>40k</cell><cell>79.2</cell></row><row><cell cols="2">SGNet-8s* ResNet101</cell><cell>65k</cell><cell>80.6</cell></row><row><cell cols="2">SGNet-8s* ResNet101</cell><cell>65k</cell><cell>81.2  ?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.cityscapes-dataset.com/anonymous-results/?id= d772c1227bd7cfe7b841805796490cab82bb7c7749e5b1ec8e69e6e86134bfb3</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research was supported by Major Project for New Generation of AI under Grant No. 2018AAA0100400, NSFC (61620106008), Tianjin Natural Science Foundation (17JCJQJC43700), and S&amp;T innovation project from Chinese Ministry of Education.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynaslam: Tracking, mapping, and inpainting in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bescos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>F?cil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4076" to="4083" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autonomous learning of semantic segmentation from internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Sin Inform</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>in chinese</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4980" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view deep learning for consistent semantic mapping with rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning common and specific features for rgb-d semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="664" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="541" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer depth cameras for computer vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6819" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward achieving robust low-level and high-level scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1378" to="1390" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic segmentation with context encoding and multi-path decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3520" to="3533" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">S4net: Single stage salient-instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="204" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="5099" to="5108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Lsanet: Feature learning on point sets by local spatial aware layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05442</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="828" to="838" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1000" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3029" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Geometryaware distillation for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="826" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4837" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2025" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="667" to="675" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8885" to="8894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoderdecoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
