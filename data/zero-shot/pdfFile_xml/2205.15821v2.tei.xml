<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Image Representation Learning with Deep Latent Particles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Daniel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Image Representation Learning with Deep Latent Particles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new representation of visual data that disentangles object position from appearance. Our method, termed Deep Latent Particles (DLP), decomposes the visual input into low-dimensional latent "particles", where each particle is described by its spatial location and features of its surrounding region. To drive learning of such representations, we follow a VAE-based approach and introduce a prior for particle positions based on a spatial-softmax architecture, and a modification of the evidence lower bound loss inspired by the Chamfer distance between particles. We demonstrate that our DLP representations are useful for downstream tasks such as unsupervised keypoint (KP) detection, image manipulation, and video prediction for scenes composed of multiple dynamic objects. In addition, we show that our probabilistic interpretation of the problem naturally provides uncertainty estimates for particle locations, which can be used for model selection, among other tasks. Videos and code are available: https://taldatech.github.io/ deep-latent-particles-web/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2205.15821v2 [cs.CV] 26 Jul 2022</head><p>Unsupervised Image Representation Learning with Deep Latent Particles reconstructing the image, and an uncertainty estimate for the particles that is based on the probabilistic interpretation of the latent variables. Meanwhile, as particles are jointly encoded and decoded, DLP does not require iterative inference, and can work with a larger number of objects (&gt; 30) than prior VAE-based methods. Our method is also more flexible than recent patch-based approaches <ref type="bibr" target="#b44">(Smirnov et al., 2021;</ref><ref type="bibr" target="#b32">Lin et al., 2020)</ref>, which anchor objects to the center of the patch -our encoder allows particles to be freely located on the canvas, and for multiple particles to jointly model a single object (e.g., a large object).</p><p>Key for making our method work are two novel ideas. The first is to add a prior on particle positions that is based on the spatial-softmax architecture. The second is to learn the posterior of particle positions based on a modification of the evidence lower bound (ELBO) that is inspired by the Chamfer distance between particles, which we term the Chamfer KL. Building on these two ideas, we propose a VAE-inspired model where particles act as latent variables.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The spatial positions of various parts of an image contain useful information for decision making. Examples include the positions of objects in video games <ref type="bibr" target="#b44">(Smirnov et al., 2021)</ref>, rigid object poses in robotic manipulation <ref type="bibr" target="#b4">(Byravan &amp; Fox, 2017)</ref>, and image landmarks for human pose estimation . This observation motivates us to seek image representations where position is disentangled from other visual properties of objects in the scene.</p><p>In different image types, however, the definition of objects and their positions may vary (e.g., contrast the position of facial parts such as eyes and nose with the position of computer game sprites), leading us to pursue an unsupervised approach, where representations are data-dependent. In particular, we follow the generative approach -learn to reconstruct an image from its disentangled latent representation .</p><p>This problem has recently gained attention in the variational autoencoding (VAE) literature, motivated by the observation that the typical single-vector representation of the entire scene has issues in scenes with multiple, varying number of objects. A common remedy is decomposing the scene into a pre-determined number of objects <ref type="bibr" target="#b26">Kipf et al., 2019)</ref>, and then learning a separate representation for each object. The main caveats with these methods are their strong assumptions on the number of objects, and their complexity, as the object discovery process is usually iterative and initializationdependent <ref type="bibr" target="#b26">Kipf et al., 2019</ref>).</p><p>An alternative representation is based on object landmarks, or keypoints, an idea that dates back to classical computer vision <ref type="bibr" target="#b36">(Lowe, 1999)</ref>. A keypoint (KP) representation is an unordered set of geometrical points (i.e., (x, y) locations in 2D scenes and (x, y, z) in 3D scenes). Several recent works investigated learning keypoints with deep learning methods <ref type="bibr" target="#b48">Thewlis et al., 2017a;</ref><ref type="bibr" target="#b57">Zhang et al., 2018;</ref>, exploiting the translational-invariance and spatial-locality of convolutional-based architectures. Predominantly, <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> apply the spatial-softmax over feature maps extracted from a convolutional neural network (CNN) to determine the location of the keypoints. This idea became a building block in other methods that use KP for downstream tasks <ref type="bibr" target="#b29">(Kulkarni et al., 2019;</ref><ref type="bibr" target="#b14">Gopalakrishnan et al., 2020;</ref><ref type="bibr" target="#b0">Boney et al., 2021;</ref><ref type="bibr" target="#b17">He et al., 2021)</ref>, and has proven to be a promising alternative to the single-vector representation.</p><p>In this work, we propose a new representation method that draws inspiration from both KP and VAEs. Our idea is to view the keypoints themselves as the latent space of the VAE. To sufficiently capture image information, we accompany each KP with a set of features to describe the content in its vicinity, and refer to it as a "particle". Our method, termed Deep Latent Particles (DLP), inherits favorable traits from the VAE approach, such as a natural decoding scheme for We demonstrate DLP on datasets that contain scenes with and without explicit objects. Our results show that the learned latent space effectively disentangles position from appearance. For example, when trained on CelebA <ref type="bibr" target="#b33">(Liu et al., 2015)</ref> data, moving the particle located on the nose only controls the nose region in the output image. Importantly, we show that incorporating the particle uncertainty can benefit downstream tasks such as supervised landmark regression, where we demonstrate state-of-the-art results. Finally, we demonstrate that our method can be used to manipulate scenes with multiple objects by changing the location of the particles, and how this idea can be used for video prediction, by training a graph neural network (GNN) to predict the change in the particles.</p><p>Our contributions are summed as follows: (1) we propose a new unsupervised particle-based latent representation, trained with a novel modification of the VAE loss function based on the Chamfer distance for a set of points; (2) we show that our method is capable of extracting objects and their masks from multiple-object scenes without any supervision; (3) we experiment with various image datasets, showing the method's applications in keypoint discovery, image manipulation and video prediction; and (4) we demonstrate the benefits of incorporating the learned uncertainty information for model selection in the task of KP regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is inspired by ideas from unsupervised keypoint detection and unsupervised scene decomposition to objects.</p><p>Unsupervised keypoint detection: <ref type="bibr" target="#b48">Thewlis et al. (2017a)</ref> represent the structure of an object as a set of keypoints learned via image deformations under equivariance con-straints. <ref type="bibr" target="#b57">Zhang et al. (2018)</ref> proposed an autoencodingbased landmark discovery approach with a constrained bottleneck, to improve upon <ref type="bibr" target="#b48">Thewlis et al. (2017a)</ref>. The method does not require pairs of images, but introduces several equivariance and separation constraints, making it more complex. <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> proposed KeyNet 1 to learn KP using a tight bottleneck. KeyNet is simpler -the constraints of <ref type="bibr" target="#b57">Zhang et al. (2018)</ref> are removed, but it requires pairs of images (video frames or augmented images). KeyNet outperformed <ref type="bibr" target="#b57">Zhang et al. (2018)</ref> by a large margin. Transporter <ref type="bibr" target="#b29">(Kulkarni et al., 2019)</ref> extends KeyNet by learning to transport image features between two frames for tracking objects and object parts across long time-horizons. The aforementioned methods learn deterministic keypoints, without uncertainty information as our DLP. Moreover, we show that in the single-image setting we outperform <ref type="bibr" target="#b57">Zhang et al. (2018)</ref> without requiring their constraints, and outperform <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> in the image-pairs setting as well.</p><p>Recently, several methods complimented KP discovery with modules that model object parts . These methods disentangle shape and appearance by using various types of augmentations, and introduce components that mask foreground and background. This additional complexity proved useful, demonstrating outstanding results in various computer applications. Our method is simpler, as it uses standard reconstruction loss terms, and provides a natural uncertainty measure for the keypoints. Additionally, as DLP represents the whole image using the latent particles, it allows to explicitly control the generated scene, unlike previous methods that rely on feature maps from the original image, and can therefore only modify small local regions around the KP.</p><p>Unsupervised object-centric representations: unsupervised discovery of objects in scenes has mainly relied on sequential inference of objects, where in each iteration a new part of the input is attended to, or patch-based inference, where each patch can contain an object that needs to be represented. AIR <ref type="bibr" target="#b10">(Eslami et al., 2016)</ref>, SQAIR <ref type="bibr" target="#b28">(Kosiorek et al., 2018)</ref>, R-SQAIR <ref type="bibr" target="#b46">(Stani? &amp; Schmidhuber, 2019)</ref> and SPAIR <ref type="bibr" target="#b5">(Crawford &amp; Pineau, 2019)</ref> are based on sequential inference of objects and explicitly representing objects as 'what', 'where', and 'presence' latent variables, where the latter also adds a 'depth' variable. APEX (Wu et al., 2021b) leverages a similar approach but exploits temporal information in videos for better performance. These models are limited to a moderate number of objects, and typically struggle with modelling the scene's background. In contrast, our inference happens all at once, and the 'where' representation is replaced with a spatial prior over keypoint locations. MONet ) uses a sequential attention mechanism to allocate objects to slots, while Slot Attention <ref type="bibr" target="#b34">(Locatello et al., 2020)</ref> replaces MONet's multi-step procedure with a single step using iterated attention between slots. IODINE <ref type="bibr" target="#b15">(Greff et al., 2019)</ref> adds iterative refinement to objects, exhibiting similar performance to MONet, but requires more memory and is arguably more complex. GEN-ESIS <ref type="bibr" target="#b8">(Engelcke et al., 2019)</ref> and GENESISv2 <ref type="bibr" target="#b9">(Engelcke et al., 2021)</ref> build on MONet and IODINE and introduce a generative model that captures relations between scene components with an autoregressive prior. Our method does not contain autoregressive components or iterative inference.</p><p>Finally, MarioNette <ref type="bibr" target="#b44">(Smirnov et al., 2021)</ref> and SPACE <ref type="bibr" target="#b32">(Lin et al., 2020)</ref> are non-sequential patch-based approaches. SPACE factors patches into 'what', 'where', 'depth', and 'presence' in parallel and is thus more scalable than the aforementioned methods, but has a tendency to embed objects in the background. MarioNette takes a different approach and learns a deterministic dictionary of objects or sprites that can appear in a scene. However, the dictionary approach is discrete in nature and is limited to objects seen during training. Our method is also non-sequential, but is not limited to patches or a static dictionary of learned objects.</p><p>Latent video prediction: recent advances in generative modelling (e.g., <ref type="bibr" target="#b41">Razavi et al., 2019;</ref><ref type="bibr" target="#b23">Karras et al., 2020;</ref><ref type="bibr" target="#b6">Daniel &amp; Tamar, 2021)</ref> have inspired a large body of video prediction methods that employ prediction in a learned latent space <ref type="bibr" target="#b38">(Minderer et al., 2019b;</ref><ref type="bibr">Wu et al., 2021a;</ref><ref type="bibr">Walker et al., 2021;</ref><ref type="bibr">Villegas et al., 2019;</ref><ref type="bibr">Yan et al., 2021)</ref>.  <ref type="bibr" target="#b31">(Li et al., 2020)</ref> performs video prediction of physical interaction by building a causal graph from keypoints learned with Transporter <ref type="bibr" target="#b29">(Kulkarni et al., 2019)</ref>. Closely related to our work, <ref type="bibr" target="#b38">Minderer et al. (2019b)</ref> uses KeyNet  to learn keypoints, and propose a variational RNN to model stochastic dynamics. We empirically compare with this approach, and report improved performance on datasets with varying number of objects, which we attribute to the GNN in our method. However, the dynamics model in <ref type="bibr" target="#b38">Minderer et al. (2019b)</ref> is orthogonal to our work, and can potentially be used with our DLP representation as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Variational Autoencoders (VAEs): VAEs <ref type="bibr" target="#b25">(Kingma &amp; Welling, 2014</ref>) learn an approximate model of data p ? (x) using variational inference by maximizing the evidence lower bound (ELBO), which states that for any approximate pos-terior distribution q(z|x):</p><formula xml:id="formula_0">log p ? (x) ? E q(z|x) [log p ? (x|z)] ? KL(q(z|x) p(z)) . = ELBO(x),<label>(1)</label></formula><p>where the Kullback-Leibler (KL) divergence is KL(q(z|x) p(z)) = E q(z|x) log q(z|x) p(z) . Typically, Gaussian distributions are used to model the approximate posterior q ? (z|x), likelihood p ? (z|x), and prior p(z). The approximate posterior q ? (z|x) is also known as the encoder, while p ? (x|z) is termed the decoder. The ELBO can be maximized using the reparameterization trick, and in what follows, the term reconstruction error refers to log p ? (x|z).</p><p>KeyNet: The objective in <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> is to produce a set of K 2D coordinates (a.k.a. keypoints/landmarks) y = (u 1 , ..., u K ), u k ? ? for a given image, where ? = [?1, 1] 2 denotes the normalized space of 2D positions in the image. Consider an image x ? R H?W ?3 . To extract keypoints, an encoder network (CNN) outputs K feature maps S u (x; k) ? R H ?W , k = 1, ..., K. The keypoint u k is finally generated from S u (x; k) using a spatial softmax (SSM) layer <ref type="bibr" target="#b12">(Finn et al., 2016)</ref>.</p><p>Gaussian Heatmaps: To backpropagate through the keypoint positions, <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> broadcast each keypoint u k into a Gaussian-like 2D heatmap centered around u k with a small and constant standard deviation ?: ? u (x; k) = exp ? 1 2? 2 ||u ? u k (x)|| 2 . These maps are then used as part of the image reconstruction process.</p><p>Chamfer distance: the distance between point clouds S 1 and S 2 of arbitrary size can be calculated with the Chamfer distance: d CH (S 1 , S 2 ) = x?S1 min y?S2 ||x ? y|| 2 2 + y?S2 min x?S1 ||x ? y|| 2 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Our objective is to design a VAE for images x ? R H?W ?C where the latent representation is structured as a set of particles z ? R K?(2+d) , where K is the number of particles, the first two components of each particle contain information about the positions of objects, and d additional features encode information about object appearance. Explicitly, we denote z = [z p , z a ], where z p ? R K?2 and z a ? R K?d denote the position and appearance components, respectively.</p><p>There are several challenges in doing so. The first is how to disentangle the positional information from other content in the particle features, without sacrificing reconstruction quality. Our idea is to exploit the probabilistic interpretation of the VAE, and view each particle position as coming from a distribution, where its prior is given by the standard method of <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref>, which we already know produces reasonable keypoints. The posterior neural network, in contrast, is not restricted to any particular structure, to allow maximal expressiveness for accurately reconstructing the scene. The KL term in the VAE loss, which forces the posterior to be close to the prior, will drive the posterior to have positional meaning. This idea brings about another challenge -the prior can generate more keypoints than required by the posterior, so we need a method to enforce similarity between two sets of keypoint of different sizes. Finally, we need to also encode (and decode) the appearance information around the predicted keypoints. For this we propose the appearance encoder, which encodes 'glimpses' of the image around the keypoint in a differentiable way.</p><p>In the following, we explain each of these components in detail. The model is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Patch-wise Conditional Prior</head><p>In the standard VAE setting, the prior distribution is fixed and usually set to p(z) = N (0, I). Here we consider a conditional setting <ref type="bibr" target="#b45">(Sohn et al., 2015)</ref>, where we explicitly learn a prior for the keypoints given an image x,</p><formula xml:id="formula_1">p ? (z|x) = p ? (z p |x) ? p ? (z a |x). 2 We set p ? (z a |x) to the unit Gaussian prior N (0, I d ).</formula><p>The prior for the positions, however, requires more sophistication, as we describe next.</p><p>First, note that for computing the ELBO in Eq.</p><p>(1), we do not need the explicit form of the prior, but only the KL divergence between the prior and approximate posterior. In the following, the prior distribution will be defined implicitly, through a set of prior keypoint proposals, and a particular KL divergence term to be described in Section 4.3.</p><p>To generate keypoint proposals, the input x ? R H?W ?3 is split into K p (in general K p = K) patches of size D ?D (in our experiments D ? {8, 16}), and for each patch, the prior network outputs a distribution for the coordinates of a single keypoint proposal. This distribution is Gaussian N (? p , ? p ), where the standard deviation is chosen to be a fixed small constant ? p = 0.1, and the mean is the output of SSM, as described in Section 3. Therefore, for each image we obtain a set of K p unordered keypoint proposals. In practice, as the set of proposals can grow large with the number of patches, we found it useful to consider only a subset of L prior keypoints, where L is a hyper-parameter and we term the set of prior keypoints keypoint proposals. To filter out L keypoints, we explored uniform sampling of L from the set of K p proposals, and a heuristic where we keep the top-L distant keypoints from the center of their respective center. The logic behind this heuristic is that applying SSM in smooth patches (e.g., a solid color background) will result in a keypoint in the center of the patch, which might be uninformative. Both filtering methods resulted in similar performance, with a slight advantage to the heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Position Encoder and Appearance Encoder</head><p>The encoder models the approximate posterior, q ? (z|x) = q ? (z p |x) ? q ? (z a |x, z p ). The position encoder q ? (z p |x) has a similar architecture to KeyNet : the input image is downsampled with convolutional layers ending with K feature maps ? enc (x) ? R H ?W ?K . Unlike KeyNet, however, we do not apply SSM on these feature maps, but flatten them and use a fully-connected (FC) layer to map to K keypoints. The output of the FC layer is ?, log(? 2 ) ? R K?2 , the means and log variances of K independent Gaussians that make up q ? (z p |x).</p><p>For the appearance encoder, q ? (z a |x, z p ), we desire the features to encode the visual properties of the vicinity of each keypoint, in a differentiable manner. To that end, we use a Spatial Transformer Network ( <ref type="bibr">STN Jaderberg et al. 2015)</ref>, similarly to SPAIR <ref type="bibr" target="#b5">(Crawford &amp; Pineau, 2019)</ref>, to extract regions of size S ? S from the original input at the locations specified by z p , where the region size S around the keypoint is a hyperparameter (in our work, S ? {16, 32}).</p><p>These glimpses go through a small CNN ending with a FC layer mapping to the parameters ? f , log(? 2 f ) ? R d of a Gaussian distribution for the features of each particle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Chamfer-KL Distance</head><p>The use of a FC layer instead of SSM in the encoder gives the model additional freedom when choosing the ideal keypoint locations for reconstruction. Note, however, that the number of posterior keypoints K does not necessarily equal L, the number of prior keypoint proposals from p ? (z p |x).</p><p>We constrain the posterior to be close to the prior despite differences in the number and ordering of elements in each set, using a novel loss that we term the Chamfer-KL distance. Chamfer-KL views each point cloud as a set of Gaussian distributions and calculates the KL divergence between a keypoint in set S 1 and every keypoint in set S 2 :</p><formula xml:id="formula_2">d CH?KL (S 1 ,S 2 ) = x?S1 min y?S2 KL(x y)+ y?S2 min x?S1 KL(x y).</formula><p>Note that unlike the L 2 distance, the KL is asymmetrical and thus not a metric, a property which is maintained in the Chamfer-KL as d CH?KL (S 1 , S 2 ) = d CH?KL (S 2 , S 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Decoder</head><p>The decoder in a VAE maps the latent variable z into an image. For our particle-based latents, we found that different decoder architectures work better for different types of scenes, depending on the variation of the background in the data, and whether the scene is composed of many separated objects or not. We next describe three basic decoder components, and how to combine them for different scenes.</p><p>The basic decoder component is an upsampling CNN <ref type="figure">Figure 1</ref>. DLP architecture. Image is processed by the position encoder to produce the posterior probability of latent particle positions. These positions are used to extract glimpses from the original images using a STN, which are then processed by an appearance encoder to produce the appearance features for each particle. The input image (or an augmented view of it) is also processed by the prior network, producing keypoint proposals via SSM. To reconstruct the image, the particles are (1) transformed to differentiable Gaussian heatmaps and (2) go through a PointNet++ to produce feature maps ? graph . For the Masked model, the heatmaps are used as binary masks to combine local regions from ? graph with bypass features from the encoder. For the Object model, a separate glimpse decoder is used to decode RGBA patches, which are then combined with feature maps ? graph to produce the output image. See text for full details. D upsample taking in input feature maps and outputting the reconstructed imagex ? R H?W ?3 . The input feature maps of D upsample are comprised of a concatenation of Gaussian heatmaps ? heatmap constructed from the coordinates of each particle as described in Section 3, and feature maps. We explored two methods for generating the feature maps, the graph component ? graph and the bypass component ? bypass .</p><p>Additionally, for multi-object scenes, we introduce a separate glimpse decoder D glimpse , taking in a single particle and outputting an RGBA patch of the surrounding region around the particle. The output of D upsample and the output of D glimpse can be stitched together to create the final reconstructed image. We next overview each component; a full technical description is in the Appendix.</p><p>Graph component ? graph : The goal of this component is to use the particles for modelling the global structure of the scene. We create a KNN graph, where nodes are the particles, and edges are connected based on the Euclidean distance between particle positions. This graph is processed by a PointNet++ <ref type="bibr" target="#b40">(Qi et al., 2017)</ref>, which, using global max pooling, outputs feature maps ? p (x) ? R H ?W ?M . 3</p><p>Bypass component ? bypass : The goal of this component is to supply the background features that are not modelled by the particles. We simply take the feature maps from the encoder ? enc (x). A similar component was employed in <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref>, but was used to model the whole scene <ref type="bibr">3</ref> In all our experiments we arbitrarily chose M = K. without considering the features around the keypoints.</p><p>Glimpse decoder D glimpse : This component reconstructs each object's appearance independently, as a combination of RGB values and a mask (alpha channel). We use a fully-connected layer followed by a small upsampling CNN that takes in a single latent particle z i and decodes an RGBA patch of the surrounding region around the particl? x p i ? R S?S?4 . The decoded patches are positioned in the full H ? W canvas according to their respective particle's position, using a STN.</p><p>Equipped with the decoder components defined above, we now describe two decoder combinations that we explored:</p><formula xml:id="formula_3">1. Masked model: concat[? heatmap , ? ? ? graph , (1 ? ?) ? ? bypass ] ? D upsample .</formula><p>The masks ? are generated from the Gaussian heatmaps ? heatmap , and represent foreground.</p><formula xml:id="formula_4">2. Object-based model: ? ? D glimpse + (1 ? ?) ? (concat[? heatmap , ? graph ] ? D upsample ).</formula><p>The masks ? are given by the alpha channel of D glimpse . 4</p><p>The Object-based model has a pure bottleneck -all information from the encoder to the decoder flows through the latent representation. In the Masked model, however, the bypass component skips the bottleneck. As we show in our experiments, the pure bottleneck model allows the particles more control over the output image, while the Masked model excels in reconstruction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training the Model</head><p>The training procedure is based on maximizing the ELBO (1). Since all of our distributions are Gaussians, the KL divergence has a closed form solution. Similar to the ?-VAE <ref type="bibr" target="#b18">(Higgins et al., 2017)</ref>, we multiply the KL and Chamfer-KL terms in the loss by hyperparameters ? KL and ? CKL , respectively. To obtain better image quality, we replace the typical MSE reconstruction error with a variant of the VGG perceptual loss <ref type="bibr" target="#b19">(Hoshen et al., 2019</ref>) that calculates the L 2 distance between the extracted features instead of the L 1 , similarly to <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref>.</p><p>The model is optimized end-to-end by Adam <ref type="bibr">(Kingma &amp; Ba, 2014)</ref>, using the reparametrization trick, and is implemented in PyTorch <ref type="bibr" target="#b39">(Paszke et al., 2017)</ref>. Extended implementation details can be found in the Appendix. Our code is available publicly 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our method produces latent particles to represent an input image. We design our set of experiments to answer the following questions: (1) does our method effectively disentangle position from appearance in various scene types;</p><p>(2) how important are our novel Chamfer-KL and the conditional SSM prior components; <ref type="formula">(3)</ref> what is the quality of our particles compared to other unsupervised keypoint discovery methods; (4) can our approach be used for downstream tasks such as image manipulation; and (5) can we exploit our probabilistic formulation to infer uncertainty estimates for the particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Linear Regression on Face Landmarks</head><p>A standard quantitative evaluation of unsupervised keypoint discovery is based on the error in predicting annotated keypoints from the discovered keypoints.</p><p>The common benchmark for this task uses the CelebA train set while excluding the MAFL <ref type="bibr" target="#b53">(Zhang et al., 2014b)</ref> subset which includes annotations for 5 facial landmarks-eyes, nose and mouth corners. We train our Masked-model with a similar architecture and pre-processing as Jakab et al. (2018a) on 128 ? 128 face images from CelebA <ref type="bibr" target="#b33">(Liu et al., 2015)</ref>. For the prior keypoint proposals we follow <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> and use their proposed thin-plate-spline (TPS) augmentation for the prior image x p , which is split to patches of size 8 ? 8 (D = 8), resulting in a total of K p = 256 which are then filtered to L = 50 KP pro-  <ref type="bibr">(2018a)</ref> and use the unsupervised keypoints to regress from K = {25, 30, 50} to the annotated keypoints in the MAFL dataset. The linear regressor is learned without the bias term. For the regressor input, we experiment with using just the mean ? as features (deterministic KP, as in all previous works) and using the mean ? and the log-variance log(? 2 ) as features. In <ref type="table" target="#tab_1">Table 1</ref> we report the results in terms of the standard MSE normalized by the inter-ocular distance expressed as a percentage. It can be seen that our method improves upon the SOTA in unsupervised keypoint discovery. Complete results, hyperparameters and an extended comparison can be found in Appendix E.</p><p>Information from uncertainty: to test whether the learned variance of each particle contains meaningful information, we performed two experiments. First, we trained our model with K = 25 particles and used the mean ? and the logvariance log(? 2 ) as features for the supervised regression task described above. As seen in <ref type="table" target="#tab_1">Table 1</ref>, we outperform <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> with K = 50, even though the number of input features to the regressor is the same. In the second experiment, we used the model trained on K = 30 keypoints and chose the 10 keypoints with the highest variance and 10 keypoints with lowest variance, and used their means ? as the input features to the regressor. For the lowvariance batch, we report an error of 5.75%, while for the high-variance batch the error was 7.54%. The results from both experiments indicate that the posterior variance is related to uncertainty in the location of the keypoints, and can be useful for decision making in downstream tasks.</p><p>We further illustrate the connection between the location of the particles and their variance in Appendix E.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Scene Decomposition and Image Manipulation</head><p>A hallmark of latent space generative models is the ability to change the image by controlling the latents <ref type="bibr" target="#b6">(Daniel &amp; Tamar, 2021;</ref><ref type="bibr" target="#b23">Karras et al., 2020)</ref>. Our model allows to modify the image in an intuitive way, by simply moving around the particles.</p><p>We first demonstrate the latent control with the Masked model, trained with K = 30 particles on CelebA <ref type="bibr" target="#b33">(Liu et al., 2015)</ref> (cf. Section 5.1), and compare with KeyNet , using their published pre-trained model with K = 30 keypoints <ref type="bibr" target="#b22">(Jakab et al., 2018b)</ref>. To perform manipulation, we visually locate the keypoints on the nose and mouth, slightly change their coordinates (leaving their features the same), and decode a new image. For a fair comparison, as keypoints differ between the models, we manually chose and moved the keypoints of <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref> to produce the most visually pleasing results. In <ref type="figure">Figure 2</ref> we show the keypoints detected by each model, the reconstruction, and the resulting reconstruction after performing the above manipulation. As our latent space is structured to disentangle position and appearance, changing the position only affects the respective area of the particle by performing a smooth interpolation. KeyNet, on the other hand, has limited controllability as the latent space is only represented by the keypoints, and the features are propagated from the encoder, resulting in a blurry area near the keypoint position. Note that the manipulation in our model had a semantic effect -moving the lip particle closed the mouth and hid the teeth, while moving the nose particle up exposed the nostrils.</p><p>As can be observed in <ref type="figure">Figure 2</ref>, several particles may be located in a small region (e.g., multiple particles near the nose). This is an attribute of the model, when the number of keypoints chosen is larger than the natural number of keypoints required to represent the variation in the data. In such a case, not all particles have 'control': manipulating them will have no effect on the reconstructed image. Interestingly, we found that controllable, or salient, particles are assigned lower uncertainty, and a simple filtering heuristic can be used to automatically select the top-K salient particles, as we show in the supplementary material.</p><p>Next, we train our Object model on two multiple-object datasets: CLEVRER (Yi et al., 2019) dataset and Traffic -a self-collected traffic camera dataset. The CLEVRER dataset is composed of 5-second (128 frames) video of rigid objects colliding, where each frame can contain up to 8 objects of various shapes and colors. For this dataset, we learn K = 10 particles with feature dimension d = 5. Traffic is composed of 44,000 frames containing cars of different sizes and shapes. For this dataset, we learn K = 15 particles with feature dimension d = 20. We emphasize that while these datasets contain videos, our method works on single images, and therefore ignores any temporal relation between the frames. We downscale the frames in both datasets to 128 ? 128, use a glimpse size S = 32, and do not use augmentations, i.e., x p = x.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref> we visualize a sample of the detected KP, the reconstructed images, the detected objects and their masks, and image manipulation by moving the KP (features remain the same). Evidently, our method can learn to decompose scenes with a varying number of objects of different shapes and sizes, and allows for particle-based manipulation of scenes where particles control objects.</p><p>Additional results can be found in Appendix E.3 6 .</p><p>We finally remark that empirically, we found that applying the Masked model in object-based scenes resulted in worse reconstructions, where objects were reconstructed as blurry blobs. Alternatively, using the Object-based model for non-object-based scenes reduced the manipulation ability. We noticed that the model tended to assign 'objects' to high-contrast parts of the image, such as the hairline and eyebrows, and ignored smoother parts such as the nose. We believe this is since high-contrast objects are easier to reconstruct using the alpha channel approach in the Object-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Particle-based Video Prediction</head><p>The image manipulation results above suggest that our particles effectively control scene generation. We capitalize on this observation, and suggest to use particles for video prediction. Recall that particles are learned per image; our idea is to also learn a predictor for the temporal change in particles, from video sequence data. A natural predictor that can exploit the disentangled position and appearance is a Graph Convolutional Network (GCN, Kipf &amp; Welling 2016), where each particle is a node, and connectivity is based on the Euclidean distance between particle positions. In this work, for simplicity, we chose a deterministic prediction model. Stochastic prediction, such as in <ref type="bibr" target="#b38">Minderer et al. (2019b)</ref>, can also be used with our model, and we leave that to future work.</p><p>We demonstrate our approach on the Traffic dataset, using our DLP model from Section 5.2. We employ a 2-layer Gated GCN <ref type="bibr" target="#b1">(Bresson &amp; Laurent, 2017)</ref> to predict the change in position ?z p and appearance features ?z a for each particle. To reduce drift in appearance, we constrain the maximal ?z a to a small value. The GCN is trained to predict particles of two consecutive frames [t, t + 1] from particles in two previous frames [t ? 1, t]. Since DLP is trained per-image, particles in consecutive frames do not necessarily match. Therefore, we do not have a ground truth for ?z p , ?z a . 7 Instead, we decode an image from the predicted particles, and train the GCN to minimize the perceptual loss <ref type="bibr" target="#b19">(Hoshen et al., 2019)</ref> with the ground truth future frame. Video pre- <ref type="figure">Figure 2</ref>. Image manipulation comparison with KeyNet . We visualize the keypoints learned by each model, the reconstruction, and the effect that moving keypoints on the nose and the mouth has on the output image. dictions are generated by rolling out the GCN, starting from the particles of the first two images in the video, and using the decoder to generate images from predicted particles. We provide the complete technical description of our method in the supplementary. As a baseline, we trained the method of <ref type="bibr" target="#b38">Minderer et al. (2019b)</ref>, where keypoints are learned with KeyNet , on the Traffic dataset with the recommended hyperparameters <ref type="bibr" target="#b37">(Minderer et al., 2019a)</ref>.</p><p>As can be seen in <ref type="figure" target="#fig_2">Figure 4</ref>, our approach produces sharp predictions, even for significantly longer horizons than trained on. We see this as a promising approach to video prediction, which is often prone to blurriness <ref type="bibr" target="#b30">(Lee et al., 2018;</ref><ref type="bibr">Yan et al., 2021;</ref><ref type="bibr" target="#b16">Hafner et al., 2020)</ref>. We provide more details and results in the appendix. Note that the video prediction quality of the <ref type="bibr" target="#b38">Minderer et al. (2019b)</ref> baseline is significantly worse. In the appendix, we verify that the baseline obtains good reconstructions per single frames. We hypothesize that the reason for the poor video prediction is the varying number of objects (cars) in the scene: in all of the experiments in <ref type="bibr" target="#b38">Minderer et al. (2019b)</ref>, the number of objects in the scene was fixed, and thus their recurrent neural network approach was reasonable. Our GNN approach can better account for a variable number of objects. Additionally, as the baseline uses KeyNet, it is less capable of manipulability, as we demonstrated above.</p><p>For more complex scenes such as CLEVRER, we found that our simple approach does not work well enough, as we describe in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablative Analysis</head><p>We evaluate the importance of our novel components, the Chamfer-KL and SSM prior. The ablation of Chamfer-KL uses the conventional KL calculation performed by flattening the particle representation to a vector. For this comparison, we chose a model with the same number of prior and posterior keypoints, K p = K = 30. For an ablation of the SSM-based conditional prior, we experimented with two alternative priors: (1) a Gaussian prior N (0, 0.1 2 ), similar to the standard VAE setting; and (2) prior keypoint proposals sampled uniformly U[?1, 1] instead of using the SSM. We experiment with the supervised KP regression task using the Masked-model. We run the training for 50 epochs and keep the rest of the hyperparameters similar to Section 5.1. As shown in <ref type="table" target="#tab_2">Table 2</ref>, using the Chamfer-KL and the SSMbased prior significantly improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Future Work</head><p>We illustrate several limitations of our method, which we observed when trying to predict video on CLEVRER.   <ref type="table" target="#tab_1">Table  1</ref>, see text for full information.</p><p>A particle shared between two objects: when the data contains objects that frequently appear together, a single particle can be assigned to more than one object. This is due to our single-image formulation -there is no signal for our method to separate the two objects. We believe that our method can be extended to a sequential setting, using the prior x p , which would potentially resolve this issue for objects that change position during the video. Large objects and occlusions: several particles are assigned to large objects. This can become a problem when objects get partially occluded, requiring drastic changes to the particle features. More expressive GCNs trained on longer horizons may potentially mitigate this problem. An alternative is to modify the model to better account for occlusions. Changing background: the proposed model will have difficulty when the background changes dramatically, such as when the camera is moving. Incorporating some background detection into our method is one direction for addressing this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work we showed that the classical concept of keypoint detection can be viewed in the lens of deep generative modelling, by viewing the keypoints themselves as the latent variables in a variational autoencoder model. Beyond the elegance of the formulation, we showed that our method can generate SOTA results in keypoint discovery, and be used for intriguing image manipulations.</p><p>Many questions remain. For example, extending the method to handle more complex video prediction, where objects change appearance dramatically, or occlude other objects. Another exciting direction is to leverage developments in VAEs, such as the vector-quantized VAE <ref type="bibr" target="#b41">(Razavi et al., 2019)</ref> for improved performance. We also intend to explore the use of the uncertainty estimate in our model for decision making. More broadly, we are hopeful that this new connection between generative models and keypoint detection will spur up interesting developments in image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>The authors would like to thank Zhang, Z., Luo, P., Loy, C. C., and Tang, X. Facial landmark detection by deep multi-task learning. In European conference on computer vision, pp. 94-108. Springer, 2014b.</p><p>Zhang, Z., Luo, P., Loy, C. C., and Tang, X. Learning deep representation for face alignment with auxiliary attributes. IEEE transactions on pattern analysis and machine intelligence, 38(5):918-930, 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended Architecture Details</head><p>Encoders: all of the encoders described in this work follow a similar scheme to <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref>. Unless mentioned otherwise, input images are assumed to be of 128 ? 128 resolution. The position encoder is composed of a CNN with convolutional blocks where each block contains a convolutional layer, followed by Batch Normalization and ReLU activation. Downsmapling is performed by using strided (s = 2) convolutional layers with 'replication' padding. The channels of each convolutional block are <ref type="bibr">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256]</ref> and the feature maps are of shape 16 ? 16 ? 256. These maps are flattened and fed into 3-layer fully-connected network with hidden layers of size <ref type="bibr">[256,</ref><ref type="bibr">128]</ref>, each activated with ReLU, outputting K ? 4 position values, reshaped to [?, log(? 2 )]. The prior encoder, operating on patches of sizes 16 ? 16 or 32 ? 32, is composed of a CNN with a similar structure to the position encoder with channels <ref type="bibr">[16,</ref><ref type="bibr">32,</ref><ref type="bibr">64]</ref> followed by a spatial softmax (SSM) block. Finally, the glimpse encoder has similar structure as the prior encoder, but the SSM is replaced with a FC network similar to the one in the position encoder.</p><p>? graph -PointNet++: to decode feature maps from the latent particles, we use a PointNet++ <ref type="bibr" target="#b40">(Qi et al., 2017)</ref> implemented as a GNN <ref type="bibr" target="#b42">(Scarselli et al., 2009)</ref>. First, a KNN graph with K = 10 is built from the position of the particles. This graph is then processed by a 4-layer PointNet++ layers, composed of 1-D convolution, ReLU activation and Batch <ref type="bibr">Normalization,</ref><ref type="bibr">with channels [64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>. The output of the convolutional blocks is then max-pooled to produce a vector which is reshaped to <ref type="bibr">[K, 8, 8</ref>] maps, where K is the number of particles. This is implemented efficiently with the torch geometric library <ref type="bibr" target="#b11">(Fey &amp; Lenssen, 2019)</ref>. Finally, these maps are upsampled with a convolutional layer to <ref type="bibr">[16, 16, K]</ref> maps.</p><p>D upsample -Upsampling CNN: this component takes in a concatenation of feature maps; For the Masked model, the concatenation is of the Gaussian heatmaps ? heatmap , masked feature maps from the encoder ? enc and masked feature maps from ? graph . The final shape of the input to D upsample is <ref type="bibr">[16, 16, K ? 3]</ref>. For the Object model, the concatenation is of the Gaussian heatmaps ? heatmap and feature maps ? graph . The final shape of the input to D upsample is [16, 16, K ? 2]. The upsampling CNN network has a symmetric structure to the CNN in the position encoder, but in reverse, namely, the CNN channels are: <ref type="bibr">[256,</ref><ref type="bibr">128,</ref><ref type="bibr">64,</ref><ref type="bibr">32]</ref>. Finally, the output is aggregated with a 1 ? 1 convolutional layer to produce a RGB image of shape: <ref type="bibr">[128,</ref><ref type="bibr">128,</ref><ref type="bibr">3]</ref>.</p><p>D glimpse -Glimpse decoder: this component takes in a latent vector which is mapped with a 2-layer fully-connected network with 256 hidden units each and ReLU activated to a vector of size 8 ? 8 ? 32. This vector is reshaped to feature maps of shape <ref type="bibr">[8,</ref><ref type="bibr">8,</ref><ref type="bibr">32]</ref> that are upsampled with a 2-layer CNN (similar blocks as before) with 64 channels in each layer to maps of shape <ref type="bibr">[S, S, 64]</ref>, where S is the glimpse size. This output is aggregated with a 1 ? 1 convolutional layer and activated with a Sigmoid activation to produce RGBA patches of shape [S, S, 4].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extended Implementation and Training Details</head><p>In this section, we describe important implementation and training details for convergence of our method.</p><p>Training objective: the complete training objective follows the ?-VAE <ref type="bibr" target="#b18">(Higgins et al., 2017)</ref> formulation:</p><formula xml:id="formula_5">L = L rec (x,x) + ? CKL CH ? KL(q ? (z p |x) p ? (z|x)) + ? KL KL(q ? (z a |x) p(z)).</formula><p>We found it crucial to balance the two KL terms, where usually ? CKL &gt; ? KL . We report the exact values for each task in Appendix D.</p><p>Initialization, LR scheduling and latent activation: in our implementation, we initialized the convolutional layers with values from N (0, 0.01). We used a general multi-step learning scheduler that decreases the learning by 0.5 in each milestone. The milestones are reported in Appendix D. Moreover, to constrain the values of the particles' position to be in the range of [?1, 1], we used a TanH activation on ?.</p><p>Binary masks from Gaussian heatmaps: for the Masked model, we use a binary mask M u (x) created from the Gaussian as follows: Warm-up, noisy masks and transparency variable: for the Object model, we found it beneficial to have a short warm-up stage of 2 epochs where only the glimpse encoder and glimpse decoder are training to encode and decode patches. This warm-up step prevents D upsample to take full responsibility of the reconstruction and ignore D glimpse . Moreover, at the beginning of the training (5-10 epochs) we add a small Gaussian noise N (0, 0.01) to the alpha channel of the decoded patches following <ref type="bibr" target="#b44">Smirnov et al. (2021)</ref>. This additional step encourages learning sharper object masks. Finally, following <ref type="bibr" target="#b44">Smirnov et al. (2021)</ref>, we learn an additional 'transparency' parameter for each particle z on ? [0, 1] which is multiplied by the decoded glimpse prior to stitching the final reconstructed image. This implemented by extending the output dimension of the FC layer in ? enc and using Sigmoid activation for that variable.</p><formula xml:id="formula_6">M i u (x) = 1, ? i heatmap (x) ? ? 0, else , Algorithm 1 Stitching Algorithm Input: alpha maps a K i=1 , RGB maps r K i=1 ,</formula><p>Frozen prior network Interestingly, even though the prior network can be trained (the SSM is differentiable), we found that in some datasets (e.g. Traffic and CLEVRER), keeping the network frozen, with the initial random weights, worked better. This is inline with the findings of <ref type="bibr" target="#b13">Frankle et al. (2020)</ref>.</p><p>Stitching the image: we follow a layer-wise approach <ref type="bibr" target="#b44">(Smirnov et al., 2021)</ref> and describe our stitching algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets</head><p>In this section we provide a detailed description of the datasets we used throughout this paper.</p><p>CelebA <ref type="bibr" target="#b33">(Liu et al., 2015)</ref>: this dataset contains 200k images of celebrity faces which are cropped and resized to 128 ? 128 following <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref>; <ref type="bibr" target="#b48">Thewlis et al. (2017a)</ref>. The dataset provides annotations for 5 facial landmarks -eyes, nose and mouth corners, which are not required during training. As per the setting of <ref type="bibr" target="#b48">Thewlis et al., 2017a)</ref>, the MAFL <ref type="bibr" target="#b53">(Zhang et al., 2014b)</ref> test-set is excluded from training. <ref type="bibr">Yi et al., 2019)</ref>: this dataset contains 20,000 synthetic videos of moving and colliding objects, separated to 10,000 train video, 5,000 validation videos and 5,000 test videos, where each video is 5 seconds long and contains 128 frames with resolution 480 ? 320. In our work, we resize the frames to 128 ? 128 and use a subset of the frames created by skipping every second frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLEVRER (</head><p>Traffic: a self-collected traffic camera dataset composed of 44,000 frames resized to 128 ? 128 containing cars of different sizes and shapes, where we take the first 90% of the frames for train and the rest for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameters Details</head><p>In this section we provide the complete set of hyperparameters used for the experiments in this work. The shared hyperparameters between all of the experiments are described below, and the rest can be found in <ref type="table" target="#tab_5">Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Complete Results</head><p>In this section, we provide extended results for the various tasks we presented in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Supervised Regression on Face Landmarks</head><p>In <ref type="table">Table 4</ref> we present the complete set of results for the supervised KP regression task. It can be seen that our method outperforms the supervised and unsupervised benchmark for K = {30, 50}, and combined with the uncertainty information and the learned features, the results are further improved. It is worth noting that simply learning features without the notion of location results in bad performance as reported by <ref type="bibr" target="#b21">Jakab et al. (2018a)</ref>, stressing that the learned features are only informative when learned with respect to their position information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Uncertainty Information Analysis</head><p>In this section, we demonstrate a visual connection between the location of each particle and its learned variance. Each keypoint u k is defined by the Gaussian parameters (? k , ? k ), where ? 2 k can be interpreted as the variance in the location of this keypoint. Intuitively, for common patterns in the data, we should expect the variance to be small. Accordingly, we define the per-keypoint uncertainty as follows:</p><formula xml:id="formula_7">V (u k ) . = i log(? 2 ki ),</formula><p>where ? ki is the standard deviation in the i th axis (i.e., the x and y coordinates) of u k . 8</p><p>To test our hypothesis, we use two trained models: (1) Masked model from Section 5.1 on 128 ? 128 face images from CelebA <ref type="bibr" target="#b33">(Liu et al., 2015)</ref> and <ref type="formula">(2)</ref> Object model from Section 5.2 on the Traffic dataset. The Masked was trained with K = 30 particles and the Object model with K = 15. In <ref type="figure" target="#fig_4">Figure 5</ref> we plot the K keypoints learned by our model and display the top-10 keypoints with highest confidence. It can be seen that the keypoints with the highest confidence lie on locations that are common across the dataset (i.e., eyes, nose and mouth) while the rest lie in regions of higher variability (e.g., hair and background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Scene Decomposition and Image Manipulation</head><p>We provide more results for the different experiments described in Section 5. First, we compare image manipulation of the Masked model with KeyNet  on CelebA in <ref type="figure">Figure 6</ref>. The experimental setting, where both models learn K = 30 keypoints, is the same as in Section 5.2.  <ref type="table">Table 4</ref>. Comparison with state-of-the-art on MAFL. K is the number of unsupervised landmarks. Reported results are the MSE in % between predicted and ground-truth (lower is better). In <ref type="figure">Figure 7</ref> we provide more manipulations on CelebA produced by moving the particles in the face area.</p><p>Next, we provide extended results of our Object model. In <ref type="figure">Figures 8 and 9</ref> we visualize the detected particles, the reconstructed images, the objects and masks -output of the glimpse decoder, and image manipulation based on moving the particles on CLEVRER and Traffic, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Video Prediction</head><p>We present more results for the video prediction experiment we presented in Section 5.3. Video predictions are generated by rolling out the GCN, starting from the particles of the first two images in the video, and using the decoder to generate images from predicted particles. As can be seen in <ref type="figure">Figures 10 and 11</ref>, our approach produces sharp predictions, even for significantly longer horizons than trained on. For animated sequences please see supplementary material. In <ref type="figure" target="#fig_1">Figure 13</ref> we plot show single-frame reconstructions and video prediction of the method proposed in <ref type="bibr" target="#b38">Minderer et al. (2019b)</ref> trained on the Traffic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Video Prediction Experiment Details</head><p>In this section, we provide technical details of the video prediction experiment we described in Section 5.3. The objective in this experiment is to learn a predictor for the temporal change of the particles, from video sequence data. Recent advances in graph neural networks (GNN, <ref type="bibr" target="#b42">Scarselli et al. 2009</ref>) provide a natural framework to learn on graph-structured data. GNNs, such as Graph Convolutional Network (GCN, <ref type="bibr" target="#b27">Kipf &amp; Welling 2016)</ref>, can learn to extract details relevant to interaction between nodes in the graph. With DLP, we model each particle as a node and the connectivity between nodes is based on the Euclidean distance between particle positions. Furthermore, each node is described by the particle's features, namely, its position and appearance features.</p><p>To predict the change in position ?z p and appearance features ?z a for each particle, we employ a 2-layer Gated GCN (Bresson &amp; Laurent, 2017), with 128 hidden units each and activated with ReLU. To make the network more expressive, we add a 2-layer 1D convolutional network implemented as a shared MLP operating on each node separately, with 128 units each and activated with ReLU. The GCN is trained to predict particles of two consecutive frames [t, t + 1] from particles in two previous frames [t ? 1, t] as follows: first, for each frame, we extract its respective particle-representation using a pre-trained DLP model and we concatenate a one-hot vector to its features indicating the time-step (e.g., [1, 0] is concatenated to the features of the particle representation at time-step t ? 1). Then, we build a radius (r = 0.2) graph from the resulting set of nodes based on the Euclidean distance between the particles. This graph is fed into the GCN, which via the message-passing process outputs ?z p and ?z a for the consecutive time-steps [t, t + 1]. These updates are activated with a TanH activation and multiplied by constants ? p = 0.2 and ? a = 0.02 to constrain the maximal ?z p and ?z a , respectively. Finally, the ? is added to the original particles (a residual connection) and the new particle is decoded back to the image space using the</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Walker et al. (2021); Yan et al. (2021) model a sequence of discrete latent variables in the latent space of a vector-quantized VAE, while Wu et al. (2021a) and Villegas et al. (2019) focus on scaling-up latent autoregressive video prediction, the first via hierarchical VAEs and the latter via large stochastic recurrent neural networks (RNNs). V-CDN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Scene decomposition and manipulation with Object model. Left -CLEVRER, right -Traffic. We show the detected particles, the reconstructed images, the objects and masks -output of the glimpse decoder, and image manipulation based on moving the particles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Video prediction on the Traffic dataset. We use the pre-trained Object model to provide the particle-representation and a GNN to predict the temporal change in particles.Top -ground truth, middle -Minderer et al. (2019b)  prediction, bottom -DLP (ours) prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>background map b Initialize currM ask = a[1], masks = []. for i = 2 to K do availableSpace = 1.0 ? currM ask currM askT mp = M in(availableSpace, a[i]) Append currM askT mp to masks currM ask = currM ask + currM askT mp end for alphaM ask ? Sum(masks) rec = (1 ? alphaM ask) * bg + masks * r Return rec for some threshold ? (? = 0.2 in our experiments). Now, the input of D upsample is a concatenation of ? heatmap , M ? graph and (1 ? M ) ? enc (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Information from uncertainty. We show the K = 30 particles (second row) learned from two models and top-10 particles with the highest confidence (third row): (1) Masked model trained on CelebA (left); and (2) Object model trained on Traffic (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Department of Electrical and Computer Engineering, Technion -Israel Institute of Technology, Haifa, Israel. Correspondence to: Tal Daniel &lt;taldanielm@campus.technion.ac.il&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art on MAFL. K is the number of unsupervised landmarks. We report the MSE in % between predicted and ground-truth (lower is better) obtained from the Masked-model.</figDesc><table><row><cell>Method</cell><cell>K MAFL</cell></row><row><cell>Zhang (Zhang et al., 2018)</cell><cell>30 3.16</cell></row><row><cell cols="2">KeyNet (Jakab et al., 2018a) 30 2.58</cell></row><row><cell></cell><cell>50 2.54</cell></row><row><cell>Ours</cell><cell>25 2.87</cell></row><row><cell></cell><cell>30 2.56</cell></row><row><cell></cell><cell>50 2.43</cell></row><row><cell>Ours+ (with log-variance)</cell><cell>25 2.52</cell></row><row><cell></cell><cell>30 2.49</cell></row><row><cell></cell><cell>50 2.42</cell></row></table><note>5 https://github.com/taldatech/ deep-latent-particles-pytorchposals. We follow Thewlis et al. (2017a;b); Jakab et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study. The effect of the choice of prior and KL implementation on the supervised KP regression on the MAFL dataset. Reported results are the MSE in % between predicted and ground-truth KP (lower is better). Setting is different than in</figDesc><table><row><cell>Prior/ KL</cell><cell cols="2">Standard KL Chamfer-KL</cell></row><row><cell cols="2">Constant N (0, 0.1 2 ) 3.18</cell><cell>3.07</cell></row><row><cell>Random U[?1, 1]</cell><cell>3.39</cell><cell>2.99</cell></row><row><cell>SSM</cell><cell>3.24</cell><cell>2.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Thewlis, J., Bilen, H., and Vedaldi, A. Unsupervised learn-  ing of object frames by dense equivariant image labelling. arXiv preprint arXiv:1706.02932, 2017b.Villegas, R., Pathak, A., Kannan, H., Erhan, D., Le, Q. V., and Lee, H. High fidelity video prediction with large stochastic recurrent neural networks.</figDesc><table><row><cell>Advances in Neural</cell><cell></cell></row><row><cell>Information Processing Systems, 32, 2019.</cell><cell></cell></row><row><cell>Walker, J., Razavi, A., and Oord, A. v. d. Predicting video</cell><cell></cell></row><row><cell>with vqvae. arXiv preprint arXiv:2103.01950, 2021.</cell><cell></cell></row><row><cell>Watters, N., Matthey, L., Bosnjak, M., Burgess, C. P.,</cell><cell></cell></row><row><cell>and Lerchner, A. Cobra: Data-efficient model-based</cell><cell></cell></row><row><cell>rl through unsupervised object discovery and curiosity-</cell><cell></cell></row><row><cell>driven exploration. arXiv preprint arXiv:1905.09275,</cell><cell></cell></row><row><cell>2019.</cell><cell></cell></row><row><cell>Wiles, O., Koepke, A., and Zisserman, A. Self-supervised</cell><cell></cell></row><row><cell>learning of a facial attribute embedding from video. arXiv</cell><cell></cell></row><row><cell>preprint arXiv:1808.06882, 2018.</cell><cell></cell></row><row><cell>Wu, B., Nair, S., Martin-Martin, R., Fei-Fei, L., and Finn,</cell><cell></cell></row><row><cell>C. Greedy hierarchical variational autoencoders for large-</cell><cell></cell></row><row><cell>scale video prediction. In Proceedings of the IEEE/CVF</cell><cell></cell></row><row><cell>Conference on Computer Vision and Pattern Recognition,</cell><cell></cell></row><row><cell>pp. 2318-2328, 2021a.</cell><cell></cell></row><row><cell>Wu, Y., Jones, O. P., Engelcke, M., and Posner, I.</cell><cell></cell></row><row><cell>Apex: Unsupervised, object-centric scene segmentation</cell><cell></cell></row><row><cell>and tracking for robot manipulation. arXiv preprint</cell><cell></cell></row><row><cell>arXiv:2105.14895, 2021b.</cell><cell></cell></row><row><cell>Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. Videogpt:</cell><cell></cell></row><row><cell>Video generation using vq-vae and transformers. arXiv</cell><cell></cell></row><row><cell>preprint arXiv:2104.10157, 2021.</cell><cell></cell></row><row><cell>Yi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A.,</cell><cell></cell></row><row><cell>and Tenenbaum, J. B. Clevrer: Collision events for</cell><cell></cell></row><row><cell>video representation and reasoning. arXiv preprint</cell><cell></cell></row><row><cell>arXiv:1910.01442, 2019.</cell><cell></cell></row><row><cell>Zhang, J., Shan, S., Kan, M., and Chen, X. Coarse-to-fine</cell><cell></cell></row><row><cell>auto-encoder networks (cfan) for real-time face alignment.</cell><cell></cell></row><row><cell>In ECCV, 2014a.</cell><cell>Ron Mokadi for collect-</cell></row><row><cell>Zhang, Y., Guo, Y., Jin, Y., Luo, Y., He, Z., and Lee, H.</cell><cell>ing and sharing the Traffic dataset used in this work. This</cell></row><row><cell>Unsupervised discovery of object landmarks as structural</cell><cell>work is partly funded by the Israel Science Foundation (ISF-</cell></row><row><cell>representations. In Proceedings of the IEEE Conference</cell><cell>759/19), the Open Philanthropy Project Fund, an advised</cell></row><row><cell>on Computer Vision and Pattern Recognition, pp. 2694-</cell><cell>fund of Silicon Valley Community Foundation, and by the</cell></row><row><cell>2703, 2018.</cell><cell>European Union. Views and opinions expressed are however</cell></row><row><cell></cell><cell>those of the author(s) only and do not necessarily reflect</cell></row><row><cell></cell><cell>those of the European Union or the European Research</cell></row><row><cell></cell><cell>Council. Neither the European Union nor the granting au-</cell></row><row><cell></cell><cell>thority can be held responsible for them.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>datasetModel K K p L ? CKL ? KL prior patch size glimpse size S feature dim d epochs</figDesc><table><row><cell>CelebA</cell><cell>Masked 10 256 15</cell><cell>20</cell><cell>20  *  0.001</cell><cell>8</cell><cell>16</cell><cell>30</cell><cell>90</cell></row><row><cell>CelebA</cell><cell>Masked 25 256 30</cell><cell>30</cell><cell>30  *  0.001</cell><cell>8</cell><cell>16</cell><cell>10</cell><cell>90</cell></row><row><cell>CelebA</cell><cell>Masked 30 256 50</cell><cell>40</cell><cell>40  *  0.001</cell><cell>8</cell><cell>16</cell><cell>10</cell><cell>90</cell></row><row><cell>CelebA</cell><cell>Masked 50 256 50</cell><cell>50</cell><cell>50  *  0.001</cell><cell>8</cell><cell>16</cell><cell>10</cell><cell>90</cell></row><row><cell>Traffic</cell><cell>Object 15 64 64</cell><cell>30</cell><cell>30  *  0.001</cell><cell>16</cell><cell>32</cell><cell>20</cell><cell>100</cell></row><row><cell cols="2">CLEVRER Object 10 64 64</cell><cell>40</cell><cell>40  *  0.001</cell><cell>16</cell><cell>32</cell><cell>5</cell><cell>120</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Detailed hyperparameters used for the various experiments in the paper.Shared hyperparameters: all models were trained with an initial learning rate of 2e ? 4 and a a batch size of 32 per GPU (we used 1 to 4 GPUs). For all datasets, we used a multi-step learning rate scheduler with the following milestones (in epochs): [30, 60] with learning rate decreasing by 0.5 on each milestone. The warm-up stage described in Appendix B was only used for the Object model, where we used 1 warm-up epoch for CLEVRER and 2 for Traffic, and the number of 'noisy masks' epochs was 5 times the warm-up epochs-5 and 10 for CLEVRER and Traffic, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"> While Jakab et al. (2018a)  did not officialy name their method, it is often referred to as KeyNet (e.g.,<ref type="bibr" target="#b14">Gopalakrishnan et al., 2020)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b45">Sohn et al. (2015)</ref> show that such a conditional prior complies with the ELBO formulation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The careful reader may wonder why we used ?graph instead of ?bypass for the background. Empirically, we found that ?bypass can be too expressive, causing some of the objects to be represented by it instead of by Dglimpse.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We have implemented a graphical use interface (GUI) for manipulating the images, please visit https://taldatech. github.io/deep-latent-particles-web/ for videos.7  In principle, the Chamfer distance can be used to resolve this, but in practice it only worked well for short horizon predictions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">One may also consider the variance in the features for each particle. However, to illustrate our idea of disentangling position from appearance, we only consider position uncertainty.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">End-to-end learning of keypoint representations for continuous control from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07995</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Unsupervised scene decomposition and representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Se3-nets: Learning rigid body motion using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatially invariant unsupervised object detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Soft-introvae: Analyzing and improving the introspective variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="4391" to="4400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised disentanglement of pose, appearance and background from images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pottorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Genesis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13052</idno>
		<title level="m">Generative scene inference and sampling with object-centric latent representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Genesis-V2</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09958</idno>
		<title level="m">ferring unordered object representations without iterative refinement</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3225" to="3233" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep spatial autoencoders for visuomotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Training</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00152</idno>
		<title level="m">On the expressive power of random features in cnns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised object keypoint learning using local spatial predictability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12930</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mastering atari with discrete world models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02193</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Latentkeypointgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15812</idno>
		<title level="m">Controlling gans via latent keypoints</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<title level="m">Learning basic visual concepts with a constrained variational framework. ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-adversarial image synthesis with generative latent nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5811" to="5819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4020" to="4031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<ptr target="https://github.com/tomasjakab/imm" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">A method for stochastic optimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Unsupervised Image Representation Learning with Deep Latent Particles Kingma,</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12247</idno>
		<title level="m">Contrastive learning of structured world models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01794</idno>
		<title level="m">Sequential attend, infer, repeat: Generative modelling of moving objects</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised learning of object keypoints for perception and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11883</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<title level="m">Stochastic adversarial video prediction</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Causal discovery in physical systems from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00631</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised objectoriented scene representation via spatial attention and decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Space</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02407</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15055</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised part-based disentangling of object shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10955" to="10964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.1999.790410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/google-research/tree/master/video_structure" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object structure and dynamics from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<title level="m">Deep hierarchical feature learning on point sets in a metric space</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deforming autoencoders: Unsupervised disentangling of shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahasrabudhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="650" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised sprite learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marionette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stani?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R-Sqair</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05231</idno>
		<title level="m">relational sequential attend, infer, repeat</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks by factorized spatial embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5916" to="5925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Method</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rcpr</forename><surname>Supervised</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burgos-Artizzu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cfan (zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">84</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cnn (</forename><surname>Cascaded</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tcdcn (zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mtcnn (zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised / Self-supervised Thewlis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thewlis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thewlis</surname></persName>
		</author>
		<idno>frames) - 5.83</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shu</surname></persName>
		</author>
		<idno>5.45</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiles</surname></persName>
		</author>
		<idno>3.44</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Keynet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jakab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lorenz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dundar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
