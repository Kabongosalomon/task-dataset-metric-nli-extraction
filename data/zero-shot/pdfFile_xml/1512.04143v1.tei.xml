<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
							<email>sbell@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research *</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3 rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reliably detecting an object requires a variety of information, including the object's fine-grained details and the context surrounding it. Current state-of-the-art detection approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> only use information near an object's region of interest (ROI). This places constraints on the type and accuracy of objects that may be detected.</p><p>We explore expanding the approach of <ref type="bibr" target="#b7">[8]</ref> to include two additional sources of information. The first uses a multiscale representation <ref type="bibr" target="#b18">[19]</ref> that captures fine-grained details by pooling from multiple lower-level convolutional layers in a ConvNet <ref type="bibr" target="#b34">[35]</ref>. These skip-layers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref> span multiple spatial resolutions and levels of feature abstraction. The information gained is especially important for small objects, which require the higher spatial resolution provided by lower-level layers. * Ross <ref type="bibr">Girshick</ref>   <ref type="figure">Figure 1</ref>. Inside-Outside Net (ION). In a single pass, we extract VGG16 <ref type="bibr" target="#b34">[35]</ref> features and evaluate 2000 proposed regions of interest (ROI). For each proposal, we extract a fixed-size descriptor from several layers using ROI pooling <ref type="bibr" target="#b7">[8]</ref>. Each descriptor is L2-normalized, concatenated, scaled, and dimension-reduced (1x1 convolution) to produce a fixed-length feature descriptor for each proposal of size 512x7x7. Two fully-connected (fc) layers process each descriptor and produce two outputs: a one-of-K class prediction ("softmax"), and an adjustment to the bounding box ("bbox"). Our second addition is the use of contextual information. It is well known in the study of human and computer vision that context plays an important role in visual recognition <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5]</ref>. To gather contextual information we explore the use of spatial Recurrent Neural Networks (RNNs). These RNNs pass spatially varying contextual information both horizontally and vertically across an image. The use of at least two RNN layers ensures information may be propagated across the entire image. We compare our approach to other common methods for adding contextual information, including global average pooling and additional convolutional layers. Global average pooling provides information about the entire image, similar to the features used for scene or image classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Following previous approaches <ref type="bibr" target="#b8">[9]</ref>, we use object proposal detectors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref> to identify ROIs in an image. Each ROI is then classified as containing one or none of the objects of interest. Using dynamic pooling <ref type="bibr" target="#b12">[13]</ref> we can efficiently evaluate thousands of different candidate ROIs with a single forwards pass of the network. For each candidate ROI, the multi-scale and context information is concatenated into a single layer and fed through several fully connected layers for classification.</p><p>We demonstrate that both sources of additional information, context and multi-scale, are complementary in nature. This matches our intuition that context features look broadly across the image, while multi-scale features capture more fine-grained details. We show large improvements on the PASCAL VOC <ref type="bibr" target="#b5">[6]</ref> and Microsoft COCO <ref type="bibr" target="#b21">[22]</ref> object detection datasets and provide a thorough evaluation of the gains across different object types. We find that the they are most significant for object types that have been historically difficult. For example, we show improved accuracy for potted plants which are often small and amongst clutter. In general, we find that our approach is more adept at detecting small objects than previous state-of-the-art methods. For heavily occluded objects like chairs, gains are found when using contextual information.</p><p>While the technical methods employed (spatial RNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40]</ref>, skip-layer connections <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref>) have precedents in the literature, we demonstrate that their well-executed combination has an unexpectedly positive impact on the detector's accuracy. As always, the devil is in the details <ref type="bibr" target="#b2">[3]</ref> and thus our paper aims to provide a thorough exploration of design choices and their outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior work</head><p>ConvNet object detectors. ConvNets with a small number of hidden layers have been used for object detection for the last two decades (e.g., from <ref type="bibr" target="#b38">[39]</ref> to <ref type="bibr" target="#b33">[34]</ref>). Until recently, they were successful in restricted domains such as face detection. Recently, deeper ConvNets have led to radical improvements in the detection of more general object categories. This shift came about when the successful application of deep ConvNets to image classification <ref type="bibr" target="#b19">[20]</ref> was transferred to object detection in the R-CNN system of Girshick et al. <ref type="bibr" target="#b8">[9]</ref> and the OverFeat system of Sermanet et al. <ref type="bibr" target="#b32">[33]</ref>. Our work builds on the rapidly evolving R-CNN ("region-based convolutional neural network") line of work. Our experiments are conducted with Fast R-CNN <ref type="bibr" target="#b7">[8]</ref>, which is an end-to-end trainable refinement of He et al.'s SPPnet <ref type="bibr" target="#b12">[13]</ref>. We discuss the relationship of our approach to other methods later in the paper in the context of our model description and experimental results.</p><p>Spatial RNNs. Recurrent Neural Networks (RNNs) exist in various extended forms, including bidirectional RNNs <ref type="bibr" target="#b31">[32]</ref> that process sequences left-to-right and rightto-left in parallel. Beyond simple sequences, RNNs exist in full multi-dimensional variants, such as those introduced by Graves and Schmidhuber <ref type="bibr" target="#b9">[10]</ref> for handwriting recognition. As a lower-complexity alternative, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref> explore running an RNN spatially (or laterally) over a feature map in place of convolutions. These papers examine spatial RNNs for the tasks of semantic segmentation and image classification, respectively. We employ spatial RNNs as a mechanism for computing contextual features for use in object detection.</p><p>Skip-layer connections. Skip-layer connections are a classic neural network idea wherein activations from a lower layer are routed directly to a higher layer while bypassing intermediate layers. The specifics of the wiring and combination method differ between models and applications. Our usage of skip connections is most closely related to those used by Sermanet et al. <ref type="bibr" target="#b33">[34]</ref> (termed "multi-stage features") for pedestrian detection. Different from <ref type="bibr" target="#b33">[34]</ref>, we find it essential to L2 normalize activations from different layers prior to combining them.</p><p>The need for activation normalization when combining features across layers was recently noted by <ref type="bibr">Liu et al. (ParseNet [23]</ref>) in a model for semantic segmentation that makes use of global image context features. Skip connections have also been popular in recent models for semantic segmentation, such as the "fully convolutional networks" in <ref type="bibr" target="#b23">[24]</ref>, and for object instance segmentation, such as the "hypercolumn features" in <ref type="bibr" target="#b11">[12]</ref>.   <ref type="figure">Figure 3</ref>. Four-directional IRNN architecture. We use "IRNN" units <ref type="bibr" target="#b20">[21]</ref> which are RNNs with ReLU recurrent transitions, initialized to the identity. All transitions to/from the hidden state are computed with 1x1 convolutions, which allows us to compute the recurrence more efficiently (Eq. 1). When computing the context features, the spatial resolution remains the same throughout (same as conv5). The semantic segmentation regularizer has a 16x higher resolution; it is optional and gives a small improvement of around +1 mAP point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture: Inside-Outside Net (ION)</head><p>In this section we describe ION, a detector with an improved descriptor both inside and outside the ROI. An image is processed by a single deep ConvNet, and the convolutional feature maps at each stage of the ConvNet are stored in memory. At the top of the network, a 2x stacked 4-directional IRNN (explained later) computes context features that describe the image both globally and locally. The context features have the same dimensions as "conv5." This is done once per image. In addition, we have thousands of proposal regions (ROIs) that might contain objects. For each ROI, we extract a fixed-length feature descriptor from several layers ("conv3", "conv4", "conv5", and "context features"). The descriptors are L2-normalized, concatenated, re-scaled, and dimension-reduced (1x1 convolution) to produce a fixed-length feature descriptor for each proposal of size 512x7x7. Two fully-connected (FC) layers process each descriptor and produce two outputs: a one-of-K object class prediction ("softmax"), and an adjustment to the proposal region's bounding box ("bbox").</p><p>The rest of this section explains the details of ION and motivates why we chose this particular architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pooling from multiple layers</head><p>Recent successful detectors such as Fast R-CNN, Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>, and SPPnet, all pool from the last convolutional layer ("conv5 3") in VGG16 <ref type="bibr" target="#b34">[35]</ref>. In order to extend this to multiple layers, we must consider issues of dimensionality and amplitude.</p><p>Since we know that pre-training on ImageNet is important to achieve state-of-the-art performance <ref type="bibr" target="#b0">[1]</ref>, and we would like to use the previously trained VGG16 network <ref type="bibr" target="#b34">[35]</ref>, it is important to preserve the existing layer shapes. Therefore, if we want to pool out of more layers, the final feature must also be shape 512x7x7 so that it is the correct shape to feed into the first fully-connected layer (fc6). In addition to matching the original shape, we must also match the original activation amplitudes, so that we can feed our feature into fc6.</p><p>To match the required 512x7x7 shape, we concatenate each pooled feature along the channel axis and reduce the dimension with a 1x1 convolution. To match the original amplitudes, we L2 normalize each pooled ROI and re-scale back up by an empirically determined scale. Our experiments use a "scale layer" with a learnable per-channel scale initialized to 1000 (measured on the training set). We later show in Section 5.2 that a fixed scale works just as well.</p><p>As a final note, as more features are concatenated together, we need to correspondingly decrease the initial weight magnitudes of the 1x1 convolution, so we use "Xavier" initialization <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context features with IRNNs</head><p>Our architecture for computing context features in ION is shown in more detail in <ref type="figure">Figure 3</ref>. On top of the last convolutional layer (conv5), we place RNNs that move laterally across the image. Traditionally, an RNN moves left-to-right along a sequence, consuming an input at every step, updating its hidden state, and producing an output. We extend this to two dimensions by placing an RNN along each row and along each column of the image. We have four RNNs in total that move in the cardinal directions: right, left, down, up. The RNNs sit on top of conv5 and produce an output with the same shape as conv5.</p><p>There are many possible forms of recurrent neural networks that we could use: gated recurrent units (GRU) <ref type="bibr" target="#b3">[4]</ref>, long short-term memory (LSTM) <ref type="bibr" target="#b13">[14]</ref>, and plain tanh recurrent neural networks. In this paper, we explore RNNs composed of rectified linear units (ReLU). Le et al. <ref type="bibr" target="#b20">[21]</ref> recently showed that these networks are easy to train and are good at modeling long-range dependencies, if the recurrent weight matrix is initialized to the identity matrix. This means that at initialization, gradients are propagated backwards with full strength. Le et al. <ref type="bibr" target="#b20">[21]</ref> call a ReLU RNN initialized this way an "IRNN," and show that it performs almost as well as an LSTM for a real-world language modeling task, and better than an LSTM for a toy memory problem. We adopt this architecture because it is very simple to implement and parallelize, and is much faster than LSTMs or GRUs to compute.</p><p>For our problem, we have four independent IRNNs that move in four directions. To implement the IRNNs as efficiently as possible, we split the internal IRNN computations into separate logical layers. Viewed this way, we can see that the input-to-hidden transition is a 1x1 convolution, and that it can be shared across different directions. Sharing this transition allows us to remove 6 conv layers in total with a negligible effect on accuracy (?0.1 mAP). The bias can be shared in the same way, and merged into the 1x1 conv layer. The IRNN layer now only needs to apply the recurrent matrix and apply the nonlinearity at each step. The output from the IRNN is computed by concatenating the hidden state from the four directions at each spatial location. This is the update for an IRNN that moves to the right; similar equations exist for the other directions:</p><formula xml:id="formula_0">h right i,j ? max W right hh h right i,j?1 + h right i,j , 0 .<label>(1)</label></formula><p>Notice that the input is not explicitly shown in the equation, and there is no input-to-hidden transition. This is because it was computed as part of the 1x1 convolution, and then copied in-place to each hidden layer. For each direction, we can compute all of the independent rows/columns in parallel, stepping all IRNNs together with a single matrix multiply. On a GPU, this results in large speedups compared to computing each RNN cell one at a time. We also explore using semantic segmentation labels to regularize the IRNN output. When using these labels, we add the deconvolution and crop layer as implemented by Long et al. <ref type="bibr" target="#b23">[24]</ref>. The deconvolution upsamples by 16x with a 32x32 kernel, and we add an extra softmax loss layer with a weight of 1. This is evaluated in Section 5.3.</p><p>Variants and simplifications. We explore several further simplifications.</p><p>1. We fixed the hidden transition matrix to the identity W right hh = I, which allows us to entirely remove it:</p><formula xml:id="formula_1">h right i,j ? max h right i,j?1 + h right i,j , 0 .<label>(2)</label></formula><p>This is like an accumulator, but with ReLU after each step. In Section 5.5 we show that removing the recurrent matrix has a surprisingly small impact.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To prevent overfitting, we include dropout layers (p = 0.25) after each concat layer in all experiments. We later found that in fact the model is underfitting and there is no need for dropout anywhere in the network. 3. Finally, we trained a separate bias b 0 for the first step in the RNN in each direction. However, since it tends to remain near zero after training, this component is not really necessary.</p><p>Interpretation. After the first 4-directional IRNN (out of the two IRNNs), we obtain a feature map that summarizes nearby objects at every position in the image. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, we can see that the first IRNN creates a summary of the features to the left/right/top/bottom of every cell. The subsequent 1x1 convolution then mixes this information together as a dimension reduction. After the second 4-directional IRNN, every cell on the output depends on every cell of the input. In this way, our context features are both global and local. The features vary by spatial position, and each cell is a global summary of the image with respect to that specific spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We train and evaluate our dataset on three major datasets: PASCAL VOC 2007, VOC 2012, and on MS COCO. We demonstrate state-of-the-art results on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>All of our experiments use Fast R-CNN <ref type="bibr" target="#b7">[8]</ref> built on the Caffe <ref type="bibr" target="#b16">[17]</ref> framework, and the VGG16 architecture <ref type="bibr" target="#b34">[35]</ref>, all of which are available online. As is common practice, we use the publicly available weights pre-trained on ILSVRC2012 <ref type="bibr" target="#b30">[31]</ref> downloaded from the Caffe Model Zoo. <ref type="bibr" target="#b0">1</ref> We make some changes to Fast R-CNN, which give a small improvement over the baseline. We use 4 images per mini-batch, implemented as 4 forward/backward passes of single image mini-batches, with gradient accumulation. We sample 128 ROIs per image leading to 512 ROIs per model update. We measure the norm of the parameter gradient vector and rescale it if its L2 norm is above 20 (80 when accumulating over 4 images).   <ref type="bibr" target="#b6">[7]</ref>, D: remove all dropout layers. *We use a longer training schedule, resulting in a higher score than the preliminary numbers in <ref type="bibr" target="#b7">[8]</ref>. ? test-dev scores for our submission to the 2015 MS COCO Detection competition, and post-competition improvements, trained on "trainval35k", described in the Appendix.</p><p>To accelerate training, we use a two-stage schedule. As noted by Girshick <ref type="bibr" target="#b7">[8]</ref>, it is not necessary to fine-tune all layers, and nearly the same performance can be achieved by fine-tuning starting from conv3 1. With this in mind, we first train for 40k iterations with conv1 1 through conv5 3 frozen, and then another 100k iterations with only conv1 1 through conv2 2 frozen. All other layers are fine-tuned. When training for COCO, we use 80k and 320k iterations respectively. We found that shorter training schedules are not enough to fully converge.</p><p>We also use a different learning rate (LR) schedule. The LR exponentially decays from 5 ? 10 ?3 to 10 ?4 in the first stage, and from 10 ?3 to 10 ?5 in the second stage. To reduce the effect of random variation, we fix the random seed so that all variants see the same images in the same order. For PASCAL VOC we use the same pre-computed selective search boxes from Fast R-CNN, and for COCO we use the boxes precomputed by Hosang et al. <ref type="bibr" target="#b15">[16]</ref>. Finally, we modified the test thresholds in Fast R-CNN so that we keep only boxes with a softmax score above 0.05, and keep at most 100 boxes per images.</p><p>When re-running the baseline Fast R-CNN using the above settings, we see a +0.8 mAP improvement over the original settings on VOC 2007 test. We compare against the baseline using our improved settings where possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PASCAL VOC 2007</head><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, we evaluate our detector (ION) on PASCAL VOC 2007, training on the VOC 2007 trainval dataset merged with the 2012 trainval dataset, a common practice. Applying our method described above, we obtain a mAP of 76.5%. We then make some simple modifications, as described below, to achieve a higher score of 79.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR-CNN [7] introduces a bounding box regression</head><p>Extra-small Objects</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medium Objects</head><p>Extra-large Objects Our detector has a particularly large improvement for small objects. See Hoiem <ref type="bibr" target="#b14">[15]</ref> for details on these metrics. scheme to improve results on VOC, where bounding boxes are evaluated twice: (1) the initial proposal boxes are evaluated and regressed to improved locations and then (2) the improved locations are passed again through the network. All boxes are accumulated together, and non-max supression is applied. Finally, a weighted vote is computed for each kept box (over all boxes, including those suppressed), where boxes that overlap a kept box by at least 0.5 IoU contribute to the average. For our method, we use the softmax scores as the weights. When adding this scheme to our method, our mAP rises from 76.5% to 78.5%. Finally, we observed that our models are underfitting and we remove dropout from all layers to get a further gain up to 79.2%. MR-CNN also uses context and achieves 78.2%. However, we note that their method requires that pieces are evaluated individually, and thus has a test runtime around 30 seconds per image, while our method is significantly faster, taking 0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding box regression and 1.15s per image with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">PASCAL VOC 2012</head><p>We also evaluate on the slightly more challenging VOC 2012 dataset, submitting to the public evaluation server. <ref type="bibr" target="#b1">2</ref> In <ref type="table">Table 2</ref>, we show the top methods on the public leaderboard as of the time of submission. Our detector obtains a mAP of 76.4%, which is several points higher than the next best submission, and is the most accurate for most categories. <ref type="bibr" target="#b1">2</ref> Anonymous URL: http://host.robots.ox.ac.uk:8080/anonymous/B3VFLE.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MS COCO</head><p>Microsoft has recently released the Common Objects in Context dataset, which contains 80k training images ("2014 train") and 40k validation images ("2014 val"). There is an associated MS COCO challenge with a new evaluation metric, that averages mAP over different IoU thresholds, from 0.5 to 0.95 (written as "0.5:0.95"). This places a significantly larger emphasis on localization compared to the PASCAL VOC metric which only requires IoU of 0.5.</p><p>We are only aware of one baseline performance number for this dataset, as published in the Fast R-CNN paper, which cites a mAP of 19.7% on the 2015 test-dev set <ref type="bibr" target="#b7">[8]</ref>. We trained our own Fast R-CNN model on "2014 train" using our longer training schedule and obtained a higher mAP of 20.5% mAP on the same set, which we use as a baseline. As shown in <ref type="table" target="#tab_3">Table 3</ref>, when trained on the same images with the same schedule, our method obtains a large improvement over the baseline with a mAP of 24.9%.</p><p>We tried applying the same bounding box voting scheme <ref type="bibr" target="#b6">[7]</ref> to COCO, but found that performance decreases on the COCO metric (IOU 0.5:0.95, second row of <ref type="table" target="#tab_3">Table 3</ref>). Interestingly, the scheme increases performance at IoU 0.5 (the PASCAL metric). Since the scheme heuristically blurs together box locations, it can find the general location of objects, but cannot predict precise box locations, which is important for the new COCO metric. As described in the Appendix, we fixed this for our competition submission by raising the voting IoU threshold from 0.5 to ? 0.85.</p><p>We submitted ION to the 2015 MS COCO Detection Challenge and won the Best Student Entry with 3 rd place overall. Using only a single model (no ensembling), our submission achieved 31.0% on test-competition score and 31.2% on test-dev score ( <ref type="table" target="#tab_3">Table 3)</ref>. After the competition, we further improved our test-dev score to 33.1% by adding left-right flipping and adjusting training parameters. See the Appendix for details on our challenge submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Improvement for small objects</head><p>In general, small objects are challenging for detectors: there are fewer pixels on the object, they are harder to localize, and there can be many more of them per image. Small objects are even more challenging for proposal methods. For all experiments, we are using selective search <ref type="bibr" target="#b37">[38]</ref> for object proposals, which performs very poorly on small objects in COCO with an average recall under 10% <ref type="bibr" target="#b26">[27]</ref>.</p><p>We find that our detector shows a large relative improvement in this category. For COCO, if we look at small 3 objects, average precision and average recall improve from 4.1% to 7.0% and from 7.3% to 10.7% respectively. We highlight that this is even higher than the baseline proposal method, which is only possible because we perform bound-  <ref type="bibr" target="#b14">[15]</ref>, and see similarly large improvements on this dataset as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Design evaluation</head><p>In this section, we explore changes to our architecture and justify our design choices with experiments on PAS-CAL VOC 2007. All numbers in this section are VOC 2007 test mAP, trained on 2007 trainval + 2012 trainval, with the settings described in Section 4.1. Note that for this section, we use dropout in all networks, and a single round of bounding box regression at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pool from which layers?</head><p>As described in Section 3.1, our detector pools regions of interest (ROI) from multiple layers and combines the result. A straightforward approach would be to concatenate the ROI from each layer and reduce the dimensionality using a 1x1 convolution. As shown in <ref type="table">Table 4</ref> (left column), this does not work. In VGG16, the convolutional features at different layers can have very different amplitudes, so that naively combining them leads to unstable learning. While it is possible in theory to learn a model with inputs of very different amplitude, this is ill-conditioned and does not work well in practice. It is necessary to normalize the amplitude such that the features being pooled from all layers have similar magnitude. Our method's normalization scheme fixes this problem, as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">How should we normalize feature amplitude?</head><p>When performing L2 normalization, there are a few choices to be made: do you sum over channels and perform one normalization per spatial location (as in ParseNet <ref type="bibr" target="#b22">[23]</ref>), or should you sum over all entries in each pooled ROI and normalize it as a single blob. Further, when re-scaling the features back to an fc6-compatible magnitude, should you use a fixed scale or should you learn a scale per channel? The reason why you might want to learn a scale per channel is that you get more sharing than you would if you relied on the 1x1 convolution to model the scale. We evaluate this in <ref type="table">Table 5</ref>, and find that all of these approaches perform about the same, and the distinction doesn't matter for this problem. The important aspect is whether amplitude is taken into account; the different schemes we explored in <ref type="table">Table 5</ref> are all roughly equivalent in performance.</p><p>To determine the initial scale, we measure the mean scale of features pooled from conv5 on the training set, and use that as the fixed scale. Using Fast R-CNN, we measured the mean norm to be approximately 1000 when summing over all entries, and 130 when summing across channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">How much does segmentation loss help?</head><p>Although our target task is object detection, many datasets also have semantic segmentation labels, where the object class of every pixel is labeled. Many images in PAS-CAL VOC and every image in COCO has these labels. This is valuable information that can be incorporated into a training algorithm to improve performance.</p><p>As shown in <ref type="figure">Figure 3</ref>, when adding stacked IRNNs it is possible to have them also predict a semantic segmentation output-a multitask setup. In <ref type="table">Table 6</ref>, we see that these extra labels consistently provide about a +1 point boost in mAP for object detection. This is because we are training the network with more bits of supervision, so even though we are adding extra labels that we do not care about during inference, the features inside the network are trained to contain more information than they would have otherwise if only trained on object detection. Since this is an extra layer used only for training, we can drop the layer at test time and get a +1 mAP point boost with no change in runtime.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context method</head><p>Seg. mAP  <ref type="table">Table 7</ref>. Comparing approaches to adding context. All rows also pool out of conv3, conv4, and conv5. Metric: detection mAP on VOC07 test. Seg: if checked, the top layer received extra supervision from semantic segmentation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">How should we incorporate context?</head><p>While RNNs are a powerful mechanism of incorporating context, they are not the only method. For example, one could simply add more convolutional layers on top of conv5 and then pool out of the top convolutional layer. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, stacked 3x3 convolutions add two cells worth of context, and stacked 5x5 convolutions add 6 cells. Alternatively, one could use a global average and unpool (tile or repeat spatially) back to the original shape as in ParseNet <ref type="bibr" target="#b22">[23]</ref>.</p><p>We compared these approaches on VOC 2007 test, shown in <ref type="table">Table 7</ref>. The 2x stacked 4-dir IRNN layers have fewer parameters than the alternatives, and perform better on the test set (both with and without segmentation labels). Therefore, we use this architecture to compute "context features" for all other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Which IRNN architecture?</head><p>When designing the IRNN for incorporating context, there are a few basic decisions to be made, namely how many layers and how many hidden units per layer. In ad- dition, we explore the idea of entirely removing the recurrent transition (equivalent to replacing it with the identity matrix), so that the IRNN consist of repeated steps of: accumulate, ReLU, accumulate, etc. Note that this is not the same as an integral/area image, since each step has ReLU. As shown in <ref type="table">Table 8</ref>, using 2 IRNN layers performs the best on VOC 2007 test. While stacking more convolution layers tends to make ConvNets perform better, the same is not always true for RNNs <ref type="bibr" target="#b17">[18]</ref>. We also found that the number of hidden units did not have a strong effect on the performance <ref type="table">(Table 9</ref>), and chose 512 as the baseline size for all other experiments.</p><p>Finally, we were surprised to discover that removing the recurrent W hh transition performs almost as well as learning it <ref type="table">(Table 9</ref>). It seems that the input-to-hidden and hidden-to-output connections contain sufficient context that the recurrent transition can be removed and replaced with an addition, saving a large matrix multiply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Other variations</head><p>There are some other variations on our architecture that perform almost as well, which we summarize in <ref type="table" target="#tab_2">Table 10</ref>. For example, (a) the first IRNN only processes two directions left/right and the second IRNN only processes up/down. This kind of operation was explored in ReNet <ref type="bibr" target="#b39">[40]</ref> and performs the same as modeling all four directions in both IRNN layers. We also explored (b) pooling out of both IRNNs, and (c) pooling out of both stacked convolutions and the IRNNs. None of these variations perform better than our main method.  <ref type="table" target="#tab_2">Table 10</ref>. Other variations. Metric: VOC07 test mAP. We list some other variations that all perform about the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduces the Inside-Outside Net (ION), an architecture that leverages context and multi-scale knowledge for object detection. Our architecture uses a 2x stacked 4-directional IRNN for context, and multi-layer ROI pooling with normalization for improved object description. To justify our design choices, we conducted extensive experiments evaluating choices like the number of layers combined, using segmentation loss, normalizing feature amplitudes, different IRNN architectures, and other variations. We achieve state-of-the-art results on both PASCAL VOC and COCO, and find our proposed architecture is particularly effective at improving detection of small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: 2015 MS COCO Competition</head><p>In this section, we describe our submission to the 2015 MS COCO Detection Challenge, which won Best Student Entry and finished 3 rd place overall, with a score of 31.0% mAP on 2015 test-challenge and 31.2% on 2015 test-dev. Later in this section we describe further post-competition improvements to achieve 33.1% on test-dev. Both models use a single ConvNet (no ensembling).</p><p>For our challenge submission, we made several improvements: used a mix of MCG (Multiscale Combinatorial Grouping <ref type="bibr" target="#b27">[28]</ref>) and RPN (Region Proposal Net <ref type="bibr" target="#b29">[30]</ref>) proposal boxes, added two extra 512x3x3 convolutional layers, trained for longer, and used two rounds of bounding box regression with a modified version of weighted voting <ref type="bibr" target="#b6">[7]</ref>. At test time, our model runs in 2.7 seconds/image on a single Titan X GPU (excluding proposal generation).</p><p>We describe all changes in more detail below (note that many of these choices were driven by the need to meet the challenge deadline, and thus may be suboptimal):</p><p>1. Train+val. For the competition, we train on both train and validation sets. We hold out 5000 images from the validation as our new validation set called "minival." 2. MCG+RPN box proposals. We get the largest improvement by replacing selective search with a mix of MCG <ref type="bibr" target="#b27">[28]</ref> and RPN <ref type="bibr" target="#b29">[30]</ref> boxes. We modify RPN from the baseline configuration described in <ref type="bibr" target="#b29">[30]</ref> by adding more anchor boxes, in particular smaller ones, and using a mixture of 3x3 (384) and 5x5 (128) convolutions. Our anchor configuration uses a total of 22 anchors per location with the following shapes: 32x32 and aspect ratios {1:2, 1:1, 2:1} ? scales {64, 90.5, 128, 181, 256, 362, 512}. We also experiment with predicting proposals from concatenated conv4 3 and conv5 3 features (after L2 normalization and scaling), rather than from conv5 3 only. We refer to these two configurations as RPN1 (without concatenated features) and RPN2 (with concatenated features). Using VGG16, RPN1 and RPN2 achieve average recalls of 44.1% and 44.3%, respectively, compared to selective search, 41.7%, and MCG, 51.6%. Despite their lower average recalls, we found that RPN1 and RPN2 give comparable detection results to MCG. We also found that mixing 1000 MCG boxes with 1000 RPN1 or RPN2 boxes performs even better than 2000 of either method alone, suggesting that the two methods are complementary. Thus, we train with 1000 RPN1 and 1000 MCG boxes. At test time, we use 1000 RPN2 and 2000 MCG boxes, which gives a +0.3 mAP improvement on minival compared to using the same boxes as training. 3. conv6+conv7. We use the model listed in row (c) of <ref type="table" target="#tab_2">Table 10</ref>: two 512x3x3 convolutions on top of conv5 3 which we call "conv6" and "conv7". We also pool out of conv7, so in total we pool out of conv3 3, conv4 3, conv5 3, conv7, and IRNN. 4. Longer training. Since training on COCO is very slow, we explored ideas by initializing new networks from the weights of the previous best network. While we have not tried training our best configuration starting from VGG16 ImageNet weights, we do not see any reason why it would not work. Here is the exact training procedure we used:</p><p>(a) Train using 2000 selective search boxes, using the schedule described in Section 4.1, but stopping after 220k iterations of fine-tuning. This achieves 24.8% on 2015 test-dev. (b) Change the box proposals to a mix of 1000 MCG boxes and 1000 RPN1 boxes. Train for 100k iterations with conv layers frozen (learning rate: exp decay 5 ? 10 ?3 ? 10 ?4 ), and for 80k iterations with only conv1 and conv2 frozen (learning rate: exp decay 10 ?3 ? 10 ?5 ).</p><p>5. 2xBBReg + WV. We use the iterative bounding box regression and weighted voting scheme described in <ref type="bibr" target="#b6">[7]</ref>, but as noted in Section 4.4, this does not work outof-the-box for COCO since boxes are blurred together and precise localization is lost. We solve this by adjusting the thresholds so that only very similar boxes are blurred together with weighted voting. We jointly optimized the NMS (non-max suppression) and voting threshold on the validation set, by evaluating all boxes once and then randomly sampling hundreds of thresholds. On our minival set, the optimal IoU (intersectionover-union) threshold is 0.443 for NMS and 0.854 for weighted voting. Compared to using a single round of standard NMS (IoU threshold 0.3), these settings give +1.3 mAP on minival.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-competition improvements</head><p>We have an improved model which did not finish in time for the challenge, which achieves 33.1% on 2015 test-dev.</p><p>We made these further adjustments:</p><p>1. Training longer with 0.99 momentum. We found that if the momentum vector is reset when re-starting training, the accuracy drops by several points and takes around 50k iterations to recover. Based on this observation, we increased momentum to 0.99 and correspondingly decreased learning rate by a factor of 10. We initialized from our competition submission (above), and:</p><p>(a) further trained for 80k iterations with only conv1 and conv2 frozen, (learning rate: exp decay 10 ?4 ? 10 ?6 ). By itself, this gives a boost of +0.4 mAP on minival. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Challenging detections on COCO 2015 test-dev using our model trained on COCO "2014 train."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>conv5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Output of rst IRNN (b) Slice of single cell</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Interpretation of the first IRNN output. Each cell in the output summarizes the features to the left/right/top/bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>VOC 2007 normalized AP by size. Left to right: increasing complexity. Left-most bar in each group: Fast R-CNN; right-most bar: our best model that achieves 79.2% mAP on VOC 2007 test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Receptive field of different layer types. When considering a single cell in the input, what output cells depend on it? (a) If we add two stacked 3x3 convolutions on top of conv5, then a cell in the input influences a 5x5 window in the output. (b) Similarly, for a 5x5 convolution, one cell influences a 9x9 window in the output. (c) For global average pooling, every cell in the output depends on the entire input, but the output is the same value repeated. (d) For IRNNs, every cell in the output depends on the entire input, but also varies spatially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Left-right then up-down 76.5 (b) Pool out of both IRNNs 75.9 (c) Combine 2x stacked 512x3x3 conv and IRNN 76.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 .</head><label>2</label><figDesc>(b) We trained for another 160k iterations with no layers frozen (learning rate: exp decay 10 ?4 ? 10 ?6 ) which gives +0.7 mAP on minival. Left-right flipping. We evaluate the model twice, once on the original image, and once on the left-right flipped image. To merge the predictions, we average both the softmax scores and box regression shifts (after flipping back). By itself, this gives a boost of +0.8 mAP on minival. 3. More box proposals. At test time, we use 4000 proposal boxes (2000 RPN2 and 2000 MCG boxes). For the model submitted to the competition, this performs +0.1 mAP better than 3000 boxes (1000 RPN2 and 2000 MCG boxes). At test time, the above model runs in 5.5 seconds/image on a single Titan X GPU (excluding proposal generation). Most of the slowdown is from the left-right flipping. Table 11 provides a breakdown of the gains due to the various competition and post-competition changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and C. Lawrence Zitnick are now at Facebook AI Research.</figDesc><table><row><cell>conv1</cell><cell cols="2">conv2</cell><cell cols="2">conv3</cell><cell>conv4</cell><cell>conv5</cell><cell></cell><cell>4-dir IRNN</cell><cell>4-dir IRNN</cell><cell>context features</cell></row><row><cell cols="3">ROI Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fc</cell><cell>softmax</cell></row><row><cell cols="2">L2 normalize</cell><cell cols="2">concat</cell><cell cols="2">scale</cell><cell>1x1 conv</cell><cell>fc</cell><cell>fc</cell><cell>bbox For each ROI fc</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Method R S W D Train mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv FRCN [8] 07+12 70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 RPN [30] 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 MR-CNN [7] 07+12 78.2 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 ION [ours] 07+12 74.6 78.2 79.1 76.8 61.5 54.7 81.9 84.3 88.3 53.1 78.3 71.6 85.9 84.8 81.6 74.3 45.6 75.3 72.1 82.6 81.4 ION [ours] 07+12 75.6 79.2 83.1 77.6 65.6 54.9 85.4 85.1 87.0 54.4 80.6 73.8 85.3 82.2 82.2 74.4 47.1 75.8 72.7 84.2 80.4 ION [ours] 07+12+S 76.5 79.2 79.2 77.4 69.8 55.7 85.2 84.2 89.8 57.5 78.5 73.8 87.8 85.9 81.3 75.3 49.7 76.9 74.6 85.2 82.1 ION [ours] 07+12+S 78.5 80.2 84.7 78.8 72.4 61.9 86.2 86.7 89.5 59.1 84.1 74.7 88.9 86.9 81.3 80.0 50.9 80.4 74.1 86.6 83.3 ION [ours] 07+12+S 79.2 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 Detection results on VOC 2007 test. Legend: 07+12: 07 trainval + 12 trainval, 07+12+S: 07+12 plus SBD segmentation labels<ref type="bibr" target="#b10">[11]</ref>, R: include 2x stacked 4-dir IRNN (context features), S: regularize with segmentation labels, W: two rounds of bounding box regression and weighted voting<ref type="bibr" target="#b6">[7]</ref>, D: remove all dropout layers.</figDesc><table><row><cell>Method</cell><cell>R S W D Train</cell><cell cols="9">mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv</cell></row><row><cell>FRCN [8]</cell><cell>07++12</cell><cell cols="9">68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2</cell></row><row><cell>RPN [30]</cell><cell>07++12</cell><cell cols="9">70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5</cell></row><row><cell>FRCN+YOLO [29]</cell><cell>07++12</cell><cell cols="9">70.4 83.0 78.5 73.7 55.8 43.1 78.3 73.0 89.2 49.1 74.3 56.6 87.2 80.5 80.5 74.7 42.1 70.8 68.3 81.5 67.0</cell></row><row><cell>HyperNet</cell><cell>07++12</cell><cell cols="9">71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7</cell></row><row><cell cols="6">MR-CNN [7] 73.Method 07+12 R S W D Train 0.5:0.95 0.50 0.75 Small Med. Large Avg. Precision, IoU: Avg. Precision, Area:</cell><cell cols="5">Avg. Recall, # Dets: 1 10 100 Small Med. Large Avg. Recall, Area:</cell></row><row><cell>FRCN [8]*</cell><cell>train</cell><cell>20.5 39.9 19.4</cell><cell>4.1</cell><cell>20.0</cell><cell cols="2">35.8 21.3 29.5</cell><cell>30.1</cell><cell>7.3</cell><cell>32.1</cell><cell>52.0</cell></row><row><cell>FRCN [8]*</cell><cell>train</cell><cell>20.0 40.3 18.1</cell><cell>4.1</cell><cell>19.6</cell><cell cols="2">34.5 20.8 29.1</cell><cell>29.8</cell><cell>7.4</cell><cell>31.9</cell><cell>50.9</cell></row><row><cell>ION [ours]</cell><cell>train</cell><cell>23.0 42.0 23.0</cell><cell>6.0</cell><cell>23.8</cell><cell cols="2">37.3 23.0 32.4</cell><cell>33.0</cell><cell>9.7</cell><cell>37.0</cell><cell>53.5</cell></row><row><cell>ION [ours]</cell><cell>train</cell><cell>23.6 43.2 23.6</cell><cell>6.4</cell><cell>24.1</cell><cell cols="2">38.3 23.2 32.7</cell><cell>33.5</cell><cell>10.1</cell><cell>37.7</cell><cell>53.6</cell></row><row><cell>ION [ours]</cell><cell>train</cell><cell>24.9 44.7 25.3</cell><cell>7.0</cell><cell>26.1</cell><cell cols="2">40.1 23.9 33.5</cell><cell>34.1</cell><cell>10.7</cell><cell>38.8</cell><cell>54.1</cell></row><row><cell>ION comp.  ?</cell><cell>trainval35k</cell><cell>31.2 53.4 32.3</cell><cell>12.8</cell><cell>32.9</cell><cell cols="2">45.2 27.8 43.1</cell><cell>45.6</cell><cell>23.6</cell><cell>50.0</cell><cell>63.2</cell></row><row><cell>ION post.  ?</cell><cell>trainval35k</cell><cell>33.1 55.7 34.6</cell><cell>14.5</cell><cell>35.2</cell><cell cols="2">47.2 28.9 44.8</cell><cell>47.4</cell><cell>25.5</cell><cell>52.4</cell><cell>64.3</cell></row></table><note>9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 ION [ours] 07+12+S 76.4 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 Table 2. Detection results on VOC 2012 test (comp4). Legend: 07+12: 07 trainval + 12 trainval, 07++12: 07 trainvaltest + 12 trainval, 07+12+S: 07+12 plus SBD segmentation labels [11], R: include 2x stacked 4-dir IRNN (context features), S: regularize with segmentation labels, W: two rounds of bounding box regression and weighted voting [7], D: remove all dropout layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Detection results on COCO 2015 test-dev. Legend: R: include 2x stacked 4-dir IRNN (context features), S: regularize with segmentation labels, W: two rounds of bounding box regression and weighted voting</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Combining features from different layers. Approaches to normalizing feature amplitude. Metric: detection mAP on VOC07 test. All methods are regularized with loss from predicting segmentation.</figDesc><table><row><cell cols="4">ROI pooling from: Merge features using:</cell></row><row><cell>C2 C3 C4 C5</cell><cell cols="3">1x1 L2+Scale+1x1</cell></row><row><cell></cell><cell>*70.8</cell><cell>71.5</cell></row><row><cell></cell><cell>69.7</cell><cell>74.4</cell></row><row><cell></cell><cell>63.6</cell><cell>74.6</cell></row><row><cell></cell><cell>59.3</cell><cell>74.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Metric: De-</cell></row><row><cell cols="4">tection mAP on VOC07 test. Training set: 07 trainval + 12 train-</cell></row><row><cell cols="4">val. 1x1: combine features from different layers using a 1x1 con-</cell></row><row><cell cols="4">volution. L2+Scale+1x1: use L2 normalization, scaling (initial-</cell></row><row><cell cols="4">ized to 1000), and 1x1 convolution, as described in section 3.1.</cell></row><row><cell cols="4">These results do not include "context features." *This entry is the</cell></row><row><cell cols="4">same as Fast R-CNN [8], but trained with our hyperparameters.</cell></row><row><cell cols="2">L2 Normalization method Seg.</cell><cell cols="2">Scale: Learned Fixed</cell></row><row><cell>Sum across channels</cell><cell></cell><cell>76.4</cell><cell>76.2</cell></row><row><cell>Sum over all entries</cell><cell></cell><cell>76.5</cell><cell>76.6</cell></row></table><note>ing box regression to predict improved box locations. Sim- ilarly, we show a size breakdown for VOC2007 test in Fig- ure 5 using Hoiem's toolkit for diagnosing errors</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 (Table 6 .</head><label>46</label><figDesc>right column). Effect of segmentation loss. Metric: detection mAP on VOC07 test. Adding segmentation loss tends to improve detection performance by about 1 mAP, with no test-time penalty.</figDesc><table><row><cell>ROI pooling from:</cell><cell cols="2">Use seg. loss?</cell></row><row><cell>C2 C3 C4 C5 IRNN</cell><cell>No</cell><cell>Yes</cell></row><row><cell></cell><cell>69.9</cell><cell>70.6</cell></row><row><cell></cell><cell>73.9</cell><cell>74.2</cell></row><row><cell></cell><cell>75.1</cell><cell>76.2</cell></row><row><cell></cell><cell>75.6</cell><cell>76.5</cell></row><row><cell></cell><cell>74.9</cell><cell>76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Varying the number of IRNN layers. Metric: mAP on VOC07 test. Segmentation loss is used to regularize the top IRNN layer. All IRNNs use 512 hidden units. Varying the hidden transition. We vary the number of units and try either learning recurrent transition W hh initialized to the identity, or entirely removing it (same as setting W hh = I).</figDesc><table><row><cell>ROI pooling from: C2 C3 C4 C5 IRNN</cell><cell>Seg.</cell><cell cols="2"># IRNN layers 1 2 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.6</cell></row><row><cell></cell><cell cols="2">74.3</cell></row><row><cell></cell><cell cols="3">75.8 76.2</cell></row><row><cell></cell><cell cols="3">76.1 76.5 75.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>76.8</cell></row><row><cell cols="3">ROI pooling from: Seg. # units C3 C4 C5 IRNN</cell><cell>Include W hh ? Yes No</cell></row><row><cell></cell><cell>128</cell><cell></cell><cell>76.4</cell><cell>75.5</cell></row><row><cell></cell><cell>256</cell><cell></cell><cell>76.5</cell><cell>75.3</cell></row><row><cell></cell><cell>512</cell><cell></cell><cell>76.5</cell><cell>76.1</cell></row><row><cell></cell><cell>1024</cell><cell></cell><cell>76.2</cell><cell>76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 11. Breakdown of gains for the post-competition model. The reported metric is Avg. Precision, IoU: 0.5:0.95. The training set "trainval35k" includes all of train together with approximately 35k images from val, after removing the 5k minival set. All entries use a single ConvNet model (no ensembling). The majority of the gains come from the ION model (20.5 ? 24.9) and better proposals with more training data (MCG + RPN: 25.1 ? 29.0). Two rounds of bounding box regression with weighted voting and longer training with improved hyperparameters also yield important gains. Note that we use a modified version of RPN<ref type="bibr" target="#b29">[30]</ref>, described in the Appendix text.</figDesc><table><row><cell>Method</cell><cell>Proposals</cell><cell cols="3">Train: Test: minival test-dev train train trainval35k trainval35k minival test-dev</cell></row><row><cell>baseline VGG16</cell><cell>Sel. Search</cell><cell>20.3</cell><cell>20.5</cell></row><row><cell>ION VGG16</cell><cell>Sel. Search</cell><cell>24.4</cell><cell>24.9</cell></row><row><cell>ION VGG16 + conv7</cell><cell>Sel. Search</cell><cell></cell><cell>25.1</cell></row><row><cell>ION VGG16 + conv7</cell><cell>MCG + RPN</cell><cell></cell><cell>28.4</cell><cell>29.0</cell></row><row><cell>+ W (box refinement)</cell><cell></cell><cell></cell><cell>30.0</cell><cell>30.6</cell></row><row><cell>+ flipping + more, better training</cell><cell></cell><cell></cell><cell>32.5</cell><cell>33.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/BVLC/caffe/wiki/Model-Zoo</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">"Small" means area ? 32 2 px; about 40% of COCO is "small."</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contributions. We make the following contributions:</p><p>1. We introduce the ION architecture that leverages context and multi-scale skip pooling for object detection. 2. We achieve state-of-the-art results on PASCAL VOC 2007, with a mAP of 79.2%, VOC 2012, with a mAP of 76.4%, and on COCO, with a mAP of 24.9%. 3. We conduct extensive experiments evaluating choices like the number of layers combined, using a segmentation loss, normalizing feature amplitudes, different IRNN architectures, and other variations. 4. We analyze the detector's performance and find improved accuracy across the board, but, in particular, for small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project was the result of an internship at Microsoft Research (MSR). We would like to thank Abhinav Shrivastava and Ishan Misra for helpful discussions while at MSR. We thank NVIDIA for the donation of K40 GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoderdecoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion &amp; semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05082</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bio. cybernetics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579[cs.CV</idno>
		<title level="m">ParseNet: Looking wider to see better. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring tiny images: The roles of appearance and contextual information for machine and human object recognition. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U. the difficulty of training deep feedforward neural networks. Xavier glorot and yoshua bengio</title>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Contextual priming for object detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proc. on Vision, Image, and Signal Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00393</idno>
		<title level="m">ReNet: A recurrent neural network based alternative to convolutional networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
