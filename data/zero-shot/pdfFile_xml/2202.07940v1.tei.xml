<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Meta Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies pointed out that knowledge distillation (KD) suffers from two degradation problems, the teacher-student gap and the incompatibility with strong data augmentations, making it not applicable to training state-of-the-art models, which are trained with advanced augmentations. However, we observe that a key factor, i.e., the temperatures in the softmax functions for generating probabilities of both the teacher and student models, was mostly overlooked in previous methods. With properly tuned temperatures, such degradation problems of KD can be much mitigated. However, instead of relying on a naive grid search, which shows poor transferability, we propose Meta Knowledge Distillation (MKD) to meta-learn the distillation with learnable meta temperature parameters. The meta parameters are adaptively adjusted during training according to the gradients of the learning objective. We validate that MKD is robust to different dataset scales, different teacher/student architectures, and different types of data augmentation. With MKD, we achieve the best performance with popular ViT architectures among compared methods that use only ImageNet-1K as training data, ranging from tiny to large models. With ViT-L, we achieve 86.5% with 600 epochs of training, 0.6% better than MAE that trains for 1,650 epochs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref> is an extensively studied technique in recent years, whose initial goal is to to transfer the "dark knowledge" from a teacher model to a student model. It works by providing the student with more intuitive targets than the ground truth one-hot labels to learn. It has been widely explored in various fields, including model compression , semi- <ref type="table">Table 1</ref>. Knowledge distillation is not compatible with strong augmentation. <ref type="bibr" target="#b11">(Das et al., 2020;</ref><ref type="bibr" target="#b10">Cui &amp; Yan, 2021)</ref> Augmentation</p><p>Teacher acc. Student acc.</p><p>Normal 73.3 70.4 LabelSmooth <ref type="bibr" target="#b33">(Szegedy et al., 2016)</ref> 73.3 68.1 Mixup <ref type="bibr" target="#b42">(Zhang et al., 2017)</ref> 74.3 67.9 CutMix <ref type="bibr" target="#b40">(Yun et al., 2019)</ref> 76.0 68.8 <ref type="table">Table 2</ref>. Knowledge distillation suffers from teacher-student gap. <ref type="bibr" target="#b26">(Mirzadeh et al., 2020;</ref><ref type="bibr" target="#b8">Cho &amp; Hariharan, 2019)</ref> Teacher Teacher acc. Student (ResNet20) acc.</p><p>ResNet32 <ref type="bibr" target="#b14">(He et al., 2016)</ref> 71.0 70.8 ResNet56 <ref type="bibr" target="#b14">(He et al., 2016)</ref> 73.3 70.4 ResNet110 <ref type="bibr" target="#b14">(He et al., 2016)</ref> 75.4 70.5 ResNet164 <ref type="bibr" target="#b14">(He et al., 2016)</ref> 75.8 69.9 supervise learning <ref type="bibr" target="#b29">(Pham et al., 2021)</ref>, domain adaptation <ref type="bibr" target="#b34">(Tian et al., 2019)</ref>. Besides of those applications of KD, many other works aims at improving the vanilla KD via introducing extra supervisions, including intermediate features <ref type="bibr" target="#b30">(Romero et al., 2014)</ref>, attention maps <ref type="bibr" target="#b41">(Zagoruyko &amp; Komodakis, 2016)</ref>, relations <ref type="bibr" target="#b28">(Park et al., 2019)</ref>, etc.</p><p>However, recent papers pointed out that knowledge distillation suffers from two problems: (1) Teacher-student gap <ref type="bibr" target="#b26">(Mirzadeh et al., 2020;</ref><ref type="bibr" target="#b8">Cho &amp; Hariharan, 2019)</ref> Even though the performance of a teacher model can improve, the performance of the student model does not necessarily follow the same trend.</p><p>(2) The incompatibility with strong augmentation <ref type="bibr" target="#b11">(Das et al., 2020;</ref><ref type="bibr" target="#b10">Cui &amp; Yan, 2021)</ref> When the teacher's performance improves via strong augmentation, the performance of the distilled student model might actually degrade. To demonstrate the two problems, we conduct a series of pilot studies. We train ResNet56 teacher models with different data augmentations, and use ResNet20 to distill those teacher models, with results shown in <ref type="table">Table 1</ref>. Besides, we train a series of ResNet models with different depths, and use those teachers to distill ResNet20 student, with results shown in <ref type="table">Table 2</ref>. In <ref type="table">Table 1</ref>, we can see that the strongest teacher (trained with CutMix <ref type="bibr" target="#b40">(Yun et al., 2019)</ref>) actually results in worse student performance than the teacher trained with normal data augmentation. Besides, as shown in <ref type="table">Table 2</ref>, the distilled student's performance decline with the improvement of teacher's performance.</p><p>However, in the pilot studies, we observe that a key fac-arXiv:2202.07940v1 <ref type="bibr">[cs.</ref>LG] 16 Feb 2022 tor, the temperature hyperparameters, ? , of both the teacher and student, were mostly overlooked by previous methods. It is actually the main cause for the two above mentioned problems. All the mentioned problems reported by previous methods are based on experiments with a fixed recommended temperature, i.e. 4 in <ref type="bibr" target="#b34">(Tian et al., 2019;</ref><ref type="bibr" target="#b8">Cho &amp; Hariharan, 2019)</ref>. The temperature is used to control the degree of softness of the targets. With lower temperatures, the student pays much more attention to match the maximal logits of the teacher outputs. On the other hand, higher temperatures encourage the student to also focus on the logits other than the maximal ones. With different augmentations or teacher architectures, the distribution of the teachers' output logits may vary significantly. We empirical find the soften logits of a teacher trained with CutMix is too noisy to provide effective supervision for distillation. This problems become severe for larger teacher models, because those models are usually trained with augmentations of data deformations of larger degrees. Besides, prior arts <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref> assigned the same temperature to both the student and the teacher, trying to match them with the same degree of softness. These approaches do not take the capacity gap between the teacher and the student into consideration. For example, it might be harder for a student (e.g. ResNet18 <ref type="bibr" target="#b14">(He et al., 2016)</ref>), to distill from a larger teacher (e.g. ResNet101) than a smaller one (e.g. ResNet50), given that the same temperature is used for both the teacher and student.</p><p>One of our key findings is that, with a proper temperature, the aforementioned degradation problems in KD can be much mitigated. We show that vanilla KD can beat state-ofthe-art distillation methods on distilling vision transformers <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>.</p><p>Previous works <ref type="bibr" target="#b8">Cho &amp; Hariharan, 2019)</ref> simply conducts a grid search to identify an optimal temperature, and fix it for the entire learning process. In this paper, we propose Meta Knowledge Distillation (MKD) to learn the distillation function with learnable meta-parameters ?, including the temperatures {? t , ? s } of the teacher and the student, respectively. The meta-parameters are adjusted online during the students' training process with a meta objective that minimizes the validation loss on a preserved validation set, allowing the distillation function to dynamically adapt over time to the gradients of learning objective. We apply MKD to various distillation scenarios, and obtain consistent gains over the manually provided metaparameters suggested by previous methods. Empirically, we validate that MKD is robust to different scales of the datasets, architectures of the teacher or student models, and different types of data augmentation. While previous work <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> shows that vanilla KD does not work on Vision Transformer (ViT), we show that MKD improves ViT consistently. We achieve the best accuracy of ViT with-out changing architectures when training on ImageNet-1K <ref type="bibr" target="#b12">(Deng et al., 2009</ref>) dataset only. Specifically, we achieve 77.1% top-1 accuracy with ViT-T, 2% better than Manifold Distillation <ref type="bibr" target="#b19">(Jia et al., 2021)</ref>. We also achieve 86.5% top-1 accuracy with ViT-L, surpassing the reported performance 85.15% in <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>, which is trained with 10x larger ImageNet-21k <ref type="bibr" target="#b12">(Deng et al., 2009</ref>).</p><p>In summary, our contributions are as follows:</p><p>1. We identify the temperature hyperparameters of both the teacher and student are overlooked by previous studies of the problems of knowledge distillation, and find that proper temperatures can mitigate those problems.</p><p>2. We propose MKD to learn the distillation function by dynamically adapting the temperatures over time to the gradients of the training objective.</p><p>3. With MKD, we achieve the best accuracy of ViT without changing architectures when training on ImageNet-1K dataset only, surpassing the results trained with 10x larger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Temperature Matters for Knowledge Distillation</head><p>Although the problems of knowledge distillation have been widely discussed in different literatures (as stated in Section 1), we find that the conclusions were reached by previous methods based on a fixed temperature. A properly tuned temperature can sometimes lead to opposite conclusions. We perform a grid search over temperatures to verify their impacts on the performance of KD. We train ResNet56 teacher models with different data augmentations, and use ResNet20 to distill those teacher models. Besides, we train a series of ResNet models with different depths, and use those teachers to distill ResNet20 student. The setting is the same as Tables 1 and 2, but different temperatures are applied. We use the same temperature for the teacher and the student by default.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the optimal temperatures vary for different teachers, and we can obtain opposite conclusions under different temperatures. For example, in <ref type="figure" target="#fig_0">Figure 1</ref> (left), R32 is the best teacher when the temperature is 4, but the worst teacher when using temperature of 9. Besides, in <ref type="figure" target="#fig_0">Figure 1</ref> (right), the teacher using normal data augmentation performs best when using temperatures of 3 among other teachers, but performs worst when the temperature is set as 0.75.</p><p>We also grid search different temperatures for both the teacher (ResNet56) and student (ResNet20), with results shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We find that using the same temperature  for the teacher and the student leads to sub-optimal results, even sometimes the worst results. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the worst performance is obtained when using the same temperatures 0.75 for both the teacher and student.</p><p>We notice some recent papers try to study the compatibility of label smooth and KD. <ref type="bibr" target="#b27">M?ller et al. (2019)</ref> showed that teachers trained with label smoothing lead to inferior student networks compared to teachers trained with hard targets. In contrast, <ref type="bibr" target="#b31">Shen et al. (2021)</ref> reached a different conclusion showing that teachers trained with label smoothing produce better students. In our study, we can reach both conclusion by using different temperatures. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (left), when using lower temperatures (i.e. ? ? 1), the teacher trained with label smoothing produces slightly better results. However, when using higher temperatures (i.e. ? &gt; 1), the conclusion is opposite.</p><p>The temperatures for the teacher and the student play important roles in KD, and the optimal temperatures vary a lot under different distillation setups, as shown in our empirical studies. To address this problem, we introduce our Meta Knowledge Distillation in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Meta Knowledge Distillation</head><p>In contrast to prior work improving knowledge distillation, which has sought to design new distillation loss <ref type="bibr" target="#b34">(Tian et al., 2019)</ref>, new distillation pipeline <ref type="bibr" target="#b26">(Mirzadeh et al., 2020)</ref>, and new distillation targets <ref type="bibr" target="#b29">(Pham et al., 2021)</ref>, we aim to improve knowledge distillation <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref> via meta learning the temperature parameters to automatically adapt to various datasets, teacher-student pairs, and augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>In the original formulation of knowledge distillation (KD) <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref>, a student model is trained to match the targets provided by a teacher model, which tries to provide more information than the ground truth one-hot labels. The targets are usually softened by a fixed temperature in the softmax function and the entropy increases as the temperature increases.</p><p>Formally, we consider a student model S with parameters ? s for distilling the teacher. We denote the output logits from the teacher and the student as z s and z t , respectively. The student is trained to minimize the cross-entropy (CE) loss between its predicted probability p s and the teacher's output probability p t , where p s and p t are softened by their corresponding temperatures ? s and ? t respectively.</p><formula xml:id="formula_0">min ?s CE(p s , p t ),<label>(1)</label></formula><p>where p s = softmax(z s /? s ), p t = softmax(z t /? t ).</p><p>In prior arts, ? s and ? t are manually chosen as the same constant greater than 1. However, as we discussed in Sec. 2, the temperatures play a key role in distillation, and the optimal temperatures vary in different distillation setups, which are difficult to tune manually. Besides, the temperatures control the degree of softness of the targets provided by the teacher. Using a fixed temperature for different distillation setups, including teach/student architectures, augmentation types, datasets, can be sub-optimal. Intuitively, the temperatures are expected to be adjusted according to according to the different training setups.</p><p>We take an explicit approach to tackle this problem: we aim at meta-learning the temperatures of both the teacher and student to dynamically adjust the softness of the learning targets according to the student's on-the-fly performance on the validation set,</p><formula xml:id="formula_1">min ? L v (? s , ?), s.t. ? s = argmin ?s L t (? s ),<label>(2)</label></formula><p>where L t and L v denote the training and validation losses, and ? = {? s , ? t } are meta-parameters (temperatures) to be optimized on the meta level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meta-learning Temperatures for Better Distillation</head><p>Our Meta Knowledge Distillation (MKD) aims at jointly updating the student's parameters ? s and temperature metaparameters ?. We apply the underlying distillation loss to a batch of training samples to pre-update the student. We then measure the performance of the pre-updated student with a batch of validation samples to update the meta-parameters. The student would be finally optimized with the updated meta-parameters.</p><p>Specifically, starting from the current ? s and ?, MKD first pre-updates ? s by optimizing the distillation objective using a batch of training samples, resulting in pre-updated parameters ? s ,</p><formula xml:id="formula_2">? s = ? s ? ? ?L t (? s , ?) ?? s ,<label>(3)</label></formula><p>where ? is the step size. Note that we approximate argmin ?s L t (? s ) in Equation <ref type="formula" target="#formula_1">(2)</ref> with an one-step update following Equation 3, as updating ? s until full convergence is infeasible.</p><p>MKD then measures the performance of the pre-updated parameters ? s with a new batch of validation samples, and utilizes the differentiable meta-objective L v (? s ) to update ?. The gradient of ? can be obtained by applying the chain rule of gradients,</p><formula xml:id="formula_3">?L v (? s ) ?? = ?L v (? s ) ?? s ?? s ?? .<label>(4)</label></formula><p>The meta-optimization is conducted via stochastic gradient descent, such that the meta-parameters ? are updated as</p><formula xml:id="formula_4">? ? ? ? ? ?L v (? s ) ?? ,<label>(5)</label></formula><p>where ? is the meta step size. Intuitively, our proposed MKD aims at updating meta-parameters in the direction that minimizes the validation loss along the normal updates.</p><p>After that, MKD updates the student's original parameters ? s with the updated meta-parameters ?,</p><formula xml:id="formula_5">? s ? ? s ? ? ?L t (? s , ?) ?? s .<label>(6)</label></formula><p>In contrast to MPL <ref type="bibr" target="#b29">(Pham et al., 2021)</ref>, our approach does not reuse the student's update, as it make the student's updates follow those of the meta-parameters. We illustrate our method in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temperature Prediction Network</head><p>Instead of directly updating the temperatures, we opt to make the tempuratures output by an extra small network, </p><formula xml:id="formula_6">{? s , ? t } = ? init + ?(MLP(e)) ? 0.5,<label>(7)</label></formula><p>where ? init denotes the initial temperature. We found that the extra small network has larger capacity than simply updating the two temperature and thus can adapt the training process much faster, leading to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Alternative Meta-learning Objective</head><p>The meta-parameters ? are optimized to minimize the CE loss between the student's output and the ground-truth onehot labels on a preserved validation set. However, in our experiments, we find that the CE loss might not truly indicate the actual performance. As long as the logit value of the correct class decreases, the loss decreases, but the performance does not necessarily increase. To address this problem, we also propose an alternative and more aggressive meta-objective that only optimizes the incorrect samples,</p><formula xml:id="formula_7">L v (? s ) = i?G c j=1 (p (ij) s ? y (ij) ) 2 ,<label>(8)</label></formula><p>where G denotes the indexes of the incorrectly classified samples in a validation batch, and c denotes the number of classes, p (ij) s returns the jth predicted probability value of ith sample, and y (ij) returns the jth ground-truth probability value of ith sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>We apply Meta Knowledge Distillation (MKD) on various teacher-student pairs and augmentations setups, and compare MKD against state-of-the-art distillation methods on standard benchmarks such as CIFAR-100 <ref type="bibr" target="#b23">(Krizhevsky et al., 2009)</ref>, and ImageNet-1K <ref type="bibr" target="#b12">(Deng et al., 2009</ref>).</p><p>Datasets. CIFAR-100 contains 50K images from 100 classes for training and 10K for testing. ImageNet-1K provides 1.2 million images of 1k classes for training and 50k images for validation. For those datasets, we preserve 10% of the training set for optimization of the meta parameters, i.e. 5k images for CIFAR-100 and 128k for ImageNet-1K.</p><p>Experimental setup. For the small-scale CIFAR-100 benchmark, we follow CRD <ref type="bibr" target="#b34">(Tian et al., 2019)</ref>, and experiment with different teacher-student pairs, including ResNet <ref type="bibr" target="#b14">(He et al., 2016)</ref> of different depths and widths. We mainly compare our results with vanilla KD <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref> and state-of-the-art distillation method CRD <ref type="bibr" target="#b34">(Tian et al., 2019)</ref>.</p><p>For large-scale ImageNet benchmark, we experiment with the state-of-the-art architecture, Vision Transformer (ViT) <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>, and mainly use the following three setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">For fair comparison with previous distillation methods</head><p>for ViTs <ref type="bibr" target="#b35">(Touvron et al., 2021a;</ref><ref type="bibr" target="#b19">Jia et al., 2021)</ref>, we employ the same teacher-student model pairs, i.e. ViT-T and ViT-S <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> to distill CaiT-S24 <ref type="bibr" target="#b36">(Touvron et al., 2021b)</ref>.</p><p>2. To compare with state-of-the-art ViT performances <ref type="bibr" target="#b32">Steiner et al., 2021)</ref>, we use a stronger teacher BEiT-L <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> with students ranging from ViT-T to ViT-L <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>.</p><p>3. We also experiment with a fixed student ViT-T, and use teachers of different performances (i.e. ViT-S, ViT-B, and ViT-L), to verify that whether a better teacher consistently leads to better distilled students.</p><p>Training details. On CIFAR-100 benchmark, all the models are trained with SGD for 300 epochs, initial learning rate of 0.05, minimal learning rate of 0.0005 with cosine decay, weight decay of 5 ? 10 ?4 . The teacher models are trained with CutMix <ref type="bibr" target="#b40">(Yun et al., 2019)</ref>, while the students are not. Besides, we employ regular random crop and flip for data augmentation. For CRD, we use the nice official code 1 and the default hyperparameters of CRD.</p><p>As ViT is sensitive to the choice of training hyperparameters, we mainly follow the popular training recipe in <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref>, but employ different dropping patch ratios <ref type="bibr" target="#b17">(Huang et al., 2016)</ref> for different students. We add a new distillation token to the student network for distillation. It works similarly to the class token <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref> but its supervision is from the teacher. We do not use extra network augmentations such as relative position <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> or layer scaling <ref type="bibr" target="#b3">(Bao et al., 2021;</ref><ref type="bibr" target="#b36">Touvron et al., 2021b)</ref>. We distill ViT-T or ViT-S using a regular setting of training for 300 epochs, and distill ViT-base or ViT-large for 600 epochs (MAE  uses much more epochs, 1 https://github.com/HobbitLong/RepDistiller i.e. 1600). When comparing with previous best results, i.e. <ref type="table" target="#tab_3">Table 5</ref>, we train ViT-S for 600 epochs. In contrast to previous works that rely on a weight hyper-parameter to balance the cross-entropy loss term on ground-truth one-hot labels and the loss term on soft labels, we employ a simpler form that only minimizes the cross-entropy loss on soft labels from the teacher as Equation <ref type="formula" target="#formula_0">(1)</ref>. We employ AdamW optimizer <ref type="bibr" target="#b25">(Loshchilov &amp; Hutter, 2017)</ref> to optimize metaparameters, with a learning rate of 3 ? 10 ?4 and weight decay of 5 ? 10 ?4 . For more details, please refer to the Appendix A.</p><p>For ViT, we find that the changing the temperatures during the training process is less important than the final converged temperatures, as ViT is a powerful network and can adapt to new temperatures quickly. Therefore, we fix and adopt the temperature learned from ViT-T. We only activate our proposed meta-learning strategy in the last 100 training epochs to maintain high training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Distilling ViT on ImageNet-1K</head><p>Comparisons with ViT distillation methods. In <ref type="table" target="#tab_1">Table 3</ref>, we compare Meta Knowledge Distillation (MKD) with previous Vision Transformer (ViT) <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref> distillation methods <ref type="bibr" target="#b35">(Touvron et al., 2021a;</ref><ref type="bibr" target="#b19">Jia et al., 2021)</ref> using the same teacher-student pairs. MKD improves the performances by large margins (i.e. 4.2% for ViT-T, and 2.3% for ViT-S), indicating the potential of ViT when trained with better supervisions.</p><p>Previous work <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> shows that the vanilla KD does not work on ViT, and proposes a hard distillation method to pursue better performance. In contrast, our MKD is based on the vanilla KD, and achieves even better performance by adapting the temperatures. For instance, we achieve 76.4% with ViT-T, +2% and +4.2% better than the hard and soft distillation strategies in <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref>, respectively.</p><p>Manifold distillation <ref type="bibr" target="#b19">(Jia et al., 2021</ref>) designs more complex distillation targets, and requires specific teacher architectures. Comparing with their approach, our MKD is more accurate while being simpler and more flexible. Our MKD is able to be applied to various teacher-student pairs, and can take advantage of much stronger teacher, as shown in <ref type="table" target="#tab_2">Table 4</ref>.</p><p>Better teachers lead to better students. In <ref type="table" target="#tab_2">Table 4</ref>, we experiment on distilling ViT-T with teachers of different capacities. We observe that the performance of the student improves as the teachers' capacities increase. Specifically, distilling from BEiT-L, the performance of the student improves 1.2%, compared with distilling from DeiT-S.  <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> 72.2 Hard <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> 74.4 Manifold <ref type="bibr" target="#b19">(Jia et al., 2021)</ref> 75.1 MKD 76.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No distillation</head><p>ViT-S 79.8 Soft (KD) <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> 80.0 Hard <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> 81.3 Manifold <ref type="bibr" target="#b19">(Jia et al., 2021)</ref> 81.5 MKD 82.1 DeiT-S <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> 22.1 79.8 75.9 CaiT-S24 <ref type="bibr" target="#b36">(Touvron et al., 2021b)</ref> 46.9 82.7 76.4 DeiT-B-Dist. <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref> 87.3 83.4 76.8 BEiT-L <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> 304.4 87.5 77.1</p><p>We achieve 77.1% with ViT-T by training for 300 epochs, 0.5% better than previous best result <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref>, which trains for 1000 epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MKD overcomes degradations.</head><p>Prior arts point out that KD suffers from the teacher-student gap and is incompatible with strong data augmentation. Those problems prevent KD from learning from better teachers or advanced data augmentations. In contrast, our MKD mitigates those degradation problems by adaptively adjusting the temperatures. We do not notice the degradation of students even when the teacher is 50? larger than the student. Note that both the teachers and the students in <ref type="table" target="#tab_2">Table 4</ref> are trained with strong data augmentations, such as Mixup <ref type="bibr" target="#b42">(Zhang et al., 2017)</ref> and CutMix <ref type="bibr" target="#b40">(Yun et al., 2019)</ref>. Those results demonstrate the robustness and the effectiveness of our MKD.</p><p>Comparisons with previous results of ViT. In <ref type="table" target="#tab_3">Table 5</ref>, we compare our distilled ViT models with previous approaches, including distillation, pre-training with large scale datasets followed by finetuning, and self-supervised methods. All MKD results are obtained by distilling BEiT-L (87.5%).</p><p>With MKD, we achieve the best performance on popular ViT architectures among compared methods that use only ImageNet-1K as training data, across tiny to large models. Our MKD distilled ViT-L obtains 86.5% accuracy, +3.9% better than training without distillation . Compared with MAE that employs self-supervised training for 1,600 epochs, our MKD achieves better results with much fewer training epochs.</p><p>Training large ViT models is nontrivial <ref type="bibr" target="#b32">(Steiner et al., 2021)</ref>, and previous works proposed different techniques <ref type="bibr" target="#b35">(Touvron et al., 2021a;</ref><ref type="bibr" target="#b15">He et al., 2021)</ref> to achieved good results. We provide a promising approach to train ViTs by distillation, which is more accurate, simpler, and faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">CIFAR-100 Results</head><p>In <ref type="table">Table 6</ref>, we compare our MKD with other distillation methods on CIFAR-100 with different teacher-student pairs. Our method outperforms compared methods for different teacher-student pairs.</p><p>Besides using MKD alone, we also experiment with combining CRD <ref type="bibr" target="#b34">(Tian et al., 2019)</ref> and the proposed MKD. As shown in <ref type="table">Table 6</ref>, compared to CRD, vanilla KD produces worse results when combined with CRD. In contrast, we obtain consistent gains over the strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Studies</head><p>In this section, we investigate individual components of our Meta Knowledge Distillation on the CIFAR-100 benchmark. The ablation studies are conducted with the setup of distilling ResNet32x4 to ResNet8x4. The teacher and the student are trained with CutMix for data augmentation. For other details, please see more details in Section 4.   <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref> 71.2 CRD <ref type="bibr" target="#b34">(Tian et al., 2019)</ref> 70.9 KD (grid search) <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref> 73.7 MKD 74.4 MKD vs. grid search We list the baseline results in <ref type="table" target="#tab_4">Table  7</ref>, including the results of vanilla Knowledge Distillation (KD), CRD <ref type="bibr" target="#b34">(Tian et al., 2019)</ref>, and our MKD. We see that the default KD setting provided by CRD fail to improve the student (71.2% vs. the train-from-scratch result of 73.4%). However, after grid searching the temperatures, vanilla KD obtains 73.7%, being slightly better than the train-fromscratch result. In contrast, our MKD improves our grid searched results, and achieves 74.4%.</p><p>Meta-learning targets. The meta-learning targets of MKD can be flexibly designed, as shown in <ref type="table" target="#tab_5">Table 8</ref>. As for the capacity gap between the teacher and the student, using different temperatures for them allows more flexible control of the their softness of outputs. We see that jointly learning separate temperatures for the teacher and the student achieves best results. In contrast, using a shared temperatures for both the teacher and student is widely used in previous works and leads to inferior performances, which demonstrates that a shared temperature is insufficient for better knowledge transfer. We further visualize the adaptation of the temperatures over the course of training in <ref type="figure" target="#fig_2">Figure  3</ref>.</p><p>Initial temperatures. The influence of the initial temperature is studied in <ref type="table">Table 9</ref>. We experiment with the widely used temperatures, and presents the accuracy difference after applying MKD. We see that MKD is robust to initial temperatures, and obtains consistent performance gains over the initial temperatures. Note that for ImageNet-1K, comparing to the best grid-searched temperature, MKD also obtains 0.5% accuracy gain.</p><p>Ways to generate temperatures. We experiment on two ways to generate temperatures, 1) two learnable parameters directly updated via back-propagation, and 2) generating temperatures from a small neural network, as expressed in Equation <ref type="formula" target="#formula_6">(7)</ref>. The later approach obtains 1% performance gain compared with the former.</p><p>Previous meta-hyperparameter-optimization works <ref type="bibr" target="#b38">(Xu et al., 2018;</ref><ref type="bibr" target="#b24">Li et al., 2017;</ref><ref type="bibr" target="#b2">Baik et al., 2020;</ref><ref type="bibr" target="#b20">Khodak et al., 2019)</ref> usually employ the first approach to optimize their hyperparameters. In contrast, we propose to generate the hyperparameters to be learned with a small neural network, e.g. 2-layer MLP network. The neural network has larger capacity, and is able to adapt faster, leading to better performance. <ref type="bibr" target="#b2">Baik et al. (Baik et al., 2020)</ref> employ similar strategy to generate hyperparameters, but requires additional state as the input.</p><p>Meta loss functions. We compare different meta loss functions in <ref type="table">Table 11</ref>. Using cross-entropy (CE) loss on the validation set to optimize is a natural choice, and has been utilized in previous works <ref type="bibr" target="#b29">(Pham et al., 2021)</ref>. However, as stated in Section 3.4, the CE loss may not indicate the true performance. As a result, we propose an alternative that only optimizes the incorrect samples as express in Equation <ref type="formula" target="#formula_7">(8)</ref>, which leads to better results on CIFAR-100.</p><p>Transfer temperatures across dataset We apply the grid searched temperatures on a source dataset and transfer the searched temperatures to to a target dataset. As shown in <ref type="table" target="#tab_6">Table 12</ref>, transferring the temperatures leads to inferior performance. For instance, directly searching on ImageNet-1K is 5.8% better than transferring from CIFAR-100. The results also suggest that the temperatures should be adjusted for different datasets. The results provide the strong justification of adaptively and separately searching the temperatures for different distillation setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Works</head><p>Knowledge distillation The initial idea of knowledge distillation (KD) is introduced by the work of <ref type="bibr" target="#b5">(Bucilu? et al., 2006)</ref> and <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref>, attempting to transfer the knowledge of a cumbersome model (teacher) to a lightweight model (student). <ref type="bibr" target="#b5">Bucilu? et al. (2006)</ref> achieve the knowledge transferring by matching logits. <ref type="bibr" target="#b16">Hinton et al. (2015)</ref> propose to soft the logits before softmax with a temperature, and match the soften probabilities of the teacher and the student for distillation. The soften probabilities provide useful information about the learned representation of the teacher model, and the temperature is able to control which part of the information to focus on. After that, a lot of works try to improve KD with various techniques. FitNets <ref type="bibr" target="#b30">(Romero et al., 2014)</ref> proposes to match the intermediate features for better knowledge transfer. Zagoruyko et al. <ref type="bibr" target="#b41">(Zagoruyko &amp; Komodakis, 2016)</ref> transfer the attention map of intermediate features from the teacher to the student. CRD <ref type="bibr" target="#b34">(Tian et al., 2019)</ref> introduce contrastive objective for representation transfer, which achieves state-of-the-art results. Other papers <ref type="bibr" target="#b28">(Park et al., 2019;</ref><ref type="bibr" target="#b39">Yim et al., 2017;</ref><ref type="bibr" target="#b18">Huang &amp; Wang, 2017;</ref><ref type="bibr" target="#b21">Kim et al., 2018;</ref><ref type="bibr" target="#b0">Ahn et al., 2019;</ref><ref type="bibr" target="#b22">Koratana et al., 2019)</ref> have proposed various distillation criteria based on representations.</p><p>Meta-learning for hyperparameter optimization Hyperparameters are hard to tune manually, and lots of works try to automate the tuning by meta-learning. Meta-SGD <ref type="bibr" target="#b24">(Li et al., 2017)</ref> optimize the learning rate along model parameters. <ref type="bibr" target="#b38">Xu et al. (Xu et al., 2018)</ref> propose to learn the return function in Reinforcement Learning with tunable meta-parameters, and use specialization form for different scenarios. Baik et al. <ref type="bibr" target="#b2">(Baik et al., 2020)</ref> propose to update the hyperparameters (i.e. learning rate and weight decay coefficients) with meta-optimization. Their work uses the learning state as the input of the meta-model to produce the hyperparameters, but our work does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we propose Meta Knowledge Distillation to meta-learn the distillation loss with learnable temperatures. While recent works point out that knowledge distillation suffers from two degradation problems, our study show that those problems can be much mitigated with proper temperatures to the teacher and the student. Our MKD can adaptively adjust the temperatures over the training process according to the gradients of the learning objective. With extensive experiments, we show that our MKD is robust to different scales of the datasets, architectures of the teacher or student models, and different types of data augmentation. We achieve the best performance with popular ViT architectures among compared methods that use only ImageNet-1K as training data, across tiny to large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Training details on CIFAR-100</head><p>The train setting on CIFAR-100 is in <ref type="table" target="#tab_1">Table 13</ref>.  <ref type="bibr" target="#b33">(Szegedy et al., 2016)</ref> 0.1 Mixup * <ref type="bibr" target="#b42">(Zhang et al., 2017)</ref> 0.2 CutMix * <ref type="bibr" target="#b40">(Yun et al., 2019)</ref> 1.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training details on ImageNet</head><p>We follow the standard ViT architecture <ref type="bibr" target="#b13">(Dosovitskiy et al., 2020)</ref>. It consists of a stack of Transformer blocks <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, and each block consists of a multi-head self-attention block and an FFN block, with LayerNorm <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>. The ViT-T and ViT-S are introduced in DeiT <ref type="bibr" target="#b35">(Touvron et al., 2021a)</ref>.</p><p>Our training of ViT follows common practice of supervised ViT training. The default setting is in <ref type="table" target="#tab_2">Table 14</ref>.  <ref type="bibr" target="#b33">(Szegedy et al., 2016)</ref> 0.1 Mixup <ref type="bibr" target="#b42">(Zhang et al., 2017)</ref> 0.8 CutMix <ref type="bibr" target="#b40">(Yun et al., 2019)</ref> 1.0 drop path <ref type="bibr" target="#b17">(Huang et al., 2016)</ref> 0.0 (T/S/B), 0.3 (L)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training details of meta-parameters</head><p>For our proposed temperature prediction network, we employ a 2-layer MLP with ReLU activation between the layers. The output is scaled with sigmoid function. The input embedding dimension is set to 8, and the hidden dimension is set to 16.</p><p>The default setting to train meta-parameters is in <ref type="table" target="#tab_2">Table 14</ref>.</p><p>Meta Knowledge Distillation </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Grid search over temperatures of different teachers. On the left, the results shown with different lines are distilled from the teachers trained with different data augmentations. On the right, different lines represent ResNet teachers with different depths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Grid search results of applying different temperatures to the teacher and the student.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The evolution curves of temperatures during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Meta Knowledge DistillaionRequire: student parametrized with ? s and metaparameters ? Require: step size hyperparameters ? and ? 1: while not done do 2:pre-update student: ? s = ? s ? ? ?Lt(?s,?)</figDesc><table><row><cell></cell><cell cols="2">??s</cell></row><row><cell>3:</cell><cell>update meta-parameters: ? ? ? ? ?</cell><cell>?Lv(? s ) ??</cell></row><row><cell>4:</cell><cell cols="2">update student: ? s ? ? s ? ? ?Lt(?s,?) ??s</cell></row><row><cell cols="2">5: end while</cell><cell></cell></row><row><cell cols="3">which takes a learnable embedding e as input. The extra</cell></row><row><cell cols="3">small network is implemented as a 2-layer multi-layer per-</cell></row><row><cell cols="3">ceptron (MLP) followed by a sigmoid function ?,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison with previous ViT distillation methods on ImageNet-1K. All models are distilled with CaiT-S24 (82.7%) as the teacher and trained from scratch with ImageNet-1K.</figDesc><table><row><cell>Distillation Method</cell><cell>Student Top-1 Acc. (%)</cell></row><row><cell>No distillation</cell><cell>72.2</cell></row><row><cell>Soft (KD)</cell><cell></cell></row><row><cell></cell><cell>ViT-T</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Distilling ViT-T with teachers of different capacities on ImageNet-1K. Ordered by model capacity from top to bottom.</figDesc><table><row><cell>Teacher</cell><cell>Params. (M)</cell><cell>Top-1 Acc. (%)</cell><cell>Student Top-1 Acc. (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison with previous results of ViT on ImageNet-1K. The previous best results are underlined.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">ViT-T ViT-S ViT-B ViT-L</cell></row><row><cell>Train-from-scratch</cell><cell></cell><cell>72.2</cell><cell>79.8</cell><cell>81.8</cell><cell>82.6</cell></row><row><cell cols="6">TrainViT (Steiner et al., 2021) 73.75 80.46 83.96 83.98</cell></row><row><cell cols="2">Manifold (Jia et al., 2021)</cell><cell>75.1</cell><cell>81.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DeiT (Touvron et al., 2021a)</cell><cell>76.6</cell><cell>82.6</cell><cell>84.2</cell><cell>-</cell></row><row><cell cols="2">DINO (Caron et al., 2021)</cell><cell></cell><cell>81.5</cell><cell>82.8</cell><cell>-</cell></row><row><cell cols="2">MoCo v3 (Chen et al., 2021)</cell><cell>-</cell><cell>-</cell><cell>83.2</cell><cell>84.1</cell></row><row><cell cols="2">BEiT (Bao et al., 2021)</cell><cell>-</cell><cell>-</cell><cell>83.2</cell><cell>85.2</cell></row><row><cell>MAE (He et al., 2021)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>83.6</cell><cell>85.9</cell></row><row><cell>MKD</cell><cell></cell><cell>77.1</cell><cell>83.1</cell><cell>85.1</cell><cell>86.5</cell></row><row><cell cols="6">Table 6. Comparison with previous distillation methods on</cell></row><row><cell cols="3">CIFAR-100. Average over 5 runs.</cell><cell></cell><cell></cell></row><row><cell>Teacher</cell><cell cols="5">ResNet110 ResNet56 ResNet110 ResNet32x4</cell></row><row><cell>Student</cell><cell cols="4">ResNet20 ResNet20 ResNet32</cell><cell>ResNet8x4</cell></row><row><cell>Teacher</cell><cell>77.52</cell><cell>76.04</cell><cell>77.52</cell><cell></cell><cell>81.96</cell></row><row><cell>Student</cell><cell>68.55</cell><cell>68.55</cell><cell>70.74</cell><cell></cell><cell>71.58</cell></row><row><cell>KD (Hinton et al., 2015)</cell><cell>68.71</cell><cell>68.89</cell><cell>72.34</cell><cell></cell><cell>70.15</cell></row><row><cell>CRD (Tian et al., 2019)</cell><cell>71.14</cell><cell>71.91</cell><cell>73.43</cell><cell></cell><cell>73.75</cell></row><row><cell>CRD+KD</cell><cell>71.06</cell><cell>71.98</cell><cell>72.54</cell><cell></cell><cell>73.55</cell></row><row><cell>MKD</cell><cell>70.64</cell><cell>70.93</cell><cell>72.74</cell><cell></cell><cell>71.6</cell></row><row><cell>CRD+MKD</cell><cell>71.58</cell><cell>72.01</cell><cell>73.75</cell><cell></cell><cell>74.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Baseline results of ablation studies.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc. (%)</cell></row><row><cell>Teacher (ResNet32x4)</cell><cell>81.9</cell></row><row><cell>Student (ResNet8x4)</cell><cell>73.4</cell></row><row><cell>KD (CRD)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Meta-learning targets. Learning different temperatures for the teacher and the student achieves better results.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Table 9. Initial temperature. ?</cell></row><row><cell></cell><cell></cell><cell cols="3">represents accuracy difference</cell></row><row><cell></cell><cell></cell><cell cols="2">after applying MKD.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Benchmark</cell><cell cols="2">? init ? acc. (%)</cell></row><row><cell>Setup Fix ? t , learn ? s Fix ? s , learn ? t</cell><cell>Top-1 Acc. (%) 74.0 72.9</cell><cell>CIFAR-100</cell><cell>1 2 3 4</cell><cell>0.7 0.7 0.5 0.6</cell></row><row><cell>Share learnable ?</cell><cell>72.6</cell><cell></cell><cell>1</cell><cell>0.5</cell></row><row><cell>Learn {? s , ? t }</cell><cell>74.4</cell><cell>ImageNet-1K</cell><cell>2 3</cell><cell>2.4 2.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>2.1</cell></row><row><cell cols="2">Table 10. Design of tempera-</cell><cell cols="3">Table 11. Design of meta loss</cell></row><row><cell cols="2">ture output approaches.</cell><cell>function.</cell><cell></cell><cell></cell></row><row><cell>Setup</cell><cell>Top-1 Acc. (%)</cell><cell cols="3">Meta loss function Top-1 Acc. (%)</cell></row><row><cell>Learnable parameter</cell><cell>73.4</cell><cell cols="2">cross-entropy loss</cell><cell>74</cell></row><row><cell>MLP network</cell><cell>74.4</cell><cell>Equation (8)</cell><cell></cell><cell>74.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 12 .</head><label>12</label><figDesc>Applying grid searched temperature of source dataset to target dataset.</figDesc><table><row><cell>Source</cell><cell>Target</cell><cell>Top-1 Acc. (%)</cell></row><row><cell>CIFAR-100</cell><cell>ImageNet-1K</cell><cell>70.1</cell></row><row><cell cols="2">ImageNet-1K ImageNet-1K</cell><cell>75.9</cell></row><row><cell>ImageNet-1K</cell><cell>CIFAR-100</cell><cell>71.4</cell></row><row><cell>CIFAR-100</cell><cell>CIFAR-100</cell><cell>73.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 13 .</head><label>13</label><figDesc>Training settings on CIFAR-100. * optional config.</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>SGD</cell></row><row><cell>learning rate</cell><cell>0.05</cell></row><row><cell>weight decay</cell><cell>0.0005</cell></row><row><cell>batch size</cell><cell>64</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell></row><row><cell>training epochs</cell><cell>300</cell></row><row><cell>augmentation</cell><cell>random crop and flip</cell></row><row><cell>LabelSmooth  *</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 14 .</head><label>14</label><figDesc>Training settings on ImageNet-1K.</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW (Loshchilov &amp; Hutter, 2017)</cell></row><row><cell>learning rate</cell><cell>0.001 (T/S/B), 0.0003 (L)</cell></row><row><cell>weight decay</cell><cell>0.05</cell></row><row><cell>batch size</cell><cell>1024</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell></row><row><cell>warmup epochs</cell><cell>5</cell></row><row><cell>training epochs</cell><cell>300 (T/S), 600 (S/B/L)</cell></row><row><cell>augmentation</cell><cell>RandAug(9, 0.5) (Cubuk et al., 2020)</cell></row><row><cell>LabelSmooth</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 15 .</head><label>15</label><figDesc>Training settings of meta-parameters</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW (Loshchilov &amp; Hutter, 2017)</cell></row><row><cell>learning rate</cell><cell>0.0003</cell></row><row><cell>weight decay</cell><cell>0.00005</cell></row><row><cell>batch size</cell><cell>1024 (ImageNet), 64 (CIFAR)</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">SenseTime Research 2 The Chinese University of Hong Kong &amp; Centre for Perceptual and Interactive Intelligence. Correspondence to: Hongsheng Li &lt;hsli@ee.cuhk.edu.hk&gt;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9163" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00209</idno>
		<title level="m">Metalearning with adaptive hyperparameters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A good teacher is patient and consistent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05237</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4794" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Isotonic data augmentation for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01412</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An empirical analysis of the impact of data augmentation on knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rekatsinas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03810</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient vision transformers via fine-grained manifold distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01378</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adaptive gradient-based meta-learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02717</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04977</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lit: Learned intermediate representation training for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koratana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3509" to="3518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to learn quickly for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5191" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
		<title level="m">When does label smoothing help</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11557" to="11568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitnets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Is label smoothing truly incompatible with knowledge distillation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00676</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09801</idno>
		<title level="m">Meta-gradient reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
