<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Super-Resolution with Recurrent Structure-Detail Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
							<email>x.jia@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
							<email>shuhanggu@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">School of Eie</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songjiang</forename><surname>Li</surname></persName>
							<email>songjiang.li@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Super-Resolution with Recurrent Structure-Detail Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Super-Resolution</term>
					<term>Recurrent Neural Network</term>
					<term>Two- Stream Block</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most video super-resolution methods super-resolve a single reference frame with the help of neighboring frames in a temporal sliding window. They are less efficient compared to the recurrent-based methods. In this work, we propose a novel recurrent video super-resolution method which is both effective and efficient in exploiting previous frames to superresolve the current frame. It divides the input into structure and detail components which are fed to a recurrent unit composed of several proposed two-stream structure-detail blocks. In addition, a hidden state adaptation module that allows the current frame to selectively use information from hidden state is introduced to enhance its robustness to appearance change and error accumulation. Extensive ablation study validate the effectiveness of the proposed modules. Experiments on several benchmark datasets demonstrate superior performance of the proposed method compared to state-of-the-art methods on video super-resolution. Code is available at https://github.com/junpan19/RSDN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Super-resolution is one of the fundamental problem in image processing, which aims at reconstructing a high resolution (HR) image from a single low-resolution (LR) image or a sequence of LR images. According to the number of input frames, the field of SR can be divided into two categories, i.e., single image super-resolution (SISR) and multi-frame super-resolution (MFSR). For SISR, the key issue is to exploit natural image prior for compensating missing details; while for MFSR, how to take full advantage from additional temporal information is of pivotal importance. In this work, we focus on the video super-resolution (VSR) task which belongs to MFSR. It draws much attention in both research and  <ref type="figure">Fig. 1</ref>. VSR results on the City sequence in Vid4. Our method produces finer details and stronger edges with better balance between speed and performance than both temporal sliding window based <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> and recurrent based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref>. Blue box represents recurrent-based and green box represents sliding window based methods. Runtimes (ms) are calculated on an HR image of size 704?576.</p><p>industrial communities because of its great value on computational photography and surveillance. In the last several years, great attempts have been made to exploit multi-frame information for VSR. One category of approaches utilize multi-frame information by conducting explicit motion compensation. These approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21]</ref> firstly compute optical flow between a reference frame and neighboring frames and then employ the aligned observations to reconstruct the high-resolution reference frame. However, estimating dense optical flow itself is a challenging and time-consuming task. Inaccurate flow estimation often leads to unsatisfactory artifacts in the SR results of these flow-based VSR approaches. In addition, the heavy computational burden also impedes the application of these applications in resource-constrained devices and time-sensitive scenarios. In order to avoid explicit motion compensation, another category of methods propose to exploit the motion information in an implicit manner. The dynamic upsampling filters <ref type="bibr" target="#b11">[12]</ref> and the progressive fusion residual blocks <ref type="bibr" target="#b28">[29]</ref> are designed to explore flow-free motion compensation. With these flexible compensation strategies, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> not only avoid heavy motion estimation step but also achieve highly competitive VSR performance. However, they still suffer from the redundant computation for several neighboring frames within a temporal window and need to cache several frames in advance to conduct VSR. Recently, for the pursuit of efficiency, there is an emerging trend of applying recurrent connection to address the VSR task.</p><p>These approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref> make use of recurrent connection to conduct video super-resolution in a streaming way, that is, output or hidden state of previous time steps is used to help super-resolve future frames. In addition, they are able to exploit temporal information from many frames in the past. By simply propagating output and hidden state of previous steps with a recurrent unit, they achieve promising VSR performance with considerably less processing time.</p><p>In this paper, we propose a novel recurrent network for efficient and effective video super-resolution. Instead of simply concatenating consecutive three frames with previous hidden state as in <ref type="bibr" target="#b3">[4]</ref>, we propose to decompose each frame of a sequence into components of structure and detail and aggregate both current and previous structure and detail information to super-resolve each frame. Such a strategy not only allows our method to address different difficulties in the structure and detail components, but also able to impose flexible supervision to recover high-frequency details and strengthen edges in the reconstruction.</p><p>In addition, we observe that hidden state in a recurrent network captures different typical appearances of a scene over time. To make full use of temporal information in hidden state, we treat the hidden state as a historical dictionary and compute correlation between the reference frame and each channel in hidden state. This allows the current frame to highlight the potentially helpful information and suppress outdated information such that information fusion would be more robust to appearance change and accumulated errors. Extensive ablation study demonstrates the effectiveness of the proposed method. It performs very favorably against state-of-the-art methods on several benchmark datasets, in both superresolution performance and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single Image Super-Resolution</head><p>Traditional SISR methods include interpolation-based methods and dictionary learning-based methods. However, since the rise of deep learning, most traditional methods are outperformed by deep learning based methods. A simple three-layer CNN is proposed by Dong <ref type="bibr" target="#b1">[2]</ref>, showing great potential of deep learning in super-resolution for the first time. Since then, plenty of new network architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30]</ref> have been designed to explore power of deep learning to further improve performance of SISR. In addition, researchers also investigate the role of losses for better perceptual quality. More discussions can be found in a recent survey <ref type="bibr" target="#b27">[28]</ref>. A very relevant work is the DualCNN method proposed by Pan et al. <ref type="bibr" target="#b21">[22]</ref>, where authors proposed a network with two parallel branches to reconstruct structure and detail components of an image, respectively. However, different from that work, our method aims at addressing the video super-resolution task. It decomposes the input frames into structure and detail components and propagates them with a recurrent unit that is composed of two interleaved branches to reconstruct the high-resolution targets. It is motivated by the assumption that structure and detail components not only suffer from different difficulties in high-resolution reconstruction but also take benefit from other frames in different ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Super-Resolution</head><p>Although SISR methods can also be used to address the video super-resolution task, they are not very effective because they only learn to explore natural prior and self-similarity within an image and ignore rich temporal information in a sequence. The key to video super-resolution is to make full use of complementary information across frames. Most video super-resolution methods can be roughly divided into two categories according to whether they conduct motion compensation in an explicit way or not.</p><p>Explicit motion compensation. Most methods with explicit motion compensation follow a pipeline of motion estimation, motion compensation, information fusion and upsampling. VESPCN <ref type="bibr" target="#b0">[1]</ref> presents a joint motion compensation and video super-resolution with a coarse-to-fine spatial transformer module. Tao et al. <ref type="bibr" target="#b24">[25]</ref> proposed an SPMC module for sub-pixel motion compensation and used a ConvLSTM to fuse information across aligned frames. Xue et al. <ref type="bibr" target="#b26">[27]</ref> proposed a task-oriented flow module that is trained together with a video processing network for video denoising, deblock or super-resolution. In <ref type="bibr" target="#b22">[23]</ref>, Sajjadi et al. proposed to super-resolve a sequence of frames in a recurrent manner, where the result of previous frame is warped to the current frame and two frames are concatenated for video super-resolution. Haris et al. <ref type="bibr" target="#b6">[7]</ref> proposed to use a recurrent encoderdecoder module to exploit explicitly estimated inter-frame motion. Wang et al. <ref type="bibr" target="#b25">[26]</ref> proposed to align multiple frames to a reference frame in feature space with a deformable convolution based module and fuse aligned frames with a temporal and spatial attention module. However, the major drawback of such methods is the heavy computational load introduced by motion estimation and motion compensation. Implicit motion compensation. As for methods with implicit motion compensation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4]</ref>, they do not estimate motion between frames and align them to a reference frame, but focus on designing an advanced fusion module such that it can make full use of complementary information across frames. Jo et al. <ref type="bibr" target="#b11">[12]</ref> proposed to use a 3D CNN to exploit spatial-temporal information and predict a dynamic upsampling filter to reconstruct HR images. In <ref type="bibr" target="#b28">[29]</ref>, Yi et al. proposed to fuse spatial-temporal information across frames in a progressive way and use a non-local module to avoid explicit motion compensation. Video super-resolution with implicit motion can also be done with recurrent connection. Huang et al. <ref type="bibr" target="#b8">[9]</ref> proposed a bidirectional recurrent convolutional network to model temporal information across multiple frames for efficient video super-resolution. In <ref type="bibr" target="#b3">[4]</ref>, Fuoli et al. proposed to conduct temporal information propagation with a recurrent architecture in feature space. Our method also adopts the recurrent way to conduct video super-resolution without explicit motion compensation. However, different from the above methods, we proposed to decompose a frame into two components of structure and detail and propagate them separately. In addition, we also compute correlation between the current frame and the hidden state to adaptively use the history information in the hidden state for better performance and less risk of error accumulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Recurrent Networks for Video-based Tasks</head><p>Recurrent networks have been widely used in different video recognition tasks. Donahue et al. <ref type="bibr" target="#b14">[15]</ref> proposed a class of recurrent convolutional architectures which combine convolutional layers and long-range temporal information for action recognition and image captioning. In <ref type="bibr" target="#b23">[24]</ref>, a bi-directional LSTM is applied after a multi-stream CNN to fully explore temporal information in a sequence for fine-grained action detection. Du et al. <ref type="bibr" target="#b2">[3]</ref> proposed a recurrent network with a pose attention mechanism which exploits spatial-temporal evolution of human pose to assist action recognition. Recurrent networks are capable of processing sequential information by integrating information from each frame in their hidden states. They can not only be used for high-level video recognition tasks but are also suitable for low-level video processing tasks.</p><formula xml:id="formula_0">? ? ? SD Block SD Block Structure-detail Decomposition C C Conv 3?3 Conv 3?3 Conv 3?3 Conv 3?3 ?1 ? ?1 C Conv 3?3 ? ReLU ? ? +1 ?1 ? ReLU ReLU (a) (b) ?1 ? ?1 SD Block Hidden-state Adaptation +1 +1 +1 +2 +2 +2 + + +1 +1 ? +2 +2 +2 ? ?1 ?1 ?1 Conv 3?3 C Concatenation Depth to space Elemental-wise Addition Input Output ?1 +1 +2 ?1 ?1 Propagation Progress +1 +2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a low-resolution video clip {I LR 1:N }, N ? 2, the goal of VSR is to produce a high-resolution video sequence {? HR 1:N } from the corresponding low-resolution one by filling in missing details for each frame. In order to process a sequence efficiently, we conduct VSR in a recurrent way similar to <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref>. However, instead of feeding a whole frame to a recurrent network at each time step, we decompose each input frame into two components, i.e., a structure component and a detail component, to the following network. Two kinds of information interact with each other in the proposed SD blocks over time, which is not only able to sharpen the structure of each frame but also manages to recovers missing details. In addition, to make full use of complementary information stored in hidden states, we treat hidden state as a history dictionary and adapt this dictionary to the demand of the current frame. This allow us to highlight the potential helpful information and suppress outdated information. The overall pipeline is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent Structure-Detail Network</head><p>Recurrent unit. Each frame can be decomposed into a structure component and a detail component. The structure component models low-frequency information in an image and motion between frames. While the detail component captures fine high-frequency information and slight change in appearance. These two components suffer from different difficulty in high-resolution reconstruction and take different benefit from other frames, hence should be processed separately.</p><p>In this work, we simply apply a pair of bicubic downsampling and upsampling operations to extract structural information from a frame I LR t , which is denoted as</p><formula xml:id="formula_1">S LR t . The detail component D LR t</formula><p>can be then computed as the difference between the input frame I LR t and the structure component S LR t . In fact, we can also use other ways such as low-pass filtering and high-pass filtering to get these two components. For simplicity, we adopt a symmetric architecture for the two components in the recurrent unit, as shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. Here we only take D-branch at time step t as an example to explain its architecture design. Detail components of the previous and current frames {D LR t?1 , D LR t } are concatenated with the previously estimated detail mapD t?1 and hidden state? SD t?1 along the channel axis. Such information is further fused by one 3 ? 3 convolutional layer and several structure-detail (SD) blocks. In this way, this recurrent unit manages to integrate together information from two consecutive input frames, output of the previous time step and historical information stored in the hidden state. h D t denotes the feature computed after several SD blocks. It goes through another 3 ? 3 convolutional layer and an upsampling layer to produce the high resolution detail componentD HR t . The S-branch is designed in a similar way. h S t and h D t are combined to produce the final high resolution image? HR t and new hidden state h SD t . The D-branch focuses on extracting complementary details from past frames for the current frame while the S-branch focuses on enhancing existed edges and textures in the current frame.</p><p>Structure-Detail block. Residual block <ref type="bibr" target="#b17">[18]</ref> and dense block <ref type="bibr" target="#b7">[8]</ref> are widely used in both high-level and low-level computer vision tasks because of their effectiveness in mining and propagating information. In this section, we compare several variants of blocks in propagating information in a recurrent structuredetail unit. For comparison, we also include a modified residual block as shown in <ref type="figure">Fig. 3(a)</ref>, which only has one branch and takes the whole frames as input. To adapt it to address two branches, the easiest way is to have two modified residual blocks that process two branches separately, as shown in <ref type="figure">Fig. 3(b)</ref>. However, in this way each branch only sees the component-specific information and can not makes full use of all information in the input frames. Therefore, we propose a new module called structure-detail (SD) block, as shown in <ref type="figure">Fig. 3(c)</ref>. The two components are first fed to two individual branches and then combined with an addition operation. In this way, it not only specializes on each component but also promotes information exchange between structure and detail components. Its advantage over the other two variants is validated in the experiment section. In a recurrent neural network, hidden state at time step t would summarize past information in the previous frames. When applying a recurrent neural network to the video super-resolution task, hidden state is expected to model how a scene's appearance evolves over time, including both structure and detail. The previous recurrent-based VSR method <ref type="bibr" target="#b3">[4]</ref> directly concatenates previous hidden state and two input frames and feeds it to several convolutional layers. However, for each LR frame to be super-resolved, it has distinct appearance and is expected to borrow complementary information from previous frames in different ways. Applying the same integration manner to all frames is not optimal and could hurt the final performance. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, different channels in hidden state describe different scene appeared in the past. They should make different contribution to different positions of different frames, especially when there are occlusion and large deformation with some channels of the hidden state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hidden State Adaptation</head><p>In this work, we proposed the Hidden State Adaptation (HSA) module to adapt a hidden state to the appearance of the current frame. As for each unit in hidden state, it should be highlighted if it has similar appearance as the current frame; otherwise, it should be suppressed if it looks very different. With this module the proposed method carefully chooses only useful information in previous frames, hence alleviate the influence of drastic appearance change and error accumulation. Since response of a filter models correlation between the filter and a neighborhood on an image, here we take similar way to compute correlation between an input frame and hidden state. Inspired by <ref type="bibr" target="#b10">[11]</ref>, we generate spatially variant and sample specific filters for each position in the current frame and use those filters to compute their correlation with the corresponding positions in each channel of hidden state. Specifically, these spatially variant filters F ? t ? R H?W ?(k?k) are obtained by feeding the current frame I LR t ? R H?W ?3 into a convolutional layer with ReLU activation function <ref type="bibr" target="#b4">[5]</ref>, where H and W are respectively height and width of the current frame, and k denotes the size of filters. Then, each filter F ? t (i, j) are applied to a k ? k window of h SD t?1 centered at position (i, j) to conduct spatially variant filtering. This process can be formulated as:</p><formula xml:id="formula_2">M t (i, j, c) = k/2 u=? k/2 k/2 v=? k/2 F ? t (i, j, u, v) ? h SD t?1 (i + u, j + v, c),<label>(1)</label></formula><p>where M t (i, j, c) represents correlation between the current frame and the c-th channel of hidden state at position (i, j). It is further fed to a sigmoid activation function ?(?) that transforms it into a similarity value in range [0, 1]. Finally, the adapted hidden state? SD t?1 is computed by:</p><formula xml:id="formula_3">? SD t?1 = M t h SD t?1 ,<label>(2)</label></formula><p>where ' ' denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss functions</head><p>Since the proposed recurrent network has two streams, the trade-off between supervision on structure and detail during training is very important. Imbalanced supervision on structure and detail might produce either sharpened frames but with less details or frames with many weak edges and details. Therefore, we propose to train the proposed network with three loss terms as shown in eq. 3, one for structure component, one for detail component, and one for the whole frame. ?, ? and ? are hyper-parameters to balance the trade-off of these three terms. The loss to train an N -frame sequence is formulated as:</p><formula xml:id="formula_4">L = 1 N N t=1 (?L S t + ?L D t + ?L I t ).<label>(3)</label></formula><p>Similar to <ref type="bibr" target="#b16">[17]</ref>, we use Charbonnier loss function to compute the difference between reconstruction and high-resolution targets. Hence, we have L S t = S HR t ?? HR </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first explain the experiment datasets and implementation details of the proposed method. Then extensive ablation study is conducted to analyze the effectiveness of the proposed SD block and hidden state adaptation module. Furthermore, the proposed method is compared with state-of-the-art video super-resolution methods in terms of both effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Datasets. Some works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> collect private training data from youtube on their own, which is not suitable for fair comparison with other methods. In this work, we adopt a widely used video processing dataset Vimeo-90K to train video super-resolution models. Vimeo-90K is a recent proposed large dataset for video processing tasks, which contains about 90K 7-frame video clips with various motions and diverse scenes. About 7K video clips select out of 90K as the test set, termed as Vimeo-90K-T. To train our model, we crop patches of size 256 ? 256 from HR video sequences as the target. Similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref>, the corresponding low-resolution patches are obtained by applying Gaussian blur with ? = 1.6 to the target patches followed by ?4 times downsampling.</p><p>To validate the effectiveness of the proposed method, we evaluate our models on several popular benchmark datasets, including Vimeo-90K-T <ref type="bibr" target="#b26">[27]</ref>, Vid4 <ref type="bibr" target="#b19">[20]</ref>  and UDM10 <ref type="bibr" target="#b28">[29]</ref>. As mentioned above, Vimeo-90K-T contains a lot of video clips, but each clip has only 7 frames. Vid4 and UDM10 are long sequences with diverse scenes, which is suitable to evaluate the effectiveness of recurrent-based method in information accumulation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Training Details. The base model of our method consists of 5 SD blocks where each convolutional layer has 128 channels, i.e., RSDN 5-128. By adding more SD blocks, we can obtain RSDN 7-128 and RSDN 9-128. The performance can be further boosted with only small increase on computational cost and runtime. We adopt K = 3 for HSA module for efficiency. To fully utilize all given frames, we pad each sequence by reflecting the second frame at the beginning of the sequence. When dealing with the first frame of a sequence, the previous estimated detailD t?1 , structure? t?1 and hidden state feature h SD t?1 are all initialized with zeros. The model training is supervised with Charbonnier penalty loss function and is optimized with Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with ? 1 = 0.9 and ? 2 = 0.999. Each mini-batch consists of 16 samples. The learning rate is initially set to 1 ? 10 ?4 and is later down-scaled by a factor of 0.1 every 60 epoch till 70 epochs. The training data is augmented by standard flipping and rotating. All experiments are conducted on a server with Python 3.6.4, PyTorch 1.1 and Nvidia Tesla V100 GPU.</p><p>Recurrent Unit. We compare three kinds of blocks for information flow in the recurrent unit, i.e., the three blocks shown in <ref type="figure">Fig. 3</ref>. For fair comparison among these blocks, we keep these three networks with almost the same parameters by setting the channel of convolutional layers in model 1 to 256, and setting the one in model 4 and 7 to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this section, we conduct several ablation studies to analyze the effectiveness of the proposed SD block and the hidden state adaptation module. In addition, we also investigate the influence of different supervision on structure and detail components on the reconstruction performance. As shown in Tab. 1, model 1 and model 4 achieves similar performance, with model 1 a little higher SSIM and model 4 a little higher PSNR. This implies that simply dividing the input into structure and detail components and processing each one individually does not work well. Although it seems that having two branches to process each component divides a difficult task into two easier ones, it makes each one blind to the other and can not make full use of the information in the input to reconstruct either component.  <ref type="table">Table 2</ref>. Ablation study on influence of different loss items. By introducing information exchange between structure and detail components, model 7 obtains better performance than model 1 and 4 in both PSNR and SSIM. Similar result can also found in comparison among model 2, 5 and 8. In addition, we experiment with taking the whole frames as input of both branches, that is, model 3 and model 6. By comparing model 3 and model 5 (and also model 6 and model 8), we show that the improvement comes not only from architecture of the two-stream block itself but also indeed from the decomposition into structure and detail components. The network with the proposed SD block allows each branch to explicitly focus on reconstructing a single component, which is easier than reconstructing a mixture of multiple components. Each branch makes use of the other one such that it can obtain enough information to reconstruct the high-resolution version for that component. The advantage of the proposed SD blocks can also be observed in the qualitative comparison as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>In addition, we show in Tab. 1 that each model can gain further boost in performance with the proposed HSA module, about 0.04 dB in PSNR and 0.002 in SSIM on average. This module does not only work for the proposed network with SD blocks but also helps improve the performance for the ones with one-stream and two-stream residual blocks. The hidden state adaptation module allows the model to selectively use the history information stored in hidden state, which makes it robust to appearance change and error accumulation to some extent.</p><p>Influence of different components. The above experiment shows that decomposing the input into two components and processing them with the proposed SD blocks brings much improvement. We also investigate the relative importance of these two components by imposing different levels of supervision on the reconstruction of two components. It implies that the relative supervision strength applied to different components also plays an important role in the super-resolution performance. As shown in Tab. 2, when the weights for structure component, detail component and the whole frame are set to (?, ?, ?) = (1, 1, 1), it achieves a good performance of 27.79/0.8474 in PSNR/SSIM. The performance degrades when the weigh for structure component more than the weight for detail component (i.e.(?, ?, ?) = (1, 0.5, 1)), and verse vise (i.e.(?, ?, ?) = (0. <ref type="figure">5, 1, 1)</ref>). The result of (1, 1, 0) is 0.06dB lower than that of (1, 1, 1), which means applying additional supervision on the combined image helps the training of the model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-arts</head><p>In this section, we compare our methods with several state-of-the-art VSR approaches, including SPMC <ref type="bibr" target="#b24">[25]</ref>, TOFlow <ref type="bibr" target="#b26">[27]</ref>, Liu <ref type="bibr" target="#b20">[21]</ref>, DUF <ref type="bibr" target="#b6">[7]</ref>, EDVR <ref type="bibr" target="#b25">[26]</ref>, PFNL <ref type="bibr" target="#b28">[29]</ref>, TGA <ref type="bibr" target="#b9">[10]</ref>, FRVSR <ref type="bibr" target="#b22">[23]</ref> and RLSP <ref type="bibr" target="#b3">[4]</ref>. The first seven methods super-resolve a single reference within a temporal sliding window. Among these methods, SPMC, TOFlow, Liu, RBPN and EDVR need to explicitly estimate the motion between the reference frame and other frames within the window, which requires redundant computation for several frames. DUF, PFNL and TGA skip the motion estimation process and partially ameliorate this issue. The last two methods FRVSR and RLSP super-resolve each frame in a recurrent way and are more efficient. We carefully implement most of these methods either on our own or by running the publicly available code, and manage to reproduce the results in their paper. The quantitative result of state-of-the-art methods on Vid4 is shown in Tab. 3, where the number is either reported in the original papers or computed with our implementation. In addition, we also include the number of parameters and FLOPs for most methods when super-resolution is conducted on an LR image of size 112 ? 64 in Tab. 3. On Vid4, our model with only 5 SD block achieves 27.61dB PSNR in Y channel and 26.13dB PSNR in RGB channels, which already outperforms most of the previous methods by a large margin. By increasing the number of SD block to 7 and 9, our methods respectively gain another 0.18dB and 0.31dB PSNR in Y channel while with only a little increase in FLOPs. We also evaluate our method on other three popular test sets. The quantitative results on UDM10 <ref type="bibr" target="#b28">[29]</ref> and Vimeo-90K-T <ref type="bibr" target="#b26">[27]</ref> two datasets are reported in Tab. 4. Our method achieves a very good balance between reconstruction performance and speed on these datasets. On UDM10 test set, RSDN 9-128 achieves new state-of-the-art, and is about 15 and 37 times faster than DUF and RBPN, respectively. RSDN 9-128 outperforms the recent proposed PFNL, where this dataset is proposed by 0.61dB in PSNR in Y channel while being 3 times faster. The proposed method is also evaluated on Vimeo-90K-T, which only contains 7-frame in each sequence. In this case, although our method can not take full of its advantage because of the short length of the sequence, it only lags behind the large model EDVR-L but is 6 times faster. We also show the qualitative comparison with other state-of-the-art methods. As shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, our method produces higher quality high-resolution images on all three datasets, including finer details and sharper edges. Other methods are either prone to generate some artifacts (e.g., wrong stripes in an image) or can not recover missing details (e.g., small windows of the building). We also examine temporal consistency of the video super-resolution results in <ref type="figure" target="#fig_7">Fig. 8</ref>, which is produced by extracting a horizontal row of pixels at the same position from consecutive frames and stacking them vertically. The temporal profile produced by our method is not only temporally smoother but also much sharper, satisfying both requirements of the video super-resolution task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work we have presented an effective and efficient recurrent network to super-resolve a video in a streaming manner. The input is decomposed into structure and detail components and fed to two interleaved branches to respectively reconstruct the corresponding components of high-resolution frames. Such a strategy allows our method to address different difficulties in the structure and detail components and to enjoy flexible supervision applied to each components for good performance. In addition we find that hidden state in a recurrent network captures different typical appearance of a scene over time and selectively using information from hidden state can enhance its robustness to appearance change and error accumulation. Extensive experiments on several benchmark datasets demonstrate its superiority in terms of both effectiveness and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) The overall pipeline of the proposed method; (b) architecture of the recurrent structure-detail unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Four channels in hidden state at a certain time step are selected for visualization. Yellow arrow denotes the difference in appearance among these four channels. Zoom in for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 ?Fig. 5 .</head><label>15</label><figDesc>Element-wise Multiplication * Convolution Design of hidden state adaptation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>t 2 + 2 + ? 2</head><label>222</label><figDesc>? 2 for structure component, L D t = for the whole frame. The effectiveness of these loss functions is validated in the experiment section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative comparison between different network structures. Zoom in to see better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative comparison on Vid4, UDM10 and Vimeo-90K-T test set for 4? SR. Zoom in for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of temporal profile for the green line on the calendar sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>SSIM 27.58/0.8410 27.65/0.8444 27.70/0.8452 27.64/0.8404 27.68/0.8429 27.73/0.8460 27.76/0.8463 27.79/0.8474 Ablation study on different network architecture.</figDesc><table><row><cell>Method</cell><cell cols="2">One Stream 7-256</cell><cell></cell><cell>Two Stream 7-128</cell><cell></cell><cell></cell><cell>SD Block 7-128</cell><cell></cell></row><row><cell>Model</cell><cell>Model 1</cell><cell>Model 2</cell><cell>Model 3</cell><cell>Model 4</cell><cell>Model 5</cell><cell>Model 6</cell><cell>Model 7</cell><cell>Model 8</cell></row><row><cell>HSA?</cell><cell>w/o</cell><cell>w/</cell><cell>w/</cell><cell>w/o</cell><cell>w/</cell><cell>w/</cell><cell>w/o</cell><cell>w/</cell></row><row><cell>Input</cell><cell>Image</cell><cell>Image</cell><cell>Image</cell><cell>S &amp; D</cell><cell>S &amp; D</cell><cell>Image</cell><cell>S &amp; D</cell><cell>S &amp; D</cell></row><row><cell>PSNR/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>PSNR/SSIM 27.56/0.8440 27.77/0.8459 27.73/0.8453 27.79/0.8474</figDesc><table><row><cell>(?, ?, ?)</cell><cell>(1, 0.5, 1)</cell><cell>(0.5, 1, 1)</cell><cell>(1, 1, 0)</cell><cell>(1, 1, 1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison (PSNR (dB) and SSIM) on Vid4 for 4? video super-resolution. Red text indicates the best and blue text indicates the second best performance. Y and RGB indicate the luminance and RGB channels, respectively. FLOPs (MAC) are calculated on an HR image of size 720?480. ' ?' means the values are either taken from paper or calculated using provided models.</figDesc><table><row><cell>UDM10</cell><cell>Bicubic</cell><cell>TOFlow [27]</cell><cell>DUF-52L [12]</cell><cell>RBPN [7]</cell><cell>PFNL  ? [29]</cell><cell cols="2">FRVSR 10-128 [23] RLSP 7-256 [4]</cell><cell>RSDN 7-128</cell><cell>RSDN 9-128</cell></row><row><cell>FLOPs [TMAC]</cell><cell>N/A</cell><cell>2.17</cell><cell>1.65</cell><cell>24.81</cell><cell>1.88</cell><cell>0.36</cell><cell>0.24</cell><cell>0.28</cell><cell>0.35</cell></row><row><cell>Runtime [ms]</cell><cell>N/A</cell><cell>1693</cell><cell>1413</cell><cell>3567</cell><cell>295</cell><cell>137</cell><cell>49</cell><cell>79</cell><cell>94</cell></row><row><cell>Average (Y)</cell><cell>28.47/0.8523</cell><cell>36.26/0.9438</cell><cell>38.48/0.9605</cell><cell>38.66/0.9596</cell><cell>38.74/0.9627</cell><cell>37.09/0.9522</cell><cell>38.48/0.9606</cell><cell>39.13/0.9645</cell><cell>39.35/0.9653</cell></row><row><cell>Average (RGB)</cell><cell>27.05/0.8267</cell><cell>34.46/0.9298</cell><cell>36.78/0.9514</cell><cell>36.53/0.9462</cell><cell>36.78/0.9514</cell><cell>35.39/0.9403</cell><cell>36.39/0.9465</cell><cell>37.26/0.9548</cell><cell>37.46/0.9557</cell></row><row><cell>Vimeo-90K-T</cell><cell>Bicubic</cell><cell>TOFlow [27]</cell><cell>DUF-52L [12]</cell><cell>RBPN [7]</cell><cell cols="3">EDVR-L  ? [26] FRVSR 10-128 [23] RLSP 7-256 [4]</cell><cell>RSDN 7-128</cell><cell>RSDN 9-128</cell></row><row><cell>FLOPs [TMAC]</cell><cell>N/A</cell><cell>0.27</cell><cell>0.20</cell><cell>3.08</cell><cell>0.30</cell><cell>0.04</cell><cell>0.03</cell><cell>0.03</cell><cell>0.04</cell></row><row><cell>Runtime [ms]</cell><cell>N/A</cell><cell>215</cell><cell>167</cell><cell>470</cell><cell>99</cell><cell>28</cell><cell>11</cell><cell>13</cell><cell>15</cell></row><row><cell>Average (Y)</cell><cell>31.30/0.8687</cell><cell>34.62/0.9212</cell><cell>36.87/0.9447</cell><cell>37.20/0.9458</cell><cell>37.61/0.9489</cell><cell>35.64/0.9319</cell><cell>36.49/0.9403</cell><cell>37.05/0.9454</cell><cell>37.23/0.9471</cell></row><row><cell>Average (RGB)</cell><cell>29.77/0.8490</cell><cell>32.78/0.9040</cell><cell>34.96/0.9313</cell><cell>35.39/0.9340</cell><cell>35.79/0.9374</cell><cell>33.96/0.9192</cell><cell>34.56/0.9274</cell><cell>35.14/0.9325</cell><cell>35.32/0.9344</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparison (PSNR(dB) and SSIM) on UDM10 and Vimeo-90K-T for 4? video super-resolution, respectively. Flops and runtimes are calculated on an HR image size of 1280?720 and 448?256 for UDM10 and Vimeo-90K-T, respectively. Red text indicates the best and blue text indicates the second best performance. Y and RGB indicate the luminance and RGB channels, respectively. ' ?' means the values are either taken from paper or calculated using provided models.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rpan: An end-to-end recurrent pose-attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient video super-resolution through recurrent latent space propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fuoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<idno>abs/1909.08080</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep back-projection networks for superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Recurrent back-projection network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Video super-resolution with temporal group attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast and accurate image superresolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2599" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning dual convolutional neural networks for low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Detail-revealing deep video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3106" to="3121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Progressive fusion video superresolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
