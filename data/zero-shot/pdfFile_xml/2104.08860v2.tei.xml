<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
							<email>huaishaoluo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
							<email>leiji@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
							<email>emchen@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
							<email>wen.lei@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<email>nanduan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
							<email>trli@swjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. The CLIP (Contrastive Language-Image Pretraining), an image-language pre-training model, has demonstrated the power of visual concepts learning from web collected image-text datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner. Several questions are investigated via empirical studies: 1) Whether image feature is enough for video-text retrieval? 2) How a postpretraining on a large-scale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyper-parameters sensitivity of the model on video-text retrieval task. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various videotext retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We release our code at https://github. com/ArrowLuo/CLIP4Clip.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing of videos uploaded online every day, video-text retrieval is becoming an emerging requirement for people to find relevant videos efficiently. Beyond the actual web application, videotext retrieval is a fundamental research task for multi-modal visual and language understanding. We can distinguish the previous works directly by their input: raw video (pixel-level) or video feature (feature-level).</p><p>Usually, the pretrain models <ref type="bibr" target="#b42">(Zhu and Yang, 2020;</ref><ref type="bibr" target="#b20">Luo et al., 2020;</ref><ref type="bibr" target="#b16">Li et al., 2020;</ref><ref type="bibr">Gabeur *</ref> This work was done during the first author's internship in <ref type="bibr">MSR Asia et al., 2020;</ref><ref type="bibr" target="#b24">Patrick et al., 2021;</ref><ref type="bibr" target="#b29">Rouditchenko et al., 2020)</ref> are feature-level because they are trained on some large-scale video-text datasets, e.g., Howto100M <ref type="bibr" target="#b22">(Miech et al., 2019)</ref>. The input is the cached video features generated via off-the-shelf frozen video feature extractors. If the input is the raw video, it makes the pretrain very slow or infeasible. Nevertheless, benefitting from the large-scale dataset, the pretrain models show a significant performance gain for video-text retrieval.</p><p>The pixel-level approach trains the model with raw video as input directly <ref type="bibr" target="#b31">(Torabi et al., 2016;</ref><ref type="bibr" target="#b12">Kiros et al., 2014;</ref><ref type="bibr" target="#b37">Yu et al., 2016a;</ref><ref type="bibr" target="#b10">Kaufman et al., 2017;</ref><ref type="bibr" target="#b40">Yu et al., 2017</ref><ref type="bibr" target="#b38">Yu et al., , 2018</ref>. Early literature almost belongs to this approach. This approach learns the video feature extractor jointly with the paired text. On the contrary, the feature-level approach highly depends on a suitable feature extractor. It can not propagate the learning back to the fixed video encoder.</p><p>Some recent works begin to pretrain the model with the pixel-level approach, making the pretrain model learn from the raw video. The big challenge is how to reduce the high computational overload of dense video input. Typically, ClipBERT <ref type="bibr" target="#b15">(Lei et al., 2021</ref>) employs a sparse sampling strategy to make the end-to-end pretrain possible. Concretely, the model only sparsely samples a single or a few short clips from a video at each training step. The performance shows that the end-to-end training can benefit the low-level feature extraction. A few sparsely sampled clips are enough to solve the video-text retrieval task. Frozen <ref type="bibr">(Bain et al., 2021)</ref> treats an image as a single-frame video and designs a curriculum learning schedule to train the model on both image and video datasets. The results show that the curriculum learning schedule learns increasingly from image to multi frames can increase efficiency. Our target is not to pretrain a new model on video-text retrieval. We mainly investigate how to transfer the knowledge from the image-text pretrained model CLIP <ref type="bibr">(Radford et al., 2021)</ref> to video-text retrieval in this paper.</p><p>We exploit the pre-trained CLIP and propose a model named CLIP4Clip (CLIP For video Clip retrieval) to solve video-text retrieval. Concretely, the CLIP4Clip is constructed on top of the CLIP and designs a similarity calculator to investigate three similarity calculation approaches: parameterfree type, sequential type, and tight type. Like our work, the concurrent work from <ref type="bibr">Portillo-Quintero et al. (2021)</ref> is also built on the CLIP for video-text retrieval. The difference is that their work directly leveraged the CLIP for zero-shot prediction without considering different similarity calculation mechanisms. However, we design some similarity calculation approaches to improve the performance and train the model in an end-to-end manner. The contributions of our work are: 1) we investigate three mechanisms of similarity calculation based on the pretrained CLIP; 2) we further post pre-train the CLIP on a noisy large-scale video-language dataset to learn a better retrieval space. The extensive experiments show our model achieves the new SOTA results on MSR-VTT <ref type="bibr" target="#b36">(Xu et al., 2016)</ref>, MSVC <ref type="bibr" target="#b5">(Chen and Dolan, 2011)</ref>, LSMDC , ActivityNet <ref type="bibr" target="#b13">(Krishna et al., 2017a)</ref>, and DiDeMo <ref type="bibr" target="#b8">(Hendricks et al., 2017)</ref> datasets.</p><p>Besides, we can conclude the following insights from our extensive experiments: 1) One single image is far from enough for video encoding for video-text retrieval.</p><p>2) Post-pretraining at a large-scale video-text dataset on the CLIP4Clip model is required and can improve the performance, especially for zeroshot prediction by a large margin.</p><p>3) With the powerful pre-trained CLIP, it is better not to introduce new parameters and adopt a mean pooling mechanism on video frames for small datasets. At the same time, it is better to introduce more parameters, e.g., the self-attention layer, to learn the temporal dependency for large datasets. 4) We carefully study the hyper-parameters and report the best setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Video Encoder Backbone Previous works mainly focus on 2D/3D spatial-temporal convolution for video representation <ref type="bibr" target="#b32">(Tran et al., 2015;</ref><ref type="bibr" target="#b35">Xie et al., 2018;</ref><ref type="bibr">Feichtenhofer et al., 2019)</ref>. <ref type="bibr">Recently ViT (Dosovitskiy et al., 2021)</ref>, a transformerbased image encoder, has attracted much attention.</p><p>The transformer based video encoder is still in its early stage for action classification <ref type="bibr" target="#b4">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b2">Arnab et al., 2021)</ref>. We mainly investigate the effective transformer based video backbone for multimodal video-text retrieval.</p><p>Visual Representation Learning from Text Supervision Visual representation learning is a challenging task and has been widely studied with supervised or self-supervised methods. Considering semantic supervision from large-scale unlabeled data, learning visual representation from text representation <ref type="bibr">(Radford et al., 2021;</ref><ref type="bibr" target="#b21">Miech et al., 2020;</ref><ref type="bibr" target="#b15">Lei et al., 2021)</ref> is an emerging research topic with the benefit of large-scale visual and linguistic pairs collected from the Internet. The prominent success of the CLIP (Contrastive Language-Image Pre-training) <ref type="bibr">(Radford et al., 2021)</ref> has demonstrated its capability of learning SOTA image representations from linguistic supervision with pretraining on large-scale image and text pairs. The pre-trained model can learn fine-grained visual concepts for images and transfer the knowledge for the retrieval task. Typically, MIL-NCE <ref type="bibr" target="#b21">(Miech et al., 2020)</ref> mainly investigated to leverage noisy large-scale Howto100M <ref type="bibr" target="#b22">(Miech et al., 2019)</ref> instructional videos to learn a better video encoder in an end-to-end manner. Furthermore, ClipBERT <ref type="bibr" target="#b15">(Lei et al., 2021)</ref> proposed an efficient end-to-end approach through sparse sampling and presented that the pretrained by image-language dataset facilitated a better initialization of video-text retrieval. Different from ClipBERT, we adopt CLIP with transformer based visual backbone and extend this image-language pre-trained model to videolanguage pre-training for video-text retrieval. Considering the temporal sequence of video, a 2D/3D linear embedding and a similarity calculator attached to the visual transformer are used to capture temporal sequence features.</p><p>Video-Text Retrieval Early works on video-text retrieval <ref type="bibr" target="#b31">(Torabi et al., 2016;</ref><ref type="bibr" target="#b12">Kiros et al., 2014;</ref><ref type="bibr" target="#b37">Yu et al., 2016a;</ref><ref type="bibr" target="#b10">Kaufman et al., 2017;</ref><ref type="bibr" target="#b40">Yu et al., 2017</ref><ref type="bibr" target="#b38">Yu et al., , 2018</ref>designed intensive fusion mechanisms for cross-modal learning. Recently, the pre-trained models <ref type="bibr" target="#b42">(Zhu and Yang, 2020;</ref><ref type="bibr" target="#b1">Amrani et al., 2021;</ref><ref type="bibr" target="#b20">Luo et al., 2020;</ref><ref type="bibr" target="#b16">Li et al., 2020;</ref><ref type="bibr" target="#b21">Miech et al., 2020;</ref><ref type="bibr">Gabeur et al., 2020;</ref><ref type="bibr" target="#b24">Patrick et al., 2021;</ref><ref type="bibr" target="#b15">Lei et al., 2021;</ref><ref type="bibr">Dzabraev et al., 2021;</ref> have dominated the leaderboard of the video-text retrieval with noticeable results on zero-shot retrieval  <ref type="figure">Figure 1</ref>: The framework of CLIP4Clip, which comprises three components, including two single-modal encoders and a similarity calculator. The model takes a video-text pair as input. For the input video, we first sample the input video into ordinal frames (images). Next, these image frames are reshaped into a sequence of flattened 2D patches. These patches are mapped to the 1D sequence of embeddings with a linear patch embedding layer and input to the image encoder for representation as in <ref type="bibr">ViT (Dosovitskiy et al., 2021)</ref>. Finally, the similarity calculator predicts the similarity score between the text representation and representation sequence of these frames. We investigate three types of similarity calculators in this work, including parameter-free, sequential, and tight types. ? means cosine similarity. We initial the two single-modal encoders with CLIP (ViT-B/32) <ref type="bibr">(Radford et al., 2021)</ref>. and fine-tuned retrieval. Concurrent to our work, <ref type="bibr">Portillo-Quintero et al. (2021)</ref> applied CLIP for zero-shot prediction, and <ref type="bibr">Bain et al. (2021)</ref> proposed a transformer-based video backbone. We propose to directly transfer the powerful knowledge from the pre-trained CLIP and continue pre-train the designed video-based CLIP4Clip on a largescale video-language dataset. Empirical studies present the effectiveness of the CLIP4Clip model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework</head><p>Given a set of videos (or video clips) V and a set of captions T , our target is to learn a function s(v i , t j ) to calculate the similarity between the video (or video clip) v i ? V and the caption t j ? T . The goal is to rank all the videos (or video clips) given the query caption according to their similarity score in the text-to-video retrieval or rank all the captions given the query video (or video clip) in the task of video-to-text retrieval. The objective of the s(v i , t j ) is to calculate a high similarity for relevant videotext pairs and a low similarity score for irrelevant ones.</p><p>The video (or video clip) v i ? V is repre-sented as a sequence of frames (images) in this paper. Formally, the video (or video clip) v i is composed of |v i | sampled frames such that</p><formula xml:id="formula_0">v i = {v 1 i , v 2 i , . . . , v |v i | i }.</formula><p>Our model is an end-to-end manner (E2E) trained on pixels directly via taking the frames as input. <ref type="figure">Figure 1</ref> demonstrates our framework, which mainly contains a text encoder, a video encoder, and a similarity calculator. We introduce each part in detail in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Encoder</head><p>To get the video representation, we first extract the frames from the video clip and then encode them via a video encoder to obtain a sequence of features. In this paper, we adopt the ViT-B/32 <ref type="bibr">(Dosovitskiy et al., 2021)</ref> with 12 layers and the patch size 32 as our video encoder. Concretely, we use the pretrained CLIP (ViT-B/32) <ref type="bibr">(Radford et al., 2021)</ref> as our backbone and mainly consider transferring the image representation to video representation. The pre-trained CLIP (ViT-B/32) is effective for the video-text retrieval task in this paper.</p><p>The <ref type="bibr">ViT (Dosovitskiy et al., 2021)</ref> first extracts non-overlapping image patches, then performs a linear projection to project them into 1D tokens, and exploits the transformer architecture to model the interaction between each patch of the input image to get the final representation. Following the ViT and CLIP, we use the output from the [class] token as the image representation. For the input frame sequence of video</p><formula xml:id="formula_1">v i = {v 1 i , v 2 i , . . . , v |v i | i }, the generated representation can denote as Z i = {z 1 i , z 2 i , . . . , z |v i | i }.</formula><p>We investigate two types of linear projections in the Linear Projection of Flattened Patches module shown in <ref type="figure">Figure 1a</ref> named 2D linear and 3D linear separately. a) The linear projection of flattened patches of ViT is regarded as 2D linear, which embeds each 2D frame patch independently. Such a 2D linear ignores the temporal information among frames. b) Therefore, we investigate a 3D linear projection, similar to <ref type="bibr" target="#b2">(Arnab et al., 2021)</ref>, to enhance temporal feature extraction. The comparison between 2D and 3D is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The 3D linear embeds patches across time. Concretely, the 3D linear uses a 3D convolution with kernel [t ? h ? w] as the linear instead of the kernel of [h?w] in 2D linear, where t, h, and w are temporal, height, and width dimensions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Encoder</head><p>We directly apply the text encoder from the CLIP to generate the caption representation. The text encoder is a Transformer <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> with the architecture modifications described in <ref type="bibr" target="#b27">(Radford et al., 2019)</ref>. It is a 12-layer 512-wide model with 8 attention heads. Following CLIP, the activations from the highest layer of the transformer at the [EOS] token are treated as the feature representation of the caption. For the caption t j ? T , we denote the representation as w j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Similarity Calculator</head><p>After extracting the video representation Z i = {z 1 i , z 2 i , . . . , z |v i | i } and caption representation w j , the key point comes to the similarity calculation. Since our model is built based upon a pre-trained image-text model, we should carefully add new learnable weights in the similarity calculator module. It is hard to learn without weight initialization and may hurt the performance of the pre-trained model training with backpropagation. Therefore, we categorize the mechanisms of the similarity calculator into three categories depending on whether the module introduces new parameters to learn. The parameter-free approach, i.e., meaning pooling, fuses the video representation without new parameters. Additionally, two other approaches introduce new weights to learn including a sequential type and a tight type methods with different sizes of new weights. <ref type="figure">Figure 1b</ref> illustrates the detailed structure of the three mechanisms. The parameterfree type and sequential type similarity calculators belong to the loose type that adopts two separate branches for video and text representation independently to calculate cosine similarity. While the tight type similarity calculator uses the transformer model for multi-modal interaction and further calculates the similarity via a linear projection, both of which consist of new weights to learn.</p><p>Parameter-free type According to the CLIP, the frames representation Z i and the caption representation w j have been layer normalized and linearly projected into a multi-modal embedding space through the large-scale pretraining on the image-text pairs. The natural idea is to employ a parameter-free type to calculate similarity directly with the image/frame from the video perspective. The parameter-free type first uses the mean pooling to aggregate the feature of all frames to obtain an 'average frame',</p><formula xml:id="formula_2">? i = mean-pooling(z 1 i , z 2 i , . . . , z |v i | i ). Then, the similarity function s(v i , t j ) is defined as the cosine similarity, s(v i , t j ) = w j ? i w j ? i .<label>(1)</label></formula><p>Sequential type The mean-pooling operation ignores the sequential information between frames.</p><p>In this way, we explore two methods to model the sequential feature for Sequential type similarity calculator. One is LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr">Gers et al., 2002)</ref>, and the other one is Transformer encoder <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> with position embedding P, Both of which are effective models for sequence features. We formulate them asZ i = LSTM(Z i ) and Z i = Transformer-Enc(Z i +P), respectively. Through the encoding, theZ i already embeds the temporal information. The subsequent operations are the same as the parameter-free type similarity calculator, and the similarity function is also the Eq. (1), and? i = mean-pooling(Z i ).</p><p>Tight type Different from above parameter-free type and sequential type, the tight type uses a Transformer Encoder <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref> for multimodal interaction between video and caption similar to <ref type="bibr" target="#b20">(Luo et al., 2020)</ref>, and predict the similarity through a linear layer, which introduces the most uninitialized weights. First, the Transformer Encoder takes the concatenated caption representation w j and frames' representation</p><formula xml:id="formula_3">Z i = {z 1 i , z 2 i , . . . , z |v i | i } as the fused feature U i formulated as, U i = [w j , z 1 i , z 2 i , . . . , z |v i | i ],<label>(2)</label></formula><formula xml:id="formula_4">U i = Transformer-Enc(U i + P + T). (3)</formula><p>where [, ] denotes concatenate operation. P is the position embedding, and T is type embedding similar to Segment Embeddings in <ref type="bibr">BERT (Devlin et al., 2019)</ref>. The T contains two types of embedding, one is for caption and the other is for video frames. Next, we calculate similarity score with two linear projection layers plus an activation upon the first token output of the last layer? i [0, :]. Concretely, the similarity function</p><formula xml:id="formula_5">s(v i , t j ) = FC ReLU FC(? i [0, :]) ,</formula><p>where FC is the linear projection, and the ReLU means ReLU activation function (Agarap, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Strategy</head><p>Loss Function Given a batch of B (video, text) or (video clip, text) pairs, the model needs to generate and optimize B ? B similarities. We use a symmetric cross entropy loss over these similarity scores to train the model's parameters,</p><formula xml:id="formula_6">L v2t = ? 1 B B i log exp(s(v i , t i )) B j=1 exp(s(v i , t j ) ,<label>(4)</label></formula><formula xml:id="formula_7">L t2v = ? 1 B B i log exp(s(v i , t i )) B j=1 exp(s(v j , t i ) ,<label>(5)</label></formula><formula xml:id="formula_8">L = L v2t + L t2v .<label>(6)</label></formula><p>The loss L is the sum of video-to-text loss L v2t and text-to-video loss L t2v .</p><p>Frame Sampling Since our model is trained on pixels directly via taking the frames as input, it is an important strategy to extract frames. An effective sampling strategy is required to consider the balance between the information richness and the computational complexity (especially memory cost). To consider the sequential information in the video (or video clip), we adopt a uniform frame sampling strategy instead of a random sparse sampling strategy used in <ref type="bibr" target="#b15">(Lei et al., 2021)</ref>. The sampling rate is 1 frame per second. Besides, we also investigate different frame lengths and different extraction positions in our experiments.</p><p>Pre-training Although the CLIP is effective for learning the visual concepts of images, it is essential to learn temporal features from video. To further transfer the knowledge to video, our CLIP4Clip model is post-pretrained on Howto100M dataset <ref type="bibr" target="#b22">(Miech et al., 2019)</ref>. Pretraining on the video-text dataset is extremely challenging due to efficiency consideration. We conduct a preliminary exploration and use the 'Food and Entertaining' category, around 380k videos, as the post-pretraining dataset (called Howto100M-380k in the rest of the paper). We adopt the MIL-NCE loss <ref type="bibr" target="#b21">(Miech et al., 2020)</ref> to optimize the CLIP in our parameter-free type. The optimizer is Adam (Kingma and Ba, 2015), with a learning rate 1e-8. The token length is 32, the frame length is 12, and the batch size is 48. The training is processed on 8 NVIDIA Tesla V100 GPUs. We run 5 epochs and it takes about 2 weeks. In this paper, the postpretraining test can be regarded as a preliminary study on this direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first describe the datasets and implementation details before presenting state-of-the-art results on five datasets. We then ablate various settings of our model. Finally, we discuss some aspects of promising direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We validate our model on five datasets: MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. MSR-VTT <ref type="bibr" target="#b36">(Xu et al., 2016)</ref> is a dataset composed of 10,000 videos, each with a length that ranges from 10 to 32 seconds and 200,000 captions. We   <ref type="bibr" target="#b22">(Miech et al., 2019)</ref>, WIT <ref type="bibr">(Radford et al., 2021)</ref>, COCO Captions <ref type="bibr" target="#b6">(Chen et al., 2015)</ref>, and Visual Genome Captions <ref type="bibr" target="#b14">(Krishna et al., 2017b)</ref>. use two types of data splits, 'Training-7K' and 'Training-9K', to compare with baselines. The 'Training-7K' follows the data splits from <ref type="bibr" target="#b22">(Miech et al., 2019)</ref>  LSMDC  is comprised 118,081 videos, each with a length that ranges from two to 30 seconds. The videos were extracted from 202 movies. The validation set contains 7,408 videos, and the test set 1,000 videos from movies independent from the training and validation splits. ActivityNet <ref type="bibr" target="#b13">(Krishna et al., 2017a)</ref> consists of 20,000 YouTube videos. We follow the setting from <ref type="bibr" target="#b41">(Zhang et al., 2018;</ref><ref type="bibr">Gabeur et al., 2020)</ref> to concatenate all the descriptions of a video to form a paragraph and evaluate the model with videoparagraph retrieval on the 'val1' split.</p><p>DiDeMo <ref type="bibr" target="#b8">(Hendricks et al., 2017)</ref> contains 10,000 videos annotated with 40,000 sentences. We evaluate video-paragraph retrieval following <ref type="bibr" target="#b18">(Liu et al., 2019;</ref><ref type="bibr" target="#b15">Lei et al., 2021;</ref><ref type="bibr">Bain et al., 2021)</ref>, where all sentence descriptions for a video are concatenated into a single query. We use standard retrieval metrics: recall at rank K (R@K, higher is better), median rank (MdR, lower is better), and mean rank (MnR, lower is better) to evaluate the performance of our model. R@K (Recall at K) calculates the percentage of test samples for which the correct result is found in the top-K retrieved points to the query sample. We    <ref type="table">Table 4</ref>: Results of text-to-video retrieval on Activi-tyNet dataset. In the column 'TrainD', A, H, W, C, and G denote training on ActivityNet, HowTo100M <ref type="bibr" target="#b22">(Miech et al., 2019)</ref>, WIT <ref type="bibr">(Radford et al., 2021)</ref>, COCO Captions <ref type="bibr" target="#b6">(Chen et al., 2015)</ref>, and Visual Genome Captions <ref type="bibr" target="#b14">(Krishna et al., 2017b)</ref>. The column 'E2E' with means training from raw video in an end-to-end manner. The baseline methods are a FSE,HSE <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref>, b CE <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, c MMT (Gabeur et al., 2020), d SSB <ref type="bibr" target="#b24">(Patrick et al., 2021)</ref>, e HiT , f ClipBERT <ref type="bibr" target="#b15">(Lei et al., 2021)</ref>, g TT-CE+ <ref type="bibr" target="#b7">(Croitoru et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>TrainD <ref type="formula" target="#formula_3">E2E</ref>   <ref type="table">Table 5</ref>: Results of text-to-video retrieval on DiDeMo dataset. In the column 'TrainD', D, H, W, C, and G denote training on DiDeMo, HowTo100M <ref type="bibr" target="#b22">(Miech et al., 2019)</ref>, WIT <ref type="bibr">(Radford et al., 2021)</ref>, COCO Captions <ref type="bibr" target="#b6">(Chen et al., 2015)</ref>, and Visual Genome Captions <ref type="bibr" target="#b14">(Krishna et al., 2017b)</ref>, CW means CC3M <ref type="bibr" target="#b30">(Sharma et al., 2018)</ref> plus WebVid-2M <ref type="bibr">(Bain et al., 2021)</ref>. The column 'E2E' with means training from raw video in an endto-end manner. ? means that the candidate video is concatenated using ground truth proposals. The baseline methods are a S2VT <ref type="bibr" target="#b34">(Venugopalan et al., 2015)</ref>,  <ref type="figure">Figure 3</ref>: Retrieval results on different batch sizes, frame length, freeze layer, and learning rate. Batch size: freeze layer is 6. Learning rate: batch size is 128, freeze layer is 6, frame length is 12. Freeze layer: Fz-[NO.] means freeze layers below [NO.]-th layer (inclusive), Fz-Linear means only freeze the linear layer at the bottom, No-Fz is training without freeze, batch size is 128, frame length is 12, learning rate is 5e-8. Frame length: batch size is 128, freeze layer is 6, learning rate is 5e-8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Details</head><p>We initial the text encoder and video encoder with CLIP (ViT-B/32) <ref type="bibr">(Radford et al., 2021)</ref> in this paper. The practical question is how to initialize the parameters in the similarity calculator, e.g., parameters in sequential type. Our solution is to reuse similar parameters from the CLIP (ViT-B/32). Concretely, for the position embedding in sequential type and tight type, we initialize them by repeating the position embedding from CLIP's text encoder. Similarly, the transformer encoder is initialized by the corresponding layers' weight of the pretrained CLIP's image encoder. The rest of the parameters, e.g., LSTM and linear projection, are initialized randomly. The temporal dimension t, height dimension h, and width dimension w of 3D linear and 2D linear in Section 3.1 are set to 3, 32, 32, respectively. For 3D linear, we set stride and padding with 1 at the temporal dimension. We initialize the 3D linear following <ref type="bibr" target="#b2">(Arnab et al., 2021)</ref> with a 'central frame initialization' strategy from the pretrained 2D linear of CLIP. Concretely, we use [0, E 2D , 0] from the 2D weight E 2D of CLIP' image encoder. We finetune the model with the Adam optimizer <ref type="bibr" target="#b11">(Kingma and Ba, 2015)</ref>. For the learning rate, we decay it using a cosine schedule <ref type="bibr" target="#b19">(Loshchilov and Hutter, 2017)</ref> following the CLIP <ref type="bibr">(Radford et al., 2021)</ref>. If no otherwise specified, the initial learning rate is 1e-7 for text encoder and video encoder (including linear projection) and 1e-4 for new modules, e.g., LSTM, the caption token length is 32, the frame length is 12, the batch size is 128, and running 5 epochs. The layer of LSTM is 1, and the layer of Transformer Encoder in both sequential type and tight type is 4 in our experiments. All finetune experiments are carried out on 4 NVIDIA Tesla V100 GPUs. Note that the ActivityNet and the DiDeMo are regarded as video-paragraph retrieval, so we set the caption token length and the frame length 64. The experiments on them are carried out on 16 NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to the State of the Art</head><p>We compare all types of similarity calculator based on the pretrained CLIP against the state-of-the-art (SOTA): '-meanP', '-seqLSTM', '-seqTransf', and '-tightTransf' are short for parameter-free type (i.e., mean pooling), sequential type of LSTM, Transformer Encoder, and tight type mentioned in Section 3.3. <ref type="table" target="#tab_2">Table 1</ref>-5 present the text-to-video retrieval results of our model on MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. The baselines of each dataset are listed in the caption of each table for clarification. We achieve the SOTA results on all five datasets by a large margin compared with all baselines. We find that the growth of retrieval performance benefits from the pretrained CLIP via our results and the concurrent CLIP-straight <ref type="bibr">(Portillo-Quintero et al., 2021)</ref>. Besides, the improvement from our end-to-end finetune proves the potential of the image-text pretrain model on video-text retrieval.</p><p>For the MSR-VTT dataset, the model with the parameter-free type (-meanP) achieves the best results for the 'Training-7k' data split, while the model with the sequential type (-seqTransf) outperforms other methods for the 'Training-9k' data split. We think that it is hard to learn extra parameters beyond the pre-trained parameters given a small dataset. With a large dataset, it is capable of learning the extra parameters. For the LSMDC dataset, the model with the sequential type performs better than the other two types. The two sequential types, -seqLSTM and -seqTransf, achieve  <ref type="table">Table 6</ref>: Study on sampling strategy. 'Head', 'Tail' and 'Uniform' are three sample strategies to select frames from a video. Batch size is 128, freeze layer is 0, frame length is 12, and learning rate is 5e-8.</p><p>comparable results. For the MSVD dataset, the performance of the parameter-free type is the best. We notice that the MSVD training data is smaller than the MSR-VTT and MSVD datasets at least 2 times, and consider the reason is that the extra parameters need extra large dataset to keep the advance from pretrained weight. The performance of videoparagraph retrieval on the ActivityNet and DiDeMo further proves the advantage of the parameter-free type when utilizing the pretrained model. Among five datasets, almost all the results of the tight type (-tightTransf) are the worst among all calculators. We think that the tight type is still hard to learn the cross-modality interaction without enough dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyperparameters and Learning Strategy</head><p>We run extensive experiments on studying the hyperparameters and learning strategies to search for the best settings. <ref type="figure">Figure 3</ref> shows the resulting chart.</p><p>With increasing of Batch size in 3a, the performance increases and it achieves comparable result for batch size 128 and 256. In our experiment, we set the batch size to 128. About the study on frame length in 3d, we can see a significant increase between 1 and 6 frames which shows that it is required for video to actually model with a sequence of multiple frames instead of one single frame. We sampled 12 frames for our experiment, which is both efficient and effective. We also study whether we should freeze the parameters of each layer pre-trained by CLIP. From the <ref type="figure">Figure 3c,</ref>    the best learning rate is 1e-7, which can not be too large or too small.A large learning rate hurts the performance. Even more, it can not leverage the advantage of pre-trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Post-pretraining on Video Dataset</head><p>Our model is built on the pre-trained CLIP, which is an image pre-training model. To solve this data type (image v.s. video) variance, we conduct a preliminary exploration on the post-pretraining of our model on Howto100M-380k video dataset, and report the results for both zero-shot and fine-tune. From the table 7, we can see that the performance increases for both zero-shot and fine-tuning setting. The increase of zero-shot is much larger, which shows that post-pretraining with the same data type (video) can learn general knowledge and directly transfer to the task. In addition, the fine-tuning on the post-pretrained model also improves the performance on both LSMDC and MSVD datasets and achieves approximate results for the MSR-VTT dataset. In future work, we will explore the capability of pre-training with an even larger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Sampling strategy</head><p>We conduct three different sampling strategies for video. 'Head' is to sample the first frames at the beginning of the video, 'Tail' is to select the last frames at the end of the video, and 'Uniform' is to sample the whole frames of the video uniformly. The experimental results show that 'Uniform' is relatively a good choice, and 'Head' is comparable. The 'Tail' sampling strategy is unlikely to be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">2D/3D Patch Linear</head><p>We conduct the comparison on 2D and 3D linear mentioned in Section 3.1. <ref type="table" target="#tab_11">Table 8</ref> presents the performance of them. Against our expectation that 3D patch linear can extract temporal information among frames and generate better discriminant features and performance, the 3D linear generate worse results than 2D linear on both MSR-VTT and MSVD. We suppose that the CLIP is trained for 2D linear instead of 3D linear, and the discrepant initialization on 3D linear makes it hard to learn the temporal information. We will pretrain on a large video-text dataset to unleash its potential in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we use the pretrained CLIP as our backbone to solve the video clip retrieval task from frame-level input. We employ parameter-free type, sequential type, and tight type similarity calculator to obtain the final results. The experimental results demonstrate the effectiveness of our model and achieve the SOTA results on MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. Besides, we give serval insights from our empirical studies: 1) image feature can also promote the video-text retrieval, 2) post-pretrain on even outstanding imagetext pretrained CLIP can further improve the performance on video-text retrieval, 3) 3D patch linear projection and sequential type similarity are promising approaches on the retrieval task, and 4) The CLIP used on video-text retrieval is learningrate sensitivity.</p><p>A Video-to-Text Retrieval  <ref type="table" target="#tab_2">Table A1</ref>: Results of video-to-text retrieval on MSR-VTT dataset. 'Training-7K' follows the data splits from <ref type="bibr" target="#b22">(Miech et al., 2019)</ref> and 'Training-9K' follows the data splits from <ref type="bibr">(Gabeur et al., 2020)</ref>. They have the same test set but different training set. The column 'TrainD' shows the datasets used for pre-training and training, where M, H, W denote MSR-VTT, HowTo100M <ref type="bibr" target="#b22">(Miech et al., 2019)</ref> and WIT <ref type="bibr">(Radford et al., 2021)</ref>.   <ref type="table" target="#tab_5">Table A2</ref>: Results of video-to-text retrieval on MSVD dataset. In the column 'TrainD', M and W denote training on MSVD and WIT <ref type="bibr">(Radford et al., 2021)</ref>, and CW means CC3M <ref type="bibr" target="#b30">(Sharma et al., 2018)</ref> plus WebVid-2M <ref type="bibr">(Bain et al., 2021)</ref>. The column 'E2E' with means training from raw video in an end-to-end manner. The baseline method is a CLIP-straight (Portillo-Quintero et al., 2021), b TT-CE+ <ref type="bibr" target="#b7">(Croitoru et al., 2021)</ref>.  <ref type="table" target="#tab_6">Table A3</ref>: Results of video-to-text retrieval on LSMDC dataset. In the column 'TrainD', L and W denote training on LSMDC and WIT <ref type="bibr">(Radford et al., 2021)</ref>, MD used in (Dzabraev et al., 2021) denotes a combined multidomain dataset containing MSR-VTT, LSMDC, HowTo100M, etc., and CW means CC3M <ref type="bibr" target="#b30">(Sharma et al., 2018)</ref> plus WebVid-2M <ref type="bibr">(Bain et al., 2021)</ref>. The column 'E2E' with means training from raw video in an end-to-end manner. The baseline methods are a JSFusion <ref type="bibr" target="#b38">(Yu et al., 2018)</ref>, b CLIP-straight <ref type="bibr">(Portillo-Quintero et al., 2021)</ref>, c TT-CE+ <ref type="bibr" target="#b7">(Croitoru et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>TrainD <ref type="formula" target="#formula_3">E2E</ref>   <ref type="table">Table A4</ref>: Results of video-to-text retrieval on Activ-ityNet dataset. In the column 'TrainD', A, H, and W denote training on ActivityNet, HowTo100M <ref type="bibr" target="#b22">(Miech et al., 2019)</ref>, and WIT <ref type="bibr">(Radford et al., 2021)</ref>. The column 'E2E' with means training from raw video in an end-to-end manner. The baseline methods are a FSE,HSE <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref>, b CE <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, c MMT (Gabeur et al., 2020), d SSB <ref type="bibr" target="#b24">(Patrick et al., 2021)</ref>, e TT-CE+ <ref type="bibr" target="#b7">(Croitoru et al., 2021)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different views of Linear Projection of Flattened Patches in Video Encoder. Dotted boxes with color are kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Besides, MD used inMDMMT(Dzabraev et al., 2021) denotes a combined multidomain dataset including MSR-VTT, LSMDC, HowTo100M, etc., and CW means CC3M<ref type="bibr" target="#b30">(Sharma et al., 2018)</ref> plus WebVid-2M(Bain et al., 2021). The column 'E2E' with means training from raw video in an end-to-end manner. The baseline methods are a C+LSTM+SA<ref type="bibr" target="#b31">(Torabi et al., 2016)</ref>, b VSE (Kiros et al., 2014), c SNUVL (Yu et al., 2016b), d Kaufman et al. Kaufman et al. (2017), e CT-SAN (Yu et al., 2017), f JSFusion (Yu et al., 2018), g HowTo100M (Miech et al., 2019), h ActBERT (Zhu and Yang, 2020), i NoiseE (Amrani et al., 2021), j UniVL (Luo et al., 2020), k HERO (Li et al., 2020), l ClipBERT (Lei et al., 2021), m MIL-NCE (Miech et al., 2020), n CLIP-straight (Portillo-Quintero et al., 2021), o CE (Liu et al., 2019), p MMT (Gabeur et al., 2020), q AVLnet (Rouditchenko et al., 2020), r SSB (Patrick et al., 2021), s MDMMT (Dzabraev et al., 2021), t Frozen (Bain et al., 2021), u HiT, v TT-CE+<ref type="bibr" target="#b7">(Croitoru et al., 2021)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>b FSE (Zhang et al., 2018), c CE (Liu et al., 2019), d ClipBERT (Lei et al., 2021), e Frozen (Bain et al., 2021), f TT-CE+ (Croitoru et al., 2021).median of the ground-truth results in the ranking. Similarly, Mean Rank calculates the mean rank of all correct results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The column 'E2E' with means training from raw video in an end-to-end manner. The baseline methods are a CLIP-straight (Portillo-Quintero et al., 2021), b HowTo100M (Miech et al., 2019), c CE (Liu et al., 2019), d MMT (Gabeur et al., 2020), e AVLnet (Rouditchenko et al., 2020), f SSB<ref type="bibr" target="#b24">(Patrick et al., 2021)</ref>, g HiT, h TT-CE+<ref type="bibr" target="#b7">(Croitoru et al., 2021)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Results of text-to-video retrieval on MSR-VTT dataset. Table (a) and (c) present the results on different splits of the dataset. 'Training-7K' follows the data splits from (Miech et al., 2019) and 'Training-9K' follows the data splits from (Gabeur et al., 2020). They have the same test set but different training sets. For each table, the column 'TrainD' shows the datasets used for pre-training and training, where M, H, W, C, G denote MSR- VTT, HowTo100M</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Results of text-to-video retrieval on MSVD</cell></row><row><cell cols="4">dataset. In the column 'TrainD', M, H, and W de-</cell></row><row><cell cols="4">note training on MSVD, HowTo100M (Miech et al.,</cell></row><row><cell cols="4">2019), and WIT (Radford et al., 2021), and CW means</cell></row><row><cell cols="4">CC3M (Sharma et al., 2018) plus WebVid-2M (Bain</cell></row><row><cell cols="4">et al., 2021). The column 'E2E' with means training</cell></row><row><cell cols="4">from raw video in an end-to-end manner. The base-</cell></row><row><cell cols="4">line methods are Methods TrainD E2E R@1? R@5? R@10? MdR? MnR?</cell></row><row><cell>CT-SAN a</cell><cell>L</cell><cell>5.1 16.3 25.2 46.0</cell><cell>-</cell></row><row><cell>JSFusion b</cell><cell>L</cell><cell>9.1 21.2 34.1 36.0</cell><cell>-</cell></row><row><cell>CE c</cell><cell>L</cell><cell cols="2">11.2 26.9 34.8 25.3 96.8</cell></row><row><cell>MMT d</cell><cell>H+L</cell><cell cols="2">12.9 29.9 40.1 19.3 75.0</cell></row><row><cell>NoiseE e</cell><cell>H+L</cell><cell>6.4 19.8 28.4 39.0</cell><cell>-</cell></row><row><cell>CLIP-straight f</cell><cell>L</cell><cell>11.3 22.7 29.2 56.5</cell><cell>-</cell></row><row><cell>MDMMT g</cell><cell>MD+L</cell><cell cols="2">18.8 38.5 47.9 12.3 58.0</cell></row><row><cell>Frozen h</cell><cell>CW+L</cell><cell>15.0 30.8 39.8 20.0</cell><cell>-</cell></row><row><cell>HiT i</cell><cell>H+L</cell><cell>14.0 31.2 41.6 18.5</cell><cell>-</cell></row><row><cell>TT-CE+ j</cell><cell>L</cell><cell>17.2 36.5 46.3 13.7</cell><cell>-</cell></row><row><cell>(Ours)-meanP</cell><cell>W+L</cell><cell cols="2">20.7 38.9 47.2 13.0 65.3</cell></row><row><cell cols="2">(Ours)-seqLSTM W+L</cell><cell cols="2">21.6 41.8 49.8 11.0 58.0</cell></row><row><cell cols="2">(Ours)-seqTransf W+L</cell><cell cols="2">22.6 41.0 49.1 11.0 61.0</cell></row><row><cell cols="2">(Ours)-tightTransf W+L</cell><cell cols="2">18.9 37.8 46.7 13.0 61.6</cell></row></table><note>a Multi Cues (Mithun et al., 2018),b CE (Liu et al., 2019), c SSB (Patrick et al., 2021),d NoiseE (Amrani et al., 2021), e CLIP-straight (Portillo- Quintero et al., 2021), f Frozen (Bain et al., 2021), g TT- CE+ (Croitoru et al., 2021).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Results of text-to-video retrieval on LSMDC</cell></row><row><cell>dataset. In the column 'TrainD', L, H, and W de-</cell></row><row><cell>note training on LSMDC, HowTo100M (Miech et al.,</cell></row><row><cell>2019), and WIT (Radford et al., 2021), MD used in (Dz-</cell></row><row><cell>abraev et al., 2021) denotes a combined multidomain</cell></row><row><cell>dataset containing MSR-VTT, LSMDC, HowTo100M,</cell></row><row><cell>etc., and CW means CC3M (Sharma et al., 2018) plus</cell></row><row><cell>WebVid-2M (Bain et al., 2021). The column 'E2E'</cell></row><row><cell>with means training from raw video in an end-to-end</cell></row><row><cell>manner. The baseline methods are a CT-SAN (Yu et al.,</cell></row><row><cell>2017), b JSFusion (Yu et al., 2018), c CE (Liu et al.,</cell></row><row><cell>2019), d MMT (Gabeur et al., 2020), e NoiseE (Amrani</cell></row><row><cell>et al., 2021), f CLIP-straight (Portillo-Quintero et al.,</cell></row><row><cell>2021), g MDMMT (Dzabraev et al., 2021), h Frozen</cell></row><row><cell>(Bain et al., 2021), i HiT (Liu et al., 2021), j TT-CE+</cell></row><row><cell>(Croitoru et al., 2021).</cell></row><row><cell>report results for R@1, R@5, and R@10 (or R@50</cell></row><row><cell>for the ActivityNet). Median Rank calculates the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="6">: Test on post-pretraining (P-PT) on (Ours)-</cell></row><row><cell cols="6">meanP model with HowTo100M-380k dataset. ZS:</cell></row><row><cell cols="3">zero-shot, FT: fine-tuning.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">2D/3D R@1? R@5? R@10? MdR? MnR?</cell></row><row><cell></cell><cell></cell><cell cols="2">MSR-VTT</cell><cell></cell><cell></cell></row><row><cell>2D</cell><cell>43.1</cell><cell>70.4</cell><cell>80.8</cell><cell>2</cell><cell>16.2</cell></row><row><cell>3D</cell><cell>41.6</cell><cell>69.9</cell><cell>79.5</cell><cell>2</cell><cell>17.3</cell></row><row><cell></cell><cell></cell><cell cols="2">MSVD</cell><cell></cell><cell></cell></row><row><cell>2D</cell><cell>46.2</cell><cell>76.1</cell><cell>84.6</cell><cell>2</cell><cell>10.0</cell></row><row><cell>3D</cell><cell>44.0</cell><cell>73.6</cell><cell>83.0</cell><cell>2</cell><cell>11.3</cell></row><row><cell></cell><cell></cell><cell cols="2">LSMDC</cell><cell></cell><cell></cell></row><row><cell>2D</cell><cell>20.7</cell><cell>38.9</cell><cell>47.2</cell><cell>13</cell><cell>65.3</cell></row><row><cell>3D</cell><cell>20.8</cell><cell>40.6</cell><cell>49.3</cell><cell>11</cell><cell>61.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Test 2D and 3D patch linear on (Ours)-meanP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A1 -</head><label>A1</label><figDesc>A3 present the video-to-text retrieval results of CLIP4Clip on MSR-VTT, LSMDC, MSVD, ActivityNet, and DiDeMo.</figDesc><table><row><cell>Methods</cell><cell cols="4">TrainD E2E R@1? R@5? R@10? MdR? MnR?</cell></row><row><cell></cell><cell></cell><cell>Zero-shot</cell><cell></cell><cell></cell></row><row><cell>CLIP-straight a</cell><cell>W</cell><cell>27.2 51.7 62.6</cell><cell>5</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Training-7K</cell><cell></cell><cell></cell></row><row><cell>HowTo100M b</cell><cell>H+M</cell><cell>16.8 41.7 55.1</cell><cell>8</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Training-9K</cell><cell></cell><cell></cell></row><row><cell>CE c</cell><cell>M</cell><cell>20.6 50.3 64.0</cell><cell>5.3</cell><cell>-</cell></row><row><cell>MMT d</cell><cell>H+M</cell><cell>27.0 57.5 69.7</cell><cell>3.7</cell><cell>-</cell></row><row><cell>AVLnet e</cell><cell>H+M</cell><cell>28.5 54.6 65.2</cell><cell>4</cell><cell>-</cell></row><row><cell>SSB f</cell><cell>H+M</cell><cell>28.5 58.6 71.6</cell><cell>3</cell><cell>-</cell></row><row><cell>HiT g</cell><cell>H+M</cell><cell>32.1 62.7 74.1</cell><cell>3</cell><cell>-</cell></row><row><cell>TT-CE+ h</cell><cell>M</cell><cell>32.1 62.7 75.0</cell><cell>3</cell><cell>-</cell></row><row><cell>(Ours)-meanP</cell><cell>W+M</cell><cell>43.1 70.5 81.2</cell><cell>2</cell><cell>12.4</cell></row><row><cell cols="2">(Ours)-seqLSTM W+M</cell><cell>42.8 71.0 80.4</cell><cell>2</cell><cell>12.3</cell></row><row><cell cols="2">(Ours)-seqTransf W+M</cell><cell>42.7 70.9 80.6</cell><cell>2</cell><cell>11.6</cell></row><row><cell cols="2">(Ours)-tightTransf W+M</cell><cell>40.6 69.5 79.5</cell><cell>2</cell><cell>13.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table A5</ref><p>: Results of video-to-text retrieval on DiDeMo dataset. In the column 'TrainD', D and W denote training on DiDeMo and WIT <ref type="bibr">(Radford et al., 2021)</ref>. The column 'E2E' with means training from raw video in an end-to-end manner. ? means that the candidate video is concatenated using ground truth proposals. The baseline methods are a S2VT <ref type="bibr" target="#b34">(Venugopalan et al., 2015)</ref>, b FSE <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref>, c CE <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, d ClipBERT <ref type="bibr" target="#b15">(Lei et al., 2021)</ref>, e Frozen <ref type="bibr">(Bain et al., 2021)</ref>, d TT-CE+ <ref type="bibr" target="#b7">(Croitoru et al., 2021)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agarap</forename><surname>Abien Fred</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<title level="m">Vivit: A video vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">G?l Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Teachtext: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08271</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal tessellation: A unified approach for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learningvia sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">HERO: Hierarchical encoder for video+ language omnirepresentation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengsheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15049</idno>
		<title level="m">Hit: Hierarchical transformer with momentum contrast for video-text retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arsha Nagrani, and Andrew Zisserman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
	</analytic>
	<monogr>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UniVL: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for crossmodal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s Andr?s</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Carlos</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12443</idno>
		<title level="m">2021. A straightforward framework for video retrieval using clip</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9358</biblScope>
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angie</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Avlnet: Learning audio-visual language representations from instructional videos</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Msrvtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="487" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video captioning and retrieval models with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CVLSMDC2016 Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3261" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crossmodal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
