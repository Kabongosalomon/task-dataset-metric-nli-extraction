<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic 3D Human Shape and Pose Estimation from Multiple Unconstrained Images in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ignas Budvytis University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic 3D Human Shape and Pose Estimation from Multiple Unconstrained Images in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of 3D human body shape and pose estimation from RGB images. Recent progress in this field has focused on single images, video or multi-view images as inputs. In contrast, we propose a new task: shape and pose estimation from a group of multiple images of a human subject, without constraints on subject pose, camera viewpoint or background conditions between images in the group. Our solution to this task predicts distributions over SMPL body shape and pose parameters conditioned on the input images in the group. We probabilistically combine predicted body shape distributions from each image to obtain a final multi-image shape prediction. We show that the additional body shape information present in multi-image input groups improves 3D human shape estimation metrics compared to single-image inputs on the SSP-3D dataset and a private dataset of tape-measured humans. In addition, predicting distributions over 3D bodies allows us to quantify pose prediction uncertainty, which is useful when faced with challenging input images with significant occlusion. Our method demonstrates meaningful pose uncertainty on the 3DPW dataset and is competitive with the state-of-the-art in terms of pose estimation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human body shape and pose estimation from RGB images is a challenging problem with potential applications in augmented and virtual reality, healthcare and fitness technology and virtual retail. Recent solutions have focused on three types of inputs: i) single images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>, ii) video <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b15">16]</ref> with temporal constraints on pose, camera viewpoint and background conditions and iii) multi-view images <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46]</ref> with a fixed subject pose captured from multiple viewpoints. In contrast, we aim to estimate 3D body shape and pose from a group of images of the same human subject without any constraints on the subject's pose, camera viewpoint or back- <ref type="figure">Figure 1</ref>: Example shape and pose predictions from a group of input images. Probabilistic shape combination results in a more accurate body shape estimate than both individual single-image predictions (visualised here from SPIN <ref type="bibr" target="#b26">[27]</ref> and STRAPS <ref type="bibr" target="#b44">[45]</ref>) and naively-averaged single-image predictions, as our experiments show in <ref type="bibr">Section 5.</ref> ground conditions between the images, as illustrated in Figure 1. This task is motivated by the intuition that multiple images of the same subject should contain additional visual information about their body shape compared to a single image, regardless of whether the subject's pose or surrounding environment change between images. A suitable shape and pose estimator should leverage this information to improve shape prediction accuracy over single-image methods.</p><p>We present a probabilistic body shape and pose estimation method from a group of unconstrained images of the same subject. Inference occurs in three stages (see <ref type="figure">Figure  2</ref>). First, we predict a proxy representation from each input image in the group, consisting of the subject's silhouette and 2D joint location heatmaps, using off-the-shelf segmentation and 2D keypoint detection CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53]</ref>. Then, each proxy representation is passed through a 3D distribution prediction network that outputs a probability distribution over SMPL <ref type="bibr" target="#b32">[33]</ref> body shape and pose parameters conditioned on the input representation. Lastly, body shape distributions from each input image are prob-abilistically combined to procure a final shape prediction. This yields a better estimate of the subject's body shape than current single-image body shape and pose estimators <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b44">45]</ref>, which may be inaccurate or inconsistent, as shown in <ref type="figure">Figure 1</ref>.</p><p>Moreover, most single-image body model parameter regressors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b9">10]</ref> do not consider the uncertainty associated with each pose parameter estimate. If certain body parts are occluded or out-of-frame in the input image, the estimator can only guess about the pose parameters corresponding to these body parts. Such situations further motivate our approach of predicting a distribution over body pose, since the variance of the distribution quantifies the uncertainty associated with each pose parameter prediction, as shown in <ref type="figure" target="#fig_0">Figures 3 and 4</ref>.</p><p>Training body model parameter regressors to accurately predict body shape is challenging due to the lack of suitable training datasets of in-the-wild images paired with accurate and diverse body shape labels. Collecting such data is practically difficult, particularly for our proposed task of shape estimation from a group of unconstrained images. Recent works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref> propose using synthetic input-label pairs to overcome the lack of suitable training datasets. We adopt the same synthetic training approach as STRAPS <ref type="bibr" target="#b44">[45]</ref> to train our 3D distribution prediction network, but extend the data augmentations used to bridge the gap between synthetic and real inputs. In particular, our synthetic training data better models occluded and out-of-frame body parts in silhouettes and joints such that the domain gap to real occluded data is smaller. This allows our method to estimate pose prediction uncertainty and also results in improved single-input pose prediction metrics on challenging evaluation datasets, such as 3DPW <ref type="bibr" target="#b51">[52]</ref>.</p><p>In summary, our main contributions are as follows:</p><p>? We propose a novel task: predicting body shape from a group of images of the same human subject, without imposing any constraints on subject pose, camera viewpoint or backgrounds between the images.</p><p>? We present a solution to the proposed task which predicts a distribution over 3D human body shape and pose parameters conditioned on the input images in the group. Body shape distributions from each image are probabilistically combined to yield a final body shape estimate which leverages multi-image shape information, resulting in a more accurate body shape estimate compared to single-input methods.</p><p>? To the best of our knowledge, our method is the first to output uncertainties alongside associated SMPL <ref type="bibr" target="#b32">[33]</ref> shape and pose parameter predictions, which are shown to be useful when input images contain occluded or out-of-frame body parts.</p><p>? We extend the synthetic training framework introduced by <ref type="bibr" target="#b44">[45]</ref> to better model occlusion and missing body parts, allowing our synthetically-trained distribution prediction neural network to yield better 3D shape and pose metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section discusses recent approaches to 3D human shape and pose estimation from single images, multi-view images and video. Single-image shape and pose estimation methods can be classified into 2 categories: optimisation-based and learning-based. Optimisation-based methods fit a parametric 3D body model <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref> to 2D observations (e.g. 2D joints <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>, surface landmarks <ref type="bibr" target="#b29">[30]</ref>, silhouettes <ref type="bibr" target="#b29">[30]</ref> or part segmentations <ref type="bibr" target="#b54">[55]</ref>) via optimisation. They can accurately estimate 3D poses without requiring expensive 3Dlabelled datasets, however they are susceptible to poor initialisation and tend to be slow at test-time.</p><p>Learning-based methods can be further classified as model-free or model-based. Model-free approaches directly predict a 3D body representation from an image, such as a voxel occupancy grid <ref type="bibr" target="#b49">[50]</ref>, vertex mesh <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56]</ref> or implicit surface representation <ref type="bibr" target="#b43">[44]</ref>. Model-based approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> predict the parameters of a 3D body model <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref>, which provides a useful prior over human body shape. Several methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b53">54]</ref> overcome the scarcity of in-the-wild 3D-labelled training data by incorporating weak supervision with datasets of labelled 2D keypoints. <ref type="bibr" target="#b26">[27]</ref> extends this further by integrating optimisation into the model training loop to lift 2D keypoint labels to self-improving 3D pose and shape labels. Such approaches predict impressive 3D poses but fail to predict accurate body shapes (particularly for non-average humans) since 2D keypoints do not densely inform shape. Recently, <ref type="bibr" target="#b44">[45]</ref> used random synthetic training data to overcome data scarcity and demonstrated improved shape predictions. Video shape and pose estimation methods may be classified similarly to their single-image counterparts. Optimisation-based video methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> extend singleimage optimisation over time, while learning-based video methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b39">40]</ref> modify single-image predictors to take sequences of frames as inputs. However, video inputs allow these methods to enforce consistent body shapes and smooth motions across frames, e.g. using motion discriminators <ref type="bibr" target="#b10">[11]</ref>, optical flow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>, or texture consistency <ref type="bibr" target="#b39">[40]</ref>. Learning-based video methods also overcome 3D data scarcity by incorporating weak 2D supervision <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20]</ref>, or with self-supervision enforcing visual consistency between frames <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b0">1]</ref>. Nevertheless, current methods are unable to predict accurate body shapes, particularly for nonaverage humans. Multi-view shape and pose estimation. <ref type="bibr" target="#b31">[32]</ref> extends the <ref type="figure">Figure 2</ref>: Overview of our shape and pose distribution prediction network. Each image I n in the input group is converted into a silhouette and joint proxy representation X n , which is passed through a distribution prediction network to obtain multivariate distributions over SMPL <ref type="bibr" target="#b32">[33]</ref> shape and pose parameters, ? and ? n , conditioned on the input. Shape distributions from each individual input are probabilistically combined to form a multi-input shape distribution. The encoder and distribution MLP are trained using randomly-generated synthetic data <ref type="bibr" target="#b44">[45]</ref>. The per-vertex uncertainty visualisations (in cm) are obtained by sampling SMPL parameters from the predicted distributions, computing the SMPL vertex mesh for each sample and determining the average Euclidean distance from the mean for each vertex. Black dots indicate left hands.</p><p>iterative regressor of <ref type="bibr" target="#b18">[19]</ref> to predict body model parameters from multiple input images of the same subject in a fixed pose, captured from varying camera angles. They use synthetic data to overcome data scarcity, resulting in more accurate body shape estimates, particularly under clothing. <ref type="bibr" target="#b45">[46]</ref> uses synthetic data to learn to predict body model parameters from A-pose silhouettes.</p><p>Contrary to the above approaches, our method estimates shape and pose from a group of images without any temporal or absolute constraints on the subject's pose, camera viewpoint or background between images. Shape and pose distribution estimation. While substantial progress has been made in predicting probability distributions using neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>, prediction of distributions over 3D human shape and pose is still under-explored. Recently, <ref type="bibr" target="#b35">[36]</ref> used lixel-based 1D heatmaps to quantify uncertainty in predicted 3D human mesh vertex locations. <ref type="bibr" target="#b4">[5]</ref> predicted a categorical distribution over multiple SMPL hypotheses given an ambiguous image. In contrast, we aim to explicitly output separable uncertainties per predicted pose and shape parameter, since the shape uncertainties, specifically, are used for shape prediction from multiple unconstrained images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section provides a brief overview of the SMPL parametric human body model <ref type="bibr" target="#b32">[33]</ref>, presents our three-stage method for probabilistic body shape and pose estimation from a group of unconstrained images of the same human subject (illustrated in <ref type="figure">Figure 2</ref>) and finally discusses the synthetic training framework and loss functions used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SMPL model</head><p>SMPL <ref type="bibr" target="#b32">[33]</ref> provides a differentiable function M(?, ?, ?) which takes pose parameters ?, global rotation ? and identity-dependent body shape parameters ? as inputs and outputs a vertex mesh V ? R 6890?3 . ? ? R 69 and ? ? R 3 represent axis-angle rotation vectors for 23 SMPL body joints and the root joint respectively. ? ? R 10 represents coefficients of a PCA body shape basis. Given the vertex mesh V, 3D joint locations may be obtained using a linear regressor, J 3D = J V where J ? R L?6890 is a regression matrix for L joints of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proxy representation computation</head><p>Given a group of N RGB input images {I n } N n=1 of the same subject, we first compute proxy representations {X n } N n=1 . DensePose <ref type="bibr" target="#b12">[13]</ref> is used to obtain body part segmentations, which are converted into silhouettes. Keypoint-RCNN from Detectron2 <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b52">53]</ref> is used to obtain 2D joint locations, which are converted into Gaussian heatmaps, and associated confidence scores. Heatmaps corresponding to joint detections with confidence scores less than a threshold t = 0.025 are set to 0. Thresholding is essential for modelling uncertainty in 3D pose predictions as it typically removes invisible 2D joints from the input representations. The predicted silhouette and joint heatmaps from each image are stacked along the channel dimension to form each proxy representation X n ? R H?W ?(L+1) .</p><p>The use of silhouette and joint heatmap representations as inputs instead of RGB images is inspired by <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref> and allows us to train our distribution prediction network using a simple synthetic training framework (see Section 3.5), overcoming the lack of shape diversity in current datasets. We follow <ref type="bibr" target="#b44">[45]</ref> and use simple silhouettes and 2D joint heatmaps as our proxy representation, instead of more complex alternatives (e.g. part segmentations or IUV maps), since this leads to a smaller synthetic-to-real domain gap which is more readily bridged by data augmentation <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Body shape and pose distribution prediction</head><p>We aim to estimate probability distributions <ref type="bibr" target="#b36">[37]</ref> over SMPL pose parameters {? n } N n=1 (which are free to change between inputs) and the subject's identity-dependent shape ?, both conditional upon {X n } N n=1 . We assume simple multivariate Gaussian distributions</p><formula xml:id="formula_0">p(? n |X n ) = N (? n ; ? ? (X n ), ? ? (X n )) p(?|X n ) = N (?; ? ? (X n ), ? ? (X n )).<label>(1)</label></formula><p>Covariance matrices are constrained to be diagonal, i.e.</p><formula xml:id="formula_1">? ? (X n ) = diag(? 2 ? (X n )) and ? ? (X n ) = diag(? 2 ? (X n )). Formally, ? 2</formula><p>? (X n ) and ? 2 ? (X n ) represent estimates of the heteroscedastic aleatoric uncertainty <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> in the SMPL parameters explaining the input observations X n , which arises particularly due to occlusion.</p><p>We also predict deterministic estimates of the global rotations {? n } N n=1 and weak-perspective camera parameters {c n } N n=1 , where c n = [s n , t x n , t y n ] representing scale and xy translation respectively. Global rotation and camera parameters are unconstrained across images.</p><p>Hence, we require a function mapping each input proxy representation X n to the desired set of outputs Y(X n ) = {? ? , ? ? , ? 2 ? , ? 2 ? , ? n , c n }. This function is represented using a deep neural network f with learnable weights W:</p><formula xml:id="formula_2">Y = f (X n ; W).</formula><p>(2)</p><p>f consists of a convolutional encoder for feature extraction followed by a simple multi-layer perceptron that predicts the set of outputs Y, as illustrated in <ref type="figure">Figure 2</ref>. The network training procedure is detailed in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Body shape combination</head><p>We combine the conditional body shape distributions output by f given each individual input, p(?|X n ) for n = 1, ..., N , into a final distribution p(?|{X n } N n=1 ) that aggregates shape information across the input group. Formally,</p><formula xml:id="formula_3">p(?|{X n } N n=1 ) ? N n=1 p(?|X n )<label>(3)</label></formula><p>which follows from the conditional independence assumption (X i ? ? X j )|? for i, j ? {1, ..., N } and i = j. This is justifiable since we do not impose any relationship between the subject's pose or camera viewpoint across inputs -only the body shape is fixed. Further details are in the supp. material. Since the product of Gaussians is an un-normalised</p><formula xml:id="formula_4">Gaussian, p(?|{X n } N n=1 ) ? N (?; m, S) where S = N n=1 ? ?1 ? (X n ) ?1 m = S N n=1 ? ?1 ? (X n )? ? (X n ) .<label>(4)</label></formula><p>The combined mean m is a final point estimate of the subject's body shape from the input group {X n } N n=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Network training</head><p>Loss functions. While inference occurs using a group of inputs, the model is trained with a dataset of individual inputlabel pairs,</p><formula xml:id="formula_5">{X k , {? k , ? k , ? k }} K k=1 , with K i.i.d training samples.</formula><p>The negative log-likelihood is given by where ? ?i , ? ?i , ? 2 ?i and ? 2 ?i represent elements of the predicted SMPL mean and variance vectors ? ? (X k , W), ? ? (X k , W), ? 2 ? (X k , W) and ? 2 ? (X k , W), which are output by the neural network f with weights W. We maximise the log-likelihood of the model w.r.t W by minimising the loss function L NLL . Intuitively, each squared error term in Eqn. 5 is adaptively-weighted by the corresponding predicted variance <ref type="bibr" target="#b21">[22]</ref>. This mitigates the ill-posed nature of a naive squared error loss on SMPL parameters when training inputs are occluded, since the network learns to predict large variances for the parameters corresponding to invisible body parts, thus down-weighting the respective squared error terms. Furthermore, adaptive weighting means that our network is able to train stably without additional "global" losses on 3D vertices or 3D joints, as is common in most other recent methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b56">57]</ref>.</p><formula xml:id="formula_6">L NLL = ? K k=1 log p(? k |X k ) + log p(? k |X k ) ? K k=1 69 i=1 log(2?? 2 ?i ) + (? ki ? ? ?i ) 2 ? 2 ?i + 10 j=1 log(2?? 2 ?j ) + (? kj ? ? ?j ) 2 ? 2 ?j<label>(5)</label></formula><p>Our network also predicts deterministic estimates of the global rotations ? k . Predictions? k are supervised by</p><formula xml:id="formula_7">L glob = K k=1 R(? k ) ? R(? k ) 2 F .<label>(6)</label></formula><p>Figure 5: Example predictions on groups of images from SSP-3D <ref type="bibr" target="#b44">[45]</ref>. Single-image predictions from SPIN <ref type="bibr" target="#b26">[27]</ref>, STRAPS <ref type="bibr" target="#b44">[45]</ref> and our method are visualised, along with probabilistically combined body shapes from our method.</p><formula xml:id="formula_8">R(?) ? SO(3)</formula><p>is the rotation matrix corresponding to ?. Finally, our network estimates weak-perspective camera parameters c k = [s k , t x k , t y k ], which are supervised using a 2D joint reprojection loss. Target 2D joint coordinates J k ? R L?2 are computed from {? k , ? k , ? k } during synthetic data generation (see Section 3.5). Predicted 2D joint coordinates are obtained by first differentiably sampling?</p><formula xml:id="formula_9">i k ? p(? k |X k ) and? i k ? p(? k |X k )</formula><p>using the reparameterisation trick <ref type="bibr" target="#b23">[24]</ref>, for i = 1, ..., B samples. These are converted into 2D joint samples using the SMPL model and weak-perspective projection</p><formula xml:id="formula_10">J i k = s k ?(J M(? i k ,? i k ,? k )) + [t x k , t y k ]<label>(7)</label></formula><p>where ?() represents an orthographic projection. A squared error reprojection loss is imposed between the predicted 2D joint samples and the target 2D joints</p><formula xml:id="formula_11">L 2D = K k=1 B i=1 ? k (J k ?? i k ) 2 2<label>(8)</label></formula><p>where ? k ? {0, 1} L denote the visibilities of the target joints (1 if visible, 0 otherwise), which are computed during synthetic data generation. We apply a reprojection loss on samples from the predicted body shape and pose distributions, instead of only on the means of the distributions, because any 3D body sampled from the distributions must match the 2D joint locations present in the input X k . Our overall loss function is given by L = L NLL + ? glob L glob + ? 2D L 2D where ? glob , ? 2D are weighting terms. Synthetic data generation. To train our distribution prediction network using the proposed losses, we require training data consisting of input proxy representations paired with target SMPL shape, pose and global rotation parameters, {X k , {? k , ? k , ? k }} K k=1 . We employ a similar synthetic training data generation process as <ref type="bibr" target="#b44">[45]</ref>. In short, within each iteration of the training loop, ? k and ? k are sampled from any suitable dataset with SMPL pose parameters <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52]</ref>, while ? k are randomly sampled from a suitably high-variance Gaussian distribution to ensure body shape diversity. These are converted into synthetic silhouette and joint heatmap representations X k and target 2D joint coordinates J k using the SMPL model, a renderer <ref type="bibr" target="#b20">[21]</ref> and randomly sampled perspective camera parameters. The clean synthetic inputs are corrupted to model the failure modes of the off-the-shelf detection and segmentation CNNs used at test-time, such as noisy keypoint locations, and occluded silhouettes. Examples are given in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>We improve the data generation process of <ref type="bibr" target="#b44">[45]</ref> in two ways. First, we significantly increase the severity of the occlusion and cropping augmentations to match the occlusions seen in challenging test datasets such as 3DPW (illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>, first row). Second, we explicitly compute a joint visibility vector ? k (1 if visible, 0 otherwise) for each J k and set the heatmaps corresponding to invisible joints to 0, unlike <ref type="bibr" target="#b44">[45]</ref>. This is necessary for our distribution prediction network to learn to be uncertain about pose parameters corresponding to invisible body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>Network Architecture. We use a ResNet-18 <ref type="bibr" target="#b14">[15]</ref> encoder followed by a multi-layer perceptron (MLP) to predict SMPL parameter distributions. The MLP is comprised of one hidden layer with 512 neurons and ELU <ref type="bibr" target="#b7">[8]</ref> activation and one output layer with 164 neurons, which predicts the set of outputs Y. Predicted variances are forced to be positive using an exponential activation function. Training dataset. Synthetic training data is generated by sampling SMPL pose parameters from the training sets of UP-3D <ref type="bibr" target="#b29">[30]</ref>, 3DPW <ref type="bibr" target="#b51">[52]</ref>, and Human3.6M <ref type="bibr" target="#b16">[17]</ref> ( <ref type="bibr">Subjects 1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8)</ref>. Training details. We use Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 1e-4 and a batch size of 120, and train for 100 epochs, which takes 1.5 days on a 2080Ti GPU. Inference runs at 4fps, 90% of which is silhouette and joint prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">53]</ref>. Evaluation datasets. We use the test set of 3DPW to evaluate pose prediction accuracy. It consists of 35515 RGB im-   <ref type="table">Table 2</ref>: PVE-T-SC (mm) results on SSP-3D <ref type="bibr" target="#b44">[45]</ref> comparing i) probabilistic shape combination (PC) versus simple averaging (Mean) and ii) effect of increasing input group size from 1 to 5. ages of 7 subjects with paired ground-truth SMPL parameters. We report mean per joint position error after scale correction (MPJPE-SC) and after Procrustes analysis (MPJPE-PA). We use the scale correction technique introduced in <ref type="bibr" target="#b44">[45]</ref> to combat the ambiguity between subject scale and distance from camera. MPJPE-SC measures 3D joint error up to scale and MPJPE-PA measures 3D joint error up to scale and global rotation. We also report scale-corrected per-vertex Euclidean error in a T-pose (PVE-T-SC) on the SSP-3D dataset <ref type="bibr" target="#b44">[45]</ref> to evaluate identity-dependent shape prediction accuracy. SSP-3D consists of 311 images of 62 subjects and pseudo-ground-truth SMPL parameters. In addition, we evaluate body shape prediction accuracy on a private dataset consisting of 6 subjects (4 male, 2 female) with 4 RGB images of each and ground-truth body measurements obtained using a tape measure or full 3D body scanning technology. The subjects' body poses, clothing, surrounding environments and camera viewpoints vary between images. Example images are in the supplementary material.</p><p>Finally, we create a synthetic dataset for our experimen-  <ref type="table">Table 3</ref>: Comparison with the state-of-the-art in terms of PVE-T-SC (mm) on SSP-3D <ref type="bibr" target="#b44">[45]</ref>. Our method surpasses the state-of-the-art when using single-image inputs. Probabilistic shape combination (PC) outperforms simple averaging of predictions from other methods when using groups of up to 5 images, as well as video predictions from <ref type="bibr" target="#b25">[26]</ref>.</p><p>Method 3DPW MPJPE-SC MPJPE-PA HMR <ref type="bibr" target="#b18">[19]</ref> 102.8 71.5 GraphCMR <ref type="bibr" target="#b27">[28]</ref> 102.0 70.   <ref type="figure" target="#fig_0">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we present our ablation studies, where we investigate uncertainty predictions, compare probabilis- tic shape combination with simple averaging and explore the effects of varying input group sizes and global rotation variation within groups. We also compare our method to other approaches in terms of shape and pose accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation studies</head><p>Pose and shape uncertainty. SMPL pose and shape prediction uncertainties are represented by the predicted variances ? 2 ? and ? 2 ? . Rows 2 and 5 of <ref type="figure" target="#fig_0">Figure 3</ref> illustrate pose prediction uncertainty on clean and corrupted synthetic inputs. Heavily-occluded inputs (e.g. the corrupted input in column 4), result in large predicted variances for the pose parameters corresponding to the occluded joints, while predicted variances are smaller for visible joints. This behaviour is replicated on real inputs from the 3DPW dataset, as shown by <ref type="figure" target="#fig_1">Figure 4</ref> where the network is uncertain about the 3D locations of occluded and out-of-frame body parts. <ref type="figure" target="#fig_0">Figure 3</ref> also showcases shape parameter prediction uncertainty on synthetic data, in rows 3 and 6. The network is more uncertain about body shape when the subject is heavily occluded and/or in a challenging pose, seen by comparing the sitting pose in column 2 with the standing poses in columns 1 and 3. This behaviour is also seen on real inputs in <ref type="figure">Figure 5</ref>, e.g. by comparing the crouching pose in row 1 with the standing pose in row 2. Body shape combination method. We compare probabilistic body shape combination (from Section 3.4) with a simpler heuristic combination, where we obtain combined body shape estimates from a group of inputs {X n } N n=1 by simply averaging (i.e. taking the mean of) the shape distribution means {? ? (X n )} N n=1 . Rows 3-4 in <ref type="table" target="#tab_1">Table 1</ref> show that better shape estimation metrics are attained using probabilistic combination versus simple averaging on synthetic input quadruplets (examples in <ref type="figure" target="#fig_0">Figure 3</ref>). This is replicated on groups of real inputs from SSP-3D, as shown in <ref type="table">Table 2</ref>, row 5 versus row 6. Since probabilistic combination may be interpreted as uncertainty-weighted averaging (Eqn. 4), these experiments suggest that inaccurate mean body shape predictions are generally accompanied by large prediction uncertainty, and subsequently down-weighted during probabilistic combination. This may explain why probabilistic  combination actually gives better shape metrics when evaluating on corrupted synthetic inputs compared to clean inputs in <ref type="table" target="#tab_1">Table 1</ref>, since heavy input corruption results in inaccurate but highly-uncertain shape estimates. Input group size. <ref type="table" target="#tab_1">Table 1</ref> also investigates the effect of the input group size, evaluated on our synthetic dataset, by comparing single inputs (i.e. group size of 1) with body shape combination applied to pairs and quadruplets (i.e. input group sizes of 2 and 4). Body shape metrics are significantly improved when using pairs compared to single images, suggesting that probabilistic combination is successfully using shape information from the multiple inputs. A smaller improvement is seen when using quadruplets versus pairs. <ref type="table">Table 2</ref> shows that increasing the input group size on real data (from SSP-3D) also results in a consistent but diminishing improvement in shape prediction metrics. Global rotation variation. To investigate whether variation in global rotation of the subject between inputs is correlated with shape prediction accuracy, we split each group of 4 inputs from our synthetic dataset into groups of 2 in two ways: (front, left) + (back, right) and (front, back) + (left, right). We expect the latter split to be less informative for shape prediction as the pairs contain more redundant visual shape information. This is corroborated by the experiments labelled "Pairs" in <ref type="table" target="#tab_1">Table 1</ref>, where the former split yields better shape metrics, particularly for corrupted inputs where the amount of visual shape information in each individual input is lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the state-of-the-art</head><p>Shape prediction. Our method surpasses the state-of-theart on SSP-3D in the single-input case (group size of 1), as shown in <ref type="table">Table 3</ref> and <ref type="figure">Figure 5</ref>. The improvement over the similar synthetic training method in STRAPS <ref type="bibr" target="#b44">[45]</ref> is primarily due to our improved training data augmentations. When using groups of multiple images as inputs (with group size = 5), probabilistic combination outperforms simple averaging of predictions from all other methods. The dis-tribution of errors per SSP-3D sample, shown in <ref type="figure" target="#fig_2">Figure  6</ref>, suggests that probabilistic combination particularly improves errors for challenging samples, where the uncertainty weighting is more meaningful. <ref type="table" target="#tab_6">Table 5</ref> compares tape measurement errors computed using shape predictions from our method and competitors on a private dataset. Probabilistic combination results in the lowest measurement errors for large body parts, such as the chest, stomach, waist and hips. However, it is less accurate on smaller body parts (e.g. biceps and forearms), which are significantly obscured by clothing. In general, our method may over-estimate measurements for subjects with loose clothing, since silhouette-based inputs don't distinguish between clothing and the human body. Pose prediction. While we focus on body shape estimation, <ref type="table" target="#tab_4">Table 4</ref> shows that our method is competitive with the stateof-the-art on 3DPW, and surpasses other methods that do not require training images paired with 3D labels. <ref type="figure" target="#fig_2">Figure 6</ref> demonstrates that our method does well on low-to-medium difficulty samples, but struggles with the most challenging ones, which typically exhibit very severe occlusions leading to degraded proxy representations. Nevertheless, we outperform STRAPS <ref type="bibr" target="#b44">[45]</ref> on these challenging samples due to improved data augmentation and the adaptive loss weighting discussed in Section 3.5, which results in a more stable improvement of pose metrics during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed the novel task of human body shape estimation from a group of images of the same subject, without imposing any constraints on body pose, camera viewpoint or backgrounds. Our solution predicts multivariate Gaussian distributions over SMPL <ref type="bibr" target="#b32">[33]</ref> body shape and pose parameters conditioned on the input images. We probabilistically combine predicted body shape distributions from each image to obtain a final multi-image shape prediction, and experimentally show that probabilistically combined estimates are more accurate than both individual single-image predictions, as well as naively-averaged single-image predictions, when evaluated on SSP-3D and a private dataset of tape-measured humans. Furthermore, predicting distributions over SMPL parameters allows us to estimate the heteroscedastic aleatoric uncertainty associated with pose predictions, which is useful when faced with input images containing occluded or out-of-frame body parts. Future work can consider using (i) clothed and textured synthetic data to further close the synthetic-to-real domain gap, and (ii) more expressive predicted distributions than the simple Gaussians proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Probabilistic 3D Human Shape and Pose Estimation from Multiple Unconstrained Images in the Wild</head><p>This document provides additional material supplementing the main manuscript. Section 7 contains details regarding training data generation, evaluation protocols and probabilistic shape combination. Section 8 discusses qualitative results on the SSP-3D <ref type="bibr" target="#b44">[45]</ref> and 3DPW <ref type="bibr" target="#b51">[52]</ref> datasets, as well as providing examples from our private evaluation dataset of tape-measured humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Implementation Details</head><p>Training. <ref type="table" target="#tab_8">Table 6</ref> lists the data augmentation methods used to bridge the synthetic-to-real domain gap during synthetic training data generation, along with associated hyperparameter values. <ref type="table" target="#tab_9">Table 7</ref> lists additional hyperparameter values not given in the main manuscript. Uncertainty Visualisation. <ref type="figure">Figures 8 and 9</ref> in this supplementary material, as well as several figures in the main manuscript, visualise per-vertex prediction uncertainties. These are computed from the predicted SMPL <ref type="bibr" target="#b32">[33]</ref> pose and shape parameter distributions by i) sampling 100 SMPL parameter vectors from the predicted distributions, ii) passing each of these samples through the SMPL function to get the corresponding vertex meshes, iii) computing the mean location of each vertex over all the samples and iv) determining the average Euclidean distance from the mean for each vertex over all the samples, which is ultimately visualised in the scatter plots as a measure of uncertainty. SSP-3D Evaluation Groups. SSP-3D <ref type="bibr" target="#b44">[45]</ref> contains 311 images of 62 subjects, where subjects can have a different number of associated images. To evaluate our multi-input shape prediction method, the images for each subject were split into groups of maximum size equal to N , where N ranged from 1 to 5. For example, if a subject has 6 associated images and N = 4, the images would be split into two groups with 4 and 2 images respectively. Splitting/group assignment was done after random shuffling of the images to prevent sequential images with similar poses/global orientations from always being in the same group. Tape measurement normalisation by height. There is an inherent ambiguity between 3D subject size/scale and distance from camera. Since the true camera location relative to the 3D subject (and the focal length) is unknown, it is not possible to estimate the absolute size of the subject given an image. This is accounted for by the PVE-T-SC <ref type="bibr" target="#b44">[45]</ref> metric used to evaluate shape prediction accuracy on synthetic data and SSP-3D in the main manuscript. For our evaluation dataset of tape-measured humans (see <ref type="figure" target="#fig_3">Figure 7)</ref>, scale correction is done using the subject's height.   set to 0) and measuring the y-axis distance between the top of the head and bottom of the feet. The ratio between the subject's true height and this predicted height is then used to scale all the predicted body measurements derived from the neutral-pose mesh. Probabilistic shape combination. The main manuscript presents our method to probabilistically combine individual body shape distributions, p(?|X n ) for n = 1, ..., N , into a final distribution p(?|{X n } N n=1 ). The full derivation is given below:</p><formula xml:id="formula_12">p(?|{X n } N n=1 ) ? p({X n } N n=1 |?)p(?) = N n=1 p(X n |?) p(?) ? N n=1 p(?|X n ) p(?) N ?1 ? N n=1 p(?|X n ).<label>(9)</label></formula><p>The first and third lines use Bayes' theorem. The second line follows from the conditional independence assumption (X i ? ? X j )|? for i, j ? {1, ..., N } and i = j. This assumption is reasonable because only the subject's body shape is fixed across inputs -hence, the inputs are independent given the body shape parameters. The final line follows from assuming an (improper) uniform prior over the shape parameters p(?) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental Results</head><p>Evaluation using ground-truth vs predicted inputs. The synthetic training data augmentations listed in <ref type="table" target="#tab_8">Table 6</ref> and the main manuscript are used to increase the robustness of our distribution prediction neural network to noisy and occluded test data, as demonstrated in <ref type="figure">Figure 9</ref>. However, the synthetic-to-real domain gap still persists, as evidenced by <ref type="table">Table 8</ref>, which compares body shape and pose prediction metrics when using ground-truth, synthetic groundtruth and predicted input proxy representations. A significant improvement in both body shape and pose metrics is observed when using synthetic inputs, instead of predicted inputs. This is mostly because predicted input silhouettes and 2D joints can be very inaccurate in cases with challenging poses, significant occlusion or occluding humans, such that the synthetic training data augmentations are not sufficient. Moreover, synthetic SMPL human silhouettes are not clothed, while silhouette predictors generally classify clothing pixels as part of the human body. This is particularly detrimental to body shape prediction metrics when subjects are dressed in loose clothing, as can be seen in <ref type="figure">Figure 9</ref> (left side, rows 3 and 4), where our method tends to over-estimate the subject's body proportions. SSP-3D qualitative results. <ref type="figure">Figure 8</ref> shows qualitative results, particularly focusing on shape prediction, on groups of input images from SSP-3D <ref type="bibr" target="#b44">[45]</ref> corresponding to subjects with a wide range of body shapes. The first column in each cell shows the input images in the group. The second column shows the predicted SMPL <ref type="bibr" target="#b32">[33]</ref> body (rendered) for each individual image, obtained by passing the mean of predicted SMPL parameter distributions through the SMPL function. The third and fourth columns visualise the 3D per-vertex uncertainty (or variance) in the individual SMPL shape distribution predictions (in a neutral pose i.e. pose parameters/joint rotations set to 0). The fifth column shows the combined body shape prediction, which are obtained by probabilistically combining the individual shape distributions.</p><p>In particular, note the relationship between challenging poses with significant self-occlusion (e.g. right side, row 4 of <ref type="figure">Figure 8</ref>) and uncertainty in the predicted SMPL shape distribution. 3DPW qualitative results. <ref type="figure">Figure 9</ref> shows qualitative results, particularly focusing on pose prediction, using singleimage inputs from 3DPW <ref type="bibr" target="#b51">[52]</ref>. The first column on each side shows the input images. The second column shows the corresponding silhouette and joint heatmap proxy representation predictions. The third column shows the predicted SMPL <ref type="bibr" target="#b32">[33]</ref> body (rendered) for each image, obtained by passing the mean of predicted SMPL parameter distributions through the SMPL function. The fourth column visualises the 3D per-vertex uncertainty (or variance) in the SMPL pose and shape distribution predictions (per-vertex uncertainties are mostly due to pose variance rather than shape).</p><p>Specifically, note the large uncertainties of vertices belonging to body parts that are invisible in the image (and corresponding proxy presentations), either due to occluding objects, self-occlusion or being out-of-frame. Furthermore, large uncertainties also occur when the proxy representation prediction is highly-degraded, such as left side, row 7 of <ref type="figure">Figure 9</ref>. <ref type="figure">Figure 8</ref>: Qualitative results on groups of input images from SSP-3D <ref type="bibr" target="#b44">[45]</ref>. Black dots indicate left hands. Within each cell: 1st column is group of input images, 2nd column is predicted SMPL body, 3rd and 4th columns show 3D per-vertex uncertainty in the SMPL shape distribution prediction, 5th column is the probabilistically-combined body shape. Challenging poses lead to large shape prediction uncertainty. 11 <ref type="figure">Figure 9</ref>: Qualitative results using single-image inputs from 3DPW <ref type="bibr" target="#b51">[52]</ref>. Black dots indicate left hands. On each side: 1st column is input image, 2nd column is predicted proxy representation, 3rd column is predicted SMPL body and 4th column is 3D per-vertex uncertainty in the SMPL pose and shape distribution prediction. Vertices of occluded and out-of-frame body parts have higher prediction uncertainties.  <ref type="table">Table 8</ref>: Comparison between ground-truth (GT), synthetic ground-truth and predicted input silhouettes and 2D joints, in terms of MPJPE-SC and MPJPE-PA (both in mm) on 3DPW <ref type="bibr" target="#b51">[52]</ref>, as well as PVE-PA and PVE-T-SC (both in mm) on SSP-3D <ref type="bibr" target="#b44">[45]</ref>. Predicted silhouettes are obtained using DensePose <ref type="bibr" target="#b12">[13]</ref> and predicted 2D joint coordinates and confidences (for thresholding) are obtained using Keypoint-RCNN from Detectron2 <ref type="bibr" target="#b52">[53]</ref>. Synthetic ground-truth inputs are obtained by rendering the SMPL <ref type="bibr" target="#b32">[33]</ref> body mesh labels given by SSP-3D and 3DPW, using ground-truth camera parameters, into silhouette and 2D joint input representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Clean and corrupted versions of an example group of inputs from our synthetic evaluation dataset, and corresponding (single-image) shape and pose distribution predictions. Black dots indicate left hands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Predictions on single images from 3DPW<ref type="bibr" target="#b51">[52]</ref>. 3D locations of invisible parts are uncertain due to large predicted variances for the corresponding pose parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Comparison with other methods using sorted distributions of a) PVE-T-SC per SSP-3D evaluation sample and b) MPJPE-PA per 3DPW evaluation sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Example images from our private dataset of humans with body measurements obtained using a tape measure or 3D body scanners. The subjects' body pose, clothing, surrounding environment and camera viewpoints vary between images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3DPWSSP-3D MPJPE-SC MPJPE-PA PVE-PA PVE-T-SC GT Synthetic Silh. + 2D Joint Heatmaps 64</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">PVE-T-SC (mm) results on synthetic data (see Fig-</cell></row><row><cell cols="3">ure 3) investigating: i) probabilistic shape combination (PC)</cell></row><row><cell cols="3">versus simple averaging (Mean), ii) effect of increasing in-</cell></row><row><cell cols="3">put group size from 1 to 2 (Pairs) to 4 (Quadruplets) and iii)</cell></row><row><cell cols="3">effect of global rotation variation within pairs of inputs.</cell></row><row><cell>Max. input</cell><cell>Method</cell><cell>SSP-3D</cell></row><row><cell>group size</cell><cell></cell><cell>PVE-T-SC</cell></row><row><cell>1</cell><cell>Ours</cell><cell>15.2</cell></row><row><cell>2</cell><cell>Ours + PC</cell><cell>13.9</cell></row><row><cell>3</cell><cell>Ours + PC</cell><cell>13.6</cell></row><row><cell>4</cell><cell>Ours + PC</cell><cell>13.5</cell></row><row><cell>5</cell><cell>Ours + Mean</cell><cell>13.6</cell></row><row><cell></cell><cell>Ours + PC</cell><cell>13.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>tal analysis. It consists of 1000 synthetic humans with ran-</cell></row><row><cell>domly sampled SMPL body shapes, each posed in 4 differ-</cell></row><row><cell>ent SMPL poses sampled from Human3.6M [17] subjects 9</cell></row><row><cell>and 11. Global orientations are set such that the camera is</cell></row><row><cell>facing the human's front, back, left or right. A group of 4</cell></row><row><cell>clean synthetic inputs and 4 corrupted inputs is rendered for</cell></row><row><cell>each human, where the corruptions used are the same as the</cell></row><row><cell>data augmentations applied during training. Examples are</cell></row><row><cell>given in</cell></row></table><note>Comparison with the state-of-the-art in terms of MPJPE-SC and MPJPE-PA (both mm) on 3DPW [52]. Methods in the top half require training images paired with 3D ground-truth, methods in the bottom half do not.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art in terms of</figDesc><table><row><cell>tape measurement RMSE (cm) on our private shape evalu-</cell></row><row><cell>ation dataset. Errors are reported for the chest (C), stomach</cell></row><row><cell>(S), hips (H), biceps (B), forearms (F) and thighs (T).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>List of synthetic training data augmentations and their associated hyperparameter values. Body part occlusion uses the 24 DensePose<ref type="bibr" target="#b12">[13]</ref> parts. Joint L/R swap is done for shoulders, elbows, wrists, hips, knees, ankles.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Shape parameter sampling mean</cell><cell>0</cell></row><row><cell>Shape parameter sampling var.</cell><cell>2.25</cell></row><row><cell>Cam. translation sampling mean</cell><cell>(0, -0.2, 2.5) m</cell></row><row><cell>Cam. translation sampling var.</cell><cell>(0.05, 0.05, 0.25) m</cell></row><row><cell>Cam. focal length</cell><cell>300.0</cell></row><row><cell>Proxy representation dimensions</cell><cell>256 ? 256 pixels</cell></row><row><cell>2D joint confidence threshold</cell><cell>0.025</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>List of hyperparameter values not provided in the main manuscript.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Dr. Yu Chen (Metail) and Dr. David Bruner (SizeStream) for providing body shape evaluation data and 3D body scans. This research was sponsored by SizeStream UK Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optical flow-based 3D human motion estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Kassubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Conference on Pattern Recognition</title>
		<meeting>the German Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SCAPE: Shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Anurag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">*</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of probability distributions by neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilczek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuRIPS)</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D multibodies: Fitting sets of plausible 3D models to ambiguous image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Erhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuRIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Aleatoric or epistemic? Does it matter? Structural Safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ove</forename><surname>Dalager Ditlevsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">MoVi: A Large Multipurpose Motion and Video Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimia</forename><surname>Mahdaviani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Kording</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">James</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Blohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014-07" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural 3D mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuRIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. PointRend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Appearance consensus driven self-supervised human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unite the People: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3D human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shape-aware human pose and shape reconstruction using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>1-248:16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH Asia</title>
		<meeting>ACM SIGGRAPH Asia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic orientation estimation with matrix fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mohlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rald</forename><surname>Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuRIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating the mean and variance of the target probability distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Neural Networks (ICNN)</title>
		<meeting>IEEE International Conference on Neural Networks (ICNN)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep directional statistics: Pose estimation with uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Prokudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning in an uncertain world: Representing ambiguity through multiple hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Baust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Synthetic training for accurate 3d human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards accurate 3D human body reconstruction from silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision, ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Vince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC</title>
		<meeting>the British Machine Vision Conference (BMVC</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Danet: Decompose-and-aggregate network for 3D human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
