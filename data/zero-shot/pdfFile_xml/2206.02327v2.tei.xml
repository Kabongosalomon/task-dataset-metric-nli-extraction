<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JigsawHSI: a network for Hyperspectral Image classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Moraga</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sebnem Duzgun</surname></persName>
						</author>
						<title level="a" type="main">JigsawHSI: a network for Hyperspectral Image classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Hyperspectral Image Classification</term>
					<term>Convolu- tional Neural Network</term>
					<term>Remote Sensing</term>
					<term>JigsawHSI</term>
					<term>Pavia Uni- versity</term>
					<term>Indian Pines</term>
					<term>Salinas</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes Jigsaw, a convolutional neural network (CNN) used in geosciences and based on Inception [1] but tailored for geoscientific analyses. Introduces JigsawHSI (based on Jigsaw) and uses it on the land-use land-cover (LULC) classification problem with the Indian Pines, Pavia University and Salinas hyperspectral image data sets. The network is compared against HybridSN [2], a spectral-spatial 3D-CNN followed by 2D-CNN that achieves state-of-the-art results on the datasets. This short article proves that JigsawHSI is able to meet or exceed HybridSN's performance in all three cases. Additionally, the use of jigsaw in geosciences is highlighted, while the code and toolkit are made available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H YPERSPECTRAL image (HSI) classification is a classical task for remote sensing and machine learning practitioners, it consists in classifying the pixels from a hyperspectral image (HSI) into classes based on a given ground truth. For this tasks, several freely available data sets have been released, including Salinas Valley, Pavia University <ref type="bibr" target="#b2">[3]</ref>, and Indian Pines <ref type="bibr" target="#b3">[4]</ref>.</p><p>Image classification, or semantic segmentation, is a machine learning task that has been tackled extensively in the literature, its application to HSI is an interesting problem because it is a difficult task for machines that humans can do. This allows for manual labeling of the images to create a ground truth, which makes the availability of data sets for supervised learning possible. For machines, the task is complex because of the high dimensionality of HSI, and the spatial and spectral characteristics of the classification problem. This makes na?ve approaches to the problem to be subpar, not achieving good results.</p><p>To solve this problem, many approaches exist in the literature, with hundreds of publications in 2022 alone. The first problem is high dimensionality, so different pre-processing algorithms have been proposed, for example dimensionality reduction by using decomposition functions like Principal Component Analysis (PCA), Factor Analysis (FA), Single Value Decomposition (SVD), and others <ref type="bibr" target="#b4">[5]</ref>. Pre-processing <ref type="bibr">(</ref> steps also include the application of wavelet functions, Fourier transforms <ref type="bibr" target="#b5">[6]</ref> and others. The task of image classification and semantic segmentation has been studied in machine learning and image analysis for decades. There are two main ways to approach the problem, by using pixel-based methods or area-based methods. In pixelbased methods, each pixel is classified independently of the surrounding pixels, this has the drawback of missing all the spatial and only analyzing the spectral information. A competing approach has been the use of both spatial and spatial information by analyzing cubes of data as a whole.</p><p>Since the seventies, neural networks have shown that they are specially well suited for this type of problem. Since the neocognitron <ref type="bibr" target="#b6">[7]</ref>, various shallow and deep neural and convolutional neural networks (CNN) have been used, with great success, to classify images with different number of bands (or channels) of data. In general, these (for example AlexNet <ref type="bibr" target="#b7">[8]</ref>, ResNet <ref type="bibr" target="#b8">[9]</ref> and GoogLeNet <ref type="bibr" target="#b9">[10]</ref>) have been limited to the red, green and blue (RGB) bands of visible light, but there is no reason to limit the analysis to just those three bands. GoogLeNet's inception module, for example, analyzes data across bands independent of depth ( <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Inception architecture</head><p>Among the CNN approaches applied to multispectral and hyperspectral data are 2-dimensional CNN and, lately, 3dimensional neural networks. There have also been attempts to use hybrid approaches, as in the case of the HybridSN <ref type="bibr" target="#b1">[2]</ref> which uses 3-dimensional kernels to both reduce dimensionality of the input and capture spectral information, and 2dimensional kernels to extract spatial information (e.g. image textures).</p><p>Another network proposed for general multispectral and multi-variate data is the Jigsaw network, first used to evaluate the environmental impact of an iron mine dam collapse in Brazil <ref type="bibr" target="#b10">[11]</ref>. This network has also been used successfully in identifying geothermal potential of two sites in Nevada (Brady and Desert Peak) <ref type="bibr" target="#b11">[12]</ref>. This network is capable of identifying patterns both across the channel and spatial dimensions of an image.</p><p>This short article examines the use of a variant of the Jigsaw network, the JigsawHSI network, that is highly configurable in its hyper-parameters and depth to deal with a variety of inputs. The questions to answer are whether the network can tackle HSI classification problems, whether the results are comparable to more complex hybrid or 3-dimensional approaches, what dimensionality reduction functions can be used to achieve competitive results and what hyper-parameters are relevant to achieve such results. The network, configuration routines and sample configurations are made available to the public. <ref type="figure">Figure 1</ref>) is a network that was proposed by <ref type="bibr" target="#b9">[10]</ref>, further refined in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b0">[1]</ref>, and had 2 main objectives: a) to obtain a convolutional structure for a translation invariant vision network; and b) to introduce sparsity by using the Network-in-network idea in <ref type="bibr" target="#b13">[14]</ref>, that is, using multilayer perceptron (MLP) implemented as 1x1 convolutions to reduce the dimension of the inputs or outputs and thus reduce the number of operations of the network while introducing a network that captures non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE JIGSAW NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inception (depicted in</head><p>The network has been used successfully for image classification in implementations like GoogLeNet <ref type="bibr" target="#b9">[10]</ref>. For HSI classification, several attempts have been made to use inception as a module, including recently the 3D AI <ref type="bibr" target="#b14">[15]</ref> who uses two 3D inception modules, one for spatial classification and one for spectral in each layer.</p><p>We proposed Jigsaw in 2019 <ref type="bibr" target="#b15">[16]</ref> to address Land-use landcover (LULC) problems with multispectral images as inputs. The core of the network is an Inception-like module. The original jigsaw network is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The Jigsaw network's design had the same initial 2 considerations as Inception, a) the visual representation that is translation independent (for spatial analysis), and b) introduce sparsity to allow to go deeper if required. An additional 2 considerations for this network are c) to capture the spectral information and d) not to lose resolution. For this purpose, the A and C modules in <ref type="figure" target="#fig_0">Figure 2</ref> were introduced. The A module uses the Network-in-network concept of <ref type="bibr" target="#b13">[14]</ref>: an MLP to capture linear relationship among bands in the HSI, this module can contain one or more (1 ? 1) convolutions in series. The C module acts as a ResNet <ref type="bibr" target="#b8">[9]</ref> -making the initial image available for classification in deeper layers -, also highlighting both spatial and spectral information. This C module can increase the size and complexity of the network substantially -especially for hyperspectral images where the number of bands can be in the hundreds --, so there are two ways to reduce that impact, the first is to crop the image keeping just the center pixel (which is the determinant for the network's classification), and then sending that pixel's spectrum through the dense connections; the second way is to preprocess the input to reduce the number of bands by doing dimensionality reduction with algorithms like principal component analysis (PCA) or factor analysis (FA).</p><p>Finally, module D concatenates the results of the previous layers and feeds two Dense layers finalizing in a Softmax activation function to produce the resulting classes.</p><p>The whole process is depicted in <ref type="figure" target="#fig_1">Figure 3</ref>. The first step is to acquire an HSI, the initial dimensions will be called H (height), W (width), and B (bands), to define a figure of dimensions</p><formula xml:id="formula_0">(H ? W ? B), representing pixels p i,j,k in R 3 . From this image, each (1 ? 1 ? B)</formula><p>slice represents the profile of the spectrum for that pixel, that is, the intensity at each bandwidth represented by each band in B. From this image, tiles of side S are extracted, to form (H ? W ) tiles of dimensions (S ? S ? B). These (S ? S ? B) tiles (the "jigsaw pieces") are processed by the Jigsaw network and the results of training and prediction are mapped back to an (H ? W ? 1) image ("completing the jigsaw puzzle"). To reduce overfitting, L2 kernel regularization is added at each layer, and 40% dropout layers are added after each dense layer. The additional purpose of having module A is to create a single set of filters from the spectral information, to present them to the spatial analysis module. The logic of adding a (7 ? 7) filter to the Inceptionlike module B is to capture more of the spatial information around the center pixel. This, knowing that the pixels in the surroundings are spatially correlated with the center pixel. In its first application, this corresponded to a region of radius 35 meters from the center pixel, that matched our spatial statistics analysis of the image.</p><p>In summary, the originality of Jigsaw lies in: spatial-spectral focus for multispectral and hyperspectral data by adding specific modules that handle the channels or bands with MLPs; use of inception-like module with bigger filters to manage spatially correlated pixels around the center pixel; use of spatial statistics to inform the number and size of the additional filters in module B; and the use of Tikhonov (L2) kernel regularization in convolutional layers, and dropout after dense layers, to limit the overfitting of the network. This last point may reduce overall accuracy in the training set, but increases generalization and thus accuracy on the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE JIGSAWHSI NETWORK</head><p>The JigsawHSI network <ref type="figure" target="#fig_2">(Figure 4</ref>), is an adaptation of two networks based on the original Jigsaw network. The first network was used to determine the surface impact of a tailings dam collapse in Brumadinho, Brazil by classifying multispectral pixels from Sentinel-2 in before and after pictures, identifying the areas affected by the dam's iron ore waste tailings <ref type="bibr" target="#b10">[11]</ref>. The second application was in the Geothermal AI where, by using multi-variate input layers (Mineral Markers, Temperature and Faults) that were the result of machine learning preprocessing, the network classifies pixels in an image as having or not geothermal potential <ref type="bibr" target="#b11">[12]</ref>. In both cases, the inception network <ref type="bibr" target="#b0">[1]</ref> was adapted to better capture the correlation in surface anomalies by increasing the number of internal filters of the inception network and adding a first 1x1 convolution after the input layer to better manage relationships across bands.</p><p>The JigsawHSI's architecture builds upon this idea by both varying the number of internal (n ? n) filters and making the first (1 ? 1) convolutional layer optional, as shown in <ref type="figure" target="#fig_2">Figure  4</ref>.</p><p>The network uses two parallel blocks (modules A and B on the left and module C on the right) that are merged in the second concatenate layer. The left side block uses the Inception-like module to capture spatial and spectral information, increasing in module B the number of (n ? n) filters from the same block in Jigsaw <ref type="figure" target="#fig_0">(Figure 2</ref>), finishing with a concatenate and average pooling layers; while the right in module C introduces a Crop layer that only uses the central pixel of the image, this module then uses a flattening layer to discard spatial information and analyze all the spectral information of each pixel looking for linear and non-linear relationships. The bottom of the network (module D) has two fully connected dense layers and uses Softmax as the activation function, turning the results into classes as in the case of the vanilla version of Jigsaw.</p><p>This network's code and Jupyter Notebook are available in https://github.com/jmoraga-mines/JigsawHSI. The network is highly flexible, and can be configured by editing the config.ini file in the root directory.</p><p>The configuration includes the ability to define an analysis window that will partition the HSI (width, height, bands) in squares of side S, creating cubes for analysis by the network of dimensions (S, S, bands), a decomposition function can also be specified to reduce the dimensionality to c channels. The final input to the network will be of dimensions (S, S, c). Optionally, a layer with (1 ? 1) convolutions can be added to perform filtering in the dimension of the channels (as in <ref type="bibr" target="#b13">[14]</ref>), this starts the analysis in the spectral dimension of the HSI by the neural network, in these cases, the new value of c will be the number of filters defined for this step.</p><p>For training, several hyperparameters can be defined, these include the optimizer, learning rate of the optimizer, batch size, maximum number of epochs and a patience parameter to determine the number of epochs with no improvement required for early stopping of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONFIGURATION AND TRAINING</head><p>The configuration parameters for training are in <ref type="table" target="#tab_1">Table I</ref> for each of the datasets: Indian Pines (IP), Pavia University (PU), and the Salinas scene (SA) . Depending on the dataset, windows of different sizes can be optimal, in this case, IP uses a (27 ? 27) window, while PU and SA use (25 ? 25).</p><p>To simplify the analysis of the network, 4 decomposition functions can be used for dimensionality reduction: Principal Component Analysis (PCA), Functional Analysis (FA), Truncated Single Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF), the number of final components can also be defined. For IP, FA and 9 factors are selected; PU uses SVD and 9 values; while SA uses FA and 12 factors.</p><p>An HSI layer with 512 filters is specified for PU. Therefore, the input's shapes will be <ref type="bibr">(27,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b8">9)</ref> for IP, (25, 25, 512) for PU, and (25, 25, 9) for SA.</p><p>The internal filters in the Inception-like network will go up to (9 ? 9) for IP and PU, and (7 ? 7) for SA.</p><p>The optimizers are also flexible, accepting Stochastic Gradient Descent (SGD), Adam and Adadelta; the learning rate of these optimizers is also configurable. The configuration used is Adadelta(0.1) for IP and PU, and SGD(0.01) for SA.</p><p>Batch sizes are 106 for IP, 120 for PU and 132 for SA. Maximum number of epochs is 500 for all cases. Patience is 20 for IP and SA, while PU requires to wait longer, with a patience of 40.</p><p>The training was performed in a desktop computer running Ubuntu 20.4, with a single NVDIA card, Pyhton 3.8 and the latest version of CUDA.</p><p>For comparison, we used HibridSN (https://github.com/ gokriznastic/HybridSN, published in IEEE GEOSCIENCE AND REMOTE SENSING LETTERS, VOL. 17, NO. 2, FEBRUARY 2020). Both networks were trained using 30% of the samples for training and 70% of the samples for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION AND DISCUSSION</head><p>The convolutional neural network (CNN) is one of the most used and successful networks in computer vision and visual data processing. Traditionally, these networks have been designed with 2-dimensional (2D) CNN filters due to the datasets having relatively shallow three band (RGB) images, and in this avoiding the curse of dimensionality. In the case of HSI, even after dimensionality reduction, the number of bands or channels are significantly larger than three.</p><p>HybridSN is a hybrid spectral-spatial 3-dimensional (3D) CNN followed by a a spatial 2D-CNN <ref type="bibr" target="#b1">[2]</ref>. In theory, this should ease the representation of joint spatial-spectral information by the network. The HybridSN network was tested on all three datasets and results compared to five additional networks, achieving state-of-the-art results.</p><p>In <ref type="table" target="#tab_1">Table II</ref>, we can see that the JigsawHSI obtains results that are equivalent or better than HybridSN. In the case of Indian Pines, JigsawHSI achieves lower scores in overall accuracy (OA), Cohen Kappa (Kappa) and average accuracy (AA), but in the first two cases the difference is near 0.01% and well within the margin of error of HybridSN. For the Pavia University dataset, the JigsawHSI obtains better results than HybridSN, achieving a 100% accuracy for the test set. In the case of Salinas, both networks obtain the maximum accuracy.</p><p>In <ref type="figure">Figure 5</ref> the confusion matrix heatmaps show that in all cases the accuracy was perfect or almost perfect. This is a surprising result at first sight because the use of 3D-CNN for spectral analysis cannot be replicated in full by the JigsawHSI. This implies that either the 3D-CNN is not needed for this case, or the 3D-CNN is not helping the spectral representation of the image. We theorize that by using dimensionality reduction as a first step in both networks, the 3D-CNN is not needed to represent in full the relationships between channels.</p><p>The other anomaly is in the AA score. This comes as a result of JigsawHSI not being able to discriminate well the oats samples in IP. This is most probably caused by the sparsity of the class, where only 14 test samples are provided.</p><p>In general, the JigsawHSI is able to match or improve on the results of the HybridSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICATIONS IN GEOSCIENCE, A GENERALIZED JIGSAW</head><p>The Jigsaw network was first published in <ref type="bibr" target="#b10">[11]</ref>, as explained in the previous section, where it was used to estimate the environmental effects of a tailings dam collapse in Brazil. In this application, two multispectral images were captured before and after the dam's collapse. When training on the "before" image and applying it to the "after" image, the network was able to generalize well achieving an 86% classification accuracy. When training with a small subset of points of the "after" image, the network was able to generalize to the rest of the image with a 98% accuracy. The network performed especially well when discriminating the Mine and Tailings class, with a 98.4% accuracy, being this the most critical class to determine the impact to populated and unpopulated areas <ref type="bibr" target="#b10">[11]</ref>.</p><p>The second application in geosciences was the use of the same concepts in geothermal exploration. In <ref type="bibr" target="#b11">[12]</ref>, we introduced the Geothermal AI ( <ref type="figure">Figure 6</ref>).</p><p>By using the Geothermal AI for exploration we were able to obtain robust results in classification on two sites, the Brady  The network has unique characteristics in that the input was not a multispectral or hyperspectral image but a multi-modal series of bands that were the results of preprocessing with other ML algorithms or geological/geophysical information.</p><p>In this paper, we also introduced a semi-automated labeling process to create the outputs required for supervised training of the Geothermal AI, and described the preprocessing used to create input layers to the AI. The Geothermal AI was able to achieve 92-95% accuracy in the training sets, and 72-76% accuracy when applied to the opposite site, showing promising ability to generalize. The network was able to highlight the areas where both operating geothermal plants were located in each case.</p><p>In this paper, we show a third application in which the network shows promising results to classify HSI images for LULC. And we believe this network can be used in other geoscience classification applications given its ability to discriminate channel information and generalize spatially.</p><p>A generalized network is shown in <ref type="figure" target="#fig_4">Figure 7</ref>. This generalized network is the result of merging JigsawHSI and Geother-mal AI. The Input layer can be multispectral, hyperspectral, multi-modal or the result of the application of dimensionality reduction algorithms. Module A is an optional module that can contain one or more (1 ? 1) 2D or 3D convolutions. In module B, network-in-network can be applied before and/or after the larger (k ? k) convolutions, and these convolutions can be of size (1 ? 1), (3 ? 3), ..., up to (n ? n) based on the spatial analysis of the Input image, this spatial analysis also informs the size of the window (S) used to create the input tiles. In module C, an optional Crop layer captures only the center pixel to reduce the size of that module, while capturing non-linearity in the bands of the input image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this article, the JigsawHSI network was introduced, a modification of the Jigsaw network and Geothermal AI networks, both based on the Inception architecture. The network was tested against the Indian Pines, Pavia University and Salinas Valley datasets, three freely datasets widely used in hyperspectral image classification.</p><p>The JigsawHSI network achieves results that are either equivalent or better than HybridSN under the same conditions, showing that with 2D-CNNs results match the state-of-theart results a 3D-2D-CNN when dimensionality reduction is applied as a first step.</p><p>It has also been shown how the Jigsaw architecture performs well in geosciences when processing images for pixel by pixel classification (in land-use land-cover and geothermal exploration), and we proposed a generalized Jigsaw architecture that merges the design of JigsawHSI and the Geothermal AI.</p><p>Additional research directions include to test of the networks using smaller test samples, eliminating the dimensionality reduction and using spatial cross validation to better assess the generalization capabilities and required hyperparameters of the networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. BIOGRAPHY SECTION</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Jigsaw's original architecture, adapted from<ref type="bibr" target="#b10">[11]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Jigsaw's process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>JigsawHSI architecture. The dotted lined layers are optional</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Confusion matrix heatmaps for (a) Indian Pines, (b) Pavia U., (c) Salinas The Geothermal AI and Desert Peak geothermal sites in Churchill County, NV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>The generalized Jigsaw network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Corresponding author: Jaime Moraga, jmoraga@mines.edu) J. Moraga is with the Department of Mining Engineering, Colorado School of Mines, 1610 Illinois St., Golden, CO 80401, USA H. Sebnem Duzgun is the Fred Banfield Distinguished Endowed Chair and Professor, Mining Engineering, Colorado School of Mines, Golden, CO 80401, USA Document created June 2, 2022</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EXPERIMENTAL</head><label>I</label><figDesc></figDesc><table><row><cell cols="4">PARAMETERS FOR EACH JIGSAWHSI RESULT</cell></row><row><cell></cell><cell></cell><cell>Values</cell><cell></cell></row><row><cell>Parameters</cell><cell>IP</cell><cell>PU</cell><cell>SA</cell></row><row><cell>Window size</cell><cell>27</cell><cell>25</cell><cell>25</cell></row><row><cell>Decomposition</cell><cell>FA</cell><cell>SVD</cell><cell>FA</cell></row><row><cell>Input channels</cell><cell>9</cell><cell>9</cell><cell>12</cell></row><row><cell>Network design:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HSI Filters</cell><cell>None</cell><cell>512</cell><cell>None</cell></row><row><cell>Filter size</cell><cell>9</cell><cell>9</cell><cell>7</cell></row><row><cell>Hyperparameters:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell>Adadelta</cell><cell>Adadelta</cell><cell>SGD</cell></row><row><cell>Learning rate</cell><cell>0.1</cell><cell>0.1</cell><cell>0.01</cell></row><row><cell>Batch size</cell><cell>106</cell><cell>120</cell><cell>132</cell></row><row><cell>Max. Epochs</cell><cell>500</cell><cell>500</cell><cell>500</cell></row><row><cell>Max. Patience</cell><cell>20</cell><cell>40</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II EXPERIMENTAL</head><label>II</label><figDesc>CLASSIFICATION METRICS (IN PERCENTAGES) FOR THE THREE DATASETS. OA: OVERALL ACCURACY, KAPPA: KAPPA FACTOR, AA: AVERAGE ACCURACY</figDesc><table><row><cell>Model</cell><cell></cell><cell>Indian Pines</cell><cell></cell><cell cols="3">Pavia University</cell><cell></cell><cell>Salinas</cell><cell></cell></row><row><cell></cell><cell>OA</cell><cell>Kappa</cell><cell>AA</cell><cell>OA</cell><cell>Kappa</cell><cell>AA</cell><cell>OA</cell><cell>Kappa</cell><cell>AA</cell></row><row><cell>HybridSN</cell><cell cols="3">99.75 ?0.1 99.71 ?0.1 99.63 ?0.2</cell><cell>99.98</cell><cell>99.98</cell><cell>99.97</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>JigsawHSI</cell><cell>99.74</cell><cell>99.70</cell><cell>98.11</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Jaime Moraga is a PhD student at Colorado School of Mines, Golden, CO H. Sebnem Duzgun H. Sebnem Duzgun is the Fred Banfield Distinguished Endowed Chair and Professor, Mining Engineering, Colorado School of Mines, Golden, CO</figDesc><table><row><cell>IX. APPENDIXES</cell></row><row><cell>APPENDIX</cell></row><row><cell>INDIAN PINES RESULTS</cell></row><row><cell>Accuracy by target (in percentages):</cell></row><row><cell>100.0000 : Alfalfa</cell></row><row><cell>99.0000 : Corn-notill</cell></row><row><cell>100.0000 : Corn-mintill</cell></row><row><cell>100.0000 : Corn</cell></row><row><cell>100.0000 : Grass-pasture</cell></row><row><cell>100.0000 : Grass-trees</cell></row><row><cell>100.0000 : Grass-pasture-mowed</cell></row><row><cell>100.0000 : Hay-windrowed</cell></row><row><cell>71.4286 : Oats</cell></row><row><cell>100.0000 : Soybean-notill</cell></row><row><cell>99.8255 : Soybean-mintill</cell></row><row><cell>99.5181 : Soybean-clean</cell></row><row><cell>100.0000 : Wheat</cell></row><row><cell>100.0000 : Woods</cell></row><row><cell>100.0000 : Buildings-Grass-Trees-Drives</cell></row><row><cell>100.0000 : Stone-Steel-Towers</cell></row><row><cell>APPENDIX</cell></row><row><cell>PAVIA UNIVERSITY RESULTS</cell></row><row><cell>Accuracy by target (in percentages):</cell></row><row><cell>100.0000 : Asphalt</cell></row><row><cell>100.0000 : Meadows</cell></row><row><cell>100.0000 : Gravel</cell></row><row><cell>99.9534 : Trees</cell></row><row><cell>100.0000 : Painted metal sheets</cell></row><row><cell>100.0000 : Bare Soil</cell></row><row><cell>100.0000 : Bitumen</cell></row><row><cell>100.0000 : Self-Blocking Bricks</cell></row><row><cell>100.0000 : Shadows</cell></row><row><cell>APPENDIX</cell></row><row><cell>SALINAS VALLEY, CA RESULTS</cell></row><row><cell>Accuracy by target (in percentages):</cell></row><row><cell>100.0000 : Broccoli_green_weeds_1</cell></row><row><cell>100.0000 : Broccoli_green_weeds_2</cell></row><row><cell>100.0000 : Fallow</cell></row><row><cell>100.0000 : Fallow_rough_plow</cell></row><row><cell>99.9467 : Fallow_smooth</cell></row><row><cell>100.0000 : Stubble</cell></row><row><cell>100.0000 : Celery</cell></row><row><cell>100.0000 : Grapes_untrained</cell></row><row><cell>100.0000 : Soil_vineyard_develop</cell></row><row><cell>100.0000 : Corn_senesced_green_weeds</cell></row><row><cell>100.0000 : Lettuce_romaine_4wk</cell></row><row><cell>100.0000 : Lettuce_romaine_5wk</cell></row><row><cell>100.0000 : Lettuce_romaine_6wk</cell></row><row><cell>100.0000 : Lettuce_romaine_7wk</cell></row><row><cell>100.0000 : Vineyard_untrained</cell></row><row><cell>100.0000 : Vineyard_vertical_trellis</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/11231" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017-02" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HybridSN: Exploring 3-D-2-D CNN Feature Hierarchy for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="281" />
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
	<note>conference Name: IEEE Geoscience and Remote Sensing Letters</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A collection of data for urban area characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">72</biblScope>
		</imprint>
		<respStmt>
			<orgName>pavia Universitt dataset</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indian Pine Test Site 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Baumgardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Biehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Landgrebe</surname></persName>
		</author>
		<idno type="DOI">10.4231/R7RX991C</idno>
		<ptr target="https://purr.purdue.edu/publications/1947/1" />
	</analytic>
	<monogr>
		<title level="m">220 Band AVIRIS Hyperspectral Image Data Set</title>
		<imprint>
			<date type="published" when="1992-06-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Lightweight Spectral-Spatial Convolution Module for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local binary patterns and Fourier transform based hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Miclea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Terebes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Symposium on Electronics and Telecommunications (ISETC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/BF00344251</idno>
		<ptr target="http://link.springer.com/10.1007/BF00344251" />
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<ptr target="https://doi.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monitoring The Impacts of a Tailings Dam Failure Using Satellite Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gurkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D H S</forename><surname>Duzgun</surname></persName>
		</author>
		<ptr target="http://ussd.conferencespot.org/2020/pdf/a042/03600531000071" />
	</analytic>
	<monogr>
		<title level="m">United States Society on Dams (USSD)</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>USSD Elevate Conference 2020</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Geothermal Artificial Intelligence for geothermal exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Duzgun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cavur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soydan</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S096014812200581X" />
	</analytic>
	<monogr>
		<title level="j">Renewable Energy</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="134" to="149" />
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), ser. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, Nevada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Network In Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<idno>arXiv:1312.4400 [cs] type: article</idno>
		<ptr target="http://arxiv.org/abs/1312.4400" />
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification Based on 3D Asymmetric Inception Network with Data Fusion Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/2072-4292/14/7/1711" />
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1711</biblScope>
			<date type="published" when="2022-01" />
			<publisher>Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jigsaw: A Land use Land cover classifier for multispectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gurkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D H S</forename><surname>Duzgun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Colorado School of Mines</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>unpublished, not peer reviewed</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
