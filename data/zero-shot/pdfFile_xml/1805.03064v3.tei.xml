<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Palmero</surname></persName>
							<email>crpalmec7@alumnes.ub.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Mathematics and Informatics</orgName>
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Center Campus UAB</orgName>
								<address>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Selva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Mathematics and Informatics</orgName>
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Ali</forename><surname>Bagheri</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. Electrical and Computer Eng</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Dept. Engineering</orgName>
								<orgName type="institution">University of Larestan</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadali</forename><forename type="middle">Bagheri@ucalgary</forename><surname>Ca</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
							<email>sergio@maia.ub.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. Mathematics and Informatics</orgName>
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Center Campus UAB</orgName>
								<address>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>PALMERO ET AL.: MULTI-MODAL RECURRENT CNN FOR 3D GAZE ESTIMATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gaze behavior is an important non-verbal cue in social signal processing and humancomputer interaction. In this paper, we tackle the problem of person-and head poseindependent 3D gaze estimation from remote cameras, using a multi-modal recurrent convolutional neural network (CNN). We propose to combine face, eyes region, and face landmarks as individual streams in a CNN to estimate gaze in still images. Then, we exploit the dynamic nature of gaze by feeding the learned features of all the frames in a sequence to a many-to-one recurrent module that predicts the 3D gaze vector of the last frame. Our multi-modal static solution is evaluated on a wide range of head poses and gaze directions, achieving a significant improvement of 14.6% over the state of the art on EYEDIAP dataset, further improved by 4% when the temporal modality is included.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Eyes and their movements are considered an important cue in non-verbal behavior analysis, being involved in many cognitive processes and reflecting our internal state <ref type="bibr" target="#b16">[17]</ref>. More specifically, eye gaze behavior, as an indicator of human visual attention, has been widely studied to assess communication skills <ref type="bibr" target="#b29">[28]</ref> and to identify possible behavioral disorders <ref type="bibr" target="#b8">[9]</ref>. Therefore, gaze estimation has become an established line of research in computer vision, being a key feature in human-computer interaction (HCI) and usability research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Recent gaze estimation research has focused on facilitating its use in general everyday applications under real-world conditions, using off-the-shelf remote RGB cameras and removing the need of personal calibration <ref type="bibr" target="#b27">[26]</ref>. In this setting, appearance-based methods, which learn a mapping from images to gaze directions, are the preferred choice <ref type="bibr" target="#b25">[25]</ref>. However, they need large amounts of training data to be able to generalize well to in-the-wild situations, which are characterized by significant variability in head poses, face appearances and lighting conditions. In recent years, CNNs have been reported to outperform classical methods. However, most existing approaches have only been tested in restricted HCI tasks, c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.  <ref type="bibr" target="#b43">[42]</ref> Krafka et al. <ref type="bibr" target="#b15">[16]</ref> Zhang et al. <ref type="bibr" target="#b1">(2)</ref>  <ref type="bibr" target="#b44">[43]</ref> Deng and Zhu <ref type="bibr" target="#b3">[4]</ref> Ours <ref type="table">Table 1</ref>: Characteristics of recent related work on person-and head pose-independent appearance-based gaze estimation methods using CNNs.</p><p>where users look at the screen or mobile phone, showing a low head pose variability. It is yet unclear how these methods would perform in a wider range of head poses.</p><p>On a different note, until very recently, the majority of methods only used static eye region appearance as input. State-of-the-art approaches have demonstrated that using the face along with a higher resolution image of the eyes <ref type="bibr" target="#b15">[16]</ref>, or even just the face itself <ref type="bibr" target="#b44">[43]</ref>, increases performance. Indeed, the whole-face image encodes more information than eyes alone, such as illumination and head pose. Nevertheless, gaze behavior is not static. Eye and head movements allow us to direct our gaze to target locations of interest. It has been demonstrated that humans can better predict gaze when being shown image sequences of other people moving their eyes <ref type="bibr" target="#b0">[1]</ref>. However, it is still an open question whether this sequential information can increase the performance of automatic methods.</p><p>In this work, we show that the combination of multiple cues benefits the gaze estimation task. In particular, we use face, eye region and facial landmarks from still images. Facial landmarks model the global shape of the face and come at no cost, since face alignment is a common pre-processing step in many facial image analysis approaches. Furthermore, we present a subject-independent, free-head recurrent 3D gaze regression network to leverage the temporal information of image sequences. The static streams of each frame are combined in a late-fusion fashion using a multi-stream CNN. Then, all feature vectors are input to a many-to-one recurrent module that predicts the gaze vector of the last sequence frame.</p><p>In summary, our contributions are two-fold. First, we present a Recurrent-CNN network architecture that combines appearance, shape and temporal information for 3D gaze estimation. Second, we test static and temporal versions of our solution on the EYEDIAP dataset <ref type="bibr" target="#b6">[7]</ref> in a wide range of head poses and gaze directions, showing consistent performance improvements compared to related appearance-based methods. To the best of our knowledge, this is the first third-person, remote camera-based approach that uses temporal information for this task. <ref type="table">Table 1</ref> outlines our main method characteristics compared to related work. Models and code are publicly available at https://github.com/ crisie/RecurrentGaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Gaze estimation methods are typically categorized as model-based or appearance-based <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. Model-based approaches use a geometric model of the eye, usually requiring either high resolution images or a person-specific calibration stage to estimate personal eye parameters <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b42">41]</ref>. In contrast, appearance-based methods learn a direct mapping from intensity images or extracted eye features to gaze directions, thus being potentially applicable to relatively low resolution images and mid-distance scenarios. Different mapping functions have been explored, such as neural networks <ref type="bibr" target="#b1">[2]</ref>, adaptive linear regression (ALR) <ref type="bibr" target="#b18">[19]</ref>, local interpolation <ref type="bibr" target="#b33">[32]</ref>, gaussian processes <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b36">35]</ref>, random forests <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">31]</ref>, or k-nearest neighbors <ref type="bibr" target="#b41">[40]</ref>. Main challenges of appearance-based methods for 3D gaze estimation are head pose, illumination and subject invariance without user-specific calibration. To handle these issues, some works proposed compensation methods <ref type="bibr" target="#b17">[18]</ref> and warping strategies that synthesize a canonical, frontal looking view of the face <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>. Hybrid approaches based on analysis-by-synthesis have also been evaluated <ref type="bibr" target="#b40">[39]</ref>.</p><p>Currently, data-driven methods are considered the state of the art for person-and head pose-independent appearance-based gaze estimation. Consequently, a number of gaze estimation datasets have been introduced in recent years, either in controlled <ref type="bibr" target="#b30">[29]</ref> or semicontrolled settings <ref type="bibr" target="#b7">[8]</ref>, in the wild <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">42]</ref>, or consisting of synthetic data <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b41">40]</ref>. Zhang et al. <ref type="bibr" target="#b43">[42]</ref> showed that CNNs can outperform other mapping methods, using a multimodal CNN to learn the mapping from 3D head poses and eye images to 3D gaze directions. Krafka et al. <ref type="bibr" target="#b15">[16]</ref> proposed a multi-stream CNN for 2D gaze estimation, using individual eye, whole-face image and the face grid as input. As this method was limited to 2D screen mapping, Zhang et al. <ref type="bibr" target="#b44">[43]</ref> later explored the potential of just using whole-face images as input to estimate 3D gaze directions. Using a spatial weights CNN, they demonstrated their method to be more robust to facial appearance variation caused by head pose and illumination than eye-only methods. While the method was evaluated in the wild, the subjects were only interacting with a mobile device, thus restricting the head pose range. Deng and Zhu <ref type="bibr" target="#b3">[4]</ref> presented a two-stream CNN to disjointly model head pose from face images and eyeball movement from eye region images. Both were then aggregated into 3D gaze direction using a gaze transform layer. The decomposition was aimed to avoid head-correlation overfitting of previous data-driven approaches. They evaluated their approach in the wild with a wider range of head poses, obtaining better performance than previous eye-based methods. However, they did not test it on public annotated benchmark datasets.</p><p>In this paper, we propose a multi-stream recurrent CNN network for person-and head pose-independent 3D gaze estimation for a mid-distance scenario. We evaluate it on a wider range of head poses and gaze directions than screen-targeted approaches. As opposed to previous methods, we also rely on temporal information inherent in sequential data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present our approach for 3D gaze regression based on appearance and shape cues for still images and image sequences. First, we introduce the data modalities and formulate the problem. Then, we detail the normalization procedure prior to the regression stage. Finally, we explain the global network topology as well as the implementation details. An overview of the system architecture is depicted in <ref type="figure" target="#fig_2">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-modal gaze regression</head><p>Let us represent gaze direction as a 3D unit vector g = [g x , g y , g z ] T ? R 3 in the Camera Coordinate System (CCS), whose origin is the central point between eyeball centers. Assuming a calibrated camera, and a known head position and orientation, our goal is to estimate g from a sequence of images {I (i) | I ? R W ?H?3 } as a regression problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input sequence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual Fusion</head><p>Normalization FC C on v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C on v.</head><p>C on v.</p><p>Con v.</p><p>Conv. Gazing to a specific target is achieved by a combination of eye and head movements, which are highly coordinated. Consequently, the apparent direction of gaze is influenced not only by the location of the irises within the eyelid aperture, but also by the position and orientation of the face with respect to the camera. Known as the Wollaston effect <ref type="bibr" target="#b37">[36]</ref>, the exact same set of eyes may appear to be looking in different directions due to the surrounding facial cues. It is therefore reasonable to state that eye images are not sufficient to estimate gaze direction. Instead, whole-face images can encode head pose or illumination-specific information across larger areas than those available just in the eyes region <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b44">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC</head><p>The drawback of appearance-only methods is that global structure information is not explicitly considered. In that sense, facial landmarks can be used as global shape cues to encode spatial relationships and geometric constraints. Current state-of-the-art face alignment approaches are robust enough to handle large appearance variability, extreme head poses and occlusions, being especially useful when the dataset used for gaze estimation does not contain such variability. Facial landmarks are mainly correlated with head orientation, eye position, eyelid openness, and eyebrow movement, which are valuable features for our task.</p><p>Therefore, in our approach we jointly model appearance and shape cues (see <ref type="figure" target="#fig_2">Figure 1</ref>). The former is represented by a whole-face image I F , along with a higher resolution image of the eyes I E to identify subtle changes. Due to dealing with wide head pose ranges, some eye images may not depict the whole eye, containing mostly background or other surrounding facial parts instead. For that reason, and contrary to previous approaches that only use one eye image <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b43">42]</ref>, we use a single image composed of two patches of centered left and right eyes. Finally, the shape cue is represented by 3D face landmarks obtained from a 68-landmark model, denoted by L = {(l x , l y , l z ) c | ?c ? [1, ..., 68]}.</p><p>In this work we also consider the dynamic component of gaze. We leverage the sequential information of eye and head movements such that, given appearance and shape features of consecutive frames, it is possible to better predict the gaze direction of the current frame. Therefore, the 3D gaze estimation task for a 1-frame sequence is formulated</p><formula xml:id="formula_0">as g (i) = f {I F (i) }, {I E (i) }, {L (i) } ,</formula><p>where i denotes the i-th frame, and f is the regression function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data normalization</head><p>Prior to gaze regression, a normalization step in the 3D space and the 2D image, similar to <ref type="bibr" target="#b32">[31]</ref>, is carried out. This is performed to reduce the appearance variability and to allow the gaze estimation model to be applied regardless of the original camera configuration. Let H ? R 3x3 be the head rotation matrix, and p = [p x , p y , p z ] T ? R 3 the reference face location with respect to the original CCS. The goal is to find the conversion matrix M = SR such that (a) the X-axes of the virtual camera and the head become parallel using the rotation matrix R, and (b) the virtual camera looks at the reference location from a fixed distance d n using the Z-direction scaling matrix</p><formula xml:id="formula_1">S = diag(1, 1, d n / p ). R is computed as a =p ? H T e 1 , b =? ?p, R = [?,b,p] T ,</formula><p>where e 1 denotes the first orthonormal basis and ? is the unit vector.</p><p>This normalization translates into the image space as a cropped image patch of size W n ? H n centered at p where head roll rotation has been removed. This is done by applying a perspective warping to the input image I using the transformation matrix W = C o MC n ?1 , where C o and C n are the original and virtual camera matrices, respectively.</p><p>The 3D gaze vector is also normalized as g n = Rg. After image normalization, the line of sight can be represented in a 2D space. Therefore, g n is further transformed to spherical coordinates (? , ? ) assuming unit length, where ? and ? denote the horizontal and vertical direction angles, respectively. This 2D angle representation, delimited in the range [??/2, ?/2], is computed as ? = arctan(g x /g z ) and ? = arcsin(?g y ), such that (0, 0) represents looking straight ahead to the CCS origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recurrent Convolutional Neural Network</head><p>We propose a Recurrent CNN Regression Network for 3D gaze estimation. The network is divided in 3 modules: (1) Individual, (2) Fusion, and (3) Temporal.</p><p>First, the Individual module learns features from each appearance cue separately. It consists of a two-stream CNN, one devoted to the normalized face image stream and the other to the joint normalized eyes image. Next, the Fusion module combines the extracted features of each appearance stream in a single vector along with the normalized landmark coordinates. Then, it learns a joint representation between modalities in a late-fusion fashion. Both Individual and Fusion modules, further referred to as Static model, are applied to each frame of the sequence. Finally, the resulting feature vectors of each frame are input to the Temporal module based on a many-to-one recurrent network. This module leverages sequential information to predict the normalized 2D gaze angles of the last frame of the sequence using a linear regression layer added on top of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Network details</head><p>Each stream of the Individual module is based on the VGG-16 deep network <ref type="bibr" target="#b28">[27]</ref>, consisting of 13 convolutional layers, 5 max pooling layers, and 1 fully connected (FC) layer with Rectified Linear Unit (ReLU) activations. The full-face stream follows the same configuration as the base network, having an input of 224 ? 224 pixels and a 4096D FC layer. In contrast, the input joint eye image is smaller, with a final size of 120 ? 48 pixels, so the number of parameters is decreased proportionally. In this case, its last FC layer produces a 1536D vector. A 204D landmark coordinates vector is concatenated to the output of the FC layer of each stream, resulting in a 5836D feature vector. Consequently, the Fusion module consists of 2 5836D FC layers with ReLU activations and 2 dropout layers between FCs as regularization. Finally, to model the temporal dependencies, we use a single GRU layer with 128 units.</p><p>The network is trained in a stage-wise fashion. First, we train the Static model and the final regression layer end-to-end on each individual frame of the training data. The convolutional blocks are pre-trained with the VGG-Face dataset <ref type="bibr" target="#b28">[27]</ref>, whereas the FCs are trained from scratch. Second, the training data is re-arranged by means of a sliding window with stride 1 to build input sequences. Each sequence is composed of s = 4 consecutive frames, whose gaze direction target is the gaze direction of the last frame of the sequence {I (i?s+1) , . . . , I (i) }, g <ref type="bibr">(i)</ref> . Using this re-arranged training data, we extract features of each frame of the sequence from a frozen Individual module, fine-tune the Fusion layers, and train both, the Temporal module and a new final regression layer from scratch. This way, the network can exploit the temporal information to further refine the fusion weights. We trained the model using ADAM optimizer with an initial learning rate of 0.0001, dropout of 0.3, and batch size of 64 frames. The number of epochs was experimentally set to 21 for the first training stage and 10 for the second. We use the average Euclidean distance between the predicted and ground-truth 3D gaze vectors as loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Input pre-processing</head><p>For this work we use head pose and eye locations in the 3D scene provided by the dataset. The 3D landmarks are extracted using the state-of-the-art method of Bulat and Tzimiropoulos <ref type="bibr" target="#b2">[3]</ref>, which is based on stacked hourglass networks <ref type="bibr" target="#b23">[24]</ref>.</p><p>During training, the original image is pre-processed to get the two normalized input images. The normalized whole-face patch is centered 0.1 meters ahead of the head center in the head coordinate system, and C n is defined such that the image has size of 250 ? 250 pixels. The difference between this size and the final input size allows us to perform random cropping and zooming to augment the data (explained in Section 4.1). Similarly, each normalized eye patch is centered in their respective eye center locations. In this case, the virtual camera matrix is defined so that the image is cropped to 70 ? 58, while in practice the final patches have size of 60 ? 48. Landmarks are normalized using the same procedure and further pre-processed with mean subtraction and min-max normalization per axis. Finally, we divide them by a scaling factor w such that all coordinates are in the range [0, w]. This way, all concatenated feature values are in a similar range. After inference, the predicted normalized 2D angles are de-normalized back to the original 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the cross-subject 3D gaze estimation task on a wide range of head poses and gaze directions. Furthermore, we validate the effectiveness of the proposed architecture comparing both static and temporal approaches. We report the error in terms of mean angular error between predicted and ground-truth 3D gaze vectors. Note that due to the requirements of the temporal model not all the frames obtain a prediction. Therefore, for a fair comparison, the reported results for static models disregard such frames when temporal models are included in the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training data</head><p>There are few publicly available datasets devoted to 3D gaze estimation and most of them focus on HCI with a limited range of head pose and gaze directions. Therefore, we use VGA videos from the publicly-available EYEDIAP dataset <ref type="bibr" target="#b6">[7]</ref> to perform the experimental evaluation, as it is currently the only one containing video sequences with a wide range of head poses and showing the full face. This dataset consists of 3-minute videos of 16 subjects looking at two types of targets: continuous screen targets on a fixed monitor (CS), and floating physical targets (FT ). The videos are further divided into static (S) and moving (M) head pose for each of the subjects. Subjects 12-16 were recorded with 2 different lighting conditions. For evaluation, we filtered out those frames that fulfilled at least one of the following conditions: (1) face or landmarks not detected; <ref type="bibr" target="#b1">(2)</ref> subject not looking at the target; (3) 3D head pose, eyes or target location not properly recovered; and (4) eyeball rotations violating physical constraints (|? | ? 40 ? , |? | ? 30 ? ) <ref type="bibr" target="#b22">[23]</ref>. Note that we purposely do not filter eye blinking moments to learn their dynamics with the temporal model, which may produce some outliers with a higher prediction error due to a less accurate ground truth. <ref type="figure" target="#fig_3">Figure 2</ref> shows the distribution of gaze directions and head poses for both filtered CS and FT cases.</p><p>We applied data augmentation to the training set with the following random transformations: horizontal flip, shifts of up to 5 pixels, zoom of up to 2%, brightness changes by a factor in the range [0.4, 1.75], and additive Gaussian noise with ? 2 = 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of static modalities</head><p>First, we evaluate the contribution of each static modality on the FT scenario. We divided the 16 participants into 4 groups, such that appearance variability was maximized while maintaining a similar number of training samples per group. Each static model was trained end-to-end performing 4-fold cross-validation using different combinations of input modalities. Since the number of fusion units depends on the number of input modalities, we also compare different fusion layer sizes. The effect of data normalization is also evaluated by training a not-normalized face model where the input image is the face bounding box with square size the maximum distance between 2D landmarks. Floating <ref type="table" target="#tab_3">Target  Screen Target  0  1  2  3  4  5  6  7  8  9  10  11</ref> Angle error (degrees) 6. <ref type="bibr" target="#b37">36</ref>   As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, all models that take normalized full-face information as input achieve better performance than the eyes-only model. More specifically, the combination of face, eyes and landmarks outperforms all the other combinations by a small but significant margin (paired Wilcoxon test, p &lt; 0.0001). The standard deviation of the best-performing model is reduced compared to the face and eyes model, suggesting a regularizing effect due to the addition of landmarks. The not-normalized face-only model shows the largest error, proving the impact of normalization to reduce the appearance variability. Furthermore, our results indicate that the increase of fusion units is not correlated with a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Static gaze regression: comparison with existing methods</head><p>We compare our best-performing static model with three baselines. Head: Treating the head pose directly as gaze direction. PR-ALR: Method that relies on RGB-D data to rectify the eye images viewpoint into a canonical head pose using a 3DMM. It then learns an RGB gaze appearance model using ALR <ref type="bibr" target="#b20">[21]</ref>. Predicted 3D vectors for FT-S scenario are provided by EYEDIAP dataset. MPIIGaze:. State-of-the-art full-face 3D gaze estimation method <ref type="bibr" target="#b43">[42]</ref>. They use an Alexnet-based CNN model with spatial weights to enhance information in different facial regions. We fine-tuned it with the filtered EYEDIAP subsets using our training parameters and normalization procedure.</p><p>In addition to the aforementioned FT-based evaluation setup, we also evaluate our method on the CS scenario. In this case there are only 14 participants available, so we divided them in 5 groups and performed 5-fold cross-validation. In <ref type="figure" target="#fig_5">Figure 4</ref> we compare our method to MPIIGaze, achieving a statistically significant improvement of 14.6% and 19.5% on FT and CS scenarios, respectively (paired Wilcoxon test, p &lt; 0.0001). We can observe that a restricted gaze target benefits the performance of all methods, compared to a more challenging unrestricted setting with a wider range of head poses and gaze directions. <ref type="table" target="#tab_3">Table 2</ref> provides a detailed comparison on every participant, performing leave-one-out cross-validation on the FT scenario for static and moving head separately. Results show that, as expected, facial appearance and head pose have a noticeable impact on gaze accuracy, with average error differences of up to 7.7 ? among participants.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of the temporal network</head><p>In this section, we evaluate the contribution of adding the temporal module to the static model. To do so, we trained a lower-dimensional version of the static network with comparable performance to the original, reducing the number of units of the second fusion layer to 2918. Results are reported in <ref type="figure" target="#fig_5">Figure 4</ref> and <ref type="table" target="#tab_3">Table 2</ref>. One can observe that using sequential information is helpful on the FT scenario, outperforming the static model by a statistically significant 4.4% (paired Wilcoxon test, p &lt; 0.0001). This contribution is more noticeable in the moving head setting, proving that the temporal model can benefit from head motion information. In contrast, such information seems to be less meaningful in the CS scenario, where the obtained error is already very low for a cross-subject setting and the amount of head movement declines. <ref type="figure" target="#fig_6">Figure 5</ref> further explores the error distribution of the static network and the impact of sequential information. We can observe that the accuracy of the static model drops with extreme head poses and gaze directions, which can also be correlated to having less data in those areas. Compared to the static model, the temporal model particularly benefits gaze targets from mid-range upwards. Its contribution is less clear for extreme targets, probably again due to data imbalance.</p><p>Finally, we evaluated the effect of different recurrent architectures for the temporal model. In particular, we tested 1 (128 units) and 2 (256-128 units) LSTM and GRU layers, with 1 GRU layer obtaining slightly superior results (up to 0.12 ? ). We also assessed the effect of sequence length fixing s in the range {4, 7, 10}, with s = 7 performing worse than the other two (up to 0.14 ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we studied the combination of full-face and eye images along with facial landmarks for person-and head pose-independent 3D gaze estimation. Consequently, we proposed a multi-stream recurrent CNN network that leverages the sequential information of eye and head movements. Both static and temporal versions of our approach significantly outperform current state-of-the-art 3D gaze estimation methods on a wide range of head poses and gaze directions. We showed that adding geometry features to appearance-based methods has a regularizing effect on the accuracy. Adding sequential information further benefits the final performance compared to static-only input, especially from mid-range upwards and in those cases where head motion is present. The effect in very extreme head poses is not clear due to data imbalance, suggesting the importance of learning from a continuous, balanced dataset including all head poses and gaze directions of interest. To the best of our knowledge, this is the first attempt to exploit the temporal modality in the context of gaze estimation from remote cameras. As future work, we will further explore extracting meaningful temporal representations of gaze dynamics, considering 3DCNNs as well as the encoding of deep features around particular tracked face landmarks <ref type="bibr" target="#b13">[14]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the proposed network. A multi-stream CNN jointly models full-face, eye region appearance and face landmarks from still images. The combined extracted features from each frame are fed into a recurrent module to predict last frame's gaze direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>g (FT ) (b) h (FT ) (c) g (CS) (d) h (CS)Ground-truth eye gaze g and head orientation h distribution on the filtered EYE-DIAP dataset for CS and FT settings, in terms of x-and y-angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Performance evaluation of the Static network using different input modalities (O -Not normalized, N -Normalized, F -Face, E -Eyes, L -3D Landmarks) and size of fusion layers on the FT scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Performance comparison among MPIIGaze method<ref type="bibr" target="#b43">[42]</ref> and our Static and Temporal versions of the proposed network for FT and CS scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Angular error distribution across gaze (a) and head orientation (b) spaces in the FT setting, in terms of x-and y-angles. For each space, we depict the Static model performance (left) and the contribution of the Temporal model versus Static (right). In the latter, positive difference means higher improvement of the Temporal model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1805.03064v3 [cs.CV] 17 Sep 2018</figDesc><table><row><cell>Method</cell><cell>3D gaze</cell><cell>Unrestricted</cell><cell>Full</cell><cell>Eye</cell><cell>Facial</cell><cell>Sequential</cell></row><row><cell></cell><cell>direction</cell><cell>gaze target</cell><cell>face</cell><cell>region</cell><cell>landmarks</cell><cell>information</cell></row><row><cell>Zhang et al. (1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>22.1 20.3 23.6 23.2 23.2 23.6 21.2 26.7 23.6 23.1 24.4 23.3 24.0 24.5 22.8 23.3 PR-ALR 12.3 12.0 12.4 11.3 15.5 12.9 17.9 11.8 17.3 13.4 13.4 14.3 15.2 13.6 14.4 14.6 13.9 Head 19.3 14.2 16.4 19.9 16.8 21.9 16.1 24.2 20.3 19.9 18.8 22.3 18.1 14.9 16.2 19.3 18.7</figDesc><table><row><cell>Method</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>Avg.</cell></row><row><cell cols="2">Head 23.5 MPIIGaze 5.3</cell><cell>5.1</cell><cell>5.7</cell><cell>4.7</cell><cell cols="4">7.3 15.1 10.8 5.7</cell><cell>9.9</cell><cell>7.1</cell><cell>5.0</cell><cell>5.7</cell><cell>7.4</cell><cell>3.8</cell><cell>4.8</cell><cell>5.5</cell><cell>6.8</cell></row><row><cell>Static</cell><cell>3.9</cell><cell>4.1</cell><cell>4.2</cell><cell>3.9</cell><cell>6.0</cell><cell>6.4</cell><cell>7.2</cell><cell>3.6</cell><cell>7.1</cell><cell>5.0</cell><cell>5.7</cell><cell>6.7</cell><cell>3.9</cell><cell>4.7</cell><cell>5.1</cell><cell>4.2</cell><cell>5.1</cell></row><row><cell>Temporal</cell><cell>4.0</cell><cell>4.9</cell><cell>4.3</cell><cell>4.1</cell><cell>6.1</cell><cell>6.5</cell><cell>6.6</cell><cell>3.9</cell><cell>7.8</cell><cell>6.1</cell><cell>4.7</cell><cell>5.6</cell><cell>4.7</cell><cell>3.5</cell><cell>5.9</cell><cell>4.6</cell><cell>5.2</cell></row><row><cell cols="2">MPIIGaze 7.6</cell><cell>6.2</cell><cell>5.7</cell><cell cols="5">8.7 10.1 12.0 12.2 6.1</cell><cell>8.3</cell><cell>5.9</cell><cell>6.1</cell><cell>6.2</cell><cell>7.4</cell><cell>4.7</cell><cell>4.4</cell><cell>6.0</cell><cell>7.3</cell></row><row><cell>Static</cell><cell>5.8</cell><cell>5.7</cell><cell>4.4</cell><cell>7.5</cell><cell>6.7</cell><cell cols="3">8.8 11.6 5.5</cell><cell>8.3</cell><cell>5.5</cell><cell>5.2</cell><cell>6.3</cell><cell>5.3</cell><cell>3.9</cell><cell>4.3</cell><cell>5.6</cell><cell>6.3</cell></row><row><cell>Temporal</cell><cell>6.1</cell><cell>5.6</cell><cell>4.5</cell><cell>7.5</cell><cell>6.4</cell><cell cols="3">8.2 12.0 5.0</cell><cell>7.5</cell><cell>5.4</cell><cell>5.0</cell><cell>5.8</cell><cell>6.6</cell><cell>4.0</cell><cell>4.5</cell><cell>5.8</cell><cell>6.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Gaze angular error comparison for static (top half) and moving (bottom half) head pose for each subject in the FT scenario. Best results in bold.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been partially supported by the Spanish project TIN2016-74946-P (MINECO/ FEDER, UE), CERCA Programme / Generalitat de Catalunya, and the FP7 people program (Marie Curie Actions), REA grant agreement no FP7-607139 (iCARE -Improving Children Auditory REhabilitation). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPU used for this research. Portions of the research in this paper used the EYEDIAP dataset made available by the Idiap Research Institute, Martigny, Switzerland.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Motion influences gaze direction discrimination and disambiguates contradictory luminance cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">F</forename><surname>Nicola C Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Risko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="817" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-intrusive gaze tracking using artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular free-head 3d gaze tracking with deep learning and geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3162" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low cost eye tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Ferhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence and neuroscience</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaze estimation in the 3D space using RGB-D sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Kenneth A Funes-Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="216" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Alberto Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2578153.2578190</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Eye Tracking Research and Applications</title>
		<meeting>the ACM Symposium on Eye Tracking Research and Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Alberto Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research and Applications</title>
		<meeting>the Symposium on Eye Tracking Research and Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual social attention in autism spectrum disorder: Insights from eye tracking studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Guillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nouchine</forename><surname>Hadjikhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Baduel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernadette</forename><surname>Rog?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="279" to="297" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In the eye of the beholder: A survey of models for eyes and gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Witzner Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="478" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tabletgaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Sabharwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eye tracking in human-computer interaction and usability research: Ready to deliver the promises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith S</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The mind&apos;s eye</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="573" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person-independent 3d gaze estimation using face frontalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?szl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey F</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint finetuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunjeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review and analysis of eye-gaze estimation systems, algorithms and performance evaluation methods in consumer platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuradha</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Corcoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="16495" to="16519" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchendra</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2176" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saccadic eye movements and cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John M</forename><surname>Liversedge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Findlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="14" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A head pose-free approach for appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inferring human gaze from appearance via adaptive linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eye tracking and eye-based human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?ivi</forename><surname>Majaranta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in physiological computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="39" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaze estimation from multimodal kinect data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Alberto</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Funes</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting eye position and gaze from a single camera and 2 light sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos Hitoshi</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnon</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myron</forename><surname>Flickner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 16th International Conference on</title>
		<meeting>16th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="314" to="317" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">982 (2000) guidelines on ergonomic criteria for bridge equipment and layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imo Msc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Circ</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gaze estimation from low resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Symposium on Image and Video Technology</title>
		<imprint>
			<biblScope unit="page" from="178" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic mutual gaze detection in face-to-face dyadic interaction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Palmero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><forename type="middle">A</forename><surname>Van Dam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Kelia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><forename type="middle">F</forename><surname>Lichtert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Noldus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Astrid</forename><surname>Spink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Wieringen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Measuring Behavior</title>
		<meeting>Measuring Behavior</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="158" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Turn-taking in mother-infant interaction: An examination of vocalizations and gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Rutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental psychology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gaze locking: passive eye contact detection for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree K</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual ACM symposium on User interface software and technology</title>
		<meeting>the 26th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation using visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="329" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning-by-synthesis for appearance-based 3d gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Appearance-based eye gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kar-Han</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Sixth IEEE Workshop on</title>
		<meeting>Sixth IEEE Workshop on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="191" to="195" />
		</imprint>
	</monogr>
	<note>Applications of Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eye gaze estimation from a single image of one eye</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronda</forename><surname>Venkateswarlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real time eye gaze tracking with 3d deformable eye-face model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse and semi-supervised visual mapping with the s?3gp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">on the apparent direction of eyes in a portrait</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hyde Wollaston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="247" to="256" />
			<date type="published" when="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Eyetab: Model-based gaze estimation on unmodified tablet computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research and Applications</title>
		<meeting>the Symposium on Eye Tracking Research and Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="207" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rendering of eyes for eye-shape registration and gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3756" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A 3d morphable eye region model for gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning an appearance-based gaze estimator from one million synthesised images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A novel non-intrusive eye gaze estimation using cross-ratio under large head motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung</forename><forename type="middle">Jin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="51" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4511" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">It&apos;s written all over your face: Full-face appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
