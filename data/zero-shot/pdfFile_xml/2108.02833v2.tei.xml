<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Elaborative Rehearsal for Zero-shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
							<email>shizhe.chen@inria.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
							<email>donghuang@cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Elaborative Rehearsal for Zero-shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The growing number of action classes has posed a new challenge for video understanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction. The ZSAR task aims to recognize target (unseen) actions without training examples by leveraging semantic representations to bridge seen and unseen actions. However, due to the complexity and diversity of actions, it remains challenging to semantically represent action classes and transfer knowledge from seen data. In this work, we propose an ER-enhanced ZSAR model inspired by an effective human memory technique Elaborative Rehearsal (ER), which involves elaborating a new concept and relating it to known concepts. Specifically, we expand each action class as an Elaborative Description (ED) sentence, which is more discriminative than a class name and less costly than manual-defined attributes. Besides directly aligning class semantics with videos, we incorporate objects from the video as Elaborative Concepts (EC) to improve video semantics and generalization from seen actions to unseen actions. Our ER-enhanced ZSAR model achieves state-of-the-art results on three existing benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics dataset to overcome limitations of current benchmarks and first compare with few-shot learning baselines on this more realistic setting. Our codes and collected EDs are released at https://github. com/DeLightCMU/ElaborativeRehearsal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised video action recognition (AR) has made great progress in recent years, benefited from new models such as 3D convolutional neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b30">42]</ref> and large-scale video datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">16]</ref>. These supervised models require abundant training data for each action class. However, desired action classes are continuously increasing with the explosive growth of video applications on smart phones, surveillance cameras and drones. It is prohibitively * This work was performed when Shizhe Chen was at Carnegie Mellon University. <ref type="figure" target="#fig_3">Figure 1</ref>: Attributes and word embeddings are insufficient to semantically represent action classes. Our Elaborative Rehearsal approach defines actions by Elaborative Descriptions (EDs) and associates videos with Elaborative Concepts (ECs, known concepts detected from the video), which improve video semantics and generalization video-action association for ZSAR. ($for videos, for seen actions, ? for unseen actions, and for ECs) expensive to collect annotated videos for each action class to fuel the training needs of existing supervised models. In order to alleviate such burden, Zero-Short Action Recognition (ZSAR) <ref type="bibr" target="#b39">[50]</ref> has become a thriving research direction, which aims at generalizing AR models to unseen actions without using any labeled training data of unseen classes.</p><p>A common approach for ZSAR is to embed videos and action classes into a joint semantic space <ref type="bibr">[12,</ref><ref type="bibr" target="#b38">49]</ref>, so that the associations between video and seen actions can be transferred to unseen actions. However, how to semantically represent action classes for above associations is a challenging problem due to the complexity and diversity of actions. As shown in <ref type="figure" target="#fig_3">Figure 1</ref>(a), early works employ manual-defined attributes <ref type="bibr" target="#b17">[29]</ref> to represent actions. Despite being a natural methodology, it is hard and expensive to define a complete set of atom attributes that generalizes to arbitrary actions. To overcome difficulties in attribute definition, recent works adopt word embeddings of action names <ref type="bibr" target="#b39">[50,</ref><ref type="bibr" target="#b3">4]</ref> as class semantic representations. Though simple and effective, word embeddings can be ambiguous. Words have different meanings in different context and some actions might not even be interpreted literally according to their names such as the "dumpster diving" action in <ref type="figure" target="#fig_3">Figure 1(b)</ref>, which are confusing to relating different action classes.</p><p>In addition to class semantic representations of actions, it has been under-explored in existing ZSAR works on how to learn powerful and generalizable video semantic representations. Only until recently, deep features <ref type="bibr">[19,</ref><ref type="bibr" target="#b29">41]</ref> have been used to overtake traditional hand-crafted features such as fisher vectors of improved dense trajectory descriptors <ref type="bibr" target="#b32">[43,</ref><ref type="bibr" target="#b39">50]</ref>. One line of work <ref type="bibr">[15,</ref><ref type="bibr">21]</ref> utilizes objects recognized by deep image networks as video descriptors, which assumes that object recognition in image domain is prior knowledge for more advanced action recognition. The predicted objects are naturally embedded in the semantic space and thus can be well generalized to recognize actions even without any video example <ref type="bibr">[21]</ref>. However, the video is more than collections of objects, but contains specific relationships among objects. Therefore, it is insufficient to represent video contents purely using object semantics. Another direction of works <ref type="bibr" target="#b3">[4]</ref>, instead, directly employs stateof-the-art video classification networks in ZSAR. Though powerful enough to capture spatio-temporal information in the video, they are prone to overfit on seen action classes and transfer poorly to unseen ones.</p><p>In this work, we take inspiration from a well-established human memory technique, namely Elaborative Rehearsal (ER) <ref type="bibr" target="#b2">[3]</ref>, for ZSAR. When we learn a new item such as "dumpster diving", we first expand the phrase into a readily comprehensible definition, and then relate the definition to known information in our long-term memory, thereby fostering retention of the item. In a similar manner, we propose an ER-enhanced model to generalize AR models for new actions. Our approach advances ZSAR in three main aspects under the common paradigm of joint semantic space learning <ref type="bibr">[12,</ref><ref type="bibr" target="#b38">49]</ref>: (1) For the class semantic representation of actions, we construct Elaborative Descriptions (ED) from class names to comprehensively define action classes as shown in <ref type="figure" target="#fig_3">Figure 1</ref>(c), and embed the ED leveraging prior knowledge from pre-trained language models. (2) For the video semantic representation, we propose two encoding network streams that jointly embed spatio-temporal dynamics and objects in videos. We use a pre-trained image object classification model <ref type="bibr" target="#b12">[24]</ref> to generate the Elaborative Concepts (EC) of objects. Since it is highly likely that some common objects involved in seen and unseen classes, incorporating EC in video semantics improves the generalization on unseen classes. (3) To further improve generalization of video semantic representations, we propose an ER objective to enforce the model to rehearse video contents with additional semantic knowledge from EC. The embedding of EC shares the same embedding function as the ED of action classes, which also implicitly makes our ZSAR model more generalizable to diverse class semantic representation. Our ER-enhanced ZSAR model achieves state-of-the-art performance on the widely used benchmarks including Olympic Sports <ref type="bibr" target="#b20">[32]</ref>, HMDB51 <ref type="bibr" target="#b13">[25]</ref> and UCF101 <ref type="bibr" target="#b27">[39]</ref> datasets.</p><p>Moreover, existing ZSAR benchmarks are relative small and contain overlapped classes with video datasets for feature training. In order to benchmark ZSAR progress on a more realistic scenario, we further propose a new ZSAR evaluation protocol based on a large-scale supervised action dataset Kinetics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In our Kinetics ZSAR benchmark, we demonstrate the first case where ZSAR performance is comparable to a simple but strong few-shot learning baseline under clear split of seen and unseen action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised Action Recognition. The rapid development of deep learning [18] has vigorously promoted AR research. Early deep models <ref type="bibr" target="#b11">[23,</ref><ref type="bibr" target="#b26">38,</ref><ref type="bibr" target="#b33">44]</ref> adopt 2D convolutional neural networks (CNNs) in temporal domain. To more effectively encode temporal dynamics in videos, 3D CNNs <ref type="bibr" target="#b29">[41]</ref> are proposed but are computation and parameter heavy, which require large-scale datasets to train. Therefore, different approaches have emerged to improve 3D CNNs. Carreira et al. <ref type="bibr" target="#b5">[6]</ref> propose I3D network which inflates 2D CNN to 3D CNN to learn spatio-temporal features. Tran et al. <ref type="bibr" target="#b30">[42]</ref> and Qiu et al. <ref type="bibr" target="#b24">[36]</ref> decompose 3D convolution into 2D spatial and 1D temporal convolutions. Wang et al. <ref type="bibr" target="#b35">[46]</ref> insert non-local blocks into 3D CNNs to capture long-range dependencies. Feichtenhofer et al.</p><p>[11] introduce slowfast network with two pathways operating at different frame rates, and further explores expansion of 2D CNNs along space, time, width and depth in <ref type="bibr" target="#b9">[10]</ref>. Lin et al. <ref type="bibr" target="#b16">[28]</ref> propose temporal shift module (TSM) to achieve temporal modeling at 2D computational costs and parameters. Despite strong performance, these supervised models cannot recognize new classes without training examples. In this work, we generalize the AR models to recognize unseen actions. Zero Shot Learning. Most ZSL works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b36">47,</ref><ref type="bibr" target="#b37">48,</ref><ref type="bibr" target="#b42">53]</ref> focus on the image domain to recognize unseen objects. A comprehensive survey can be found in <ref type="bibr" target="#b37">[48]</ref>. Here we mainly review joint semantic space based methods. ALE <ref type="bibr" target="#b0">[1]</ref>, DE-VISE [12] and SJE <ref type="bibr" target="#b1">[2]</ref> use bilinear compatibility function to associate visual and class representations, with different objectives for training. ESZSL <ref type="bibr" target="#b25">[37]</ref> proposes an objective function with closed form solution for linear projection. DEM <ref type="bibr" target="#b42">[53]</ref> proposes to use visual space as embedding space to address hubness problem in ZSL. Different from above approaches, Wang et al. <ref type="bibr" target="#b36">[47]</ref> predict classification weights based on knowledge graphs of classes. Except using different features, the ZSL methods in image domain can be applied for zero-shot action recognition. Zero Shot Action Recognition. As the main focus of our work is to learn better video and action semantic repre-Class Name Elaborative Description (ED) ED Source Action Class cleaning gutters cleaning gutters : make clean ; remove dirt , marks , or stains from . a shallow trough fixed beneath the edge of a roof for carrying off rainwater . Wikipedia + Dictionary + Modification clean and jerk clean and jerk : a two -movement weightlifting exercise in which a weight is raised above the head following an initial lift to shoulder level .</p><p>Object Concept chipboard chipboard : a cheap hard material made from wood chips that are pressed together and bound with synthetic resin WordNet  <ref type="bibr" target="#b3">[4]</ref> argues that end-to-end training is important for ZSAR and proposes to train a 3D CNN to predict word embedding of action names. However, word embeddings can be ambiguous and mislead knowledge transfer among action classes. The most similar work to ours is <ref type="bibr" target="#b34">[45]</ref>, which employs texts and images as alternative semantic representation for actions, but their text descriptions are rather noisy and inferior to attributes or word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In ZSAR, we are given a source dataset D s = {(v n , y n )} N n=1 of N video with labels from seen action classes S = {1, ? ? ? , S}, where v n is a video clip and y n ? S is the label. D t = {(v m )} N +M m=1+N is the target dataset of M videos with labels from unseen action classes T = {S + 1, ? ? ? , S + T }. The goal of ZSAR is to classify v m ? D t over unseen classes T with AR models only trained on D s . Following <ref type="bibr">[12,</ref><ref type="bibr" target="#b38">49]</ref>, the main architecture of our ZSAR model embedes videos and action classes (texts) into a joint semantic space, in which classes of similar semantics are closer as nearest-neighbors. Their respective embedding functions are a video embedding function ?(v) and an action class embedding function ?(y). Both func-tions are only trained on D s , and to be tested on D t .</p><p>In the rest of the section, we present the novel components of our ER-enhanced ZSAR model: Elaborative Description (ED), action class embedding function ?(y), video embedding function ?(v), and Elaborative Rehearsal (ER) loss. The framework is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Elaborative Description (ED)</head><p>For each action class name or each object concept, we concatenate a name and its sentence-based definition as an ED. Examples of EDs are listed in <ref type="table" target="#tab_0">Table 1</ref>, which are more discriminative than class names and easier to generate than attributes to semantically represent an action or an object. Justification for Human Involvement. ZSL demands class-wise semantic representations, which might involve human to construct, but costs significantly less than samplewise annotation efforts in supervised training. In fact, it is a vital step of ZSL to design a high-quality semantic representation with less class-wise annotating efforts. For ZSL on general object classification task [12, <ref type="bibr" target="#b15">27,</ref><ref type="bibr" target="#b37">48]</ref>, word embeddings of class names are gaining popularity as semantic representation, because the semantic embeddings of general object words are well-learned in pre-trained language models and can be used as prior knowledge. However, word embeddings are not applicable to other domains such as fine-grained ZSL for bird species <ref type="bibr">[20]</ref> where the class name provides little information about visual appearances. Manual-defined attributes <ref type="bibr">[20]</ref> or cleaned text descriptions <ref type="bibr" target="#b21">[33]</ref> are required in such scenarios. The situation is similar in ZSAR, where action names alone are not discriminative enough to represent context of the action. For example, the action "fidgeting" in the Kinetics dataset <ref type="bibr" target="#b5">[6]</ref> denotes "playing fidget spinner" instead of its common meaning of "making small movements". Therefore, it is necessary for human involvement to clarify the action definitions. Compared to carefully designed and annotated attributes, a more natural way for we humans is to describe the visual process of target actions in natural language, which motivates us to collect sentence-based ED for action class representation. Construction of Elaborative Description. Defining actions is more complicated than objects. In the ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>, object classes are directly linked to concepts in WordNet <ref type="bibr" target="#b19">[31]</ref>, and thus EDs of objects are straightforward to obtain. However, currently there are no such resources to define actions. To reduce manual efforts of writing EDs from scratch, we first automatically crawl candidate sentences from Wikipedia and dictionaries using action names as queries. Then we ask annotators to select and modify a minimum set of sentences from the candidates to describe the target action given few video exemplars. More details are presented in the supplementary material. It takes less than 20 seconds on average to generate the ED per action class, which is very efficient. The average length of EDs for actions in the Kinetics dataset [6] is 36 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Class Embedding ?(y)</head><p>Denote d = {w 1 , ? ? ? , w N d } the ED for action y, where w i is the composed word. The goal of action class embedding is to encode d into semantic feature z ? R K with dimension of K.</p><p>In order to capture the sequential order in d and transfer prior knowledge from large-scale text models, unlike previous works that use tf-idf <ref type="bibr" target="#b22">[34]</ref>, average word embedding <ref type="bibr" target="#b39">[50]</ref> or RNNs trained from scratch <ref type="bibr" target="#b42">[53]</ref>, we propose to employ a pre-trained BERT model <ref type="bibr" target="#b8">[9]</ref> for ED encoding. The BERT model has demonstrated excellent capability in implicitly encoding commonsense knowledge <ref type="bibr" target="#b6">[7]</ref>, which is beneficial to embed global semantics of the sentences.</p><p>Denote h i ? R 768 as the hidden state from the last layer of BERT for word w i , we apply average pooling to obtain a sentence-level featureh:</p><formula xml:id="formula_0">h = 1 N d N d i=1 h i .<label>(1)</label></formula><p>Since there are multi-layers of self-attention in BERT, the content words are more strengthened than other stopwords. Therefore, we did not observe performance gains using more complicated methods to aggregate h i than our average pooling. Then we use a linear transformation layer to translateh into the joint semantic embedding space:</p><formula xml:id="formula_1">z = W ch + b c ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">W c ? R K?768 , b c ? R K are parameters to learn.</formula><p>Finally, the action class embedding is normalized as z = z/||?|| 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multimodal Video Embedding ?(v)</head><p>Unseen action classes may involve both novel spatiotemporal features and objects. For better generalization, we propose to encode videos in two modality streams to capture both the spatio-temporal dynamics and object semantics. Spatio-Temporal Stream in Visual Modality. Encouraged by the recent success of 3D CNNs in supervised AR, we employ 3D CNNs, specifically TSM <ref type="bibr" target="#b16">[28]</ref>, to extract Spatio-Temporal (ST) features. Denotex v ? R 2048 as the output from the last pooling layer of TSM, we mapx v into the joint embedding space through linear transformation:</p><formula xml:id="formula_3">x v = W vxv + b v ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W v ? R K?2048 , b v ? R K are parameters to learn.</formula><p>We also normalize the embedding as</p><formula xml:id="formula_5">x v =x v /||x v || 2 .</formula><p>Object Stream in Text Modality. It is a widely acknowledged assumption that the objects associated with actions are prior knowledge available to the ZSAR model <ref type="bibr">[15,</ref><ref type="bibr">21]</ref>. We leverage objects automatically recognized from frames to construct a video representation in text modality. Specifically, we use the BiT model <ref type="bibr" target="#b12">[24]</ref> pretrained on Ima-geNet21k dataset to predict object probabilities from evenly sampled 8 frames from video v. The object probabilities over frames are averaged and only the top N o objects O = {o 1 , ? ? ? , o No } are preserved and embedded in a concatenated sequence:</p><formula xml:id="formula_6">x o = ?([ED(o 1 ); ? ? ? ; ED(o No )]),<label>(4)</label></formula><p>where ED(o i ) denotes ED of object o i as in <ref type="table" target="#tab_0">Table 1</ref>. Here we use the same ?(?) function as in the action class embeddings, which explicitly encourages that the object embedding x o from videos and action class embedding z from action names to lie in the same semantic space.</p><p>Multimodal Channel Attention. The awareness of object semantics can focus the spatio-temporal stream to objecthighlighted video features, while object semantics can be enriched with motion features. We thus further propose to dynamically fuse the two embeddings, x v and x o , to enhance each other. The formula of injecting x o to improve x v is as follows:</p><formula xml:id="formula_7">g vo = ?(W 2 vo RELU(W 1 vo [x v ; x o ])), (5) x vo = x v g vo /||x v g vo || 2 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">W 1 vo ? R 2K?K , W 2 vo ? R K?K are parameters, ? is the sigmoid function.</formula><p>Similarly, we obtain x ov from object embedding x o with guidance of x v . Therefore, our video encoder ?(v) produces two video embeddings x vo and x ov to comprehensively represent the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Elaborative Rehearsal (ER) enhanced Training</head><p>Following the standard ZSAR training, given video v n of seen classes S, we first generate the video embeddings [x n vo , x n ov ] = ?(v n ) and action class embedding matrix Z ? R K?S where each column z i = ?(i). Then, we compute video-action similarity scores:</p><formula xml:id="formula_9">p n v = x vo ? Z, p n o = x ov ? Z,<label>(7)</label></formula><p>where ? denote vector-matrix multiplication, p n v , p n o ? R S . As the negative score between object and action class embeddings mainly indicates that the recognized objects are irrelevant to the action, the magnitude is less important. We thus can fuse the two similarity scores as:</p><formula xml:id="formula_10">p n = p n v + max(p n o , 0).<label>(8)</label></formula><p>We use a standard contrastive loss to train the action recognition model. To be generalizable, p ? R C denotes the predicted score, q ? R C is the ground-truth label where q i = 1 if the i-th label is true otherwise q i = 0, and C is the number of classes. The contrastive loss is:</p><formula xml:id="formula_11">L(p, q) = ?1 C j=1 q j C i=1 q i log exp(p i /? ) C j=1 exp(p j /? ) ,<label>(9)</label></formula><p>where ? is a temperature hyper-parameter. To conduct action recognition on seen data D s , we convert label y n into one-hot vector q n and the loss is:</p><formula xml:id="formula_12">L ar = 1 N N n=1 L(p n , q n ) + L(p n v , q n ) + L(p n o , q n ).<label>(10)</label></formula><p>Because x vo is more powerful than x ov , the model trained on D s tends to overweight x vo in Eq. 8 and results in overfitting. Therefore, we average the three losses in Eq. 10. Moreover, as there are only S semantic labels in L ar as semantic supervision, the learned video and text representations might be less generalizable to more diverse semantics. To address this problem, we further propose an Elaborative Rehearsal (ER) loss, which rehearses the video representation with semantics from ECs obtained from frame-wise object classification. Denote O n = {o n 1 , ? ? ? , o n No } the top recognized objects in video v n , we generate semantic representation ?(ED(o n i )) for each o n i . Since the total number of all objects are large, we sample top few object classes during training for efficiency. Let Z o be the object class embeddings in a mini-batch of training, and the ER loss is computed as:</p><formula xml:id="formula_13">L er = 1 N N n=1 L(p n c , q n c ) + L(p n c,v , q n c ) + L(p n c,o , q n c ), (11) where p n c,v = x vo ? Z o , p n c,o = x ov ? Z o , p n c = p n c,v + p n c,o</formula><p>, and q n c is the ground-truth object labels for v n . Finally, we combine L ar and L er in our ZSAR model training with a balance factor ?:</p><formula xml:id="formula_14">L = L ar + ?L er .<label>(12)</label></formula><p>Comparing to Eq. 10, our model trained by Eq. 12 learns a shared ?(?) from ECs (i.e. ?(o i )), and ED (i.e., ?(y i )).</p><p>The sharing advocates to learn more comprehensive associations between videos and classes in the common semantic space defined by (?(?), ?(?)), and thus leads to better generalization to unseen classes. In inference, the action class of v m ? D t is recognized with the highest similarity score:</p><formula xml:id="formula_15">y m = arg max y?T (x m vo ? ?(y) + max(x m ov ? ?(y), 0)) (13) where x m vo , x m ov = ?(v m ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Splits</head><p>Existing ZSAR Benchmarks. Olympic Sports <ref type="bibr" target="#b20">[32]</ref>, HMDB51 <ref type="bibr" target="#b13">[25]</ref> and UCF101 <ref type="bibr" target="#b27">[39]</ref> are the three most popular datasets used in existing ZSAR papers <ref type="bibr" target="#b10">[22]</ref>, which contain 783, 6766 and 13320 videos of 16, 51, 101 action categories respectively. For robust evaluation, Xu et al. <ref type="bibr" target="#b39">[50]</ref> proposed to evaluate on 50 independent data splits and report the average accuracy and standard deviation. In each split, videos of 50% randomly selected classes are used for training and the remaining 50% classes are held unseen for testing. We adopt the same data splits as <ref type="bibr" target="#b39">[50]</ref> for fair comparison.</p><p>There are two major limitations in the above ZSAR protocols. Firstly, it is problematic to use deep features pretrained on other large-scale supervised video datasets because there exist overlapped action classes between pretraining classes and testing classes. Secondly, the size of training and testing data is small which leads to large variations among different data splits, so that abundant numbers of experiments are necessary to evaluate a model. To address these limitations, Brattoli et al. <ref type="bibr" target="#b3">[4]</ref> proposed another setting which excludes classes overlapped with the above testing dataset in pre-training dataset Kinetics. Nevertheless, their overlapped class selection algorithm is too tender, leaving the testing classes still seen in the training. Moreover, new end-to-end training of video backbones is needed because this setting does not follow the official Kinetics data split. Therefore, in this work, we propose a more realistic, convenient and clean ZSAR protocol. Our Proposed Kinetics ZSAR Benchmark. The evolution of the Kinetics dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> naturally involves increment of new action classes: Kinetics-400 and Kinetics-600 datasets contains 400 and 600 action classes, respectively. Due to some renamed, removed or split classes in Kinetics-600, we obtain 220 new action classes outside of Kinetics-400 after cleaning. Therefore, we use 400 action classes in Kinetics-400 as seen classes for training. We randomly split the 220 new classes in Kinetics-600 into 60 validation classes and 160 testing classes respectively. We independently split the classes for three times for robustness evaluation. As shown in our experiments, due to the large-size training and testing sets, the variations of different splits are significantly smaller than previous ZSAR benchmarks. In summary, our benchmark contains 212,577 training videos from Kinetics-400 training set, 2,682 validation videos from Kinetics-600 validation set and 14,125 testing videos from Kinetics-600 testing set on average of the three splits. More details of our evaluation protocol are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For action class embedding, we use a pretrained 12-layer BERT model <ref type="bibr" target="#b8">[9]</ref>, and finetune the last two layers if not specified. For video embedding, we use TSM <ref type="bibr" target="#b16">[28]</ref> pretrained on Kinetics-400 in the spatio-temporal stream for Kinetics benchmark, and BiT image model <ref type="bibr" target="#b12">[24]</ref> pretrained on Ima-geNet for the other three benchmarks to avoid overlapped action classes in Kinetics; the object stream uses BiT image model <ref type="bibr" target="#b12">[24]</ref> pretrained on ImageNet21k <ref type="bibr" target="#b7">[8]</ref> and top-5 objects are selected for each video. The above backbones are fixed for fast training. We use one Nvidia TITAN RTX GPU for the experiments. More details are presented in the supplementary material. We set the dimensionality K = 512 of the common semantic space, ? = 0.1, ? = 1 in the loss and use top-5 objects in the ER loss. We use ADAM algorithm to train the model with weight decay of 1e-4. The  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Existing ZSAR Benchmarks</head><p>We compare our model with: (1) Direct/Indirect Attribute Prediction (DAP, IAP) <ref type="bibr" target="#b14">[26]</ref>; (2) Human Actions by Attribute (HAA) <ref type="bibr" target="#b17">[29]</ref>; (3) Self-training method with SVM and semantic Embedding (SVE) <ref type="bibr" target="#b38">[49]</ref>; (4) Embarrassingly Simple Zero-Shot Learning (ESZSL) <ref type="bibr" target="#b25">[37]</ref>; <ref type="formula">(5)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Kinetics ZSAR Benchmark</head><p>Due to limitations of existing benchmarks, we further carry out extensive experiments on more realistic Kinetics ZSAR setting to evaluate the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Comparison with State of the Arts</head><p>We re-implement state-of-the-art ZSL algorithms on the proposed benchmark, including: (1) DEVISE [12]; (2) ALE <ref type="bibr" target="#b0">[1]</ref>; (3) SJE <ref type="bibr" target="#b1">[2]</ref>; (4) DEM <ref type="bibr" target="#b42">[53]</ref>; (5) ESZSL <ref type="bibr" target="#b25">[37]</ref>; and (6) GCN [17]: a very recent ZSAR work leveraging knowledge graphs of action classes to predict classification weights as <ref type="bibr" target="#b36">[47]</ref>. Details are in the supplementary material. <ref type="table" target="#tab_4">Table 3</ref> shows the ZSAR performances of above methods. When using the same Spatio-Temporal(ST) features extracted from TSM network, our ER-enhanced model with ED and ER loss significantly outperforms previous works with 13.3 and 18.3 absolute gains on top-1 and top-5 accuracies respectively. The existing methods however achieves similar performances, which might due to ambiguous word embedding representations. After fusing with object semantics in video semantic representation, the performance of our model gets another boost, demonstrating that ST visual features and object textual features are complementary.</p><p>Moreover, compared with the results in <ref type="table" target="#tab_2">Table 2</ref>, the performance variations on different splits are much lower than those in previous benchmarks, which further proves the superiority of our benchmark for future ZSAR research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Ablation Studies</head><p>We present the following Q&amp;As to prove the effectiveness of our proposed semantic representations and the ER training objectives. More hyper-parameter ablation and analysis are in the supplementary material. All the ablation studies below are carried out on the Kinetics ZSAR benchmark.</p><p>Is human involvement necessary for action class representation? In <ref type="table" target="#tab_6">Table 4a</ref>, we compare different action class representations including action class names (W N ), Wikipedia entries (Wiki), Dictionary definitions (Dict) and the manually modified EDs. All the models use TSM video features and the L ar objective in Eq. 10 for training. The W N is encoded with pre-trained Glove word embedding while others are encoded by BERT because we observe that BERT is not suitable to encode short text such as the class names. We can see that the automatically crawled texts of the action class are very noisy which are even inferior to the ambiguous class names. However, with a minimal manual clean of crawled descriptions, we achieve significant improvements such as 8.5% absolute gains on top-5 accuracy compared to W N . This proves that even such easy human involvement is beneficial to the class representation quality as justified in Section 3.1, and ED is more discriminative action class prototype than word embedding.</p><p>How much improvements are from the pre-trained BERT model? In <ref type="table" target="#tab_6">Table 4b</ref>, we compare different action class encoding modules for EDs. AvgPool, AttnPool and RNN all transfer knowledge from a pre-trained Glove word embedding, and apply average pooling, attentive weighted pooling and bi-directional GRU respectively to encode the ED sentence. Similar to Table 4a, all the models use TSM video features and are trained with L ar . The pretrained BERT significantly boosts the performance over the other three encoding modules, demonstrating its effectiveness to understand action descriptions.</p><p>Is the ER loss beneficial? <ref type="table" target="#tab_6">Table 4c</ref> compares models trained with or without ER loss. The generalization ability on unseen actions is boosted by a large margin through the ER-enhanced training for both ST and object features. The ER loss augments the semantic labels for videos from automatic elaborative concepts, making the features more generalizable to unseen classes.</p><p>Whether ST features and object features are complementary? The object features alone "Obj" in <ref type="table" target="#tab_6">Table 4d</ref> are comparable with ST features on top-1 accuracy, but are worse than ST on top-5 accuracy. Their combination "ST+Obj" via the proposed multimodal channel attention  Whether EDs are universal representations for both actions and objects? Though we show that ED is beneficial to represent action classes, it remains a question whether ED also improves semantic representation for objects. To be noted, the EDs for objects are automatically extracted from WordNet thanks to the good correspondence between ImageNet classes and WordNet concepts. Therefore, we replace the ED with the class name of the object in Eq. 4 for video object embedding, and in Eq. 11 for the ER training objective. From <ref type="table" target="#tab_6">Table 4e</ref>, we see that even though objects are less ambiguous than actions, it is still beneficial to use its ED instead of its class name.</p><p>We provide more hyper-parameter ablations and analysis in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with Supervised Learning</head><p>Previous ZSAR works mainly benchmark the progress with respect to zero-shot methods. However, it is interesting to know how well the state-of-the-arts ZSAR methods really work from a practical prospective of video action recognition. We present one of the first attempts for this purpose.</p><p>In <ref type="table" target="#tab_8">Table 5</ref>, we compare our ZSAR model with supervised models trained with different numbers of labeled videos of unseen classes in our Kinetics ZSAR benchmark. To avoid overfitting on few training samples, we use the same fixed ST features from TSM and only train a linear classification layer. It servers as a simple but strong baseline for few-shot learning as suggested in <ref type="bibr" target="#b28">[40]</ref>. Our ERenhanced ZSAR model improves over the 1-shot baseline by a large margin, but is still inferior to the model using 2 labeled videos per classes. Although our work is the new state-of-the-arts in <ref type="table" target="#tab_2">Table 2</ref>   ing point from which ZSAR models are comparable with supervised models trained on very few samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present an Elaborative Rehearsal (ER) enhanced model to advance video understanding under the zero-shot setting. Our ER-enhanced ZSAR model leverages Elaborative Descriptions (EDs) to learn discriminative semantic representation for action classes, and generates Elaborative Concepts (ECs) from prior knowledge of image-based classification to learn generalizable video semantic representations. Our model achieves state-of-the-art performances on existing ZSAR benchmarks as well as our newly proposed more realistic ZSAR setting based on the Kinetics dataset. We demonstrated the potential that our new state-of-the-art on ZSAR benchmarks start to catch up with the supervised AR baselines. In the future, we will explore the unification of zero-shot and few-shot for action recognition.</p><p>[11] <ref type="bibr">Christoph Feichtenhofer, Haoqi</ref>  [21] Mihir Jain, Jan C van Gemert, Thomas Mensink, and Cees GM Snoek. Objects2action: Classifying and localizing actions without any video example. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Proposed Kinetics ZSAR Benchmark</head><p>We use Kinetics-400 <ref type="bibr" target="#b5">[6]</ref> as the training dataset and the associated 400 action classes as seen classes. The new classes in Kinetics-600 <ref type="bibr" target="#b4">[5]</ref> are used as unseen classes. Due to some renamed, removed or split classes in the evolution from Kinetics-400 to Kinetics-600, it is problematic to obtain new classes by simply selecting action classes that are not in the original (400) action names set. In these ambiguous classes, the videos are still the same even if the class names are different in Kinetics-600. Therefore, we further use the overlapping videos as additional cues to find new classes in Kinetics-600. In this way, we obtained 220 new action classes outside of Kinetics-400.</p><p>As mentioned in <ref type="bibr" target="#b37">[48]</ref>, it is necessary to hold a validation class split that is disjoint from the training and testing classes, to tune hyper-parameters of the zero-shot methods. Therefore, we randomly split the 220 new classes in Kinetics-600 into the 60 validation and 160 testing classes. To avoid the potential bias in only one split, we independently split the classes for three times to improve the robustness of evaluation. The validation and testing videos are from the original Kinetics-600 splits respectively. To be noted, since the training set is the same for the three splits, <ref type="table" target="#tab_0"># videos  split1  split2  split3   train  400  212,577 212,577 212,577  val  60  2,670  2,712  2,663  test  160  14,131  14,078</ref> 14,167 the ZSAR methods only need to train once on the training set, and then different validation sets are used to select the best models for the respective testing sets. The dataset statistics of the three splits are shown in <ref type="table" target="#tab_10">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Our Implemented Baseline Models</head><p>As there is no baseline to compare on our newly proposed Kinetics ZSAR benchmark, we implement the following state-of-the-art ZSL algorithms: <ref type="formula" target="#formula_0">(1)</ref>   where ?(y n , y) = 0 if y n = y otherwise 0.2. ALE <ref type="bibr" target="#b0">[1]</ref> uses weighted approximate ranking objective:</p><formula xml:id="formula_16">F (v, y; W ) = ?(v) T W ?(y)<label>(14)</label></formula><formula xml:id="formula_17">y?S l r ?(v n ,y n ) r ?(v n ,y n ) [?(y n , y) + F (v n , y; W ) ? F (v n , y n ; W )] + (16) where l k = k i=1</formula><p>? i and r ?(v n ,y n ) is defined as:   </p><formula xml:id="formula_18">1 N N i=1 ||?(v n ) ? f 1 (W 2 f 1 (W 1 ?(y n ))|| 2 + ?(||W 1 || 2 + ||W 2 || 2 )<label>(19)</label></formula><p>ESZSL <ref type="bibr" target="#b25">[37]</ref> applies a square loss to the pairwise ranking formulation and adds regularization terms to optimize:</p><formula xml:id="formula_19">?||W ?(y)|| 2 + ?||?(v) T W || 2 + ?||W || 2<label>(20)</label></formula><p>There exists a closed form solution for the objective. GCN [17] is a very recent ZSAR work which builds knowledge graphs for action classes to predict classification weights as <ref type="bibr" target="#b36">[47]</ref>. We use the first type of knowledge graphs as their work, which is built based on similarity of class embeddings. Six GCN layers are used to predict classification weights from the built graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Ablation Studies</head><p>Multimodal-based Channel Attention.</p><p>In <ref type="table" target="#tab_13">Table 7</ref>, we compare our ER-enhanced models with or without multimodal-based channel attention in video semantic representation encoding in Section 3.3 of our main paper. The comparison shows that the proposed channel attention is beneficial to generate better video semantic representations from the ST and object streams. ER loss. <ref type="table" target="#tab_14">Table 8</ref> presents additional models (using spatialtemporal and object video representations) trained with or without ER loss. The trend is the same as <ref type="table" target="#tab_6">Table 4c</ref> in the main paper. The ER loss improves the generalization ability on unseen actions by 2.6% on Top-1 accuracy and 3.0% on Top-5 accuracy.   Number of Object Concepts. <ref type="table" target="#tab_16">Table 9</ref> presents ZSAR performances using different numbers of object concepts predicted in the object stream of video encoding and ER loss respectively. We can see that the ZSAR performance first increases with the number of objects and then decreases, which might result from incorrectly detected (false positive) object concepts. Different Object Concepts. We compare different sets of object concepts in <ref type="table" target="#tab_16">Table 9</ref>. In our main paper, we use the full concept set in ImageNet21k from the BiT model. We compare it with concepts in ImageNet1k from Resnext50 6 image classification model. The predicted concepts of the latter are not as accurate as the former due to less training data and fewer concept classes. When only using the object concepts as video semantic representation, we can see that ZSAR performance of the ImageNet1k concepts are much worse than that of ImageNet21k and ST features. It indicates that the object concepts set and recognition performance are important. Though objects from ImageNet1k alone are not competitive, they are still complementary to ST video features. The combination of object and ST feature in our full ER model also achieves better performance.    Different Spatio-Temporal(ST) Features. We further verify the generalization of our approach on different ST features. <ref type="table" target="#tab_0">Table 11</ref> shows the results. We compare the TSM model and an enhanced TSM with non-local attentions for ST feature extraction. Better ST features are beneficial to the ZSAR performance. Number of Finetuned Layers in BERT Model. As shown in <ref type="figure" target="#fig_5">Figure 4a</ref>, finetuning more layers in BERT continuously improves the performance, which however consumes more resources, e.g. we use 1 RTX 2080Ti to finetune 2 layers in BERT, but need 4 GPUs to finetune 6 layers. ? for Elaborative Rehearsal Loss. <ref type="figure" target="#fig_5">Figure 4b</ref> presents the performance of different ?s for the ER loss, which suggests that the ER loss is better to set as equal contributions as the action classification loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of our ER-enhanced ZSAR model. The action class embedding function (left) extends action names to EDs towards action class embedding z. The multimodal video embedding function generates spatio-temporal and object features [xvo, xov] (middle) for the video. The ER loss utilizes recognized object semantics zo (right) to match [xvo, xov] which improves the action recognition loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Structured Joint Embedding (SJE)<ref type="bibr" target="#b1">[2]</ref>;<ref type="bibr" target="#b5">(6)</ref> Multi-Task Embedding (MTE)<ref type="bibr" target="#b40">[51]</ref>;<ref type="bibr" target="#b6">(7)</ref> Zero-Shot with Error-Correcting Output Codes (ZSECOC)<ref type="bibr" target="#b23">[35]</ref>; (8) Universal Representation (UR) model<ref type="bibr" target="#b43">[54]</ref>; (9) Objects2Action (O2A) [21]; (10) Alternative Semantic Representation (ASR)<ref type="bibr" target="#b34">[45]</ref>, which uses text descriptions and images as alternative class embedding; (11) TS-GCN [15] which builds graphs among action and object classes with ConceptNet for better action class embedding; (12) End-to-End Training (E2E)<ref type="bibr" target="#b3">[4]</ref> which uses a reduced Kinetics training set by excluding part of action classes overlapped with testset. All above methods are evaluated on the inductive ZSL setting, where the videos of unseen action classes are unavailable during training. The unseen action classes are not used in training except[15].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The annotation interface to collect Elaborative Descriptions (ED) for action classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>y?S 1 (</head><label>1</label><figDesc>F (v n , y; W ) + ?(y n , y) ? F (v n , y n ; W )) (17)We use ? i = 1/i which puts a high emphasis on the top of the rank list. SJE<ref type="bibr" target="#b1">[2]</ref> uses hard negative label mining with the training objective as follows:max y?S [?(y n , y) + F (v n , y; W ) ? F (v n , y n ; W )] + (18) model Top-1 (%) Top-5 (%) w/o MCA 41.0 ? 1.7 71.9 ? 0.7 w/ MCA 42.1 ? 1.4 73.1 ? 0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) # of finetuned layers.(b) lambda of ER loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Top-1 accuracy for different hyper-parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of Elaborative Descriptions (ED) for action classes and object concepts.</figDesc><table><row><cell>sentations for ZSAR, we group existing works according</cell></row><row><cell>to types of semantic representation of actions. The first</cell></row><row><cell>type takes manual-defined attributes [29, 52] to represent</cell></row><row><cell>an action. Gan et al. [14] improve attribute detection via</cell></row><row><cell>multi-source domain generalization. Nevertheless, the at-</cell></row><row><cell>tributes of actions are harder to define compared with the</cell></row><row><cell>image counterparts. The second type then exploits objects</cell></row><row><cell>as attributes. Jain et al. [21] detect objects in videos and</cell></row><row><cell>associate videos to action category with maximum object</cell></row><row><cell>similarity. Gan et al. [13] propose to select discriminative</cell></row><row><cell>concepts. Gao et al. [15] utilize graph networks to learn</cell></row><row><cell>action-object relationships and then match objects in the</cell></row><row><cell>video with action prototypes. Though effective, the above</cell></row><row><cell>work ignore spatio-temporal relationships in videos and ac-</cell></row><row><cell>tions. The third type of approaches uses word embedding of</cell></row><row><cell>action names [4, 30, 35, 50] as semantic representation. Qin</cell></row><row><cell>et al. [35] derive error-correcting output codes for actions</cell></row><row><cell>via both category-level embedding and intrinsic data struc-</cell></row><row><cell>tures. The recent work</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ZSAR performances on the three existing benchmarks.</figDesc><table><row><cell>Video: fisher vector (FV), bag of words (BoW), object (Obj), im-</cell></row><row><cell>age spatial feature (S),</cell></row></table><note>* (trained on video datasets), ? (trained on ImageNet dataset); Class: attribute (A), word embedding of class names (WN ), word embedding of class texts (WT ), elaborative description (ED). The average top-1 accuracy (%) ? standard de- viation are reported.base learning rate is 1e-4 with warm-up and cosine anneal- ing. The model was trained for 10 epochs except on the Olympic Sports dataset where we train 100 epochs due to its small training size. The best epoch is selected accord- ing to performance on the validation set. Top-1 and top-5 accuracies (%) are used to evaluate all models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 presents</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Video</cell><cell>Class</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>DEVISE [12]</cell><cell></cell><cell></cell><cell cols="2">23.8 ? 0.3 51.0 ? 0.6</cell></row><row><cell>ALE [1]</cell><cell></cell><cell></cell><cell cols="2">23.4 ? 0.8 50.3 ? 1.4</cell></row><row><cell>SJE [2] DEM [53]</cell><cell>ST</cell><cell>WN</cell><cell cols="2">22.3 ? 0.6 48.2 ? 0.4 23.6 ? 0.7 49.5 ? 0.4</cell></row><row><cell>ESZSL [37]</cell><cell></cell><cell></cell><cell cols="2">22.9 ? 1.2 48.3 ? 0.8</cell></row><row><cell>GCN [17]</cell><cell></cell><cell></cell><cell cols="2">22.3 ? 0.6 49.7 ? 0.6</cell></row><row><cell>Ours</cell><cell>ST ST+Obj</cell><cell>ED</cell><cell cols="2">37.1 ? 1.7 69.3 ? 0.8 42.1 ? 1.4 73.1 ? 0.3</cell></row></table><note>the comparison. To avoid leaking infor- mation from features pretrained on Kinetics video dataset, we only use image features and predicted objects from a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>ZSAR performance on the proposed Kinetics benchmark. Notations are the same asTable 2; ST: spatio-temporal feature.</figDesc><table><row><cell>2D network pretrained on ImageNet [24] for video semantic</cell></row><row><cell>representation learning. The proposed ER-enhanced ZSAR</cell></row><row><cell>model achieves consistent improvements over state-of-the-</cell></row><row><cell>art approaches on three benchmarks. Our model outper-</cell></row><row><cell>forms previous best performances (without using pretrained</cell></row><row><cell>video features) with 0.4, 10.9 and 17.6 absolute gains on</cell></row><row><cell>OlympicSports16, HMDB51 and UCF101 respectively, and</cell></row><row><cell>achieves even better performance than E2E trained on large-</cell></row><row><cell>scale Kinetics dataset with 2.6 and 3.8 gains on HMDB51</cell></row><row><cell>and UCF101 datasets</cell></row></table><note>1 . This demonstrates the effectiveness of our ED as action semantic representation and the ER ob- jective to improve generalization ability of the model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>? 0.4 54.7 ? 1.2 Wiki 25.8 ? 1.1 50.4 ? 1.6 Dict 22.3 ? 0.4 49.7 ? 0.6 ED 31.0 ? 1.2 63.2 ? 0.4 ? 1.2 54.7 ? 0.6 AttnPool 28.2 ? 1.0 56.9 ? 0.2 RNN 25.4 ? 1.1 53.7 ? 1.1 BERT 31.0 ? 1.2 63.2 ? 0.4</figDesc><table><row><cell>Class Rep</cell><cell>top-1</cell><cell>top-5</cell><cell>Class Enc</cell><cell>top-1</cell><cell>top-5</cell><cell cols="2">Video ER</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell cols="2">WN 26.5 (a) Comparing action class texts.</cell><cell cols="4">AvgPool 25.3 (b) Comparing action class encoders.</cell><cell cols="4">ST ST Obj Obj (c) Comparing models with or without ER loss. w/o 31.0 ? 1.2 63.2 ? 0.4 w/ 37.1 ? 1.7 69.3 ? 0.8 w/o 34.6 ? 1.4 60.6 ? 1.1 w/ 36.7 ? 1.0 63.2 ? 0.5</cell></row><row><cell>Video</cell><cell>top-1</cell><cell>top-5</cell><cell></cell><cell></cell><cell>Video</cell><cell>Loss</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>ST</cell><cell cols="2">37.1 ? 1.7 69.3 ? 0.8</cell><cell></cell><cell></cell><cell cols="4">Obj (Name) ER (Name) 34.5 ? 1.6 61.4 ? 1.2</cell></row><row><cell>Obj</cell><cell cols="2">36.7 ? 1.0 63.2 ? 0.5</cell><cell></cell><cell></cell><cell>Obj (ED)</cell><cell cols="3">ER (Name) 36.3 ? 1.3 62.8 ? 0.9</cell></row><row><cell cols="3">ST + Obj 42.1 ? 1.4 73.1 ? 0.3</cell><cell></cell><cell></cell><cell>Obj (ED)</cell><cell>ER (ED)</cell><cell cols="2">36.7 ? 1.0 63.2 ? 0.5</cell></row><row><cell cols="3">(d) Comparing video representations.</cell><cell></cell><cell></cell><cell cols="5">(e) Comparing EDs and class names to represent object classes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the Kinetics ZSAR benchmark.</figDesc><table /><note>achieves the best performance on the Kinetics ZASR set- ting. This shows that object features alone are not discrim- inative enough, compared to ST features, to differentiate actions. But adding object features enriches ST with the shared semantic embedding among the action classes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our ER-enhanced ZSAR model and supervised few-shot baselines on the Kinetics benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE international conference on computer vision, pages 6202-6211, 2019.</figDesc><table><row><cell>1, 2</cell></row><row><cell>[12] Andrea Frome, Greg S Corrado, Jon Shlens, Samy</cell></row><row><cell>Bengio, Jeff Dean, Marc'Aurelio Ranzato, and Tomas</cell></row><row><cell>Mikolov. Devise: A deep visual-semantic embedding</cell></row><row><cell>model. In Advances in neural information processing</cell></row><row><cell>systems, pages 2121-2129, 2013. 1, 2, 3, 7, 12</cell></row><row><cell>[13] Chuang Gan, Ming Lin, Yi Yang, Gerard De Melo,</cell></row><row><cell>and Alexander G Hauptmann. Concepts not alone:</cell></row><row><cell>Exploring pairwise relationships for zero-shot video</cell></row><row><cell>activity recognition. In Thirtieth AAAI conference on</cell></row><row><cell>artificial intelligence, 2016. 3</cell></row><row><cell>[14] Chuang Gan, Tianbao Yang, and Boqing Gong. Learn-</cell></row><row><cell>ing attributes equals multi-source domain generaliza-</cell></row><row><cell>tion. In Proceedings of the IEEE conference on com-</cell></row><row><cell>puter vision and pattern recognition, pages 87-97,</cell></row><row><cell>2016. 3</cell></row><row><cell>[15] Junyu Gao, Tianzhu Zhang, and Changsheng Xu.</cell></row><row><cell>I know the relationships: Zero-shot action recogni-</cell></row><row><cell>tion via two-stream graph convolutional networks and</cell></row><row><cell>knowledge graphs. In Proceedings of the AAAI Con-</cell></row><row><cell>ference on Artificial Intelligence, volume 33, pages</cell></row><row><cell>8303-8311, 2019. 2, 3, 4, 6</cell></row><row><cell>[16] Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan.</cell></row><row><cell>Large-scale weakly-supervised pre-training for video</cell></row><row><cell>action recognition. In Proceedings of the IEEE Con-</cell></row><row><cell>ference on Computer Vision and Pattern Recognition,</cell></row><row><cell>pages 12046-12055, 2019. 1</cell></row><row><cell>[17] Pallabi Ghosh, Nirat Saini, Larry S Davis, and Abhi-</cell></row><row><cell>nav Shrivastava. All about knowledge graphs for ac-</cell></row><row><cell>tions. arXiv preprint arXiv:2008.12432, 2020. 7, 12,</cell></row><row><cell>13</cell></row><row><cell>[18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.</cell></row><row><cell>Deep learning. MIT press, 2016. 2</cell></row><row><cell>[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian</cell></row><row><cell>Sun. Deep residual learning for image recognition.</cell></row><row><cell>In Proceedings of the IEEE conference on computer</cell></row><row><cell>vision and pattern recognition, pages 770-778, 2016.</cell></row><row><cell>2</cell></row><row><cell>[20] Dat Huynh and Ehsan Elhamifar. Fine-grained gener-</cell></row><row><cell>alized zero-shot learning via dense attribute-based at-</cell></row><row><cell>tention. In Proceedings of the IEEE/CVF Conference</cell></row><row><cell>on Computer Vision and Pattern Recognition, pages</cell></row><row><cell>4483-4493, 2020. 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics of our Kinetics ZSAR benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Comparison of ER-enhanced models with or without multimodal-based channel attention (MCA) on Kinetics ZSAR benchmark.</figDesc><table><row><cell>Video</cell><cell>ER</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>ST+Obj</cell><cell cols="3">w/o 39.5 ? 1.4 70.1 ? 0.6 w/ 42.1 ? 1.4 73.1 ? 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Comparison of ER-enhanced models with or without ER loss on Kinetics ZSAR benchmark.DEM<ref type="bibr" target="#b42">[53]</ref> uses the visual space as the embedding space, which learns a non-linear mapping from class features to visual features and minimizes the model with MSE loss:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>? 1.4 70.1 ? 0.6 5 1 41.5 ? 1.9 70.9 ? 1.0 5 5 42.1 ? 1.4 73.1 ? 0.3</figDesc><table><row><cell cols="3"># objects Top-1 (%) Top-5 (%) VE ER</cell></row><row><cell cols="3">5 39.5 5 0 10 41.0 ? 1.6 72.0 ? 1.2</cell></row><row><cell>0</cell><cell>5</cell><cell>37.1 ? 1.7 69.3 ? 0.8</cell></row><row><cell>1</cell><cell>5</cell><cell>37.6 ? 1.0 68.9 ? 0.8</cell></row><row><cell>5</cell><cell>5</cell><cell>42.1 ? 1.4 73.1 ? 0.3</cell></row><row><cell>10</cell><cell>5</cell><cell>42.0 ? 1.3 72.3 ? 0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Comparison of ER-enhanced models using different numbers of objects in object stream of video encoding (VE) and ER loss (ER) on our Kinetics ZSAR benchmark.</figDesc><table><row><cell cols="2">Model Video</cell><cell>Loss</cell><cell cols="2">ObjSet Top-1 (%) Top-5 (%)</cell></row><row><cell>ST</cell><cell></cell><cell>AR</cell><cell>-</cell><cell>31.0 ? 1.2 63.2 ? 0.4</cell></row><row><cell>Obj</cell><cell cols="2">AR + ER</cell><cell>1K</cell><cell>24.8 ? 0.7 51.7 ? 0.7</cell></row><row><cell>Obj</cell><cell cols="2">AR + ER</cell><cell>21K</cell><cell>36.7 ? 1.0 63.2 ? 0.5</cell></row><row><cell cols="3">ST + Obj AR + ER</cell><cell>1K</cell><cell>34.7 ? 1.1 67.4 ? 1.0</cell></row><row><cell cols="3">ST + Obj AR + ER</cell><cell>21K</cell><cell>42.1 ? 1.4 73.1 ? 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Comparison of using different sets of object concepts ("ObjSet") in our ER model on our Kinetics ZSAR benchmark.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>? 1.2 63.2 ? 0.4 ST (NL) AR + ER 32.0 ? 0.9 63.9 ? 0.6 ST + Obj AR + ER 42.1 ? 1.4 73.1 ? 0.3 ST (NL) + Obj AR + ER 42.7 ? 1.6 73.3 ? 0.6</figDesc><table><row><cell>Model Video</cell><cell>Loss</cell><cell>Top-1 (%) Top-5 (%)</cell></row><row><cell>ST</cell><cell>AR</cell><cell>31.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11 :</head><label>11</label><figDesc>Comparison of using different Spatio-Temporal (ST) features on Kinetics ZSAR benchmark. NL denotes non-local.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We observe large performance variations with different random weight initialization, which mainly results from the small training set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://pytorch.org/docs/stable/torchvision/ models.html#classification</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract number D17PC00340.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Construction of Elaborative Description</head><p>Different from the ImageNet object concepts universally defined in standard dictionaries, there are no standard sources to define action classes. We collect Elaborative Descriptions (ED) for action classes in two steps: firstly automatically crawling candidate sentences to describe action classes from the Internet; then manually selecting or modifying a minimum set of candidate sentences as the EDs. We release the collected EDs publicly 2 .</p><p>In the first crawling step, we utilize Wikipedia and online dictionaries. Given an action class such as "dumpster diving" as query, we use Wikipedia crawling toolkit 3 to collect summary of the first page returned by Wikipedia. This page is usually useful for describing novel actions such as "photobombing" and collocations such as "clean and jerk". We also let Wikipedia find a relevant page title for the query in case no exact page is matched with the query. But to be noted, the returned page can be noisy, especially for compositional action classes. For example, the query "assembling computer" gets the page "assembly language" in computer science. Therefore, we further crawl dictionary definitions 4 for words and phrases in the query action class. We split crawled data into candidate sentences via spacy toolkit 5 , and remove non-ascii letters in each sentence.</p><p>In the second cleaning step, we represent candidate sentences and a video exemplar in a webpage to annotators as shown in <ref type="figure">Figure 3</ref>. We ask the annotator to select or modify a minimum set of candidate sentences to describe the action class. If no candidate sentences are qualified, the annotator can write a new definition. It takes less than 20s on average to generate the ED per action class. The average length of EDs for actions in the Kinetics dataset is 36 words.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the relationship between recognition speed and accuracy for words rehearsed via rote versus elaborative rehearsal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bjork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">638</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking zeroshot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4613" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A short note about kinetics-600. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03945,2020.4</idno>
		<title level="m">Does bert solve commonsense task via commonsense knowledge? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4588" to="4596" />
		</imprint>
	</monogr>
	<note>2 Proceedings of the IEEE international conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Zero-shot action recognition in videos: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><surname>Valter Lu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menotti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06423</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot recognition using dual visual-semantic mapping paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuetan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3279" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3337" to="3344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for generalized zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devraj</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Kumar Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaib</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9985" to="9993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Zest: Zero-shot learning from text descriptions using textual similarity and visual summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzuf</forename><surname>Paz-Argaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03276</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">and Anton Van Den Hengel. Less is more: zero-shot learning from online textual documents with noise suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot action recognition with error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Twostream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV 16</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Alternative semantic representations for zero-shot human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zeroshot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic embedding space for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transductive zero-shot action recognition by wordvector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="333" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-task zero-shot action recognition with prioritised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero-shot activity recognition with verb attribute induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="946" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9436" to="9445" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
