<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FOSNet: An End-to-End Trainable Deep Neural Network for Scene Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongje</forename><surname>Seong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Hyun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Members, IEEE</roleName><forename type="first">Euntai</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">FOSNet: An End-to-End Trainable Deep Neural Network for Scene Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-scene recognition</term>
					<term>convolutional neural network</term>
					<term>fusion network</term>
					<term>scene coherence</term>
					<term>end-to-end trainable</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene recognition is an image recognition problem aimed at predicting the category of the place at which the image is taken. In this paper, a new scene recognition method using the convolutional neural network (CNN) is proposed. The proposed method is based on the fusion of the object and the scene information in the given image and the CNN framework is named as FOS (fusion of object and scene) Net. In addition, a new loss named scene coherence loss (SCL) is developed to train the FOSNet and to improve the scene recognition performance. The proposed SCL is based on the unique traits of the scene that the 'sceneness' spreads and the scene class does not change all over the image. The proposed FOSNet was experimented with three most popular scene recognition datasets, and their state-ofthe-art performance is obtained in two sets: 60.14% on Places 2 and 90.37% on MIT indoor 67. The second highest performance of 77.28% is obtained on SUN 397.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S CENE recognition is one of the most spotlighted topics in image recognition, applied to image retrieval, autonomous robot, and drone. Many studies have explored the scene recognition; however, most of them have some drawbacks: (1) they consider scene recognition as a simple image recognition problem, and (2) they applied the general CNN or image recognition methods to scene recognition without exploiting the unique traits of scene images <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>.</p><p>In the last few years, some studies have used the scene image traits to improve scene recognition. For example, the scene image traits that a scene image consists of a combination of several objects and the objects in the image possesses much information about the category of the scene was used in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The traits that the scene categories labels are inherently ambiguous was used in <ref type="bibr" target="#b6">[7]</ref>. The traits of naturalness and openness in scene images were exploited in <ref type="bibr" target="#b7">[8]</ref>. Unfortunately, however, most of the existing methods used scene traits to extract scene features manually, and only a limited number of studies trained the features of scene images with the scene traits and extracted them automatically.</p><p>In this paper, a new scene recognition framework named FOSNet is proposed. The FOSNet improves the scene recognition performance by exploiting the unique traits of the scene images; the traits that FOSNet uses are summarized as follows: (T1) The sceneness spreads all over the image, and the classes of the scene are the same to the entire image. This is contrary in the object images, where the objectness appears only at specific locations in an image. Thus, the classes of the objects change from patch to patch in the same image. This trait is named scene coherence (SC) in the scene image. (T2) Various objects can appear on the scene image, and when a specific object is found, the scene image belongs to a particular class. For example, if a chair is found, the corresponding image is unlikely to be a 'mountain' or a 'playground', but it is likely to be a 'meeting room' or a 'classroom'.</p><p>Based on (T1), a scene coherence loss (SCL) is developed to train the FOSNet. This SCL favors the case in which the class of the scene is the same all over the image and it penalizes the case in which the classes of the adjacent patches in a single image are different from each other. Based on (T2), the correlative context gating (CCG) is developed to combine the object and scene features in a given image. The fusion of the object and scene features in the image is not new but it has been reported in the previous works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. However, the proposed CCG is completely different from the previous works, as the previous works combined the object and scene features in the image by simply concatenating two features and increasing the feature dimension. This kind of simple fusion or concatenation could increase the redundancy among features and might degrade the recognition performance. On the other hand, the CCG combines the two features in an efficient manner, applying the attention concept to the fusion of two features. The CCG is an improved version of the class conversion matrix (CCM) <ref type="bibr" target="#b8">[9]</ref>, and it also determines the relative importance of the features during training.</p><p>The contributions of FOSNet are as follows:</p><p>1) The traits of scene coherence (SC) in a scene image are defined, and a new loss SCL is developed based on the trait. The SCL is the only loss specialized for scene recognition and it is applied to train the FOSNet. 2) A new fusion framework named CCG is proposed to combine the object and scene features from the image. Unlike the previous fusion methods in which the two features are simply concatenated and the classifier is designed for the features, the CCG selects important features and fuses the two sets of features effectively for training.</p><p>The rest of the paper is organized as follows: Section II provides a brief review of the related studies. Section III arXiv:1907.07570v2 [cs.CV] 18 Jul 2019 explained FOSNet in detail. Section IV applies the FOSNet to three benchmark problems, and the performance of the FOSNet is demonstrated through experimentation. Section V conducts some ablation study to verify the value of our proposed SCL and CCG. Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section, we review previous works on scene recognition with an emphasis on (1) the application of scene traits and (2) a combination of the object and scene information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Traits in Scene Image</head><p>Using object information in an image is the most utilized scene traits for scene recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. When a particular object appears in an image, the chance of the image belonging to a certain category associated with the object increases. For example, if a TV is detected, the chance of the image being in a living room increases. In the previous works, the objects features were used for scene recognition instead of detecting the objects directly. To extract the object features, large image datasets for object recognition are used and they are ImageNet <ref type="bibr" target="#b9">[10]</ref>, PASCAL visual object classes (VOC) <ref type="bibr" target="#b10">[11]</ref>, or Microsoft COCO <ref type="bibr" target="#b11">[12]</ref>.</p><p>Previous studies also used other scene traits: The analysis of object scales in the scene images was utilized in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b12">[13]</ref>. In <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, the number of CNN input patches was adjusted by considering several objects in the scene image. To capture recurring visual elements and salient objects in scene recognition, the deformable part-based model (DPM) was utilized in <ref type="bibr" target="#b16">[17]</ref>. In addition, the traits that features appearing in each image region within scene images are all similar was used in <ref type="bibr" target="#b17">[18]</ref>. A super category was proposed to solve the problem that the scene categories have label ambiguity in <ref type="bibr" target="#b6">[7]</ref>. A deep gaze shifting kernel was developed to distinguish sceneries from different categories in <ref type="bibr" target="#b18">[19]</ref>. As such, the traits of the scene image are very diverse, and there seem to be still many available unused scene traits in scene recognition studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Fusion in Scene Recognition</head><p>In order to combine scene and object features for scene recognition, the effective feature fusion is great importance and several fusion methods have been reported. For example, the two features extracted by two different CNNs were combined at feature level by summing or concatenating the features in <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Then, the classical classifiers such as support vector machine (SVM) <ref type="bibr" target="#b23">[24]</ref> was applied to the fused features. Unfortunately, these methods have some drawbacks that they cannot be trained in an end-to-end manner. Moreover, the simple summation or concatenation might degrade the recognition performance owing to redundancy in two feature sets.</p><p>Recently, the CCM has learned a relationship between an object feature and scene feature through training and converted object features into scene features <ref type="bibr" target="#b8">[9]</ref>. By doing so, the two features in different domains are transformed into the same domain, and the fusion is performed through the elementwise sum operation. In the CCM, objects in the images do not need to be labeled in the scene recognition dataset; only a pre-trained CNN trained on the object recognition dataset is enough and the relationship between scene and objects is trained in a weakly supervised way. In this paper, a new fusion method named correlative context gating (CCG) is proposed. The CCG is an extended version of the CCM and it generates more accurate scene features than CCM using the concept of attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODS</head><p>In this section, a new scene recognition network named FOSNet is proposed. The overall FOSNet structure is shown in <ref type="figure">Fig. 1</ref>. As shown in the figure, FOSNet has two input streams. In the upper stream named ObjectNet, the features of the objects in the scene images are extracted. In the lower stream named PlacesNet, scene features are extracted. In a trainable fusion module, two streams of features are fused into a combined feature for scene recognition. The FOSNet consists of ObjectNet, PlacesNet, and trainable fusion modules, and all networks can be trained in an end-to-end manner. The three subnets are explained in detail in the subsequent subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ObjectNet</head><p>Based on the scene traits (T2), information about the objects that appear in the scene is exploited in FOSNet. To obtain a highly discriminating object descriptor, ObjectNet is utilized in the upper stream of <ref type="figure">Fig. 1</ref> to extract a feature of the objects in a scene image. As the ObjectNet, the popular CNN models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref> pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref> are used, as shown in <ref type="figure">Fig. 2</ref>. An object feature extracted through ObjectNet is fed into the trainable fusion module. In the structure given in <ref type="figure">Fig.  2</ref>, not only the object feature but also the object score can be fed into the trainable fusion module. Detailed description of the fusion level is given in Section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PlacesNet</head><p>PlacesNet is another CNN model and it extracts a scene feature from an image. The PlacesNet is pre-trained using Places 2 <ref type="bibr" target="#b27">[28]</ref> and its structure is the same as that of the ObjectNet, as shown in <ref type="figure">Fig. 3</ref>(a). To train the PlacesNet, scene coherence loss (SCL) is developed in this paper. The SCL is a new loss tailored for scene recognition, and it is based on the scene trait that objectness focused on specific parts of an image, whereas sceneness unfocused on specific parts. However, it spreads all over the image. In particular, the class of the scene is unchanged over the image. This trait is named the coherence in the scene of an image, and the SCL embodies this trait into a single loss. For example, let us consider an image shown in <ref type="figure">Fig. 4</ref>. When the whole image is divided into nine grids, all nine grids cannot have different scene classes and all of them have the same scene class of a baseball field. That is, the scene class should be coherent all over the image. The scene coherence is a unique trait of the scene image and it is formulated into a new loss SCL:</p><formula xml:id="formula_0">L SC L = 1 C C c=1 1 (N ? 1) M + N (M ? 1) ? N ?1 n=1 M m=1 o n+1,m,c ? o n,m,c 2 + N n=1 M?1 m=1 o n,m+1,c ? o n,m,c 2<label>(1)</label></formula><p>where N and M are the numbers of grid cells in the vertical and horizontal directions, respectively; C is the number of classes; o n,m,c denotes the classification result for the class c in the grid cell (n, m), as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. As stated, the SCL defined in Eq. 1 favors the case in which all the grids have the same scene class, whereas it penalizes the case in which the adjacent grids have the different scene classes.</p><formula xml:id="formula_1">1,1, 1,2, ? ? ? 1, , 2,1, 2,2, ? ? ? 2, , ? ? ? ? ? ? ? ? ? ,1, ,2, ? ? ? , ,</formula><p>Scene coherence loss at class</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene scores</head><p>For each class Here, when we apply SCL in Eq. 1 to the PlacesNet training, a difficulty arises; The PlacesNet should be applied to N ? M grid cells separately and repeatedly and it leads to the waste of computation time. To resolve this inefficiency, the PlacesNet in <ref type="figure">Fig. 3</ref>(a) is converted into the form of a fully convolutional network, as shown in <ref type="figure">Fig. 3</ref>(b). The conversion is motivated by class activation map (CAM) <ref type="bibr" target="#b28">[29]</ref> and it can be applied to any CNNs, in which the last layers are global average pooling (GAP) followed by fully connected (FC) layers. In the PlacesNet, the input image with the size of 224 ? 224 is reduced to 7 ? 7 feature map after going through five pooling operations in convolutional layers. Then, the scene scores for each 7 ? 7 grid cell is obtained by replacing the last GAP-FC sequence with 1 ? 1 convolution. Then, a scene score for the entire image is computed by applying the GAP to the tensors obtained from 1 ? 1 convolution.</p><p>Interestingly, it can be shown that the PlacesNet with the GAP followed by FC shown in <ref type="figure">Fig. 3</ref>(a) outputs the same result with the converted version with the 1 ? 1 convolution followed by GAP shown in <ref type="figure">Fig. 3</ref>(b). With slightly relaxed notation, the feature tensor extracted from the PlacesNet in <ref type="figure">Fig. 3</ref>(a) is represented into</p><formula xml:id="formula_2">X last = x last n,m,d = x last 1,1,1:D x last 1,2,1:D ? ? ? x last 1, M,1:D x last 2,1,1:D x last 2,2,1:D ? ? ? x last 2, M,1:D . . . . . . . . . x last N,1,1:D x last N,1,1:D ? ? ? x last N, M,1:D ? R N ?M?D<label>(2)</label></formula><p>where N?M is the feature map size extracted from convolution layers in <ref type="figure">Fig. 3(a)</ref>; D is the number of output channels of last convolution layer; x last n,m,1:D ? R D denotes a feature vector at position (n, m) of X last ; 1 : D in the third axis of x last n,m,1:D , is a collection of all the elements accumulated over D channels and it is actually a vector. Let the trainable parameters W = w c,d ? R C?D and b = (b c ) ? R C be weight and bias, respectively, for the FC layer of the model in </p><formula xml:id="formula_3">O = FC GAP X last , W, b = FC 1 N M N n=1 M m=1 x last n,m,1:D , W, b =W 1 N M N n=1 M m=1</formula><p>x last n,m,1:</p><formula xml:id="formula_4">D + b = 1 N M N n=1 M m=1 W x last n,m,1:D + b = 1 N M N n=1 M m=1 Conv 1?1 X last , W, b n,m = GAP Conv 1?1 X last , W, b = O<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">Conv 1?1 X last , W, b = W x last 1,1,1:D + b ? ? ? W x last 1, M,1:D + b . . . . . . W x last N,1,1:D + b ? ? ? W x last N, M,1:D + b ? R N ?M?C (4)</formula><p>is a tensor obtained by applying 1?1 convolution with weights (W, b) to input X last , and it is also the classification results for each grid cell shown in <ref type="figure">Fig. 3(b)</ref>. Since O and O have the same values, the model in <ref type="figure">Fig. 3</ref>(b) performs the same classification as the one in <ref type="figure">Fig. 3</ref>(a) and it has an advantage of obtaining classification results O Conv 1?1 X last , W, b ? R N ?M?C for all grid cells without applying PlacesNet to all grid cells repeatedly. Here, classification error is defined using the crossentropy loss and it is denoted by</p><formula xml:id="formula_6">L C = ? c y c log exp ( o c ) c exp ( o c ) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">O = ( o c ) =G AP o n,m,c = 1 N M N n=1 M m=1 o n,m,c ? R C<label>(6)</label></formula><p>is a vector of classification results for C classes and it is obtained by applying GAP to the result of Eq. (4); Y = (y c ) ? {0, 1} C denotes the ground truth of the class of the given scene image and it is represented by a one-hot vector. Then, the total training loss L tot al is defined as a summation of the proposed SCL L SC L and classification loss L C , and it is represented by</p><formula xml:id="formula_8">L tot al = L C + ?L SC L (7) Convolution Partial Convolution ?1,</formula><p>, ? where ? denotes the SCL rate and controls the relative weight between SCL and the classification loss.</p><p>Another key feature of PlacesNet is that partial convolution <ref type="bibr" target="#b29">[30]</ref> is applied to all convolution layers. In vanilla convolution with zero padding, boundary of the image is filled with zeros and the vanilla convolution is applied, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. Using the padded input x l?1, pad n,m,1:D l = x l?1, pad n,m,d l ? R D l , the output vector x l n,m,1:D l+1 of the l-th layer of vanilla convolutions at the position (n, m) is computed as follows:</p><formula xml:id="formula_9">x l n,m,d l = H l i=1 W l j=1 W l i, j,d l?1 ,d l x l?1, pad n+i,m+j,d l + b l d l (8) where W l = W l i, j,d l?1 ,d l ? R H l ?W l ?D l?1 ?D l and b l = b l d l</formula><p>? R D l are the filter weights of the l-th layer; H l and W l are height and width of filter size respectively; D l is the number of output channels of the l-th layer. Here, it can be seen that vanilla convolution X l = x l n,m,1:D l around the boundary of the feature might not be as accurate as that inside the feature since the vanilla convolution should include many zero paddings. Recently, a lot of convolution layers are connected sequentially, and then the performance deterioration of the boundary of the given image becomes worse. For other general classification CNNs, the accuracy degradation around the boundary of the image might not be important because GAP is used before the classification. In FOSNet, however, the SCL is used as a loss and the classification accuracy around the boundary of the image is as important as that inside the image. Thus, the partial convolution proposed in <ref type="bibr" target="#b29">[30]</ref> is used in FOSNet.</p><p>The structure of partial convolution <ref type="bibr" target="#b29">[30]</ref> is given in <ref type="figure" target="#fig_4">Fig.  6</ref>. The scaling mask S l = S l n,m is multiplied with the convolution X l , and the output of the partial convolution is computed by</p><formula xml:id="formula_10">x l n,m,d l = S l n,m H l i=1 W l j=1 W l i, j,d l?1 ,d l x l?1, pad n+i,m+j,d l + b l d l ,<label>(9)</label></formula><p>where S l n,m = is zero padded position, otherwise 0. Therefore, partial convolution adjusts for the varying amount of valid inputs by scaling, and it likely increases the accuracy of the near image boundary.</p><formula xml:id="formula_11">H l W l H l W l ? H l i=1 W l j=1 1 l;pad n+i,m+j ,<label>(10)</label></formula><p>The partial convolution is a good match with the SCL and it will be shown that the combination enhances the classification accuracy significantly. The analysis will be given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fusion of Object Feature and Scene Feature</head><p>In this subsection, a new fusion module CCG is proposed. The CCG combines object feature x ob ject containing information of objects in the image with scene feature x scene .</p><p>x ob ject is extracted from ObjectNet in <ref type="figure">Fig. 2</ref>, while x scene is extracted from PlacesNet that is trained using SCL in <ref type="figure">Fig.  3(b)</ref>. The CCG is based on a scene traits that when a specific object in an image is found, the scene is very likely to belong to a particular class associated with the object. The CCG is inspired by context gating <ref type="bibr" target="#b30">[31]</ref> and the CCM <ref type="bibr" target="#b8">[9]</ref>. The concept of CCG is depicted in <ref type="figure" target="#fig_5">Fig. 7</ref>.</p><p>Using CCM <ref type="bibr" target="#b8">[9]</ref>, CCG converts an object feature into a scene feature and outputs a pseudo scene feature x ob ject?scene . Then, an attention map is generated by applying a sigmoid function to x ob ject?scene . The scene feature x scene from PlacesNet is multiplied by the generated attention map ? x ob ject?scene in element-wise manner, and a new scene feature y scene is obtained by</p><formula xml:id="formula_12">y scene = ? x ob ject?scene x scene =? W x ob ject + b x scene<label>(11)</label></formula><p>where denotes element-wise multiplication; W and b are the trainable parameters; x ob ject?scene is a pseudo scene feature obtained by converting the object feature into the scene feature through CCM, and ? (x) = 1 1+exp(?x) is a sigmoid function. The structure of CCG is motivated by context gating <ref type="bibr" target="#b30">[31]</ref>. The context gating transforms the input feature into a new feature using a self-gating mechanism, and it demonstrated significant improvements in video understanding tasks. Motivated by context gating, CCG selectively activates the channels of scene feature x scene . The selective activation is carried out by applying a gating mechanism at the object feature x ob ject , which are relevant to scene recognition. Here CCM <ref type="bibr" target="#b8">[9]</ref> is applied to the object feature x ob ject , and it converts x ob ject into a pseudo scene feature x ob ject?scene to modify the context gating concept of the self-gating mechanism into the correlative-gating mechanism. The structure of CCG is shown in <ref type="figure" target="#fig_5">Fig. 7(b)</ref>. As applied in the batch normalization (BN) <ref type="bibr" target="#b31">[32]</ref> at the CCM in <ref type="bibr" target="#b8">[9]</ref>, batch normalization can be applied to CCG as in</p><formula xml:id="formula_13">y scene = ? BN W x ob ject x scene .<label>(12)</label></formula><p>Another variation, a mixed CCM-CCG, can also be considered. Since PlacesNet is pre-trained using Places 2 dataset <ref type="bibr" target="#b27">[28]</ref>, performance degradation might occur when PlacesNet is applied to scene recognition datasets other than Places 2 (e.g., SUN397 <ref type="bibr" target="#b32">[33]</ref>, MIT 67 <ref type="bibr" target="#b33">[34]</ref>). To obtain x scene?scene t ar get , CCM converts the scene feature extracted from the PlacesNet to the feature suitable for the target scene dataset. Then, the converted x scene?scene t ar get and object features are fused using CCG. In this case, the mixed CCM-CCG proceeds as follows:</p><formula xml:id="formula_14">y scene = ? x ob ject?scene x scene?scene t ar get =? W 1 x ob ject + b 1 W 2 x scene + b 2<label>(13)</label></formula><p>The structure of the mixed CCM-CCG is depicted in <ref type="figure" target="#fig_5">Fig. 7(c)</ref>. Fusion can be conducted at two levels: feature level and score level, as carried out in <ref type="bibr" target="#b8">[9]</ref>. For score level fusion, an object score in <ref type="figure">Fig. 2</ref> and a scene score in <ref type="figure">Fig. 3</ref> are fed into the trainable fusion module in <ref type="figure">Fig. 1</ref>. In this case, we do not apply softmax on each score vector. For feature level fusion, we use an object feature in <ref type="figure">Fig. 2</ref> and a scene feature in <ref type="figure">Fig. 3</ref> as input features to be fused. This previous study <ref type="bibr" target="#b8">[9]</ref> provides a more detailed explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The proposed FOSNet is applied to three popular scene recognition datasets, and its performance is compared with that of the previous works. The three scene datasets for the experiment are Places 2 <ref type="bibr" target="#b27">[28]</ref>, SUN 397 <ref type="bibr" target="#b32">[33]</ref>, and MIT indoor 67 <ref type="bibr" target="#b33">[34]</ref>. ImageNet dataset <ref type="bibr" target="#b9">[10]</ref> is also used for the training of ObjectNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Places 2 dataset <ref type="bibr" target="#b27">[28]</ref> is the largest dataset for scene recognition. It is an upgraded version of Places 1 <ref type="bibr" target="#b34">[35]</ref>, and it is also the latest of all the scene recognition datasets. This dataset has 365 scene categories; consisting of two versions of datasets: Places365-Challenge dataset and Places365-Standard dataset. Both versions of datasets share the same validation images and only differ in the number of training images. The Places365-Challenge dataset provides 8 million training images, whereas the Places-Standard dataset provides 1.8 million training images.</p><p>SUN 397 dataset <ref type="bibr" target="#b32">[33]</ref> was the most popular scene dataset before the Places dataset <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[35]</ref> was released. This dataset consists of 397 scene categories. Each category has at least 100 different numbers of images. The entire set has a total of 108,754 images. For fair comparison with other methods using this dataset, 10 subsets each of which has 50 training images and 50 validation images per class were used to evaluate the competing methods. The average validation accuracy over the 10 subsets were used as the overall accuracy of each method.</p><p>MIT indoor 67 dataset <ref type="bibr" target="#b33">[34]</ref> is a scene recognition dataset consisting of 67 indoor scene categories, and it comprises a total of 15,620 indoor scene images. All the experiments with the MIT indoor 67 dataset were performed according to the standard evaluation protocol: A subset that has 80 training images and 20 testing images per scene category is used for evaluation.</p><p>ImageNet dataset <ref type="bibr" target="#b9">[10]</ref> is one of the most commonly used datasets for object recognition task, and it consists of 1.2 million object images and 1000 object categories. A number of popular CNN structures were trained in the dataset, which include AlexNet <ref type="bibr" target="#b35">[36]</ref>, ResNet <ref type="bibr" target="#b24">[25]</ref>, DenseNet <ref type="bibr" target="#b25">[26]</ref>, ResNeXt <ref type="bibr" target="#b26">[27]</ref>, SE-Net <ref type="bibr" target="#b2">[3]</ref>, and others <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The FOSNet is comprised of neural networks and it was trained from scratch. All models were trained for 130 epochs. The initial learning rate was 0.15 when the mini-batch size was 256. For different mini-batch sizes, the learning rate was adjusted using the linear scaling rule <ref type="bibr" target="#b39">[40]</ref> to achieve a similar performance. The learning rate was dropped by 0.1 times every 30 epochs. The synchronous stochastic gradient descent with a momentum of 0.9 was used as the optimization method. The training data were augmented by random rescaling, cropped randomly into 224 ? 224 <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b40">[41]</ref> and horizontally flipped with a 0.5 chance. The input image was normalized by the per-color mean and standard deviation <ref type="bibr" target="#b40">[41]</ref>. In addition, the data balancing strategy <ref type="bibr" target="#b1">[2]</ref> was adopted for mini-batch sampling <ref type="bibr" target="#b2">[3]</ref>. PlacesNet was trained using Places 2 dataset, and experiments were performed using transfer learning <ref type="bibr" target="#b41">[42]</ref> on other datasets such as SUN 397 and MIT indoor 67.</p><p>A hyper-parameter ? in Eq. <ref type="formula">(7)</ref> is set to 1. Detailed explanation about ? is discussed in Section 5.1. For a backbone network, the SE-ResNeXt-101 model, which is a combination of ResNeXt <ref type="bibr" target="#b26">[27]</ref> with SE-Network <ref type="bibr" target="#b2">[3]</ref>, was used for Object-Net and PlacesNet in FOSNet. The standard 10-crop testing method <ref type="bibr" target="#b6">[7]</ref> is used for comparison with other methods, and an Adi-Red <ref type="bibr" target="#b12">[13]</ref> 41.87 -Places365-VGG <ref type="bibr" target="#b27">[28]</ref> 55.24 -CCM <ref type="bibr" target="#b8">[9]</ref> 56.82 86.92 CNN-SMN <ref type="bibr" target="#b13">[14]</ref> 57.1 -SOSF+CFA+GAF <ref type="bibr" target="#b19">[20]</ref> 57.27 -Multi-Resolution CNNs <ref type="bibr" target="#b6">[7]</ref> 58.3 87.3 Places2-365-CNN <ref type="bibr" target="#b42">[43]</ref> 58.93 88.52 SE-Resnet-152 <ref type="bibr" target="#b2">[3]</ref> 59 evaluation measurement is the average classification accuracy of 10 crops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results on the Places 2</head><p>The FOSNet is compared with other scene recognition methods using the validation set of the Places 2 <ref type="bibr" target="#b27">[28]</ref>. The FOSNet is trained using the Places365-Challenge dataset. The comparison with other methods is summarized in <ref type="table" target="#tab_1">Table I.</ref> In <ref type="table" target="#tab_1">Table I, "</ref>SCL" denotes a model trained with scene coherence loss and partial convolution <ref type="bibr" target="#b29">[30]</ref> described in Section III-B. "Sum" and "Concatenate" denote that the conventional feature fusion methods replace the trainable fusion muddle shown in <ref type="figure">Fig. 1. "</ref>CCG" denotes a model using the correlative context gating as revealed in Section III-C. The entire trainable fusion models use the batch normalization <ref type="bibr" target="#b31">[32]</ref>; The score level or feature level indicates the level at which two kinds of information are combined, as described in Section III-C.</p><p>All the methods listed in <ref type="table" target="#tab_1">Table I</ref> use CNN: Adi-Red <ref type="bibr" target="#b12">[13]</ref>, CCM <ref type="bibr" target="#b8">[9]</ref>, CNN-SMN <ref type="bibr" target="#b13">[14]</ref>, and SOSF + DFA + GAF <ref type="bibr" target="#b19">[20]</ref> used information of the objects which appear in scene images. To obtain the object information, they used the CNN pre-trained on the object recognition dataset. Multi-Resolution CNN <ref type="bibr" target="#b6">[7]</ref> created a super category by considering the label ambiguity of scene categories to train a teacher network. Places365-VGG <ref type="bibr" target="#b27">[28]</ref>, Places2-365-CNN <ref type="bibr" target="#b42">[43]</ref>, and SE-Resnet-152 <ref type="bibr" target="#b2">[3]</ref> used the vanilla CNN architecture and the scene trait was not taken into consideration. In our FOSNet, SE-ResNeXt-101 was used as a backbone network. To demonstrate the competitiveness of SCL, we also conducted experiments using only a SE-ResNeXt-101. When it is trained without considering scene traits, SE-ResNeXt-101 offers lower performance than SE-ResNet-152. However, when SCL is used for training the SE-ResNeXt-101, it outperforms SE-ResNet-152 <ref type="bibr" target="#b2">[3]</ref> with 59.80%, and this requires lower computation than SE-ResNet-152. When a mixed CCM-CCG model is used, our FOSNet achieves state-of-the-art accuracy of 60.14% on the Places 2, and it is the first time that the accuracy exceeds 60% on the dataset.  <ref type="bibr" target="#b0">[1]</ref> 56.2 Gaze Shifting-CNN+SVM <ref type="bibr" target="#b18">[19]</ref> 56.2 MetaObject-CNN <ref type="bibr" target="#b14">[15]</ref> 58.11 Places365-VGG-SVM <ref type="bibr" target="#b27">[28]</ref> 63.24 Three <ref type="bibr" target="#b4">[5]</ref> 70.17 Hybrid CNN <ref type="bibr" target="#b20">[21]</ref> 70.69 Sparse Representation <ref type="bibr" target="#b22">[23]</ref> 71.08 Multi-Resolution CNNs <ref type="bibr" target="#b6">[7]</ref> 72.0 CNN-SMN <ref type="bibr" target="#b13">[14]</ref> 72.6 PatchNet <ref type="bibr" target="#b21">[22]</ref> 73.0 SDO <ref type="bibr" target="#b5">[6]</ref> 73.41 Adi-Red <ref type="bibr" target="#b12">[13]</ref> 73.59 SOSF+CFA+GAF <ref type="bibr" target="#b19">[20]</ref> 78 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results on the SUN 397</head><p>FOSNet is applied to the SUN 397 dataset <ref type="bibr" target="#b32">[33]</ref>. An average validation accuracy of 10 subsets provided in the dataset is carried out to compare the competing scene recognition methods, and the comparison results are summarized in <ref type="table" target="#tab_1">Table  II</ref>.</p><p>The names of competing methods of FOSNet are the same as the names listed in Section 4.3. In this experiment, the object and scene features are combined only at the feature level. Score level fusion is skipped since the PlacesNet is pretrained with Places 2 dataset but it is applied to SUN 397, making the dimensions of x ob ject and x scene be different from each other. As shown in <ref type="table" target="#tab_1">Table II</ref>, the model with the mixed CCM-CCG method achieves the highest performance among our methods. Interestingly, when the features are combined using sum or concatenate methods, the performance is degraded from the 'SE-ResNeXt-101 + SCL' model which uses only a single scene feature. This shows that a simple increase in the number of features without considering the scene traits can rather hinder scene recognition.</p><p>Among the competing methods, FOSNet achieves the second best accuracy of 77.72%, slightly lower than that of the state-of-the-art SOSF + CFA + GAF method <ref type="bibr" target="#b19">[20]</ref>. Here, it should be noted that it is unfair to directly compare the results of the two methods, considering that SOSF + CFA + GAF <ref type="bibr" target="#b19">[20]</ref> includes YOLOv2 <ref type="bibr" target="#b43">[44]</ref> and 4-directional long short-term memory (LSTM) <ref type="bibr" target="#b44">[45]</ref>. To train YOLOv2, an object detection dataset Object177 <ref type="bibr" target="#b45">[46]</ref> was additionally used. Unlike the dataset used to train ObjectNet, the object detection dataset Object177 includes not only the class labels but also bounding box information for the location of objects in an image, difficult to develop. Furthermore, the method SOSF + CFA + GAF requires much more computation than our method. The FOSNet uses input images of size 224 ? 224, whereas SOSF + CFA + GAF uses input images of size 608 ? 608, thereby employing 4-directional LSTM, which is obviously </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy (100%)</head><p>RBoW <ref type="bibr" target="#b17">[18]</ref> 37.93 DPM+GIST+SP <ref type="bibr" target="#b16">[17]</ref> 43.1 Adi-Red <ref type="bibr" target="#b12">[13]</ref> 73.59 Gaze Shifting-CNN+SVM <ref type="bibr" target="#b18">[19]</ref> 75.1 ResNet-152-DFT + <ref type="bibr" target="#b3">[4]</ref> 76.5 Places365-VGG-SVM <ref type="bibr" target="#b27">[28]</ref> 76.53 DAG-CNN <ref type="bibr" target="#b0">[1]</ref> 77.5 MetaObject-CNN <ref type="bibr" target="#b14">[15]</ref> 78.9 VS-CNN <ref type="bibr" target="#b15">[16]</ref> 80.37 Hybrid CNN <ref type="bibr" target="#b20">[21]</ref> 85.97 Three <ref type="bibr" target="#b4">[5]</ref> 86.04 PatchNet <ref type="bibr" target="#b21">[22]</ref> 86.2 CNN-SMN <ref type="bibr" target="#b13">[14]</ref> 86.5 Multi-Resolution CNNs <ref type="bibr" target="#b6">[7]</ref> 86.7 SDO <ref type="bibr" target="#b5">[6]</ref> 86.76 Sparse Representation <ref type="bibr" target="#b22">[23]</ref> 87.22 SOSF+CFA+GAF <ref type="bibr" target="#b19">[20]</ref> 89 computationally very expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental Results on the MIT indoor 67</head><p>In this subsection, the FOSNet is applied to the validation set of the MIT indoor 67 <ref type="bibr" target="#b33">[34]</ref>, and <ref type="table" target="#tab_1">Table III</ref> presents a comparison result of the scene recognition for MIT 67.</p><p>In the MIT indoor 67 experiment, only the feature level fusion is performed, and the score level fusion also is skipped based on the same reason as in SUN397. Based on results of <ref type="table" target="#tab_1">Table III</ref>, it seems that the CCG combines the two features more effectively than the existing fusion methods. The result shows the value of the trainable fusion over the heuristic fusion methods.</p><p>In <ref type="table" target="#tab_1">Table III</ref>, all the existing methods except RBoW <ref type="bibr" target="#b17">[18]</ref> and DPM+GIST+SP <ref type="bibr" target="#b16">[17]</ref> use CNN. The two methods use the handcraft features. From <ref type="table" target="#tab_1">Table III</ref>, of all the competing methods, the proposed method using CCG offers the best accuracy. In particular, the FOSNet with CCG outperforms SOSF+CFA+GAF <ref type="bibr" target="#b19">[20]</ref> which is the current state-of-the-art method on MIT indoor 67. As a result, our FOSNet with CCG achieves state-of-the-art accuracy of 90.37% on the MIT indoor 67, and this is the first time that the accuracy exceeds 90% on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION STUDY</head><p>Additional experiments are performed to gain a better understanding of the effects of our proposed SCL and CCG. All ablation studies are performed using the Places365-Standard dataset <ref type="bibr" target="#b27">[28]</ref> for various experiments with fast training. Standard 224 ? 224 single-crop evaluation is employed, and ResNet-18 and ResNet-50 <ref type="bibr" target="#b24">[25]</ref> are used as the backbone architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis on Scene Coherence Loss (SCL)</head><p>In this subsection, the effects of SCL on scene recognition are demonstrated. In the experiment, only PlacesNet is used and all models in the experiments are trained from scratch for a fair comparison. The results of SCL ablation studies are shown in <ref type="table" target="#tab_1">Tables IV and V.</ref> In <ref type="table" target="#tab_1">Table IV</ref>, accuracy is computed while varying the SCL rate (?) in Eq. 7. When ? = 0, only classification loss is used as a total loss in Eq. 7. This case is a baseline. When ? ? 1, accuracy is improved from the baseline in all cases, whereas when ? = 1, the PlacesNet in FOSNet achieves the best top-1 accuracy. When ? = 10, the accuracy is degraded, revealing that too much emphasis on SCL is an obstacle to minimizing classification errors. <ref type="table">Table V</ref> shows the results of the same experiment using ResNet-50, and similar results are obtained regarding the effects of the SCL <ref type="table" target="#tab_1">with Table IV</ref>. The models with SCL always outperform the ones without SCL regardless of which CNN backbone is used. Experimental results regarding the effects of partial convolution <ref type="bibr" target="#b29">[30]</ref> on SCL are given in Tables IV and V. This partial convolution improves the performance of the baseline, and its effect on the performance is higher when it is combined with SCL. Through the ablation studies, it can be noted that scene recognition performance is improved by using SCL. Since the best performance is obtained when ? = 1, the value is used to train PlacesNet.</p><p>Another experiment is performed to show the validity of the SCL. In <ref type="figure" target="#fig_6">Fig. 8(a)</ref>, the SCL is monitored, and it is unused (not propagated backward) for the training. As shown in <ref type="figure" target="#fig_6">Fig. 8(a)</ref>, the SCL decreases until reaching 60 epochs even when SCL is unused for the training. After 60 epochs in <ref type="figure" target="#fig_6">Fig. 8(a)</ref>, the SCL increases rapidly; the validation loss is almost saturated but the training loss decreases rapidly, revealing that the PlacesNet is overfitted. From the observation, the overfitting in the scene recognition is highly related to SCL. Thus, if the PlacesNet is trained to force the SCL to be reduced, the overfitting of the  PlacesNet will be relaxed and its generalization performance will be improved. <ref type="figure" target="#fig_6">Fig. 8(b)</ref> shows the result when ResNet-18 is trained with SCL. In this case, SCL converges quickly and almost vanishes. Thus, SCL is magnified 20 times for visualization in <ref type="figure" target="#fig_6">Fig. 8(b)</ref>. After 60 epochs in <ref type="figure" target="#fig_6">Fig. 8(b)</ref>, both training and validation errors decrease gradually but consistently, implying that the PlacesNet overfitting is relaxed.   <ref type="bibr" target="#b28">[29]</ref> using ResNet-18. The first row shows the input images. Using ResNet-18 trained without and with SCL, the second and third rows show the CAM images, respectively. In the figures, red parts denote the region which is relevant and makes a contribution to the scene classification, whereas the blue parts denote the region that offer no information and no contribution to the scene classification. When trained with SCL, the red region becomes bigger than trained without the SCL. This shows that the region which would be ignored if trained without SCL is fully exploited with SCL. In the entire experiments, the SCL is an effective loss for the scene classification that enables the FOSNet to fully exploit the sceneness all over the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis on Correlative Context Gating (CCG)</head><p>The proposed feature fusion method CCG is analyzed through experimentation. For a fair comparison, in PlacesNet, CNN models without partial convolution were used for scene feature extraction, and the experimental results are presented in <ref type="table" target="#tab_1">Table VI</ref>.</p><p>All the models in <ref type="table" target="#tab_1">Table VI</ref> are trained from scratch. Compared with the existing fusion methods, such as sum, concatenation or CCM <ref type="bibr" target="#b8">[9]</ref>, our fusion method CCG improves performance in most models regardless of whether SCL is added. Although the fusion by concatenation achieves the best top-5 performance in ResNet-50, the simple fusion delivers limited performances in a new dataset that PlacesNet did not train, as explained in Sections IV-D and IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, a new scene recognition framework named FOSNet has been proposed, in which the object and the scene information have been combined in a trainable fusion module named CCG. The entire system was trained using SCL, which is a new loss developed for the scene recognition. SCL is based on the unique property of the scene, e.g., the 'sceneness' spreads and the scene class does not change all over the image. The proposed FOSNet was experimented with three most popular scene recognition datasets, and the state-of-theart performance is obtained in Places 2 and MIT indoor 67.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This research was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT (NRF-2017M3C4A7069370). (Corresponding author: Euntai Kim.) H. Seong, J. Hyun, and E. Kim are with the School of Electrical and Electronic Engineering, Yonsei University, Seoul, 120-749, South Korea (email: hjseong@yonsei.ac.kr; jhhyun@yonsei.ac.kr; etkim@yonsei.ac.kr).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>An overall architecture of FOSNet. Structure of ObjectNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of scene coherence loss (SCL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 3(a), and O ? R C , O ? R C be the classification results of the models in Figs. 3(a) and (b), respectively. Then, we can prove that O and O are the same by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of convolution with zero padding and partial convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Trainable fusion modules with object feature and scene feature. (a) CCM [9]; (b) CCG; (c) mixed CCM-CCG. where S l n,m is scaling factor of output feature x l n,m,d l ; 1 l;pad n,m is 1 if position (n, m) of the input feature x l?1, pad n,m,d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Classification loss and SCL curves of ResNet-18 trained (a) with only classification loss and (b) with classification loss and SCL. The blue line denotes the loss of the training set, and the red line denotes the loss of the validation set. The solid line represents classification loss, and the dotted line represents SCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 Fig. 9 .</head><label>99</label><figDesc>provides the results of class activation map (CAM) The class activation map (CAM) [29] results using ResNet-18. The ground truth about the scene class of the image is on top of the image. The first row shows the input image. The second row shows the CAM result using ResNet-18 trained without SCL. The third row shows the CAM result using ResNet-18 trained with SCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>PlacesNet Scene Feature Scene Score ? Scene Features GAP Convolution Layers ? ? ? Fully Connected Layer</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">PlacesNet</cell><cell></cell><cell></cell></row><row><cell>? ? ?</cell><cell>Scene Features</cell><cell>Layer 1?1 Conv</cell><cell>? ? ?</cell><cell>GAP</cell><cell>Scene Score ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Scene Scores</cell><cell></cell><cell></cell></row><row><cell>Convolution Layers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Fig. 3. Structures of PlacesNet. (a) Vanilla CNN structure; PlacesNet should</cell></row><row><cell cols="6">be applied multiple times to compute the SCL. (b) A new structure in which</cell></row><row><cell cols="4">SCL can be computed by applying PlacesNet only once.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Baseball Field</cell><cell>Baseball Field</cell><cell cols="2">Baseball Field</cell></row><row><cell></cell><cell cols="2">Baseball Field</cell><cell>Baseball Field</cell><cell cols="2">Baseball Field</cell></row><row><cell>Baseball Field</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Baseball Field</cell><cell>Baseball Field</cell><cell cols="2">Baseball Field</cell></row></table><note>Fig. 4. Scene coherence in a scene image. Even if a scene image is divided into multiple grids, each grid cell represents the same scene.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH OTHER SCENE RECOGNITION METHODS ON PLACES 2 [28] VALIDATION SET.</figDesc><table><row><cell>Methods</cell><cell>Accuracy (100%) top-1 top-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISON</head><label>II</label><figDesc>WITH OTHER SCENE RECOGNITION METHODS ON SUN 397<ref type="bibr" target="#b32">[33]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Accuracy (100%)</cell></row><row><cell>VS-CNN [16]</cell><cell>43.14</cell></row><row><cell>DAG-CNN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III COMPARISON</head><label>III</label><figDesc>WITH OTHER SCENE RECOGNITION METHODS ON THE MIT 67 [34] VALIDATION SET.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY OF SCL USING RESNET-18. THIS EXPERIMENT IS PERFORMED ON THE PLACES365-STANDARD DATASET<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell></cell><cell cols="2">Base Model</cell><cell></cell><cell></cell><cell cols="2">ResNet-18</cell></row><row><cell></cell><cell cols="2">SCL Rate (?)</cell><cell cols="2">0 (Baseline)</cell><cell>10 1</cell><cell></cell><cell>10 0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">top-1 acc top-5 acc</cell><cell cols="2">top-1 acc top-5 acc</cell><cell>top-1 acc top-5 acc</cell></row><row><cell></cell><cell>only SCL</cell><cell></cell><cell>54.438</cell><cell>84.912</cell><cell>53.775</cell><cell>84.151</cell><cell>54.942</cell><cell>85.074</cell></row><row><cell></cell><cell cols="2">SCL with Partial Conv</cell><cell>54.718</cell><cell>84.866</cell><cell>53.688</cell><cell>84.099</cell><cell>55.090</cell><cell>85.027</cell></row><row><cell></cell><cell cols="2">SCL Rate (?)</cell><cell>10 ?1</cell><cell></cell><cell>10 ?2</cell><cell></cell><cell>10 ?3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">top-1 acc top-5 acc</cell><cell cols="2">top-1 acc top-5 acc</cell><cell>top-1 acc top-5 acc</cell></row><row><cell></cell><cell>only SCL</cell><cell></cell><cell>54.770</cell><cell>84.901</cell><cell>54.548</cell><cell>84.715</cell><cell>54.616</cell><cell>84.827</cell></row><row><cell></cell><cell cols="2">SCL with Partial Conv</cell><cell>54.901</cell><cell>85.038</cell><cell>54.710</cell><cell>84.841</cell><cell>54.877</cell><cell>84.929</cell></row><row><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ABLATION STUDY OF SCL USING RESNET-50. THIS EXPERIMENT IS</cell><cell></cell><cell></cell></row><row><cell cols="5">PERFORMED ON THE PLACES365-STANDARD DATASET [28].</cell><cell></cell><cell></cell></row><row><cell>Base Model</cell><cell></cell><cell cols="2">ResNet-50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCL Rate (?)</cell><cell cols="2">0 (Baseline)</cell><cell>10 0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">top-1 acc top-5 acc</cell><cell cols="2">top-1 acc top-5 acc</cell><cell></cell><cell></cell></row><row><cell>only SCL</cell><cell>55.888</cell><cell>86.123</cell><cell>56.285</cell><cell>86.288</cell><cell></cell><cell></cell></row><row><cell>SCL with Partial Conv</cell><cell>56.227</cell><cell>86.099</cell><cell>56.337</cell><cell>86.195</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDY OF CCG ON THE PLACES365-STANDARD DATASET<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell>Fusion Level</cell><cell>Method</cell><cell cols="2">ResNet-18 top-1 acc top-5 acc</cell><cell cols="2">ResNet-18 -SCL top-1 acc top-5 acc</cell><cell cols="2">ResNet-50 top-1 acc top-5 acc</cell><cell cols="2">ResNet-50 -SCL top-1 acc top-5 acc</cell></row><row><cell></cell><cell>Baseline</cell><cell>54.438</cell><cell>84.912</cell><cell>54.942</cell><cell>85.074</cell><cell>55.888</cell><cell>86.123</cell><cell>56.285</cell><cell>86.288</cell></row><row><cell></cell><cell>Sum</cell><cell>53.485</cell><cell>84.123</cell><cell>54.570</cell><cell>84.680</cell><cell>56.115</cell><cell>86.285</cell><cell>56.630</cell><cell>86.345</cell></row><row><cell></cell><cell>Concatenate</cell><cell>54.701</cell><cell>85.071</cell><cell>55.164</cell><cell>85.145</cell><cell>56.230</cell><cell>86.411</cell><cell>56.685</cell><cell>86.441</cell></row><row><cell></cell><cell>CCM with ReLU [9]</cell><cell>54.756</cell><cell>85.107</cell><cell>55.076</cell><cell>85.090</cell><cell>56.334</cell><cell>86.375</cell><cell>56.663</cell><cell>86.529</cell></row><row><cell>Feature Level</cell><cell>CCM-BN with ReLU [9]</cell><cell>54.786</cell><cell>85.181</cell><cell>55.129</cell><cell>85.230</cell><cell>56.269</cell><cell>86.395</cell><cell>56.726</cell><cell>86.573</cell></row><row><cell></cell><cell>CCG</cell><cell>54.575</cell><cell>84.888</cell><cell>54.907</cell><cell>84.921</cell><cell>56.060</cell><cell>86.186</cell><cell>56.469</cell><cell>86.233</cell></row><row><cell></cell><cell>CCG-BN</cell><cell>54.934</cell><cell>85.206</cell><cell>55.153</cell><cell>85.088</cell><cell>56.367</cell><cell>86.395</cell><cell>56.729</cell><cell>86.397</cell></row><row><cell></cell><cell>mixed CCM-CCG-BN</cell><cell>54.504</cell><cell>84.997</cell><cell>54.959</cell><cell>85.132</cell><cell>56.060</cell><cell>86.373</cell><cell>56.800</cell><cell>86.466</cell></row><row><cell></cell><cell>CCM [9]</cell><cell>54.477</cell><cell>84.869</cell><cell>55.055</cell><cell>85.107</cell><cell>56.096</cell><cell>86.233</cell><cell>56.581</cell><cell>86.315</cell></row><row><cell>Score Level</cell><cell>CCM-BN [9] CCG</cell><cell>54.600 54.562</cell><cell>84.979 84.901</cell><cell>55.104 55.071</cell><cell>85.126 85.071</cell><cell>56.110 56.030</cell><cell>86.238 86.192</cell><cell>56.690 56.600</cell><cell>86.343 86.238</cell></row><row><cell></cell><cell>CCG-BN</cell><cell>54.677</cell><cell>85.156</cell><cell>55.211</cell><cell>85.233</cell><cell>56.203</cell><cell>86.375</cell><cell>56.685</cell><cell>86.348</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-scale recognition with DAG-CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1215" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dft-based transformation invariant pooling layer for visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="84" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scene recognition with cnns: objects, scales and dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="571" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scene recognition with objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="474" to="487" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge guided disambiguation for large-scale scene classification with multi-resolution cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deriving high-level scene descriptions from deep scene cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pomplun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing Theory, Tools and Applications (IPTA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Seventh International Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scene recognition via object-to-scene class conversion: end-to-end training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From volcano to toyshop: Adaptive discriminative region discovery for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 26th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1760" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale multi-feature context modeling for scene recognition in the semantic manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2721" to="2735" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Harvesting discriminative meta objects with deep cnn features for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1287" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene categorization model using deep visually sensitive features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scene recognition and weakly supervised object localization with deformable part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reconfigurable models for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Oberlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2775" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene categorization using deeply learned gaze shifting kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fusing object semantics and deep appearance features for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hybrid cnn and dictionary-based models for scene recognition and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1263" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised patchnets: Describing and aggregating local patches for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2028" to="2041" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A robust indoor scene recognition method based on sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laranjeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Braz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacerda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Nascimento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberoamerican Congress on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11718</idno>
		<title level="m">Partial convolution based padding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<ptr target="https://github.com/facebook/fb.resnet.torch" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Places401 and places365 models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="https://github.com/lishen-shirley/Places2-CNNs" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object bank: A highlevel image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">He is a graduate student of the combined masters and doctoral degree programs at Yonsei University. He has studied computer vision, machine learning and deep learning</title>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Seoul, Korea</pubPlace>
		</imprint>
	</monogr>
	<note>Hongje Seong received the BS degree in electrical and electronic engineering from Yonsei University</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">He is a graduate student of the combined masters and doctoral degree programs at Yonsei University. He has studied computer vision, machine learning and deep learning</title>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Seoul, Korea</pubPlace>
		</imprint>
	</monogr>
	<note>Junhyuk Hyun received the BS degree in electrical and electronic engineering from Yonsei University</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Since 2002, he has been with the faculty of the School of Electrical and Electronic Engineering, Yonsei University, where he is currently a Professor. He was also a visiting researcher at the Berkeley Initiative in Soft Computing</title>
		<editor>1970. He received B.S., M.S., and Ph.D.</editor>
		<imprint>
			<date type="published" when="1992" />
			<pubPlace>Kyonggi-do, Korea; Berkeley, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>degrees in Electronic Engineering, all from Yonsei University, Seoul, Korea ; Hankyong National University ; University of California</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include computational intelligence, statistical machine learning and deep learning and their application to intelligent robotics. autonomous vehicles, and robot vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
