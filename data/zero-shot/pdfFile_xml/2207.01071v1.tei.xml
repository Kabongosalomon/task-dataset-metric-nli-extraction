<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You Only Need One Detector: Unified Object Detector for Different Modalities based on Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hunter College</orgName>
								<orgName type="institution" key="instit2">CUNY</orgName>
								<address>
									<region>New York City</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhujun</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Graduate Center</orgName>
								<orgName type="institution">CUNY</orgName>
								<address>
									<region>New York City</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Canizales</surname></persName>
							<email>jaime.canizales97@hunter.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hunter College</orgName>
								<orgName type="institution" key="instit2">CUNY</orgName>
								<address>
									<region>New York City</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Stamos</surname></persName>
							<email>istamos@hunter.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Hunter College</orgName>
								<orgName type="institution" key="instit2">CUNY</orgName>
								<address>
									<region>New York City</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">The Graduate Center</orgName>
								<orgName type="institution">CUNY</orgName>
								<address>
									<region>New York City</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">You Only Need One Detector: Unified Object Detector for Different Modalities based on Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most systems use different models for different modalities, such as one model for processing RGB images and one for depth images. Meanwhile, some recent works discovered that an identical model for one modality can be used for another modality with the help of cross modality transfer learning. In this article, we further find out that by using a vision transformer together with cross/inter modality transfer learning, a unified detector can achieve better performances when using different modalities as inputs. The unified model is useful as we don't need to maintain separate models or weights for robotics, hence, it is more efficient. One application scenario of our unified system for robotics can be: without any model architecture and model weights updating, robotics can switch smoothly on using RGB camera or both RGB and Depth Sensor during the day time and Depth sensor during the night time . Experiments on SUN RGB-D dataset show: Our unified model is not only efficient, but also has a similar or better performance in terms of mAP50 based on SUNRGBD16 category: compare with the RGB only one, ours is slightly worse (52.3 ? 51.9). compare with the point cloud only one, we have similar performance (52.7 ? 52.8); When using the novel inter modality mixing method proposed in this work, our model can achieve a significantly better performance with 3.1 (52.7 ? 55.8) absolute improvement comparing with the previous best result. Code (including training/inference logs and model checkpoints) is available: https://github.com/liketheflower/YONOD.git</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of computer vision and artificial intelligence, more and more robotics are showing up to enrich people's lives: autonomous vehicles can The visualization is generated from the simCrossTrans <ref type="bibr" target="#b0">[1]</ref> work which trains on the pseudo images converted from point cloud. Model C can process any of the following images: RGB images, pseudo images converted from point clouds or inter-modality mixing of RGB image and pseudo images converted from point clouds. This visualization is based on YONOD with Swin-T <ref type="bibr" target="#b1">[2]</ref> as the backbone network.</p><p>drive us to our target location without any human driver/operator; Autonomous Mobile Robots working in warehouses can help us prepare orders. Although most of those robotics require more than one sensor such as cameras and 3D sensors (LiDAR or depth), not all the scenarios all of those sensors are available. For example, the camera sensor can not work well during the night time if no extra lighting is provided or in some cases when people's privacy are being protected and hence that 2D cameras are not allowed to be deployed on robotics <ref type="bibr" target="#b0">[1]</ref>. Meanwhile, Environmentally Friendly Robots are important to our planet as people and robotics are not the only residents. Reducing the light pollution used for robotics during the night time can be something meaningful to our planet. Hence, developing robotic vision systems which only use the natural light during the day time and no extra light during the night time is always one of our research goals.</p><p>Thanks to the powerful ConvNets <ref type="bibr" target="#b2">[3]</ref> based and Transformer <ref type="bibr" target="#b3">[4]</ref> based feature extractors, the camera based vision system can achieve excellent 2D detection performance. At the same time, the simCrossTrans <ref type="bibr" target="#b0">[1]</ref> work shows by converting the 3D sensor data to pseudo images, together with cross-modality transfer . This is based on the Swin-S backbone network. More results can be found in https://youtu.be/PuRQCLLSUDI learning, the 3D data based 2D object detection system's can achieve good performance by using an identical network used for the RGB images. This is promising, and a very natural question will be: can we do better if we train a unified network by using both the RGB and 3D data with a unified network (with the same architecture and same weights )? The unified network can process any of the following sensor data: 1) RGB images; 2) pseudo images converted from 3D sensor; 3) both the RGB images and pseudo images converted from 3D sensor. If we can achieve similar or even better performance compared with separate networks with each network only performing well for a specified modality, then the environment friendly system (using natural light during the day time and no extra light during the night time) mentioned in the previous paragraph will be possible. In this article, we are mainly exploring the possibilities of this system. As in the simCrossTrans <ref type="bibr" target="#b0">[1]</ref> work, the Vision Transformer based performs better than the ConvNets based network, in this article, we focus on using the vision transformer network only. In summary, in this article, we want to find answers for the following questions:</p><p>1. Whether a unified model can have a similar performance or even better performance on both RGB images and pseudo images converted from point cloud? 2. If a unified model of processing both the RGB and pseudo images converted from point cloud is feasible, whether we can further fuse the RGB images and pseudo images? By doing this we can enhance the unified model further to support processing both the RGB and point cloud data.</p><p>Through experiments, we indeed have interesting observations and achieve SOTA results on 2D object detection. Our unified model named You Only Need One Detector (YONOD) can process all of the following images: RGB images, pseudo images converted from point clouds or inter modality mixing of RGB image and pseudo images converted from point clouds. The difference of our model compared with other works can be found in <ref type="figure" target="#fig_0">Figure 1</ref>. Comparison of the performance for different methods can be found in <ref type="table">Table 3</ref>. YONOD outputs are visualized in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Here are our contributions:</p><p>1. By using vision transformers, we achieve a new SOTA on 2D object detection based on RGB images on SUN RGB-D dataset. 2. We propose two inter-modality mixing methods which can mixup the data from different modalities to further feed to our unified model. 3. We propose a unified model which can process any of the following images:</p><p>RGB images, pseudo images converted from point clouds or inter-modality mixing of RGB image and pseudo images converted from point clouds. This unified model achieves similar performance to RGB only model and point cloud only model. Meanwhile, by using the inter-modality mixing data as input, our model can achieve a significantly better 2D detection performance. <ref type="bibr" target="#b3">4</ref>. We open source our code, training/testing logs and model checkpoints to benefit the vision community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Projecting 3D sensor data to 2D Pseudo Images: There are different ways to project 3D data to 2D features. HHA was proposed in <ref type="bibr" target="#b4">[5]</ref> where the depth image is encoded with three channels: Horizontal disparity, Height above ground, and the Angle of each pixel's local surface normal with gravity direction. The signed angle feature described in <ref type="bibr" target="#b5">[6]</ref> measures the elevation of the vector formed by two consecutive points and indicates the convexity or concavity of three consecutive points. Input features converted from depth images of normalized depth(D), normalized relative height(H), angle with up-axis(A), signed angle(S), and missing mask(M) were used in <ref type="bibr" target="#b6">[7]</ref>. DHS images are used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Object Detection Based on RGB images or Pseudo images from point cloud by Vision Transformers: Object detection approaches can be summarized as two-stage frameworks (proposal and detection stages) and one-stage frameworks (proposal and detection in parallel). Generally speaking, two-stage methods such as R-CNN <ref type="bibr" target="#b9">[10]</ref>, Fast RCNN <ref type="bibr" target="#b10">[11]</ref>, Faster RCNN <ref type="bibr" target="#b11">[12]</ref>, FPN <ref type="bibr" target="#b12">[13]</ref> and mask R-CNN <ref type="bibr" target="#b13">[14]</ref> can achieve a better detection performance while onestage systems such as YOLO <ref type="bibr" target="#b14">[15]</ref>, YOLO9000 <ref type="bibr" target="#b15">[16]</ref>, RetinaNet <ref type="bibr" target="#b16">[17]</ref> are faster at the cost of reduced accuracy. For deep learning based systems, as the size of the network is increased, larger datasets are required. Labeled datasets such as PASCAL VOC dataset <ref type="bibr" target="#b17">[18]</ref> and COCO (Common Objects in Context) <ref type="bibr" target="#b18">[19]</ref> have played important roles in the continuous improvement of 2D detection systems. Most systems introduced here are based on ConvNets. Nice reviews of 2D detection systems can be found in <ref type="bibr" target="#b19">[20]</ref>. When replacing the backbone network from ConvNets to Vision Transformers, the systems will be adopted to Vision Transformers backbone based object detection systems. The most successful systems are Swin-transformer <ref type="bibr" target="#b1">[2]</ref> and Swin-transformer v2 <ref type="bibr" target="#b20">[21]</ref>. simCrossTrans <ref type="bibr" target="#b0">[1]</ref> explored the cross modality transfer learning by using both the ConvNets and Vision Transfomers based on SUN RGB-D dataset based on the mask R-CNN <ref type="bibr" target="#b13">[14]</ref> approach.</p><p>Inter modality mixing: <ref type="bibr" target="#b21">[22]</ref> learns a dynamical and local linear interpolation between the different regions of cross-modality images in data-dependent fashion to mix up the RGB and infrared (IR) images. We explored both the static and dynamic mixing methods and found the static has a better performance. <ref type="bibr" target="#b22">[23]</ref> uses an interpolation between the RGB and thermal images at pixel level. As we are training a unified model supporting both the single modality image and multiple modality images as input, we do not apply the interpolation to keep the original signal of each modality. We leverage the transformer architecture itself to automatically build up the gap between different modalities. Transfer learning with same modality or cross modality: Transfer learning is widely used in computer vision (CV), natural language processing (NLP) and biochemistry. Most transfer learning systems are based on the same modality (e.g. RGB image in CV and text in NLP). For the CV, common transfer learning is based on supervised way such as works in R-CNN <ref type="bibr" target="#b9">[10]</ref>, Fast RCNN <ref type="bibr" target="#b10">[11]</ref>, Faster RCNN <ref type="bibr" target="#b11">[12]</ref>, FPN <ref type="bibr" target="#b12">[13]</ref>, mask R-CNN <ref type="bibr" target="#b13">[14]</ref>, YOLO <ref type="bibr" target="#b14">[15]</ref>, YOLO9000 <ref type="bibr" target="#b15">[16]</ref>, RetinaNet <ref type="bibr" target="#b16">[17]</ref> use a pretrained backbone network model based on ImageNet classification task and the model is further trained based on the following task datasets such as COCO to achieve object detection or/and instance segmentation tasks. In the NLP, the transfer learning such as BERT <ref type="bibr" target="#b23">[24]</ref>, GPT <ref type="bibr" target="#b24">[25]</ref>, GPT-2 <ref type="bibr" target="#b25">[26]</ref>, GPT-3 <ref type="bibr" target="#b26">[27]</ref> are mainly based on self-supervised way and achieve great success. Inspired by the success of the self-supervised way transfer learning, the CV community is also exploring the self-supervised way to explore new possibilities, one recent work which is similar to the BERT in NLP is MAE <ref type="bibr" target="#b27">[28]</ref>. The MolGNN <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> in bioinformatics use a self-supervised way based on Graph Neural network (GNN) in the pretraining stage and achieve good performance in a few shot learning framework for the following subtasks. For this work, we explore the cross modality transfer learning from a pretrained model under the supervised learning approach. Recently, Frustum-Voxnet <ref type="bibr" target="#b7">[8]</ref> used pretrained weights from the RGB images to fine tune the point cloud converted pseudo image based on ConvNets <ref type="bibr" target="#b2">[3]</ref>. simCrossTrans <ref type="bibr" target="#b0">[1]</ref> further explored the cross modality transfer learning by using both the ConvNets <ref type="bibr" target="#b2">[3]</ref> and Vision Transfomers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2]</ref> and showed significant improvement. In order to use pretrained models based on RGB images, we convert point clouds to pseudo 2D images with 3 channels. The point clouds can be converted to HHA or any three channels from DHASM introduced in <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convert point clouds to pseudo 2D image</head><p>For this work, we follow the same approaches in Frustum VoxNet <ref type="bibr" target="#b7">[8]</ref> and sim-CrossTrans <ref type="bibr" target="#b0">[1]</ref> by using DHS to project 3D depth image to 2D due to: 1) <ref type="bibr" target="#b31">[32]</ref> shows DHS can provide a solid result; 2) have a fair comparison with the Frustum VoxNet <ref type="bibr" target="#b7">[8]</ref> and simCrossTrans <ref type="bibr" target="#b0">[1]</ref> works.</p><p>Here is a summary of the DHS encoding method <ref type="bibr" target="#b0">[1]</ref>: Similar to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>, we adopt Depth from the sensor and Height along the sensor-up (vertical) direction as two reliable measures. Signed angle was introduced in <ref type="bibr" target="#b32">[33]</ref>: denote X i,k = [x ik , y ik , z ik ] the vector of 3D coordinates of the k-th point in the i-th scanline. Knowledge of the vertical direction (axis z) is provided by many laser scanners, or even can be computed from the data in indoor or outdoor scenarios (based on line/plane detection or segmentation results from machine learning models) and is thus assumed known. Define D i,k = X i,k+1 ? X i,k (difference of two successive measurements in a given scanline i), and A ik : the angle of the vector D i,k with the pre-determined z axis (0 to 180 degrees). The Signed angle S ik = sgn(D i,k ? D i,k?1 ) * A ik : the sign of the dot product between the vectors D i,k and D i,k?1 , multiplied by V ik . This sign is positive when the two vectors have the same orientation and negative otherwise. Those three channel pseudo images are normalized to 0 to 1 for each channel. Some samples DHS images can be seen in <ref type="figure" target="#fig_2">Figure 3</ref>   In order to support using multiple modality inputs based on our unified model, we propose an inter modality mixing approach. For the inter modality mixing, we mix the images from different modality together to generate a mixed three channel image which can be consumed by our model. By doing this, we can further extend our model's capability without updating the model's architecture. Meanwhile, by training a model based on RGB, DHS images and inter modality Mixing image from RGB and DHS images, we can achieve a unified detector which can support different modality as input. Different methods can be designed to fuse the images from different modalities, we proposed two ways: The Per Patch Mixing is relatively simple to implement. In our experiment, we alternatively select a modality image for each patch. Also, we chose square patches. By doing this, the modality image selection mask looks like a chessboard, so we call our implementation as Chessboard Per Patch Mixing (CPPM). Examples of the CPPM are shown in the middle of <ref type="figure" target="#fig_4">Figure 4</ref>. For Stochastic Flood Fill Mixing, it is adjusted from the flood fill algorithm <ref type="bibr" target="#b33">[34]</ref>. For this mixing, we generate a mask of using which modality image based on the flood fill results. For the stochastic flood fill, the edge between two neighbor pixels is connected with a probability p. The RGB and DHS have separate edge connection probabilities. The Python style pseudocode of this algorithm is shown in <ref type="figure" target="#fig_6">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inter modality mixing</head><p>We can use both four neighbors (top, right, bottom, left) to build the graph or eight neighbors (adding four closest pixels along diagonal offsets besides the four neighbors) to build the graph. In our experiment, we use four neighbors. Examples of the SFFM are shown on the right of <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">2D detection framework</head><p>For 2D detection and instance segmentation, we use the classical object detection framework: Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> in mmdetection <ref type="bibr" target="#b34">[35]</ref>. It is a two-stage detection and segmentation framework <ref type="bibr" target="#b19">[20]</ref>: region proposal and detection/segmentation. We disable the training of the mask branch when fine-tuning the model on SUN RGB-D dataset. By using the default weights from the pretrained model, the mask prediction branch can still generate reasonable mask predictions as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This is consistent with the observation from the simCrossTrans <ref type="bibr" target="#b0">[1]</ref> work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">2D detection backbone networks</head><p>For the backbone network, we use Swin Transformers <ref type="bibr" target="#b1">[2]</ref>, specifically we ex- where C is the channel number of the hidden layers in the first stage. Detail of the model architecture, please check the Swin Transformers[2] paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">SUN RGB-D dataset used in this work</head><p>SUN RGB-D <ref type="bibr" target="#b35">[36]</ref> dataset is an indoor dataset which provides both the point cloud and RGB images. In this work, since we are building a 3D only object detection system, we only use the point clouds for fine tuning. The RGB images are not used during the fine tuning process. For the point clouds, they are collected based on 4 types of sensors: Intel RealSense, Asus Xtion, Kinect v1 and Kinect v2. The first three sensors are using an IR light pattern. The Kinect v2 is based on time-of-flight. The longest distance captured by the sensors are around 3.5 to 4.5 meters.</p><p>SUN RGB-D dataset splits the data into a training set which contains 5285 images and a testing set which contains 5050 images. For the training set, it further splits into a training only, which contains 2666 images and a validation set, which contains 2619 images. Similar to <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> , we are fine-tuning our model based on the training only set and evaluate our system based on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Pre-train</head><p>Both the Swin-T and Swin-S based networks 1 are firstly pre-trained on Ima-geNet <ref type="bibr" target="#b38">[39]</ref> and then pre-trained on the COCO dataset <ref type="bibr" target="#b18">[19]</ref>. Data augmentation When pre-training on COCO dataset, the images augmentation are applied during the training stage by: randomly horizontally flip the image with probability of 0.5; randomly resize the image with width of 1333 and height of several values from 480 to 800 (details see the configure file from the github repository); randomly crop the original image with size of 384 (height) by 600 (width) and resize the cropped image to width of 1333 and height of several values from 480 to 800.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Fine-tuning</head><p>Data augmentation: We follow the same augmentation with the pre-train stage. The raw input images have the width of 730 and height of 530. Those raw images are randomly resized and cropped during the training. During testing, the images are resized to width of 1120 and height of 800 which can be divided by 32. Hardware: For the fine-tuning, we use a standard single NVIDIA Titan-X GPU, which has 12 GB memory. We fine-tune the network for 133K iterations for 100 epochs. It took about 29 hours for Swin-T based network with batch size of 2 (for 133K iterations) for the RGB only model. For the YONOD without the inter modality mixing, it took about 2 days to train the model. For with the inter modality mixing, the speed depends on the number of inter modality mixing images added to the training data. Fine-tuning subtasks: We focus on the 2D object detection performance, so we fine-tune the model based on the 2D detection related labels. Similar to simCrossTrans <ref type="bibr" target="#b0">[1]</ref>, we kept the mask branch without training to further verify whether reasonable mask detection can be created by using the weights from the pre-train stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results on SUN RGB-D dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments</head><p>Our experiments mainly focus on using different inputs data to train the model and compare the performance difference. For the YONOD without the inter modality mixing, we use both the RGB and DHS images to train a unified model. For the YONOD with the inter modality mixing, besides the RGB and DHS images, we also add the inter modality mixing images to the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>Following the previous works mentioned in <ref type="table">Table 3</ref>, we firstly use the AP50: Average Precision at IoU = 0.5 as evaluation metric. We also use the COCO object detection metric which is AP75: Average Precision at IoU = 0.75 and a more strict one: AP at IoU = .50:.05:.95 to evaluate the 2D detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation subgroups</head><p>We use the same subgroups as simCrossTrans <ref type="bibr" target="#b0">[1]</ref> to evaluate the performance. The subgroups are SUNRGBD10, SUNRGBD16, SUNRGBD66 and SUNRGBD79, which have 10, 16, 66 and 79 categories. Detail list of those sub groups can be found in simCrossTrans <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The performance of YONOD without the inter modality mixing</head><p>We first evaluate the performance of YONOD without inter modality mixing. For this one, the model is trained based on both the RGB and DHS images. Our model architecture is the same as the simCrossTrans <ref type="bibr" target="#b0">[1]</ref> work, which is using only the DHS image to train the model. We train a RGB images only model based on the same network to compare with the YONOD one's performance.  The performance based on the mAP50 on the SUNRGBD79 can be found in <ref type="figure" target="#fig_9">Figure 6</ref>. From the result, we can see the YONOD can achieve excellent performance when testing on both RGB and DHS images. We can also observe that YONOD on DHS images' performance is significantly improved compared to the DHS only model. The DHS images' performance boosting should be benefited from the inter modality transfer learning from the RGB images. The cost of the DHS performance boosting is slightly performance reduce on the RGB images. Overall, the YONOD's performance is promising as only one model can work for different modalities. It is more efficient than maintain two architectures or one architectures with two different weights. This is especially meaningful for robotics and edge devices. We can build a smoothly perception system when change from day time to night time for robotics if our system is deployed. More results can be found in <ref type="table">Table 1</ref>  For the inter-modality mixing, we tried both the SFFM and CPPM. For the SFFM, we have six SFFM images for each RGB and DHS image pair. The connection probabilities for RGB and DHS are randomly selected from 0.1 to 0.9. The first pixel's RGB and DHS mask are randomly set up with equal probability. For CPPM, we use patch size of 1 by 1 and each RGB and DHS image pair has one CPPM image.</p><p>The result is shown in <ref type="table">Table 2</ref>. From the result, we can see that the simple YONOD with CPPM has a better performance than the YONOD with SFFM. The reason should be that the SFFM generates too many random images which makes the unified network fail to perform well on the RGB and DHS images. However, for using the simple CPPM one, it has similar performance compared with the plain YONOD model's results. When using the CPPM image from RGB and DHS, the 2D detection can achieve the best performance. As YONOD with CPPM can support three types of images: RGB, DHS and CPPM images from RGB and DHS, we propose to use the YONOD with CPPM as a more powerful unified model. <ref type="table">Table 2</ref> also shows the results of using Swin-T and Swin-S as the different backbone networks. Swin-S is a more powerful network, however, the performance gain is limited. Hence, we propose to use the lightweight Swin-T as the backbone network to achieve a faster inference speed (see <ref type="table" target="#tab_3">Table 4</ref>).  <ref type="table">Table 3</ref>. 2D detection results based on SUN RGB-D validation set. Evaluation metric is average precision with 2D IoU threshold of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Influence of different backbone networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Detail per category results compared with other works</head><p>We provide a comprehensive per category results comparison with previous works as shown in <ref type="table">Table 3</ref>. Compare the YONOD with CPPM to our RGB only model, the YONOD one has a slightly worse (52.3 ? 51.9) performance. compare with the point cloud only one, we have similar performance (52.7 ? 52.8); When using the novel inter modality mixing method proposed in this work, our model can achieve a significantly better performance with 3.1 (52.7 ? 55.8) absolute improvement comparing with the previous best result from the simCrossTrans <ref type="bibr" target="#b0">[1]</ref>. For the most commonly used subgroup of SUNRGBD10, we also achieve a new SOTA of 58.1 (56.8 ? 58.1) when using YONOD with CPPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">More results based on extra evaluation metrics</head><p>More results based on mAP/mAP75 can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Number of parameters and inference time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Network # Parameters (M) GFLOPs Inference Time (ms) FPS F-VoxNet <ref type="bibr" target="#b7">[8]</ref> ResNet   <ref type="table" target="#tab_3">Table 4</ref>. As we use the same network and hardware, we use the inference time from the simCrossTrans <ref type="bibr" target="#b0">[1]</ref> for our Swin-T based network. For the larger Swin-S based network, the inference time is slower as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of parameters and inference time is shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce a unified model which can process different modality data: RGB from camera, DHS from depth sensor, inter modality mixing image from RGB and DHS. This unified system is powerful with excellent performance. By using this system, it can work smoothly with different scenarios related to the availability of the sensors: such as only an RGB camera is available during the day time and only depth sensor is available during the night time. Hence, it is more robust and environment friendly. By using RGB camera during the day time and depth sensor during the night time, this unified model can be more environment friendly due to low and even no extra light needed. Meanwhile, with the novel inter-modality mixing method, we can achieve a significantly better performance with the unified model when both RGB camera and depth sensor are available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>image from RGB and pseudo image converted from point clouds Model A: only process RGB images. The visualization is generated from the model trained only based on RGB image from this work; Model B: only process pseudo images converted from point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>2D detection results based on the SUN RGB-D validation data split. It has the 2D detection visualization of four examples based on RGB image (left), pseudo images converted from point clouds (middle) and inter-modality mixing of RGB image and pseudo images converted from point clouds (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>An example of converted pseudo three channel image from the point cloud. This example is one image selected from the validation set of the SUN RGB-D dataset. The corresponding RGB one can be found inFigure 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Inter modality mixing: left: the upper one is the original RGB image and the bottom one is the original DHS image; Middle: the Chessboard Per Patch Mixing image, the upper one has the patch size of 15 by 15 pixels and the bottom one is 1 by 1 pixel. Right: the Stochastic Flood Fill Mixing image. The upper one has the edge connection probability of 0.5, 0.5 for RGB and DHS separately and the bottom one has the probability of 0.1 and 0.1 for RGB and DHS separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>-</head><label></label><figDesc>Per Patch Mixing (PPM): divide the whole image into different patches with equal patch size. Randomly or alternatively select one image source for each patch. -Stochastic Flood Fill Mixing (SFFM): Using a stochastic way to mix the images from different modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Python style of the Stochastic Flood Fill Mixing pseudocode: the mixture mask defines which image's pixel value we should use to generate the mixing image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>plored Swin-Tiny and Swin-Small's performance. The complexity of Swin-T and Swin-S are similar to those of ResNet-50 and ResNet-101, respectively. The window size is set to M = 7 by default. The query dimension of each head is d = 32, and the expansion layer of each MLP is ? = 4. The architecture hyper-parameters of these two models are: -Swin-T: C = 96, layer numbers = {2, 2, 6, 2} -Swin-S: C = 96, layer numbers = {2, 2, 18, 2}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>D 79 detection performance rgb only model test on rgb dhs only model test on dhs YONOD test on rgb YONOD test on dhs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of the unified model and separate models performance with the training epochs. The RGB only one is our new trained model based on RGB images. The DHS only model is from simCrossTrans [1]. The YONOD is trained based on both the RGB and DHS images. The backbone for all those experiments are based on the Swin-T model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>for our YONOD and single modality only models. Similar conclusion can be observed from the result. Result comparison based on mAP50 for different subgroups of YONOD and single modality only models. Result comparison based on mAP50 for different subgroups of YONOD and single modality only models.</figDesc><table><row><cell>model</cell><cell></cell><cell cols="5">test on backbone sunrgbd10 sunrgbd16 sunrgbd66 sunrgbd79</cell></row><row><cell cols="2">RGB only (ours)</cell><cell cols="2">RGB Swin-T</cell><cell>54.2</cell><cell>52.3</cell><cell>29.3</cell><cell>25.2</cell></row><row><cell cols="2">YONOD (ours)</cell><cell cols="2">RGB Swin-T</cell><cell>53.9</cell><cell>52.5</cell><cell>28.7</cell><cell>24.7</cell></row><row><cell cols="4">DHS only (simCrossTrans [1]) DHS Swin-T</cell><cell>55.8</cell><cell>52.7</cell><cell>26.1</cell><cell>22.1</cell></row><row><cell cols="2">YONOD (ours)</cell><cell cols="2">DHS Swin-T</cell><cell>56.6</cell><cell>53.4</cell><cell>27.7</cell><cell>23.5</cell></row><row><cell cols="7">4.5 The performance of YONOD with the inter modality mixing</cell></row><row><cell>model</cell><cell cols="6">test on backbone sunrgbd10 sunrgbd16 sunrgbd66 sunrgbd79</cell></row><row><cell>YONOD</cell><cell cols="2">RGB Swin-T</cell><cell>53.9</cell><cell></cell><cell>52.5</cell><cell>28.7</cell><cell>24.7</cell></row><row><cell cols="3">YONOD + SFFM RGB Swin-T</cell><cell>24.6</cell><cell></cell><cell>17.5</cell><cell>19.2</cell><cell>20.1</cell></row><row><cell cols="3">YONOD + CPPM RGB Swin-T</cell><cell>54.2</cell><cell></cell><cell>51.9</cell><cell>27.7</cell><cell>23.7</cell></row><row><cell cols="3">YONOD + CPPM RGB Swin-S</cell><cell>54.6</cell><cell></cell><cell>52.7</cell><cell>27.5</cell><cell>23.6</cell></row><row><cell>YONOD</cell><cell cols="2">DHS Swin-T</cell><cell>56.6</cell><cell></cell><cell>53.4</cell><cell>27.7</cell><cell>23.5</cell></row><row><cell cols="3">YONOD + SFFM DHS Swin-T</cell><cell>25.6</cell><cell></cell><cell>18.7</cell><cell>20.0</cell><cell>21.3</cell></row><row><cell cols="3">YONOD + CPPM DHS Swin-T</cell><cell>55.8</cell><cell></cell><cell>52.8</cell><cell>26.3</cell><cell>22.4</cell></row><row><cell cols="2">YONOD + CPPM DHS</cell><cell>Swin-S</cell><cell>57.4</cell><cell></cell><cell>52.5</cell><cell>24.8</cell><cell>21.1</cell></row><row><cell cols="3">YONOD + CPPM CPPM Swin-T</cell><cell>58.1</cell><cell></cell><cell>55.8</cell><cell>29.5</cell><cell>25.2</cell></row><row><cell cols="3">YONOD + CPPM CPPM Swin-S</cell><cell>58.4</cell><cell></cell><cell>56.1</cell><cell>28.4</cell><cell>24.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Number of parameters and inference time comparison. All speed testing are based on a standard single NVIDIA Titan-X GPU.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The pretrained weights are loaded from mmdetection<ref type="bibr" target="#b34">[35]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">simcrosstrans: A simple cross-modality transfer learning for object detection with convnets or vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">;</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<editor>Fleet, D.J., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online algorithms for classification of urban objects in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hadjiliadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Second International Conference on 3D Imaging, Modeling, Processing</title>
		<imprint>
			<publisher>Visualization Transmission</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cnn-based object segmentation in urban lidar with missing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frustum voxnet for 3d object detection from rgb-d or depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object detection and instance segmentation from 3d range and 2d color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R.</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A survey of object classification and detection based on 2d/3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<idno>abs/1905.12683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Swin transformer V2: scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2111.09883</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Modality-adaptive mixup and invariant decomposition for rgb-infrared person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<idno>abs/2203.01735</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">In: Class-Aware Modality Mix and Center-Guided Metric Learning for Visible-Thermal Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="889" to="897" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Language models are few-shot learners. CoRR abs/2005.14165 (2020</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Molgnn: Self-supervised motif learning graph neural network for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Molecules Workshop at NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Covid-19 multi-targeted drug repurposing using few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cnn-based object segmentation in urban lidar with missing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online algorithms for classification of urban objects in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hadjiliadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Second International Conference on 3D Imaging, Modeling, Processing</title>
		<imprint>
			<publisher>Visualization Transmission</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<ptr target="https://en.wikipedia.org/w/index.php?title=Floodfill&amp;oldid=1087894346" />
		<title level="m">Wikipedia contributors: Flood fill -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
	<note>Online; accessed 26</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep sliding shapes for amodal 3d object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1511.02300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
