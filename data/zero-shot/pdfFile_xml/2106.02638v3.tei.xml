<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Associating Objects with Transformers for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
							<email>zongxinyang1996@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">CCAI</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yee.i.yang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">CCAI</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Associating Objects with Transformers for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1% J &amp;F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%), while keeping more than 3? faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1 st in the 3rd Large-scale VOS Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video Object Segmentation (VOS) is a fundamental task in video understanding with many potential applications, including augmented reality <ref type="bibr" target="#b33">[34]</ref> and self-driving cars <ref type="bibr" target="#b68">[69]</ref>. The goal of semi-supervised VOS, the main task in this paper, is to track and segment object(s) across an entire video sequence based on the object mask(s) given at the first frame.</p><p>Thanks to the recent advance of deep neural networks, many deep learning based VOS algorithms have been proposed recently and achieved promising performance. STM <ref type="bibr" target="#b36">[37]</ref> and its following works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b29">30]</ref> leverage a memory network to store and read the target features of predicted past frames and apply a non-local attention mechanism to match the target in the current frame. FEELVOS <ref type="bibr" target="#b52">[53]</ref> and CFBI <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref> utilize global and local matching mechanisms to match target pixels or patches from both the first and the previous frames to the current frame.</p><p>Even though the above methods have achieved significant progress, the above methods learn to decode scene features that contain a single positive object. Thus under a multi-object scenario, they have to match each object independently and ensemble all the single-object predictions into a multi-object segmentation, as shown in <ref type="figure">Fig. 1a</ref>. Such a post-ensemble manner eases network architectures' design Computational complexity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AOT (ours)</head><p>Post-ensemble (c) Comparison <ref type="figure">Figure 1</ref>: VOS methods, e.g., <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b45">46]</ref>, process multi-object scenarios in a post-ensemble manner (a). In contrast, our AOT associates all the objects uniformly (b), leading to better efficiency (c).</p><p>since the networks are not required to adapt the parameters or structures for different object numbers. However, modeling multiple objects independently, instead of uniformly, is inefficient in exploring multi-object contextual information to learn a more robust feature representation for VOS. In addition, processing multiple objects separately yet in parallel requires multiple times the amount of GPU memory and computation for processing a single object. This problem restricts the training and application of VOS under multi-object scenarios, especially when computing resources are limited.</p><p>To solve the problem, <ref type="figure">Fig. 1b</ref> demonstrates a feasible approach to associate and decode multiple objects uniformly in an end-to-end framework. Hence, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple targets uniformly. First, an identification mechanism is proposed to assign each target a unique identity and embed multiple targets into the same feature space. Hence, the network can learn the association or correlation among all the targets. Moreover, the multi-object segmentation can be directly decoded by utilizing assigned identity information. Second, a Long Short-Term Transformer (LSTT) is designed for constructing hierarchical object matching and propagation. Each LSTT block utilizes a long-term attention for matching with the first frame's embedding and a short-term attention for matching with several nearby frames' embeddings. Compared to the methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46]</ref> utilizing only one attention layer, we found hierarchical attention structures are more effective in associating multiple objects.</p><p>We conduct extensive experiments on two popular multi-object benchmarks for VOS, i.e., YouTube-VOS <ref type="bibr" target="#b62">[63]</ref> and DAVIS 2017 <ref type="bibr" target="#b42">[43]</ref>, to validate the effectiveness and efficiency of the proposed AOT. Even using the light-weight Mobilenet-V2 <ref type="bibr" target="#b44">[45]</ref> as the backbone encoder, the AOT variant networks achieve superior performance on the validation 2018 &amp; 2019 splits of the large-scale YouTube-VOS (ours, J &amp;F 82.6? 84.5% &amp; 82.2? 84.5%) while keeping more than 2? faster multi-object run-time (27.1? 9.3FPS) compared to the state-of-the-art competitors (e.g., CFBI <ref type="bibr" target="#b66">[67]</ref>, 81.4% &amp; 81.0%, 3.4FPS). We also achieve new state-of-the-art performance on both the DAVIS-2017 validation (85.4%) and testing (81.2%) splits. Moreover, AOT is effective under single-object scenarios as well and outperforms previous methods on DAVIS 2016 <ref type="bibr" target="#b40">[41]</ref> (92.0%), a popular single-object benchmark. Besides, our smallest variant, AOT-T, can maintain real-time multi-object speed on all above benchmarks (51.4FPS on 480p videos). Particularly, AOT ranked 1 st in the Track 1 (Video Object Segmentation) of the 3rd Large-scale Video Object Segmentation Challenge.</p><p>Overall, our contributions are summarized as follows:</p><p>? We propose an identification mechanism to associate and decode multiple targets uniformly for VOS. For the first time, multi-object training and inference can be efficient as single-object ones, as demonstrated in <ref type="figure">Fig. 1c</ref>. ? Based on the identification mechanism, we design a new efficient VOS framework, i.e., Long Short-Term Transformer (LSTT), for constructing hierarchical multi-object matching and propagation. LSTT achieves superior performance on VOS benchmarks <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41]</ref> while maintaining better efficiency than previous state-of-the-art methods. To the best of our knowledge, LSTT is the first hierarchical framework for object matching and propagation by applying transformers <ref type="bibr" target="#b50">[51]</ref> to VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semi-supervised Video Object Segmentation. Given one or more annotated frames (the first frame in general), semi-supervised VOS methods propagate the manual labeling to the entire video sequence. Traditional methods often solve an optimization problem with an energy defined over a graph structure <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b1">2]</ref>. In recent years, VOS methods have been mainly developed based on deep neural networks (DNN), leading to better results.</p><p>Early DNN methods rely on fine-tuning the networks at test time to make segmentation networks focus on a specific object. Among them, OSVOS <ref type="bibr" target="#b7">[8]</ref> and MoNet <ref type="bibr" target="#b61">[62]</ref> fine-tune pre-trained networks on the first-frame ground-truth at test time. OnAVOS <ref type="bibr" target="#b53">[54]</ref> extends the first-frame fine-tuning by introducing an online adaptation mechanism. Following these approaches, MaskTrack <ref type="bibr" target="#b39">[40]</ref> and PReM <ref type="bibr" target="#b30">[31]</ref> utilize optical flow to help propagate the segmentation mask from one frame to the next. Despite achieving promising results, the test-time fine-tuning restricts the network efficiency.</p><p>Recent works aim to achieve a better run-time and avoid using online fine-tuning. OSMN <ref type="bibr" target="#b65">[66]</ref> employs one convolutional network to extract object embedding and another one to guide segmentation predictions. PML <ref type="bibr" target="#b10">[11]</ref> learns pixel-wise embedding with a nearest neighbor classifier, and VideoMatch <ref type="bibr" target="#b19">[20]</ref> uses a soft matching layer that maps the pixels of the current frame to the first frame in a learned embedding space. Following PML and VideoMatch, FEELVOS <ref type="bibr" target="#b52">[53]</ref> and CFBI <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref> extend the pixel-level matching mechanism by additionally matching between the current frame and the previous frame. RGMP <ref type="bibr" target="#b60">[61]</ref> also gathers guidance information from both the first frame and the previous frame but uses a siamese encoder with two shared streams. STM <ref type="bibr" target="#b36">[37]</ref> and its following works (e.g., EGMN <ref type="bibr" target="#b29">[30]</ref> and KMN <ref type="bibr" target="#b45">[46]</ref>) leverage a memory network to embed past-frame predictions into memory and apply a non-local attention mechanism on the memory to decode the segmentation of the current frame. SST <ref type="bibr" target="#b14">[15]</ref> utilizes attention mechanisms in a different way, i.e., transformer blocks <ref type="bibr" target="#b50">[51]</ref> are used to extract pixel-level affinity maps and spatial-temporal features. The features are target-agnostic, instead of target-aware like our LSTT, since the mask information in past frames is not propagated and aggregated in the blocks. Instead of using matching mechanisms, LWL <ref type="bibr" target="#b6">[7]</ref> proposes to use an online few-shot learner to learn to decode object segmentation.</p><p>The above methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources of single-object cases. The problem restricts the application and development of the VOS with multiple targets. Hence, we propose our AOT to associate and decode multiple targets uniformly and simultaneously, as efficiently as processing a single object.</p><p>Visual Transformers. Transformers <ref type="bibr" target="#b50">[51]</ref> was proposed to build hierarchical attention-based networks for machine translation. Similar to Non-local Neural Networks <ref type="bibr" target="#b55">[56]</ref>, transformer blocks compute correlation with all the input elements and aggregate their information by using attention mechanisms <ref type="bibr" target="#b4">[5]</ref>. Compared to RNNs, transformer networks model global correlation or attention in parallel, leading to better memory efficiency, and thus have been widely used in natural language processing (NLP) tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>. Recently, transformer blocks were introduced to many computer vision tasks, such as image classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b27">28]</ref>, object detection <ref type="bibr" target="#b8">[9]</ref>/segmentation <ref type="bibr" target="#b57">[58]</ref>, and image generation <ref type="bibr" target="#b37">[38]</ref>, and have shown promising performance compared to CNN-based networks.</p><p>Many VOS methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref> have utilized attention mechanisms to match the object features and propagate the segmentation mask from past frames to the current frames. Nevertheless, these methods consider only one positive target in the attention processes, and how to build hierarchical attention-based propagation has been rarely studied. In this paper, we carefully design a long shortterm transformer block, which can effectively construct multi-object matching and propagation within hierarchical structures for VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Revisit Previous Solutions for Video Object Segmentation</head><p>In VOS, many common video scenarios have multiple targets or objects required for tracking and segmenting. Benefit from deep networks, current state-of-the-art VOS methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b66">67]</ref> have achieved promising performance. Nevertheless, these methods focus on matching and decoding a single object. Under a multi-object scenario, they thus have to match each object independently and ensemble all the single-object predictions into a multi-object prediction, as demonstrated in <ref type="figure">Fig. 1a</ref>. Let F N denotes a VOS network for predicting single-object segmentation, and A is an ensemble function such as sof tmax or the soft aggregation <ref type="bibr" target="#b36">[37]</ref>, the formula of such a post-ensemble manner for processing N objects is like, where I t and I m denote the image of the current frame and memory frames respectively, and {Y m 1 , ..., Y m N } are the memory masks (containing the given reference mask and past predicted masks) of all the N objects. This manner extends networks designed for single-object VOS into multi-object applications, so there is no need to adapt the network for different object numbers.</p><formula xml:id="formula_0">Y = A(F N (I t , I m , Y m 1 ), ..., F N (I t , I m , Y m N )),<label>(1)</label></formula><p>Although the above post-ensemble manner is prevalent and straightforward in the VOS field, processing multiple objects separately yet in parallel requires multiple times the amount of GPU memory and computation for matching a single object and decoding the segmentation. This problem restricts the training and application of VOS under multi-object scenarios when computing resources are limited. To make the multi-object training and inference as efficient as single-object ones, an expected solution should be capable of associating and decoding multiple objects uniformly instead of individually. To achieve such an objective, we propose an identification mechanism to embed the masks of any number (required to be smaller than a pre-defined large number) of targets into the same high-dimensional space. Based on the identification mechanism, a novel and efficient framework, i.e., Associating Objects with Transformers (AOT), is designed for propagating all the object embeddings uniformly and hierarchically, from memory frames to the current frame.</p><p>As shown in <ref type="figure">Fig. 1b</ref>, our AOT associates and segments multiple objects within an end-to-end framework. For the first time, processing multiple objects can be as efficient as processing a single object ( <ref type="figure">Fig. 1c</ref>). Compared to previous methods, our training under multi-object scenarios is also more efficient since AOT can associate multiple object regions and learn contrastive feature embeddings among them uniformly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Associating Objects with Transformers</head><p>In this section, we introduce our identification mechanism proposed for efficient multi-object VOS. Then, we design a new VOS framework, i.e., long short-term transformer, based on the identification mechanism for constructing hierarchical multi-object matching and propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Identification Mechanism for Multi-object Association</head><p>Many recent VOS methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref> utilized attention mechanisms and achieved promising results. To formulate, we define Q ? R HW ?C , K ? R T HW ?C , and V ? R T HW ?C as the query embedding of the current frame, the key embedding of the memory frames, and the value embedding of the memory frames respectively, where T , H, W , C denote the temporal, height, width, and channel dimensions. The formula of a common attention-based matching and propagation is,</p><formula xml:id="formula_1">Att(Q, K, V ) = Corr(Q, K)V = sof tmax( QK tr ? C )V,<label>(2)</label></formula><p>where a matching map is calculated by the correlation function Corr, and then the value embedding, V , will be propagated into each location of the current frame.</p><p>In the common single-object propagation <ref type="bibr" target="#b36">[37]</ref>, the binary mask information in memory frames is embedded into V with an additional memory encoder network and thus can also be propagated to the current frame by using Eq. 2. A convolutional decoder network following the propagated feature will decode the aggregated feature and predict the single-object probability logit of the current frame.</p><p>The main problem of propagating and decoding multi-object mask information in an end-to-end network is how to adapt the network to different target numbers. To overcome this problem, we propose an identification mechanism consisting of identification embedding and decoding based on attention mechanisms.</p><p>First, an Identification Embedding mechanism is proposed to embed the masks of multiple different targets into the same feature space for propagation. As seen in <ref type="figure" target="#fig_1">Fig. 2b</ref>, we initialize an identity bank, D ? R M ?C , where M identification vectors with C dimensions are stored. For embedding multiple different target masks, each target will be randomly assigned a different identification vector. Assuming N (N &lt; M ) targets are in the video scenery, the formula of embedding the targets' one-hot mask, Y ? {0, 1} T HW ?N , into a identification embedding, E ? R T HW ?C , by randomly assigning identification vector from the bank D is,</p><formula xml:id="formula_2">E = ID(Y, D) = Y P D,<label>(3)</label></formula><p>where P ? {0, 1} N ?M is a random permutation matrix, satisfying that P tr P is equal to a M ? M unit matrix, for randomly selecting N identification embeddings. After the ID assignment, different target has different identification embedding, and thus we can propagate all the target identification information from memory frames to the current frame by attaching the identification embedding E with the attention value V , i.e.,</p><formula xml:id="formula_3">V = AttID(Q, K, V, Y |D) = Att(Q, K, V + ID(Y, D)) = Att(Q, K, V + E),<label>(4)</label></formula><p>where V ? R HW ?C aggregates all the multiple targets' embeddings from the propagation.</p><p>For Identification Decoding, i.e., predicting all the targets' probabilities from the aggregated feature V , we firstly predict the probability logit for every identity in the bank D by employing a convolutional decoding network F D , and then select the assigned ones and calculate the probabilities, i.e.,</p><formula xml:id="formula_4">Y = sof tmax(P F D (V )) = sof tmax(P L D ),<label>(5)</label></formula><p>where L D ? R HW ?M is all the M identities' probability logits, P is the same as the selecting matrix used in the identity assignment (Eq. 3), and Y ? [0, 1] HW ?N is the probability prediction of all the N targets.</p><p>For training, common multi-class segmentation losses, such as cross-entropy loss, can be used to optimize the multi-object Y regarding the ground-truth labels. The identity bank D is trainable and randomly initialized at the training beginning. To ensure that all the identification vectors have the same opportunity to compete with each other, we randomly reinitialize the identification selecting matrix P in each video sample and each optimization iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long Short-Term Transformer for Hierarchical Matching and Propagation</head><p>Previous methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46]</ref> always utilize only one layer of attention (Eq. 2) to aggregate singleobject information. In our identification-based multi-object pipeline, we found that a single attention layer cannot fully model multi-object association, which naturally should be more complicated than single-object processes. Thus, we consider constructing hierarchical matching and propagation by using a series of attention layers. Recently, transformer blocks <ref type="bibr" target="#b50">[51]</ref> have been demonstrated to be stable and promising in constructing hierarchical attention structures in visual tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. Based on transformer blocks, we carefully design a Long Short-Term Transformer (LSTT) block for multi-object VOS.</p><p>Following the common transformer blocks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b12">13]</ref>, LSTT firstly employs a self-attention layer, which is responsible for learning the association or correlation among the targets within the current frame. Then, LSTT additionally introduces a long-term attention, for aggregating targets' information from long-term memory frames and a short-term attention, for learning temporal smoothness from nearby short-term frames. The final module is based on a common 2-layer feed-forward MLP with GELU <ref type="bibr" target="#b18">[19]</ref> non-linearity in between. <ref type="figure" target="#fig_1">Fig. 2c</ref> shows the structure of an LSTT block. Notably, all these attention modules are implemented in the form of the multi-head attention <ref type="bibr" target="#b50">[51]</ref>, i.e., multiple attention modules followed by concatenation and a linear projection. Nevertheless, we only introduce their single-head formulas below for the sake of simplicity.</p><p>Long-Term Attention is responsible for aggregating targets' information from past memory frames, which contains the reference frame and stored predicted frames, to the current frame. Since the time intervals between the current frame and past frames are variable and can be long-term, the temporal smoothness is difficult to guarantee. Thus, the long-term attention employs non-local attention like Eq. 2. Let X t l ? R HW ?C denotes the input feature embedding at time t and in block l, where l ? {1, ..., L} is the block index of LSTT, the formula of the long-term attention is,</p><formula xml:id="formula_5">AttLT (X t l , X m l , Y m ) = AttID(X t l W K l , X m l W K l , X m l W V l , Y m |D),<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">X m l = Concat(X m1 l , ..., X m T l ) and Y m = Concat(Y m1 , ..., Y m T )</formula><p>are the input feature embeddings and target masks of memory frames with indices m = {m 1 , ..., m T }. Besides, W K l ? R C?C k and W V l ? R C?Cv are trainable parameters of the space projections for matching and propagation, respectively. Instead of using different projections for X t l and X m l , we found the training of LSTT is more stable with a siamese-like matching, i.e., matching between the features within the same embedding space (l-th features with the same projection of W K l ). Short-Term Attention is employed for aggregating information in a spatial-temporal neighbourhood for each current-frame location. Intuitively, the image changes across several contiguous video frames are always smooth and continuous. Thus, the target matching and propagation in contiguous frames can be restricted in a small spatial-temporal neighborhood, leading to better efficiency than non-local processes. Considering n neighbouring frames with indices n = {t ? 1, ..., t ? n} are in the spatialtemporal neighbourhood, the features and masks of these frames are X n l = Concat(X t?1 l , ..., X t?n l ) and Y n = Concat(Y t?1 , ..., Y t?n ), and then the formula of the short-term attention at each spatial location p is,</p><p>AttST</p><formula xml:id="formula_7">(X t l , X n l , Y n |p) = AttLT (X t l,p , X n l,N (p) , Y n l,N (p) ),<label>(7)</label></formula><p>where X t l,p ? R 1?C is the feature of X t l at location p, N (p) is a ? ? ? spatial neighbourhood centered at location p, and thus X n l,N (p) and Y n l,N (p) are the features and masks of the spatial-temporal neighbourhood, respectively, with a shape of n? 2 ? C or n? 2 ? N .</p><p>When extracting features of the first frame t = 1, there is no memory frames or previous frames, and hence we use X 1 l to replace X m l and X n l . In other words, the long-term attention and the short-term attention are changed into self-attentions without adjusting the network structures and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation Details</head><p>Network Details: For sufficiently validating the effectiveness of our identification mechanism and LSTT, we mainly use light-weight backbone encoder, MobileNet-V2 <ref type="bibr" target="#b44">[45]</ref>, and decoder, FPN <ref type="bibr" target="#b25">[26]</ref> with Group Normalization <ref type="bibr" target="#b59">[60]</ref>. The spatial neighborhood size ? is set to 15, and the number of identification vectors, M , is set to 10, which is consistent with the maximum object number in the benchmarks <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b42">43]</ref>. AOT performs well with PaddlePaddle <ref type="bibr" target="#b0">[1]</ref> and PyTorch <ref type="bibr" target="#b38">[39]</ref>. More details can be found in the supplementary material. In the experiments, we also equip AOT-L with ResNet50 (R50) <ref type="bibr" target="#b17">[18]</ref> or Swin-B <ref type="bibr" target="#b27">[28]</ref>.</p><p>AOT-S is a small model with only 2 layers of LSTT block. Compared to AOT-S, AOT-T utilizes only 1 layer of LSTT, and AOT-B/L uses 3 layers. In AOT-T/S/B, only the first frame is considered into long-term memory, which is similar to <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b66">67]</ref>, leading to a smooth efficiency. In AOT-L, the predicted frames are stored into long-term memory per ? frames, following the memory reading strategy <ref type="bibr" target="#b36">[37]</ref>. We set ? to 2/5 for training/testing.</p><p>Training Details: Following <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>, the training stage is divided into two phases: <ref type="bibr" target="#b0">(1)</ref> pre-training on sythetic video sequence generated from static image datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref> by  randomly applying multiple image augmentations <ref type="bibr" target="#b60">[61]</ref>. (2) main training on the VOS benchmarks <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b42">43]</ref> by randomly applying video augmentations <ref type="bibr" target="#b66">[67]</ref>. More details are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We evaluate AOT on popular multi-object benchmarks, YouTube-VOS <ref type="bibr" target="#b62">[63]</ref> and DAVIS 2017 <ref type="bibr" target="#b42">[43]</ref>, and single-object benchmark, DAVIS 2016 <ref type="bibr" target="#b40">[41]</ref>. For YouTube-VOS experiments, we train our models on the YouTube-VOS 2019 training split. For DAVIS, we train on the DAVIS-2017 training split. When evaluating YouTube-VOS, we use the default 6FPS videos, and all the videos are restricted to be smaller than 1.3 ? 480p resolution. As to DAVIS, the default 480p 24FPS videos are used.</p><p>The evaluation metric is the J score, calculated as the average Intersect over Union (IoU) score between the prediction and the ground truth mask, and the F score, calculated as an average boundary similarity measure between the boundary of the prediction and the ground truth, and their mean value, denoted as J &amp;F. We evaluate all the results on official evaluation servers or with official tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Compare with the State-of-the-art Methods</head><p>YouTube-VOS <ref type="bibr" target="#b62">[63]</ref> is the latest large-scale benchmark for multi-object video segmentation and is about 37 times larger than DAVIS 2017 (120 videos). Specifically, YouTube-VOS contains 3471 videos in the training split with 65 categories and 474/507 videos in the validation 2018/2019 split with additional 26 unseen categories. The unseen categories do not exist in the training split to evaluate algorithms' generalization ability. <ref type="table" target="#tab_0">Table 1a</ref>, AOT variants achieve superior performance on YouTube-VOS compared to the previous state-of-the-art methods. With our identification mechanism, AOT-S (82.6% J &amp;F)   <ref type="table" target="#tab_0">Table 1b</ref> shows that our R50-AOT-L (Y) surpasses all the competitors on both the DAVIS-2017 validation (84.9%) and testing (79.6%) splits and maintains an efficient speed (18.0FPS). Notably, such a multiobject speed is the same as our single-object speed on DAVIS 2016. For the first time, processing multiple objects can be as efficient as processing a single object over the AOT framework. We also evaluate our method without training with YouTube-VOS, and AOT-S (79.2%) performs much better than KMN <ref type="bibr" target="#b45">[46]</ref> (76.0%) by +3.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>DAVIS 2016 <ref type="bibr" target="#b40">[41]</ref> is a single-object benchmark containing 20 videos in the validation split. Although our AOT aims at improving multi-object video segmentation, we also achieve a new state-of-the-art performance on DAVIS 2016 (R50-AOT-L (Y), 91.1%). Under single-object scenarios, the multiobject superiority of AOT is limited, but R50-AOT-L still maintains an about 2? efficiency compared to KMN (18.0 vs 8.3FPS). Furthermore, our smaller variant, AOT-B (89.9%), achieves comparable performance with CFBI+ (89.9%) while running 5? faster (29.6 vs 5.9FPS).</p><p>Apart from the above results, replacing the AOT encoder from commonly used ResNet50 to SwinB can further boost our performance to higher level <ref type="table" target="#tab_0">(Table 1a</ref>, 1b, and 2).</p><p>Qualitative results: <ref type="figure" target="#fig_4">Fig. 3</ref> visualizes some qualitative results in comparison with CFBI <ref type="bibr" target="#b66">[67]</ref>, which only associates each object with its relative background. As demonstrated, CFBI is easier to confuse multiple highly similar objects. In contrast, our AOT tracks and segments all the targets accurately by associating all the objects uniformly. However, AOT fails to segment some tiny objects (ski poles and watch) since we do not make special designs for tiny objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>In this section, we analyze the main components and hyper-parameters of AOT and evaluate their impact on the VOS performance in <ref type="table">Table 3</ref>.</p><p>Identity number: The number of the identification vectors, M , have to be larger than the object number in videos. Thus, we set M to 10 in default to be consistent with the maximum object number <ref type="table">Table 3</ref>: Ablation study. The experiments are based on AOT-S and conducted on the validation 2018 split of YouTube-VOS <ref type="bibr" target="#b62">[63]</ref> without pre-training on synthetic videos. Self: the position embedding type used in the self-attention. Rel: use relative positional embedding <ref type="bibr" target="#b46">[47]</ref> on the local attention. in the benchmarks <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b42">43]</ref>. As seen in <ref type="table">Table 3a</ref>, M larger than 10 leads to worse performance since (1) no training video contains so many objects; (2) embedding more than 10 objects into the space with only 256 dimensions is difficult.</p><p>Local window size: <ref type="table">Table 3b</ref> shows that larger local window size, ?, results in better performance. Without the local attention, ? = 0, the performance of AOT significantly drops from 80.3% to 74.3%, which demonstrates the necessity of the local attention.</p><p>Local frame number: In <ref type="table">Table 3c</ref>, we also try to employ more previous frames in the local attention, but using only the t ? 1 frame (80.3%) performs better than using 2/3 frames (80.0%/79.1%). A possible reason is that the longer the temporal interval, the more intense the motion between frames, so it is easier to introduce more errors in the local matching when using an earlier previous frame.</p><p>LSTT block number: As shown in <ref type="table">Table 3d</ref>, the AOT performance increases by using more LSTT blocks. Notably, the AOT with only one LSTT block (77.9%) reaches a fast real-time speed (41.0FPS) on YouTube-VOS, although the performance is -2.4% worse than AOT-S (80.3%). By adjusting the LSTT block number, we can flexibly balance the accuracy and speed of AOT.</p><p>Position embedding: In our default setting, we apply fixed sine spatial positional embedding to the self-attention following <ref type="bibr" target="#b8">[9]</ref>, and our local attention is equipped with learned relative positional embedding <ref type="bibr" target="#b46">[47]</ref>. The ablation study is shown in <ref type="table">Table 3e</ref>, where removing the sine embedding decreases the performance to 80.1% slightly. In contrast, the relative embedding is more important than the sine embedding. Without the relative embedding, the performance drops to 79.7%, which means the motion relationship between adjacent frames is helpful for local attention. We also tried to apply learned positional embedding to self-attention modules, but no positive effect was observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper proposes a novel and efficient approach for video object segmentation by associating objects with transformers and achieves superior performance on three popular benchmarks. A simple yet effective identification mechanism is proposed to associate, match, and decode all the objects uniformly under multi-object scenarios. For the first time, processing multiple objects in VOS can be efficient as processing a single object by using the identification mechanism. In addition, a long short-term transformer is designed for constructing hierarchical object matching and propagation for VOS. The hierarchical structure allows us to flexibly balance AOT between real-time speed and stateof-the-art performance by adjusting the LSTT number. We hope the identification mechanism will help ease the future study of multi-object VOS and related tasks (e.g., video instance segmentation, interactive VOS, and multi-object tracking), and AOT will serve as a solid baseline.  <ref type="figure">Figure 4</ref>: Visualization of the cosine similarity between every two of M identification vectors in the identity bank. We use the form of a M ? M symmetric matrix to visualize all the cosine similarities, and the values on the diagonal are all equal to 1. The darker the green color, the higher the similarity.</p><p>In the case of M = 10, the similarities are stable and balanced. As the vector number M increases, The visualized matrix becomes less and less smooth, which means the similarities become unstable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 More Implementation Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Network Details</head><p>For MobileNet-V2 encoder, we increase the final resolution of the encoder to 1/16 by adding a dilation to the last stage and removing a stride from the first convolution of this stage. For ResNet-50 and SwinB encoders, we remove the last stage directly. The encoder features are flattened into sequences before LSTT. In LSTT, the input channel dimension is 256, and the head number is set to 8 for all the attention modules. To increase the receptive field of LSTT, we insert a depth-wise convolution layer with a kernel size of 5 between two layers of each feed-forward module. In our default setting of the short-term memory n, only the previous (t ? 1) frame is considered, which is similar to the local matching strategy <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b66">67]</ref>. After LSTT, all the output features of LSTT blocks are reshaped into 2D shapes and will serve as the decoder input. Then, the FPN decoder progressively increases the feature resolution from 1/16 to 1/4 and decreases the channel dimension from 256 to 128 before the final output layer, which is used for identification decoding.</p><p>Patch-wise Identity Bank: Since the spatial size of LSTT features is only 1/16 of the input video, we can not directly assign identities to the pixels of high-resolution input mask to construct a lowresolution identification embedding. To overcome this problem, we further propose a strategy named patch-wise identity bank. In detail, we first separate the input mask into non-overlapping patches of 16?16 pixels. The original identity bank with M identities is also expanded to a patch-wise identity bank, in which each identity has 16?16 sub-identity vectors corresponding to 16?16 positions in a patch. Hence, the pixels of an object region with different patch positions will have different sub-identity vectors under an assigned identity. By summing all the assigned sub-identities in each patch, we can directly construct a low-resolution identification embedding while keeping the shape information inside each patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Training Details</head><p>All the videos are firstly down-sampled to 480p resolution, and the cropped window size is 465? 465. For optimization, we adopt the AdamW <ref type="bibr" target="#b28">[29]</ref> optimizer and the sequential training strategy <ref type="bibr" target="#b66">[67]</ref>, whose sequence length is set to 5. The loss function is a 0.5:0.5 combination of bootstrapped crossentropy loss and soft Jaccard loss <ref type="bibr" target="#b34">[35]</ref>. For stabilizing the training, the statistics of BN <ref type="bibr" target="#b21">[22]</ref> modules and the first two stages in the encoder are frozen, and Exponential Moving Average (EMA) <ref type="bibr" target="#b41">[42]</ref> is used. Besides, we apply stochastic depth <ref type="bibr" target="#b20">[21]</ref> to the self-attention and the feed-forward modules in LSTT.</p><p>The batch size is 16 and distributed on 4 Tesla V100 GPUs. For pre-training, we use an initial learning rate of 4 ? 10 ?4 and a weight decay of 0.03 for 100,000 steps. For main training, the initial learning rate is set to 2 ? 10 ?4 and the weight decay is 0.07. In addition, the training steps are 100,000 for YouTube-VOS or 50,000 for DAVIS. To relieve over-fitting, the initial learning rate of encoders is reduced to a 0.1 scale of other network parts. All the learning rates gradually decay to 2 ? 10 ?5 in a polynomial manner <ref type="bibr" target="#b66">[67]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Visualization of Identity Bank</head><p>In AOT, the identity bank is randomly initialized, and all the M identification vectors are learned by being randomly assigned to objects during the training phase. Intuitively, all the identification vectors should be equidistant away from each other in the feature space because their roles are equivalent. To validate our hypothesis, we visualize the similarity between every two identification vectors in <ref type="figure">Fig. 4</ref>.</p><p>In our default setting, M = 10 <ref type="figure">(Fig. 4a</ref>), all the vectors are far away from each other, and the similarities remain almost the same. This phenomenon is consistent with our above hypothesis. In other words, the reliability and effectiveness of our identification mechanism are further verified.</p><p>In the ablation study, using more identities leads to worse results. To analyze the reason, we also visualize the learned identity banks with more vectors. <ref type="figure">Fig. 4b, 4c</ref>, and 4d demonstrate that maintaining equidistant between every two vectors becomes more difficult when the identity bank contains more vectors, especially when M = 30. There are two possible reasons for this phenomenon: <ref type="bibr" target="#b0">(1)</ref> No training video contains enough objects to be assigned so many identities, and thus the network cannot learn to associate all the identities simultaneously; (2) the used space with only 256 dimensions is difficult for keeping more than 10 objects to be equidistant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Illustration of Long Short-term Attention</head><p>To facilitate understanding our long-term and short-term attention modules, we illustrate their processes in <ref type="figure" target="#fig_7">Fig. 5</ref>. Since the temporal smoothness between the current frame and long-term memory frames is difficult to guarantee, the long-term attention employs a non-local manner to match all the locations in the long-term memory. In contrast, short-term attention only focuses on a nearby spatial-temporal neighborhood of each current-frame location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Visualization of Hierarchical Matching and Propagation</head><p>In our AOT, we propose to construct a hierarchical framework, i.e., LSTT, for multi-object matching and propagation, and the ablation study indicates that using more LSTT layers (or blocks) results in better VOS performance. To further validate the effectiveness of LSTT and analyze the behavior of each LSTT layer, we visualize long-term and short-term attention maps in each layer during inference, as shown in <ref type="figure">Fig. 6 and 7</ref>.</p><p>At the bottom of <ref type="figure">Fig. 6</ref>, the attention maps become more accurate and sharper as the index of layers increases. In the first layer, i.e., l = 1, the current features have not aggregated the multi-object mask information from memory frames, and the long-term attention map is very vague and contains a lot of wrong matches among the objects and the background. Nevertheless, as the layer index increases, the mask information of all the objects is gradually aggregated so that the long-term attention becomes more and more accurate. Similarly, the quality, especially the boundary of objects, of the short-term attention improves as the layer index increases. Notably, the short-term attention performs well even in the first layer, l = 1, which is different from the long-term attention. The reason is that the Long-term Attention Short-term Attention <ref type="figure">Figure 6</ref>: Visualization of long-term and short-term attention maps during the inference of DAVIS 2017 <ref type="bibr" target="#b42">[43]</ref>. There are three similar people, marked in different colors, in the video. To sufficiently verify the effect of long-term attention, only the first frame is considered into the long-term memory, and thus we AOT-B to conduct the experiment. For visualization, we propagate the colored multiobject masks in long-term or short-term memory to the current frame regarding the corresponding attention map. The brighter the color, the stronger the attention. l = 1, 2, and 3 denote the 3 LSTT layers of AOT-B in order.  <ref type="figure">Figure 7</ref>: Visualization of long-term and short-term attention maps during the inference of DAVIS 2017 <ref type="bibr" target="#b42">[43]</ref>. In this case, the red person is occluded in the t ? 1 frame, and thus the short-term attention fails to match the person in the current frame. However, the long-term attention generates an accurate attention map with a clean background in the last layer, l = 3, resulting in a correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-term Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short-term Attention</head><p>neighborhood matching of short-term attention is easier than the long-term matching of long-term attention. However, long-term attention is still necessary because short-term attention will fail in some cases, such as occlusion <ref type="figure">(Fig. 7)</ref>.</p><p>In short, the visual analysis further proves the necessity and effectiveness of our hierarchical LSTT. The hierarchical matching is not simply a combination of multiple matching processes. Critically, the multi-object information will be gradually aggregated and associated as the LSTT structure goes deeper, leading to more accurate attention-based matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Compare with More Methods</head><p>We compare our AOT with more VOS methods in <ref type="table" target="#tab_3">Table 4</ref> and 5. To compare with latest real-time methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>, we introduce the real-time AOT variant, i.e., AOT-T.</p><p>Even on the single-object DAVIS 2016 <ref type="bibr" target="#b40">[41]</ref>, AOT-S (89.4%, 40.0FPS) can achieve comparable speed with SAT <ref type="bibr" target="#b9">[10]</ref> (83.1%, 39FPS) and comparable performance with CFBI <ref type="bibr" target="#b66">[67]</ref> (89.4%, 6.3FPS). On the multi-object DAVIS 2017 <ref type="bibr" target="#b42">[43]</ref>, AOT-T (79.9%, 51.4FPS) significantly outperforms SAT (72.3%, 19.5FPS) and GC (71.4%, 12.5FPS). Particularly, on the large-scale multi-object YouTube-VOS <ref type="bibr" target="#b62">[63]</ref>, AOT-T/S (80.2%/82.6%) achieves superior performance compared to previous real-time methods, SAT (63.6%) and GC (73.2%), while still maintaining a real-time speed (41.0FPS/27.1FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Additional Qualitative Results</head><p>We supply more qualitative results under multi-object scenarios on the large-scale YouTube-VOS <ref type="bibr" target="#b62">[63]</ref> and the small-scale DAVIS 2017 <ref type="bibr" target="#b42">[43]</ref> in <ref type="figure">Fig. 8 and 9</ref>, respectively. As demonstrated, our AOT-L is robust to many challenging VOS cases, including similar objects, occlusion, fast motion, and motion blur, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Border Impact and Future Works</head><p>The proposed AOT framework greatly simplifies the process of multi-object VOS and achieves a significant performance of effectiveness, robustness, and efficiency. Some AOT variants can achieve promising results while keeping real-time speed. In other words, AOT may promote the applications of VOS in real-time video systems, such as video conference, self-driving car, augmented reality, etc.</p><p>Nevertheless, the speed and accuracy of AOT can still be further improved. There is still a very large accuracy gap between the real-time AOT-T and the state-of-the-art SwinB-AOT-L. Moreover, AOT uses only a lightweight encoder and decoder. How to design stronger yet efficient encoders and decoders for VOS is still an open question.</p><p>As to related areas of VOS, the simple yet effective identification mechanism should also be promising for many tasks requiring multi-object matching, such as interactive video object segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32]</ref>, video instance segmentation <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b56">57]</ref>, and multi-object tracking <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b58">59]</ref>. Besides, the hierarchical LSTT may serve as a new solution for processing video representations in these tasks.   <ref type="figure">Figure 8</ref>: Qualitative results on the validation 2019 split of YouTube-VOS <ref type="bibr" target="#b62">[63]</ref>. Our AOT-L performs well under many challenging multi-object cases, including similar objects, occlusion, and fast motion, etc. time 0% 25% 50% 75% 100% Testing Validation <ref type="figure">Figure 9</ref>: Qualitative results on DAVIS 2017 <ref type="bibr" target="#b42">[43]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) The overview of our Associating Objects with Transformers (AOT). The multi-object masks are embedded by using our Identification mechanism. Moreover, a L-layer Long Short-Term Transformer is responsible for matching multiple objects uniformly and hierarchically. (b) An illustration of the IDentity assignment (ID) designed for transferring a N -object mask into an identification embedding. (c) The structure of an LSTT block. LN: layer normalization<ref type="bibr" target="#b2">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Architecture Variants:</head><label></label><figDesc>We build several AOT variant networks with different LSTT layer number L or long-term memory size m. The hyper-parameters of these variants are: (1) AOT-Tiny: L = 1, m = {1}; (2) AOT-Small: L = 2, m = {1}; (3) AOT-Base: L = 3, m = {1}; (4) AOT-Large: L = 3, m = {1, 1 + ?, 1 + 2?, 1 + 3?, ...}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-L (Y) 84.9 82.3 87.5 18.0 SwinB-AOT-L (Y) 85.4 82.4 88.4 12.1 Testing 2017 Split CFBI [67] (Y) 75.0 71.4 78.7 5.3 CFBI * [67] (Y) 76.6 73.0 80.1 2.9 KMN * [46] -L (Y) 81.2 77.3 85.1 12.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results. (top) Compared with CFBI<ref type="bibr" target="#b66">[67]</ref>, AOT performs better when segmenting multiple highly similar objects (carousels and zebras). (bottom) AOT fails to segment some tiny objects (ski poles and watch) since AOT has no specific design for processing rare tiny objects.is comparable with CFBI+ [68] (82.8%) while running about 7? faster (27.1 vs 4.0FPS). By using more LSTT blocks, AOT-B improves the performance to 83.5%. Moreover, AOT-L further improves both the seen and unseen scores by utilizing the memory reading strategy, and our R50-AOT-L (84.1%/84.1%) significantly outperforms the previous methods on the validation 2018/2019 split while maintaining an efficient speed (14.9FPS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) M = 10 (default)(b) M = 15 (c) M = 20 (d) M = 30</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Illustrations of the long-term attention and the short-term attention. (a) The long-term attention employs a non-local manner to match all the locations in the long-term memory. (b) In contrast, the short-term attention only focus on a nearby spatial-temporal region with a shape of n? 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The quantitative evaluation on multi-object benchmarks, YouTube-VOS<ref type="bibr" target="#b62">[63]</ref> and DAVIS 2017<ref type="bibr" target="#b42">[43]</ref>. Y: using YouTube-VOS for training. : using 600p instead of 480p videos in inference. ? : timing extrapolated from single-object speed assuming linear scaling in the number of objects.</figDesc><table><row><cell></cell><cell cols="2">(a) YouTube-VOS</cell><cell></cell><cell></cell><cell></cell><cell>(b) DAVIS 2017</cell></row><row><cell></cell><cell cols="2">Seen</cell><cell cols="2">Unseen</cell><cell></cell><cell>Methods</cell><cell>J &amp;F J</cell><cell>F FPS</cell></row><row><cell>Methods</cell><cell>J &amp;F J</cell><cell>F</cell><cell>J</cell><cell cols="2">F FPS</cell><cell>Validation 2017 Split</cell></row><row><cell></cell><cell cols="2">Validation 2018 Split</cell><cell></cell><cell></cell><cell></cell><cell>CFBI [67] (Y)</cell><cell>81.9 79.3 84.5 5.9</cell></row><row><cell>STM[ICCV19] [37] KMN[ECCV20] [46] CFBI[ECCV20] [67] LWL[ECCV20] [7]</cell><cell cols="5">79.4 79.7 84.2 72.8 80.9 -81.4 81.4 85.6 75.3 83.3 -81.4 81.1 85.8 75.3 83.4 3.4 81.5 80.4 84.9 76.4 84.4 -</cell><cell>SST [15] (Y) KMN [46] KMN [46] (Y) CFBI+ [68] (Y)</cell><cell>82.5 79.9 85.1 76.0 74.2 77.8 4.2  ? -82.8 80.0 85.6 4.2  ? 82.9 80.1 85.7 5.</cell></row><row><cell>SST[CVPR21] [15]</cell><cell cols="4">81.7 81.2 -76.0 -</cell><cell>-</cell></row><row><cell cols="6">CFBI+[TPAMI21] [68] 82.8 81.8 86.6 77.1 85.6 4.0</cell></row><row><cell>AOT-T</cell><cell cols="5">80.2 80.1 84.5 74.0 82.2 41.0</cell></row><row><cell>AOT-S</cell><cell cols="5">82.6 82.0 86.7 76.6 85.0 27.1</cell></row><row><cell>AOT-B</cell><cell cols="5">83.5 82.6 87.5 77.7 86.0 20.5</cell></row><row><cell>AOT-L</cell><cell cols="5">83.8 82.9 87.9 77.7 86.5 16.0</cell></row><row><cell>R50-AOT-L</cell><cell cols="5">84.1 83.7 88.5 78.1 86.1 14.9</cell></row><row><cell>SwinB-AOT-L</cell><cell cols="5">84.5 84.3 89.3 77.9 86.4 9.3</cell></row><row><cell></cell><cell cols="2">Validation 2019 Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CFBI[ECCV20] [67]</cell><cell cols="5">81.0 80.6 85.1 75.2 83.0 3.4</cell></row><row><cell>SST[CVPR21] [15]</cell><cell cols="4">81.8 80.9 -76.6 -</cell><cell>-</cell></row><row><cell cols="6">CFBI+[TPAMI21] [68] 82.6 81.7 86.2 77.1 85.2 4.0</cell></row><row><cell>AOT-T</cell><cell cols="5">79.7 79.6 83.8 73.7 81.8 41.0</cell></row><row><cell>AOT-S</cell><cell cols="5">82.2 81.3 85.9 76.6 84.9 27.1</cell></row><row><cell>AOT-B</cell><cell cols="5">83.3 82.4 87.1 77.8 86.0 20.5</cell></row><row><cell>AOT-L</cell><cell cols="5">83.7 82.8 87.5 78.0 86.7 16.0</cell></row><row><cell>R50-AOT-L</cell><cell cols="5">84.1 83.5 88.1 78.4 86.3 14.9</cell></row><row><cell>SwinB-AOT-L</cell><cell cols="5">84.5 84.0 88.8 78.4 86.7 9.3</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The validation split of DAVIS 2017 consists of 30 videos with 59 objects, and the training split contains 60 videos with 138 objects. Moreover, the testing split contains 30 more challenging videos with 89 objects in total.</figDesc><table><row><cell>DAVIS 2017 [43] is a multi-object extension of</cell><cell cols="3">The quantitative evaluation on the</cell></row><row><cell>DAVIS 2016.</cell><cell cols="2">single-object DAVIS 2016 [41].</cell></row><row><cell></cell><cell>Methods</cell><cell>J &amp;F J</cell><cell>F FPS</cell></row><row><cell></cell><cell>CFBI+ [68] (Y)</cell><cell cols="2">89.9 88.7 91.1 5.9</cell></row><row><cell></cell><cell>KMN [46] (Y)</cell><cell cols="2">90.5 89.5 91.5 8.3</cell></row><row><cell></cell><cell>AOT-T (Y)</cell><cell cols="2">86.8 86.1 87.4 51.4</cell></row><row><cell></cell><cell>AOT-S (Y)</cell><cell cols="2">89.4 88.6 90.2 40.0</cell></row><row><cell></cell><cell>AOT-B (Y)</cell><cell cols="2">89.9 88.7 91.1 29.6</cell></row><row><cell></cell><cell>AOT-L (Y)</cell><cell cols="2">90.4 89.6 91.1 18.7</cell></row><row><cell></cell><cell>R50-AOT-L (Y)</cell><cell cols="2">91.1 90.1 92.1 18.0</cell></row><row><cell></cell><cell cols="3">SwinB-AOT-L (Y) 92.0 90.7 93.3 12.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Additional quantitative comparison on multi-object benchmarks, YouTube-VOS<ref type="bibr" target="#b62">[63]</ref> and DAVIS 2017<ref type="bibr" target="#b42">[43]</ref>.</figDesc><table><row><cell></cell><cell cols="2">(a) YouTube-VOS</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Seen</cell><cell cols="2">Unseen</cell></row><row><cell>Methods</cell><cell>J &amp;F J</cell><cell>F</cell><cell>J</cell><cell>F FPS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Additional quantitative comparison on DAVIS 2016<ref type="bibr" target="#b40">[41]</ref>.</figDesc><table><row><cell>Methods</cell><cell>J &amp;F J</cell><cell>F FPS</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Parallel distributed deep learning: Machine learning framework from industrial practice</title>
		<ptr target="https://www.paddlepaddle.org.cn/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avinash Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9739" to="9748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">State-aware tracker for real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9384" to="9393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8953" to="8962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Agss-vos: Attention guided single-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3949" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Memory aggregation networks for efficient interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Video segmentation and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimal decisions from probabilistic models: the intersection-over-union case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fast user-guided video object segmentation by interactionand-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5247" to="5256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end asr: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12894" to="12904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="496" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Boltvos: Box-level tracking for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04552</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. In: CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Towards real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<title level="m">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Segment as points for efficient online multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="264" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Collaborative video object segmentation by multi-scale foregroundbackground integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
