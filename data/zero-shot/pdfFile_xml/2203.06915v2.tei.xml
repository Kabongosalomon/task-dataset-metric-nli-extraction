<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimMatch: Semi-supervised Learning with Similarity Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimMatch: Semi-supervised Learning with Similarity Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning with few labeled data has been a longstanding problem in the computer vision and machine learning research community. In this paper, we introduced a new semisupervised learning framework, SimMatch, which simultaneously considers semantic similarity and instance similarity. In SimMatch, the consistency regularization will be applied on both semantic-level and instance-level. The different augmented views of the same instance are encouraged to have the same class prediction and similar similarity relationship respected to other instances. Next, we instantiated a labeled memory buffer to fully leverage the ground truth labels on instance-level and bridge the gaps between the semantic and instance similarities. Finally, we proposed the unfolding and aggregation operation which allows these two similarities be isomorphically transformed with each other. In this way, the semantic and instance pseudo-labels can be mutually propagated to generate more high-quality and reliable matching targets. Extensive experimental results demonstrate that SimMatch improves the performance of semi-supervised learning tasks across different benchmark datasets and different settings. Notably, with 400 epochs of training, SimMatch achieves 67.2%, and 74.4% Top-1 Accuracy with 1% and 10% labeled examples on ImageNet, which significantly outperforms the baseline methods and is better than previous semi-supervised learning frameworks. Code and pre-trained models are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Benefiting from the availability of large-scale annotated datasets and growing computational resources in the last decades, deep neural networks have demonstrated their success on a variety of visual tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56]</ref>. However, a large volume of labeled data is very expensive to collect in a real-world scenario. Learning with few labeled data has been a longstanding problem in the * Correspondence to: Shan You &lt;youshan@sensetime.com&gt;.  <ref type="figure">Figure 1</ref>. A sketch of SimMatch. The Fully-Connected layer vectors can be viewed as a semantic representative or class center for each category. However, due to the limited labeled samples, the semantic-level information is not always reliable. In SimMatch, we consider the instance-level and semantic-level information simultaneously and adopt a labeled memory buffer to fully leverage the ground truth label on instance-level. computer vision and machine learning research community. Among various methods, semi-supervised learning (SSL) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b63">63]</ref> serves as an effective solution by dint of the help of massive unlabeled data, and achieves remarkable performance.</p><p>A simple but very effective semi-supervised learning method is to pretrain a model on a large-scale dataset and then transfer the learned representation by fine-tuning the pretrained model with a few labeled samples. Thanks to the recent advances in self-supervised learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b51">52]</ref>, such pretraining and fine-tuning pipeline have demonstrated its promising performance in SSL. Most selfsupervised learning frameworks focus on the design of pretext tasks. For example, instance discrimination <ref type="bibr" target="#b52">[53]</ref> encourages different views of the same instance to share the same features, and different instances should have distinct features. Deep clustering based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> expect different augmented views of the same instance should be classified into the same cluster. However, most of these pretext tasks are designed in a completely unsupervised manner, without considering the few labeled data at hand.</p><p>Instead of standalone two-stage pretraining and fine-tuning, current popular methods directly involve the labeled data in a joint feature learning paradigm with pseudolabeling <ref type="bibr" target="#b32">[33]</ref> or consistency regularization <ref type="bibr" target="#b44">[45]</ref>. The main idea behind these methods is to train a semantic classifier with labeled samples and use the predicted distribution as the pseudo label for the unlabeled samples. In this way, the pseudo-labels are generally produced by the weakly augmented views <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46]</ref> or the averaged predictions of multiple strongly augmented views <ref type="bibr" target="#b5">[6]</ref>. The objective will be constructed by the cross entropy loss between an different strongly augmented views and the pseudo-labels. It might also be noted that the pseudo-labels will generally be sharpened or operated by argmax since every instance is expected to be classified into a category. However, when there are only very limited annotated data, the semantic classifier is no longer reliable; applying the pseudo-label method will cause the "overconfidence" issue <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b59">59]</ref>, which means the model will fit on the confident but wrong pseudo-labels, resulting in poor performance.</p><p>In this paper, we introduce a novel semi-supervised learning framework, SimMatch, which has been shown in <ref type="figure">Figure 1</ref>. In SimMatch, we bridge both sides and propose to match the similarity relationships of both semantic and instance levels simultaneously for different augmentations. Specifically, we first require the strongly augmented view to have the same semantic similarity (i.e. label prediction) with a weak augmented view; besides, we also encourage the strong augmentation to have the same instance characteristics (i.e. similarity between instances) with the weak one for more intrinsic feature matching. Moreover, different from the previous works that simply regard the predictions of the weakly augmented views as pseudo-labels. In Sim-Match, the semantic and instance pseudo-labels are allowed to interact by instantiating a memory buffer that keeps all the labeled examples. In this way, these two similarities can be isomorphically transformed with each other by introducing aggregating and unfolding techniques. Thus the semantic and instance pseudo-labels can be mutually propagated to generate more high-quality and reliable matching targets. Extensive experiments demonstrate the effectiveness of SimMatch across different settings. Our contribution can be summarized as follows:</p><p>? We proposed SimMatch, a novel semi-supervised learning framework that simultaneously considers semantics similarity and instance similarity.</p><p>? To channel both similarities, we leverage a labeled memory buffer so that semantic and instance pseudolabels can be mutually propagated with the aggregating and unfolding techniques.</p><p>? SimMatch establishes a new state-of-the-art performance for semi-supervised learning. With only 400 epochs of training, SimMatch achieves 67.2% and 74.4% Top-1 accuracy with 1% and 10% labeled examples on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-Supervised Learning</head><p>Consistency Regularization is a widely adopted method in semi-supervised learning. The main idea is to enforce the model to output a consistent prediction for the different perturbed versions of the same instance. For example, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45]</ref> achieved such consistency requirement by minimizing the mean square difference between the predicted probability distribution of the two transformed views. In this case, the transformation could be either domain-specific data augmentations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46]</ref>, or some regularization techniques in the network (e.g. drop out <ref type="bibr" target="#b46">[47]</ref> and random max-pooling <ref type="bibr" target="#b44">[45]</ref>). Moreover, <ref type="bibr" target="#b31">[32]</ref> also proposed a temporal ensemble strategy to aggregate the predictions of multiple previous networks, which makes the predicted distribution more reliable. Mean Teacher <ref type="bibr" target="#b49">[50]</ref> further extends this idea which replaced the aggregated predictions with the output of an exponential moving average (EMA) model.</p><p>MixMatch <ref type="bibr" target="#b5">[6]</ref>, ReMixMatch <ref type="bibr" target="#b4">[5]</ref>, and FixMatch <ref type="bibr" target="#b45">[46]</ref> are three augmentation anchoring based methods that fully leverage the augmentation consistency. Specifically, Mix-Match adopts a sharpened averaged prediction of multiple strongly augmented views as the pseudo label and utilizes the MixUp trick <ref type="bibr" target="#b60">[60]</ref> to further enhance the pseudo label. ReMixMatch improved this idea by generating the pseudo label with weakly augmented views and also introduced a distribution alignment strategy that encourages the pseudo label distribution to match the marginal distribution of ground-truth class labels. FixMatch simplified these ideas, where the unlabeled images are only retained if the model produces a high-confidence pseudo label. Despite its simplicity, FixMatch achieved state-of-the-art performance among the augmentation anchoring-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised Pretraining</head><p>Apart from the typical semi-supervised learning method, self-supervised and contrastive learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53]</ref> has gained much attention in this research community since fine-tuning the pre-trained model with labeled samples has shown promising classification results, especially SimCLR v2 <ref type="bibr" target="#b14">[15]</ref> shows that a big (deep and wide) pre-trained model is a strong semi-supervised learner. Most of the contrastive learning framework adopts the instance discrimination <ref type="bibr" target="#b52">[53]</ref> as the pretext task, which defines different augmented views of the same instance as positive pairs, while negative pairs are formed by sampling views from different instances. However, because of the existence of similar samples, treating different instances as negative pairs will result in a class collision problem <ref type="bibr" target="#b1">[2]</ref>, which is not conducive to the down-  <ref type="figure">Figure 2</ref>. An overview of the SimMatch pseudo-label generation process. SimMatch will use the weak augmented view to generate a semantic pseudo-label and an instance pseudo-label. Specifically, we will first compute the semantic and instance similarity by the class centers and labeled embeddings, then use the unfolding and aggregation operations to fuse these two similarities and finally get the pseudo-label. Please see more details in our method section below.</p><p>stream tasks (especially classification tasks). Some previous works addressed this issue by unsupervised clustering <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b61">61]</ref>, where similar samples will be clustered into the same class. There are also some other methods designed various negative free pretext tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b62">62]</ref> to avoid the class collision problem. Both cluster-based methods and negative-free based methods have shown significant improvements for downstream classification tasks. CoMatch <ref type="bibr" target="#b33">[34]</ref> combines the idea of consistency regularization and contrastive learning, where the target similarity of two instances is measured by the similarity between two class probability distributions, which achieves the current state-of-the-art performance on semi-supervised learning. However, it is very sensitive to the hyper-parameters, the optimal temperature and threshold is different for various datasets and settings. Compared to CoMatch, SimMatch is faster, more robust, and has higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we will first revisit the preliminary work on augmentation anchoring based semi-supervised learning frameworks; then, we will introduce our proposed method SimMatch. After that, the algorithm and the implementation details will also be explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>We define the semi-supervised image classification problem as following. Given a batch of B labeled samples</p><formula xml:id="formula_0">X = {x b : b ? (1, .</formula><p>.., B)}, we randomly apply a weak augmentation function (e.g. using only a flip and a crop) T w (?) to obtain the weakly augmented samples. Then, a convolutional neural network based encoder F (?) is employed to extract the feature information from these samples, i.e. h = F(T (x)). Finally, a fully connected class prediction head ?(?) is utilized to map h b into semantic similarities, which can be written as: p = ?(h). The labeled samples could be directly optimized by the cross entropy loss with the ground truth labels:</p><formula xml:id="formula_1">L s = 1 B H(y, p)<label>(1)</label></formula><p>Let us define a batch of ?B unlabeled samples U = {u b : b ? (1, ..., ?B)}. By following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, we randomly apply the weak and strong augmentation T w (?), T s (?) and using the same processing step as the labeled samples to get the semantic similarities for weakly augmented sample p w (pseudo label) and strongly augmented sample p s . Then the unsupervised classification loss can be defined as the crossentropy between these two predictions:</p><formula xml:id="formula_2">L u = 1 ?B 1(max DA(p w ) &gt; ? )H(DA(p w ), p s ) (2)</formula><p>Where ? is the confidence threshold. Following <ref type="bibr" target="#b45">[46]</ref>, we only retain the unlabeled samples whose largest class probability in the pseudo-labels are larger than ? . DA(?) stands for the distribution alignment strategy from <ref type="bibr" target="#b4">[5]</ref> which balanced the pseudo-labels distribution. We simply follow the implementation from <ref type="bibr" target="#b33">[34]</ref> where we maintain a moving-average of p w avg and adjust the current p w with N ormalize(p w /p w avg ). Please also noted that we do not take the sharpened or one-hot version of p w , DA(p w ) will be directly served as the pseudo-label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Similarity Matching</head><p>In SimMatch, we also consider the instance-level similarity as we have discussed previously. Concretely, we encourage the strongly augmented view to have a similar similarity distribution with the weakly augmented view. Suppose we have a non-linear projection head g(?) which maps the representation h to a low-dimensional embedding</p><formula xml:id="formula_3">z b = g(h b ).</formula><p>Following the anchoring based method, we use z w b and z s b to denote the embedding from the weakly and strongly augmented view. Now, lets assume we have K weakly augmented embeddings for a bunch of different samples {z k : k ? (1, ..., K)}, we calculate the similarities between z w and i-th instance by using a similarity function sim(?), which represents the dot product between L 2 normalized vectors sim(u, v) = u T v/ u v . A softmax layer can be adopted to process the calculated similarities, which then produces a distribution:</p><formula xml:id="formula_4">q w i = exp(sim(z w b , z i )/t) K k=1 exp(sim(z w b , z k )/t)<label>(3)</label></formula><p>Where t is the temperature parameter that controls the sharpness of the distribution. On the other hand, we can calculate the similarities between the strongly augmented view z s and z i as sim(z s b , z i ). The resulting similarity distribution can be written as:</p><formula xml:id="formula_5">q s i = exp(sim(z s b , z i )/t) K k=1 exp(sim(z s b , z k )/t)<label>(4)</label></formula><p>Finally, the consistency regularization can be achieved by minimizing the different between q s and q w . Here, we adopt the cross entropy loss, which can be formulated as:</p><formula xml:id="formula_6">L in = 1 ?B H(q w , q s )<label>(5)</label></formula><p>Please noted that the instance consistency regularization will only be applied on the unlabeled examples. The overall training objective for our model will be:</p><formula xml:id="formula_7">L overall = L s + ? u L u + ? in L in<label>(6)</label></formula><p>where ? u and ? in are the balancing factors that control the weights of the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Label Propagation through SimMatch</head><p>Although our overall training objective also considers the consistency regularization on instance-level, however, the generation of the instance pseudo-labels q w are still in a fully unsupervised manner, which is absolutely a waste of the labeled information. To improve the quality of the pseudo-labels, in this section, we will illustrate how to leverage the labeled information on instance-level and also introduce a way that allows semantic similarity and instance similarity to interact with each other.</p><p>We instantiated a labeled memory buffer to keep all the annotated examples as we have shown in <ref type="figure">Figure 2</ref> (red branch). In this way, each z k that we used in Eq.(3) and Eq.(4) could be assigned to a specific class. If we interpret the vectors in ? as the "centered" class references, the embeddings in our labeled memory buffer can be viewed as a set of "individual" class references.</p><p>By given a weakly augmented sample, we first compute the semantic similarity p w ? R 1?L and instance similarity q w ? R 1?K . (Noted that L is generally much smaller than K since we need at least one sample for each class.) To calibrate q w with p w , we need to unfold p w into K dimensional space which we denote it as p unf old . we achieved this by matching the corresponding semantic similarity for each labeled embedding:</p><formula xml:id="formula_8">p unf old i = p w j , where class(q w j ) = class(p w i )<label>(7)</label></formula><p>where class(?) is the function that returns the ground truth class. Specifically, class(q w j ) represent the label for the j th element in memory buffer and class(p w i ) means the i th class. Now, we regenerate the calibrated instance pseudo-labels by scaling q w with p unf old , which can be expressed as the following:</p><formula xml:id="formula_9">q i = q w i p unf old i K k=1 q w k p unf old k<label>(8)</label></formula><p>The calibrated instance pseudo-label q will be served as a new target and replace the old one q w in Eq. <ref type="bibr" target="#b4">(5)</ref>. On the other hand, we can also use the instance similarity to adjust the semantic similarity. To do this, we first need to aggregate q into L dimensional space which we denote it as q agg . We achieved this by sum over the instance similarities that share the same ground truth labels:</p><formula xml:id="formula_10">q agg i = K j=0 1(class(p w i ) = class(q w j ))q w j<label>(9)</label></formula><p>Now, we regenerate the adjusted semantic pseudo-label by smoothing p w with q agg , which can be written as:</p><formula xml:id="formula_11">p i = ?p w i + (1 ? ?)q agg i<label>(10)</label></formula><p>where ? is the hyper-parameter that controls the weight of the semantic and instance information. Similarly, The adjusted semantic pseudo-label will replace the old one p w i in Eq. <ref type="bibr" target="#b1">(2)</ref>. In this way, the pseudo-label p and q will both contains the semantic-level and instance-level information. As we have shown in <ref type="figure">Figure 3</ref>, when semantic and instance similarities are similar, which means these two distributions agree with the prediction of each other, then the result pseudo-label will be much sharper and produce high confidence for some classes. On the other hand, if these two similarities are different, the result pseudo-label will be much flatter and not contain high probability values. In SimMatch, we adopt the scaling and smoothing strategy for q and p respectively, we also have tried different combination for these two strategies, please see more details in our ablation study section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Similarity</head><p>Pseudo-Label <ref type="figure">Figure 3</ref>. The intuition behind label propagation. If the semantic and instance similarities are similar, the result pseudo-label will be much sharper and produce high confidence for some classes. When these two similarities are different, the result pseudo-label will be much flatter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Efficient Memory Buffer</head><p>As we have mentioned, SimMatch requires a memory buffer to keep the embeddings for labeled examples. In doing so, we are required to store both feature embeddings and the ground truth labels. Specifically, we defined a feature memory buffer Q f ? R K?D and a label memory buffer Q l ? R K?1 where K is the number of available annotated samples, D is the embedding size. The largest K in our experiments is around 10 5 (ImageNet 10% setting), which only costs 64M GPU memories for Q f . For Q l , we just need to store a scalar for each label, the aggregation and unfolding operation can be easily achieved by gather and scatter add function, which should has been efficiently implemented in recent deep learning libraries <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref>. In this case, Q l only costs less than 1M GPU memories (K = 10 5 ), which is almost negligible.</p><p>According to <ref type="bibr" target="#b24">[25]</ref>, the rapid changed feature in memory buffer will dramatically reduce the performance. In Sim-Match, we adopt two different implementation for different buffer size. When K is large, we follow MoCo <ref type="bibr" target="#b24">[25]</ref> to leverage a student-teacher based framework, we denote it as F s and F t . In this case, the labeled examples and strongly augmented samples will be passed into F s , the weakly augmented samples will be feed into F t to generate the pseudolabels. The parameters of F t will be updated by:</p><formula xml:id="formula_12">F t ? mF t + (1 ? m)F s<label>(11)</label></formula><p>On the other hand, when K is small, maintain a teacher network is not necessary, We simply adopt the temporal ensemble strategy <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">53]</ref> to smooth the features in memory buffer, which can be written as :</p><formula xml:id="formula_13">z t ? mz t?1 + (1 ? m)z t<label>(12)</label></formula><p>In this case, all the samples will be directly pass into the same encoder. The student-teacher version of SimMatch has been illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: SimMatch (Student-Teacher)</head><p>Input: x l and :x u a batch of labeled and unlabeled samples. T w (?) and T s (?): Weak and strong augmentation function. F t and F s : Teacher and student encoder. ? t and ? s : teacher and student classifier. g t and g s : teacher and student projection head. Q f and Q l : The feature and label memory buffer. </p><formula xml:id="formula_14">while network not converge do for i=1 to step do h w = F t (T w (x u )) h s = F s (T s (x u )) p w = DA(? t (h w )) p s = ? s (h s ) z w = g t (h w ) z s = g s (h s ) h l t = F t (T w (x l )) h l s = F s (T w (x l )) p l = ? s (h l s ) z l = g t (h l t ) Compute</formula><formula xml:id="formula_15">L s = 1 B H(y, p l ) L u = 1 ?B 1(max p &gt; ? )H( p, p s ) L in = 1 ?B H( q, q s ) L overall = L s + ? u L u + ? in L in</formula><p>Optimize F s , g s and ? s by L overall Momentum Update F t , g t and ? t Update Q f and Q l with z l and y end end Output: The well trained model F s and g s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will first test SimMatch on various dataset and settings to shows its superiority, then we will ablation each component to validate the effectiveness of each component in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CIFAR-10 and CIFAR-100</head><p>We first evaluation SimMatch on CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b30">[31]</ref> datasets. CIFAR-10 consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. CIFAR-100 is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. For CIFAR-10, we randomly sample 4, 25, and 400 samples from the training set as the labeled data and use the rest of the training set as the unlabeled data. For CIFAR-100, we perform the same experiments but use 4, 25, and 100 samples per class.</p><p>Implementation Details. Most our implementations follows <ref type="bibr" target="#b45">[46]</ref>. Specifically, we adopt WRN28-2, and WRN28-8 <ref type="bibr" target="#b56">[57]</ref> for CIFAR-10 and CIFAR-100 respectively. We use a standard SGD optimizer with Nesterov momen- tum <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b48">49]</ref> and set the initial learning rate to 0.03. For the learning rate schedule, we use a cosine learning rate decay <ref type="bibr" target="#b37">[38]</ref> which adjust the learning rate to 0.03 ? cos( 7?s 16S ) where s is the current training step, and S is the total number of training steps. We also report the final performance using an exponential moving average of model parameters. Note that we use an identical set of hyper-parameters for both datasets (? u = 1, ? in = 1, t = 0.1, ? = 0.9, ? = 0.95, ? = 7, m = 0.7, B = 64, S = 2 20 ) . For distribution alignment, we accumulate the past 32 steps p w for calculating the moving average p w avg . We adopt the temporal ensemble memory buffer <ref type="bibr" target="#b31">[32]</ref> since most settings for these two datasets have a relatively small K. For the implementations of the strong and weak augmentations, we strictly follow the FixMatch <ref type="bibr" target="#b45">[46]</ref>.</p><p>Results. The result has been reported in table 1. For baseline, we mainly consider methods ?-Model <ref type="bibr" target="#b31">[32]</ref>, Pseudo-Labeling <ref type="bibr" target="#b32">[33]</ref>, Mean Teacher <ref type="bibr" target="#b49">[50]</ref>, UDA <ref type="bibr" target="#b53">[54]</ref>, Mix-Match <ref type="bibr" target="#b5">[6]</ref>, ReMixMatch <ref type="bibr" target="#b4">[5]</ref>, FixMatch <ref type="bibr" target="#b45">[46]</ref>, and CoMatch <ref type="bibr" target="#b33">[34]</ref>. We compute the mean and variance of accuracy when training on 5 different "folds" of labeled data. As we can see that the SimMatch achieves state-of-the-art performance on various settings, especially on CIFAR-100. For CIFAR-10, SimMatch has a large performance gain on 40 labels setting, but the improvements for 250 and 4000 is relatively small. We doubt that this is due to the accuracy of 95% ? 96% being already quite close to the supervised performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ImageNet-1k</head><p>We also performed SimMatch on the large-scale ImageNet-1k dataset <ref type="bibr" target="#b18">[19]</ref> to show the the superiority. Specifically, we test our algorithm on 1% and 10% settings. We follow the same label generation process as in CoMatch <ref type="bibr" target="#b33">[34]</ref>, where 13 and 128 labeled samples will be selected per class for 1% and 10% settings respectively. Implementation Details. For ImageNet-1k, we adopt ResNet-50 <ref type="bibr" target="#b26">[27]</ref> and use a standard SGD optimizer with Nesterov momentum. We warm up the model for five epochs until it reaches the initial learning rate 0.03 and then cosine decay it to 0. We use the same set of hyper-parameters for both 1% and 10% settings (? u = 10, ? in = 5, t = 0.1, ? = 0.9, ? = 0.7, ? = 5, m = 0.999, B = 64). We keep the past 256 steps p w for distribution alignment. We choose the student-teacher version memory buffer and test performance on the student network. For strong augmentation, we follow the same strategy in MoCo v2 <ref type="bibr" target="#b15">[16]</ref>.</p><p>Results. We have shown the results in <ref type="table" target="#tab_1">Table 2</ref>. As we can see, with 400 epochs training, SimMatch achieves 67.2%, and 74.4% Top-1 accuracy on 1% and 10% labeled examples, which is significantly better than the previous methods. FixMatch-EMAN <ref type="bibr" target="#b7">[8]</ref> achieves a slightly lower performance (74.0%) on 10% setting. However, it requires 800 epochs of self-supervised pretrain (MoCo-EMAN) where SimMatch can directly train from scratch. The most recent work PAWS <ref type="bibr" target="#b3">[4]</ref> achieves 66.5% and 75.5% Top-1 accuracy on 1% and 10% settings with 300 epochs training. Nevertheless, PAWS requires the multi-crops strategy <ref type="bibr" target="#b9">[10]</ref> and 970 ? 7 labeled examples to construct the support set. For each epoch, the actual training FLOPS of PAWS is 4 times that of SimMatch. Hence, the reported 300 epochs PAWS should have similar training FLOPS with 1200 epochs SimMatch. Due to the limited GPU resources, we cannot push this research to such a scale, but since Sim-Match surpassed PAWS on 1% setting with 1/3 training costs (400 epochs), we believe it can already demonstrate the superiority of our method.</p><p>Transfer Learning. We also evaluate the learned representations on multiple downstream classification tasks. We follow the linear evaluation setup described in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>. Specifically, we trained an L2-regularized multinomial logistic regression classifier on features extracted from the frozen pretrained network (400 epochs 10% SimMatch), then we used L-BFGS <ref type="bibr" target="#b36">[37]</ref> to optimize the softmax crossentropy objective, and we did not apply data augmentation. We selected the best L2-regularization parameter and learning rate from validation splits and applied it to the test sets. The datasets used in this benchmark are as follows: CIFAR-10 <ref type="bibr" target="#b30">[31]</ref>, CIFAR-100 <ref type="bibr" target="#b30">[31]</ref>, Food101 <ref type="bibr" target="#b6">[7]</ref>, Cars <ref type="bibr" target="#b29">[30]</ref>, DTD <ref type="bibr" target="#b17">[18]</ref>, Pets <ref type="bibr" target="#b40">[41]</ref>, Flowers <ref type="bibr" target="#b39">[40]</ref>. The results have been shown in <ref type="table">Table 3</ref>. As we can see, with only 400 epochs of training, SimMatch achieves the best performance on CIFAR-10, CIFAR-100, Cars and Flowers datasets which is comparable with BYOL and significantly better than Sim-CLR, MoCo V2, and supervised baseline. These results further validate the representation quality of SimMatch for classification tasks.</p><p>Training Efficiency. Next, we test the actual training speed for FixMatch, CoMatch, and SimMatch. The results is shown in <ref type="table">Table 4</ref>, SimMatch is nearly 17% faster than FixMatch and CoMatch. In FixMatch, the weakly augmented U will be passed into the online network, which consumes more resources for the extra computational graphs. But in SimMatch, U only needs to be passed into the EMA network, so the computational graph does not need to be retained. Compared with CoMatch which requires two forward passes (strongly and weakly augmented U) for the EMA network, SimMatch only requires one pass. Moreover, CoMatch adopts 4 memory banks (258M Memory) to compute the pseudo-label; SimMatch only needs 2 memory banks with 6.4M / 64M Memory for 1% and 10% labels, thus the pseudo-label generation will also be faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Pseudo-Label Accuracy. Firstly, we would like to show the pseudo-label accuracy of SimMatch. In <ref type="figure">Figure 4</ref>, we visualized the training progress of FixMatch and our method. SimMatch can always generate high-quality pseudo-labels and consistently has higher performance on both unlabeled samples and validation sets.</p><p>Temperature. The temperature t in Eq. (4) and Eq. (3) controls the sharpness of the instance distribution. (Noted t = 0 is equivalent to the argmax operation). We present the result of varying different t value in <ref type="figure">Figure 5a</ref>. As can be seen, the best Top-1 accuracy comes from t = 0.1, and slightly decreased when t = 0.07. This is consist with the most recent works in contrastive learning where t = 0.1 is generally the best temperature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Smooth Parameter. We also show the effective of different smooth parameter ? Eq. (10) in <ref type="figure">Figure 5b</ref>. Specifically, we sweep over [0.8, 0.9, 0.95, 1.0] for ?, it clear to see that ? = 0.9 achieves the best result. Noted that ? = 1.0 is equivalent to directly take the original pseudo-labels p w for Eq. (2), which result in 1.8% performance drop.</p><p>Label Propagation. Next, we would like to verify the effectiveness of the label propagation. The results has been shown in <ref type="table" target="#tab_2">Table 5</ref>. When we remove p, this is the same case with ? = 1.0, so we will not discuss this setting further. If we remove q, which means the projection head will   be trained in a fully unsupervised manner as in <ref type="bibr" target="#b62">[62]</ref>, as we can see the performance is significantly worse than standard SimMatch, demonstrating the importance of our label propagation strategy. Propagation Strategy. Then, we tried different combinations of the scaling and smoothing strategy to generate the pseudo-labels p and q. From <ref type="table">Table 6</ref>, we can see that take smoothing for p and scaling for q achieves the best result. We might notice that applying smoothing to both p and q can achieve similar performance (61.5%). However, the smoothing strategy will introduce a smoothing parameter. Thus, for keeping our framework simple, we prefer to choose the scaling strategy for q.</p><p>Instance Matching Loss Design. To verify the effectiveness of the instance similarity matching term L in , we simply replace it with InfoNCE and SwAV. We show the result in <ref type="table" target="#tab_3">Table 7</ref> When working with InfoNCE loss, we sweep the temperature over [0.07, 0.1, 0.2]. In this case, the best result we can get is 53.5%, which is 8.2% lower than Sim-Match. This is due to the natural conflict between the clas- sification problem and InfoNCE objective. To be specific, the classification problem aims to group similar samples together, but InfoNCE aims to distinguish every instance. When working with SwAV, we tried to set the number of prototypes to 1000, 3000, and 10000. Finally, the best result we can get is 49.7%, which is 12% lower than SimMatch. SwAV aims to distribute the samples equally to each prototype, preventing the model from collapsing. However, distribution alignment has a similar objective, which is adopted by SimMatch in Eq <ref type="bibr" target="#b1">(2)</ref> . Moreover, the SwAV loss will be trained in a completely unsupervised manner, which will lose the power of the labels. The advantage of L in is that the label information can easily cooperate with instance similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new semi-supervised learning framework SimMatch, which considers the consistency regularization on both semantic-level and instance-level. We also introduced a labeled memory buffer to fully leverage the data annotations on instance-level. Finally, our defined unfolding and aggregation operation allows the label to propagate between semantic-level and instance-level information. Extensive experiment shows the effectiveness of each component in our framework. The results on ImageNet-1K demonstrate the state-of-the-art performance for semi-supervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>q w and q s by Eq.(3) Eq.(4) Compute p unf old and q agg by Eq.(7) Eq.(9) Compute q and p by Eq.(8) Eq.(10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Visualization of (a) pseudo-labels accuracy -the accuracy of p that has higher confidence than threshold, (b) unlabeled samples accuracy -the accuracy of all p regardless of the threshold, (c) validation accuracy for FixMatch and SimMatch on 1% and 10% setting. Results of varying t and ?. (ImageNet-1k 1% -100 ep)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Top-1 Accuracy comparison (mean and std over 5 runs) on CIFAR-10 and CIFAR-100 with varying labeled set sizes.</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Method</cell><cell>40 labels</cell><cell>250 labels</cell><cell>4000 labels</cell><cell>400 labels</cell><cell>2500 labels</cell><cell>10000 labels</cell></row><row><cell>?-Model [32]</cell><cell>-</cell><cell>45.74?3.97</cell><cell>58.99?0.38</cell><cell>-</cell><cell>42.75?0.48</cell><cell>62.12?0.11</cell></row><row><cell>Pseudo-Labeling [33]</cell><cell>-</cell><cell>50.22?0.43</cell><cell>83.91?0.28</cell><cell>-</cell><cell>42.62?0.46</cell><cell>63.79?0.19</cell></row><row><cell>Mean Teacher [50]</cell><cell>-</cell><cell>67.68?2.30</cell><cell>90.81?0.19</cell><cell>-</cell><cell>46.09?0.57</cell><cell>64.17?0.24</cell></row><row><cell>UDA [54]</cell><cell>70.95?5.93</cell><cell>91.18?1.08</cell><cell>95.12?0.18</cell><cell>40.72?0.88</cell><cell>66.87?0.22</cell><cell>75.50?0.25</cell></row><row><cell>MixMatch [6]</cell><cell>52.46?11.50</cell><cell>88.95?0.86</cell><cell>93.58?0.10</cell><cell>32.39?1.32</cell><cell>60.06?0.37</cell><cell>71.69?0.33</cell></row><row><cell>ReMixMatch [5]</cell><cell>80.90?9.64</cell><cell>94.56?0.05</cell><cell>95.28?0.13</cell><cell>55.72?2.06</cell><cell>72.57?0.31</cell><cell>76.97?0.56</cell></row><row><cell>FixMatch(RA) [46]</cell><cell>86.19?3.37</cell><cell>94.93?0.65</cell><cell>95.74?0.05</cell><cell>51.15?1.75</cell><cell>71.71?0.11</cell><cell>77.40?0.12</cell></row><row><cell>Dash [55]</cell><cell>86.78?3.75</cell><cell>95.44?0.13</cell><cell>95.92?0.06</cell><cell>55.24?0.96</cell><cell>72.82?0.21</cell><cell>78.03?0.14</cell></row><row><cell>CoMatch [34]</cell><cell>93.09?1.39</cell><cell>95.09?0.33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SimMatch(Ours)</cell><cell>94.40?1.37</cell><cell>95.16?0.39</cell><cell>96.04?0.01</cell><cell>62.19?2.21</cell><cell>74.93?0.32</cell><cell>79.42?0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results on ImageNet with 1% and 10% labeled examples.</figDesc><table><row><cell cols="2">Self-supervised Pre-training</cell><cell>Method</cell><cell></cell><cell>Epochs</cell><cell>Paramters (train/test)</cell><cell></cell><cell cols="3">Top-1 Label fraction Label fraction Top-5 1% 10% 1% 10%</cell></row><row><cell></cell><cell></cell><cell cols="2">Pseudo-label [33, 58]</cell><cell>?100</cell><cell cols="2">25.6M / 25.6M</cell><cell>-</cell><cell>-</cell><cell>51.6</cell><cell>82.4</cell></row><row><cell></cell><cell></cell><cell cols="3">VAT+EntMin. [23, 39, 58] -</cell><cell cols="2">25.6M / 25.6M</cell><cell>-</cell><cell>68.8</cell><cell>-</cell><cell>88.5</cell></row><row><cell></cell><cell></cell><cell cols="2">S4L-Rotation [58]</cell><cell>?200</cell><cell cols="2">25.6M / 25.6M</cell><cell>-</cell><cell>53.4</cell><cell>-</cell><cell>83.8</cell></row><row><cell>None</cell><cell></cell><cell>UDA [54]</cell><cell></cell><cell>-</cell><cell cols="2">25.6M / 25.6M</cell><cell>-</cell><cell>68.8</cell><cell>-</cell><cell>88.5</cell></row><row><cell></cell><cell></cell><cell cols="2">FixMatch [46]</cell><cell>?300</cell><cell cols="2">25.6M / 25.6M</cell><cell>-</cell><cell>71.5</cell><cell>-</cell><cell>89.1</cell></row><row><cell></cell><cell></cell><cell cols="2">CoMatch [34]</cell><cell>?400</cell><cell cols="3">30.0M / 25.6M 66.0</cell><cell>73.6</cell><cell>86.4</cell><cell>91.6</cell></row><row><cell cols="2">PCL [35]</cell><cell></cell><cell></cell><cell>?200</cell><cell cols="2">25.8M / 25.6M</cell><cell>-</cell><cell>-</cell><cell>75.3</cell><cell>85.6</cell></row><row><cell cols="2">SimCLR [14]</cell><cell></cell><cell></cell><cell>?1000</cell><cell cols="3">30.0M / 25.6M 48.3</cell><cell>65.6</cell><cell>75.5</cell><cell>87.8</cell></row><row><cell cols="2">SimCLR V2 [15]</cell><cell>Fine-tune</cell><cell></cell><cell>?800</cell><cell cols="3">34.2M / 25.6M 57.9</cell><cell>68.4</cell><cell>82.5</cell><cell>89.2</cell></row><row><cell cols="2">BYOL [24]</cell><cell></cell><cell></cell><cell>?1000</cell><cell cols="3">37.1M / 25.6M 53.2</cell><cell>68.8</cell><cell>78.4</cell><cell>89.0</cell></row><row><cell cols="2">SwAV [10]</cell><cell></cell><cell></cell><cell>?800</cell><cell cols="3">30.4M / 25.6M 53.9</cell><cell>70.2</cell><cell>78.5</cell><cell>89.9</cell></row><row><cell cols="2">WCL [61]</cell><cell></cell><cell></cell><cell>?800</cell><cell cols="3">34.2M / 25.6M 65.0</cell><cell>72.0</cell><cell>86.3</cell><cell>91.2</cell></row><row><cell></cell><cell></cell><cell>Fine-tune</cell><cell></cell><cell>?800</cell><cell cols="3">30.0M / 25.6M 49.8</cell><cell>66.1</cell><cell>77.2</cell><cell>87.9</cell></row><row><cell cols="2">MoCo V2 [16]</cell><cell cols="2">CoMatch [34]</cell><cell>?1200</cell><cell cols="3">30.0M / 25.6M 67.1</cell><cell>73.7</cell><cell>87.1</cell><cell>91.4</cell></row><row><cell cols="4">MoCo-EMAN [8] FixMatch-EMAN [8]</cell><cell>?1100</cell><cell cols="3">30.0M / 25.6M 63.0</cell><cell>74.0</cell><cell>83.4</cell><cell>90.9</cell></row><row><cell>None</cell><cell></cell><cell cols="2">SimMatch (Ours)</cell><cell>?400</cell><cell cols="3">30.0M / 25.6M 67.2</cell><cell>74.4</cell><cell>87.1</cell><cell>91.6</cell></row><row><cell cols="10">Table 3. Transfer learning performance using ResNet-50 pretrained with ImageNet. Following the evaluatiion protocal from [14, 24], we</cell></row><row><cell cols="9">report Top-1 classification accuracy except Pets and Flowers for which we report mean per-class accuracy.</cell></row><row><cell>Method</cell><cell></cell><cell cols="8">Epochs CIFAR-10 CIFAR-100 Food-101 Cars DTD Pets Flowers Mean</cell></row><row><cell cols="2">Supervised</cell><cell>-</cell><cell>93.6</cell><cell>78.3</cell><cell>72.3</cell><cell>66.7</cell><cell cols="2">74.9 91.5</cell><cell>94.7</cell><cell>81.7</cell></row><row><cell cols="2">SimCLR [14]</cell><cell>1000</cell><cell>90.5</cell><cell>74.4</cell><cell>72.8</cell><cell>49.3</cell><cell cols="2">75.7 84.6</cell><cell>92.6</cell><cell>77.1</cell></row><row><cell cols="2">MoCo v2 [16]</cell><cell>800</cell><cell>92.2</cell><cell>74.6</cell><cell>72.5</cell><cell>50.5</cell><cell cols="2">74.4 84.6</cell><cell>90.5</cell><cell>77.0</cell></row><row><cell cols="2">BYOL [24]</cell><cell>1000</cell><cell>91.3</cell><cell>78.4</cell><cell>75.3</cell><cell>67.8</cell><cell cols="2">75.5 90.4</cell><cell>96.1</cell><cell>82.1</cell></row><row><cell cols="2">SimMatch (10%)</cell><cell>400</cell><cell>93.6</cell><cell>78.4</cell><cell>71.7</cell><cell>69.7</cell><cell cols="2">75.1 92.8</cell><cell>93.2</cell><cell>82.1</cell></row><row><cell cols="5">Table 4. GPU hours per epoch for different methods. The speed is</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tested on 8 NVIDIA V100 GPUs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">FixMatch CoMatch SimMatch (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPU (Hours)</cell><cell>2.77</cell><cell>2.81</cell><cell>2.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Results of removing scaling and smoothing strategy.</figDesc><table><row><cell cols="2">(ImageNet-1k 1% -100 ep)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">w/o p</cell><cell>w/o q</cell><cell>Standard</cell></row><row><cell>Top-1</cell><cell>59.9</cell><cell></cell><cell>52.3</cell><cell>61.7</cell></row><row><cell cols="5">Table 6. Results of different combinations for scaling and smooth-</cell></row><row><cell cols="4">ing strategy. (ImageNet-1k 1% -100 ep)</cell></row><row><cell>p</cell><cell>q</cell><cell cols="2">Scaling</cell><cell>Smoothing</cell></row><row><cell>Scaling</cell><cell></cell><cell>56.6</cell><cell></cell><cell>59.9</cell></row><row><cell cols="2">Smoothing</cell><cell>61.7</cell><cell></cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Results of replacing Lin with InfoNCE and SwAV.</figDesc><table><row><cell cols="2">(ImageNet-1k 1% -100 ep)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>InfoNCE</cell><cell>SwAV</cell><cell>SimMatch</cell></row><row><cell>Top-1</cell><cell>53.5</cell><cell>49.7</cell><cell>61.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised learning of visual features by nonparametrically predicting view assignments with support samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13963</idno>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>Nicolas Ballas, and Michael Rabbat</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exponential moving average normalization for self-supervised and semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A continuation method for semi-supervised svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with multi-head co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04795</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="9588" to="9597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAP</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="281" to="296" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianjiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08435,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mean shift for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>Soroush Abbasi Koohpayegani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Tejankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pirsiavash</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07269</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comatch: Semi-supervised learning with contrastive graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the limited memory bfgs method for large scale optimization. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polyak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A survey on semi-supervised learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jothi</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Dr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nithya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.4645</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prioritized architecture sampling with monto-carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10968" to="10977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Solving inefficiency of self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="9505" to="9515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dash: Semi-supervised learning with dynamic thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno>PMLR, 2021. 6</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="11525" to="11536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Greedynas: Towards fast one-shot nas with greedy supernet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiaohua Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weakly supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">ReSSL: Relational self-supervised learning with weak augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Introduction to semisupervised learning. Synthesis lectures on artificial intelligence and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="130" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
