<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN EFFICIENT ANCHOR-FREE UNIVERSAL LESION DETECTION IN CT-SCANS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Sheoran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghal</forename><surname>Dani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AN EFFICIENT ANCHOR-FREE UNIVERSAL LESION DETECTION IN CT-SCANS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Universal Lesion Detection</term>
					<term>CADe/x</term>
					<term>Medical Image Analysis</term>
					<term>One-stage Detector</term>
					<term>CT-scans</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing universal lesion detection (ULD) methods utilize compute-intensive anchor-based architectures which rely on predefined anchor boxes, resulting in unsatisfactory detection performance, especially in small and mid-sized lesions. Further, these default fixed anchor-sizes and ratios do not generalize well to different datasets. Therefore, we propose a robust one-stage anchor-free lesion detection network that can perform well across varying lesions sizes by exploiting the fact that the box predictions can be sorted for relevance based on their center rather than their overlap with the object. Furthermore, we demonstrate that the ULD can be improved by explicitly providing it the domain-specific information in the form of multi-intensity images generated using multiple HU windows, followed by self-attention based feature-fusion and backbone initialization using weights learned via selfsupervision over CT-scans. We obtain comparable results to the state-of-the-art methods, achieving an overall sensitivity of 86.05% on the DeepLesion dataset, which comprises of approximately 32K CT-scans with lesions annotated across various body organs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Computer-aided detection/diagnosis (CADe/x) using computed tomography (CT) images has evolved as an emerging field of research, thanks to the tremendous advancements of deep learning techniques in the area of computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Cancer has been one of the most researched and prevalent diseases and the identification of lesions from CT-scans is an important step towards diagnosis. Due to the heterogeneous nature of lesions, their manual analysis and detection is a tedious and error-prone task that requires significant expert knowledge. In the past decade, many efforts have been made towards automated lesion detection but solutions were largely organ-specific focusing on detecting lesions in one of the organs such as liver, kidney, and lungs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Recently, the focus has shifted towards developing a Universal Lesion Detector (ULD) which can identify lesions present in different organs from a patient's CT-scan <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Authors contributed equally DeepLesion <ref type="bibr" target="#b8">[9]</ref> is a multi-organ CT-scan dataset, which consists of 32K lesions annotated across various organs of the body, available publicly for bench-marking ULD techniques.</p><p>Prior ULD methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> have utilized neighboring slice information to provide 3D-context to the network and attention to provide better features for detection by enabling the network to focus on important regions of CT-scans. Yan et al. <ref type="bibr" target="#b3">[4]</ref> proposed MULAN that fuses features from 27 input slices and jointly trains the network for lesion segmentation and tagging. Some of these works also incorporate novel negative mining techniques to remove false positives and improve the detection. Authors of MELD <ref type="bibr" target="#b10">[11]</ref> use 4 different datasets for training and further, use missing annotation matching (MAM) and negative region mining (NRM) for achieving state-of-the-art lesion detection performance on the DeepLesion test-set. We note that the previous ULD methods are anchor-boxe based such as Mask-RCNN <ref type="bibr" target="#b11">[12]</ref> and Faster-RCNN <ref type="bibr" target="#b12">[13]</ref>, etc. in which pre-defined fixed anchor-sizes and aspect-ratios are used. This makes it difficult to capture the heterogeneous sizes of lesions present in various body organs of different medical imaging datasets. Moreover, the anchor-based methods are computationally heavy and quite slow as they require running the detection and classification modules multiple times. This prompted researchers to get rid of anchor-boxes in lesion detection networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> proposed a U-Net based anchor-free ULD network in which each feature map is attached with a detection head. In another work <ref type="bibr" target="#b6">[7]</ref>, authors proposed a compute-heavy multilayer anchor-free MLANet based on hourglass network and center-to-corner transformation strategy for detecting varied sized lesions. However, both of these anchor-free ULD methods are not the state-of-the-art.</p><p>In this paper, we propose a robust and efficient one-stage anchor-free ULD network that performs at par with state-ofart ULD methods. The concept of a one-stage detector is based on centerness <ref type="bibr" target="#b13">[14]</ref> of objects and predicts all bounding boxes in one go and hence, is computationally efficient and light for clinical deployment. Next, we demonstrate that the performance of automated diagnostic systems can be further improved by imparting extra domain knowledge to the deep networks explicitly. This domain driven information enables deep networks to mimic the diagnostic pattern of doctors by focusing on features or areas where they pay attention while making the final diagnosis. To this end, we pro- vide multi-intensity images highlighting different organs of the body in CT-images using multiple HU windows, fuse the multiple features using self-attention and initialize the backbone of the detection network using medical imaging related weights learned via self-supervision on the DeepLesion <ref type="bibr" target="#b8">[9]</ref> dataset. We call our proposed network DSA-ULD (Domaindriven Self-attention based Anchor-free ULD).</p><p>To summarize, our contributions are as follows: ? We propose a one-stage anchor-free ULD network which can efficiently detect lesions present in CT-scans across different organs. ? We demonstrate that the feeding of domain information to the deep networks helps to improve detection performance. We named our network DSA-ULD: Domaindriven Self-attention based Anchor-free ULD. ? We evaluate DSA-ULD on the DeepLesion test-set and achieve better/comparable results with the state-of-the-art ULD methods. <ref type="figure" target="#fig_0">Fig. 1</ref> shows our proposed DSA-ULD pipeline. The network takes a 3-channel image I (key slice with one inferior and superior slice) as input and generates 5 intensity-images (I Ui ) using 5 HU-windows (U i ) after pre-processing. Next, the features are extracted and fused into (F F j ) using a novel featurefusion strategy based on self-attention which are eventually fed to a detection head. We utilize self-supervised weights and anchor-free protocol to make DSA-ULD generic and robust. Now, we discuss the network in detail:  <ref type="bibr" target="#b16">[17]</ref> based convolutional feature extractor. To further incorporate domain-information in our proposed ULD, we initialize the backbone of our feature extractor with weights learned via self-supervision on the DeepLesion dataset <ref type="bibr" target="#b17">[18]</ref>. Attention Based Feature Fusion: In order to fuse the feature maps (F ui ? P j ) containing multi-organ information, one can simply apply a 2D convolution layer which operates only on a local neighborhood. However, recently Vision Transformers <ref type="bibr" target="#b18">[19]</ref> have shown remarkable state-of-the-art results across various vision tasks by jointly attending to both spatial and feature sub-spaces with the use of multi-headed self-attention. Therefore, for efficient feature-fusion, we use self-attention which can capture global information across long range dependencies. At each FPN level P j , 5 feature maps each having 256 channels are fed as input to the module. We also use a 2D convolution attention layer (256 ? dv output channels) in parallel with the self-attention module (dv output channels) to reduce the computational overhead. Subsequently, the outputs from the two parallel branches are concatenated to obtain the desired number of output channels (256). To reduce computation overhead for attention, we use </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head><p>In a per-pixel prediction, for each location (x, y) in the feature map F F j , classification score p x,y is computed followed by regression prediction t x,y for every positive location via an indicator function 1 c * x,y &gt;0 . Here, L cls and L reg are the classification focal loss and regression IoU loss for location (x, y), N pos is the no. of positive samples, ? is the balance weight, and, c * and t * are ground-truth labels for classification and regression, respectively. Apart from the two conventional detector heads (classification &amp; regression), there is a third crucial head for centerness. It is based on the idea that low-level regressed boxes that have a skewed feature location in terms of their location inside the box tend to hamper overall detection results. Thus, given the regression targets l * , t * , r * &amp; b * for a location, the term centerness (as defined below) is trained with binary cross entropy (BCE) loss and added to the loss function defined in Eq. 1 for the refined results.</p><formula xml:id="formula_1">centerness = min(l * , r * ) max(l * , r * ) ? min(t * , b * ) max(t * , b * )<label>(2)</label></formula><p>3. EXPERIMENTS AND RESULTS Dataset and Metric: All the experiments are performed on standard DeepLesion dataset consisting of 32, 735 lesions bounding-box instances on 32, 120 axial key CT slices of 4, 427 unique patients <ref type="bibr" target="#b8">[9]</ref>. For a fair comparison, we conduct evaluation on the official test set (15%), and report sensitivity at various false positives (FPs) per image levels. Implementation Details: We utilize weights learned via self-supervised learning (SSL) for FPN backbone with ResNeXt-101 for all the comparison experiments. A 3-channel image input is generated by taking 3 slices of a patient's CT-scan (key slice with one superior and inferior neighboring slice). The pre-processing of images include clipping of black borders, windowing <ref type="bibr" target="#b15">[16]</ref>, and resampling voxel space to 0.8 ? 0.8 ? 2 mm 3 . We utilize transformations such as horizontal and vertical flips, resizing and translation to augment the training data.The models are trained on NVIDIA Tesla V100 GPUs having 32GB GPU-memory, with a batch size of 8. The model is trained until convergence using SGD optimizer with a learning rate and decay-factor of 0.004 and 10, respectively. Comparison with state-of-the-art: Now, we present comparison results of ULD methods such as 3DCE <ref type="bibr" target="#b5">[6]</ref>, improved RetinaNet <ref type="bibr" target="#b9">[10]</ref>, Anchor-free RPN <ref type="bibr" target="#b7">[8]</ref>, MLANet <ref type="bibr" target="#b6">[7]</ref>, MU-LAN <ref type="bibr" target="#b3">[4]</ref> and Detectron2 based ULD in <ref type="table">Table 1</ref>. Please note that D2-ULD is the network that is trained using anchor-based Detectron2 backbone in place of anchor-free FCOS and involves domain knowledge such as multi-intensity images, feature-fusion and custom-anchors relevant for lesion-sizes. We do not show a comparison with MELD <ref type="bibr" target="#b10">[11]</ref> as MELD is trained on 4 different datasets including DeepLesion and the comparison would not have been fair with our DSA-ULD which is trained on DeepLesion only. We still achieve comparable sensitivity of 86.05% similar to that of MELD (86.60%). This supports our claim that adding extra domain knowledge to the deep networks explicitly alleviates the need for large amounts of heterogeneous training data to learn robust features. As evident in <ref type="table">Table 1</ref>, we outperform all the prior methods and achieve an average sensitivity of 85.79% with only a few slices of a patient's CT-scan. The average sensitivity is further improved to 86.05% after initializing the backbone of our DSA-ULD with weights learned using self-supervision on the DeepLesion dataset. From <ref type="table">Table 1</ref>, we also observe that D2-ULD and DSA-ULD give comparable average sensitivity on DeepLesion where D2-ULD uses custom-anchors defined for lesion detection and DSA-ULD is anchor-free. Hence, it can be inferred that anchor-free ULD is more preferable as it does not require very heavy computation while giving equal detection performance.</p><p>We also show a comparison of organ-wise average sensitivity, lesion-size wise sensitivity at FP = 4 and lesion-size wise average sensitivity of DSA-ULD with previous methods in <ref type="figure" target="#fig_1">Fig. 2(a),(b)</ref>  that we are able to improve the detection of very small-sized (&lt; 10mm) lesions and sensitivity is improved across all the organs of the body. In addition to the above, we present qualitative comparison results (at FP=2) of DSA-ULD in <ref type="figure" target="#fig_1">Fig. 2(d</ref> Now, we provide an ablation study on the effect of introducing different numbers of HU windows and attention based feature-fusion in our proposed network DSA-ULD. <ref type="table" target="#tab_2">Table 2</ref> illustrates that when we use our 5 HU windows to generate a multi-intensity input image, we obtain a considerable boost in average sensitivity (84.61%) as compared to using a single HU-window (82.29%). Following this, on applying selfattention based feature fusion, we obtain a further increment in average sensitivity (85.79%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We presented an anchor-free one-stage ULD network called DSA-ULD which is also augmented with explicit domaindriven information such as multi-intensity images, featurefusion using self-attention and self-supervision techniques for efficient and robust lesion detection in CT-scans. We demonstrate that our proposed anchor-free DSA-ULD performs at par with anchor-based lesion detection methods on the DeepLesion test-set while being very simple and computationally-efficient. We also illustrate that the incorporation of domain knowledge in DSA-ULD removes the need of training on heterogeneous datasets. Going forward, we would like to propose a domain-adaptive ULD which can perform effectively on datasets coming from different scanners, domains and hospitals, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of DSA-ULD architecture. (a) An input I, consisting of 3 CT-slices of a patient, is used for generating 5 multi-intensity images (IU i ) with 5 different HU windows (U1 to U5). Next, each image is passed through a shared convolutional feature extractor (having domain-specific (DS) weight initialization) with 5 FPN sub-levels (P2 to P6) and we extract 5 feature maps (FUi ? Pj) for each FPN level (Pj). (b) These feature maps at sub-level (Pj) are fused together using proposed self-attention module into a single feature map (F Fj). Here dv, dk, dq represent dimensions for Value, Key, and Query matrix. Finally, detection from each sub-level is merged to get the final detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2</head><label>2</label><figDesc>heads with the depth of Values matrix as 4. The dimensions per head for Keys and Values matrix are fixed at 20. This convolution augmented self-attention allows us to fuse features from different pyramid levels having different resolutions, resulting in robust detection of lesions of different sizes. Anchor-free One-Stage Detector: Zhi et al. [14] proposed a fully convolutional one-stage (FCOS) detector which works on the principle of centerness to reduce the number of lowquality bounding box detections. The overall loss function is: L(p x,y , t x,y ) = 1 N pos x,y L cls (p x,y , c * x,y ) + ? N pos x,y 1 c * x,y &gt;0 L reg (t x,y , t * x,y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative and Quantitative Comparison of sensitivity on DeepLesion test-set. Please note that ULD w/o DK represents the anchorfree ULD without domain-knowledge (DK) having 3 slices as input with only one HU window ([1024, 4096]) and without attention-based feature fusion. D2-ULD is a custom-anchor based detectron2 network with DK. Here, BN, LNG, MDT, LVR, KDY, ABM, PLS and ST represent different organs such as bones, lungs, mediastinum, liver, kidney, abdomen, pelvis and soft-tissues, respectively. The green, magenta, and red color boxes represent ground-truth, true-positive (TP), and false-positive (FP) lesion detection, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Feature Generation: During manual analysis, a radiologist uses HU windowing to adjust CT intensity values to focus on organs/tissues of interest. Inspired by Masoudi et al. [15], we mimic the radiologist's behaviour in our ULD and highlight multiple organs of interest with heuristically determined 5 HU windows [16]: U 1 = [400, 2000], U 2,3 =</figDesc><table /><note>[?600, 1500], [50, 350], U 4 = [30, 150], U 5 = [50, 400] for bones, chest region including lungs &amp; mediastinum, abdomen including liver &amp; kidney, and soft-tissues, respectively. After windowing, 3-channel multi-intensity image (I Ui ) is passed as input to the ResNeXt-101 shared backbone with feature pyramid network (FPN)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and (c), respectively. It is clearly visible</figDesc><table><row><cell>Method</cell><cell>(S,W)</cell><cell>0.5</cell><cell>FP (%) 1.0 2.0</cell><cell>4.0</cell><cell>Avg.</cell></row><row><cell>3DCE [6]</cell><cell cols="5">(27,1) 62.48 73.37 80.70 85.65 75.55</cell></row><row><cell>Anchor-Free RPN [8]</cell><cell cols="5">(64,1) 68.73 77.10 83.54 88.12 79.37</cell></row><row><cell>MLANet [7]</cell><cell>(3,1)</cell><cell>--</cell><cell cols="2">77.10 83.0 88.30</cell><cell>--</cell></row><row><cell>Improved RetinaNet [10]</cell><cell>(3,1)</cell><cell cols="4">72.18 80.07 86.40 90.77 82.36</cell></row><row><cell>MVP Net [5]</cell><cell>(9,3)</cell><cell cols="4">73.83 81.82 87.60 91.30 83.64</cell></row><row><cell>MULAN (w/o tags) [4]</cell><cell cols="5">(27,1) 76.10 82.50 87.50 90.90 84.33</cell></row><row><cell>MULAN (w/ tags) [4]</cell><cell cols="5">(27,1) 76.12 83.69 88.76 92.30 85.22</cell></row><row><cell>D2-ULD (custom anchors)</cell><cell>(3,5)</cell><cell cols="4">75.09 83.88 89.28 92.83 85.27</cell></row><row><cell>D2-ULD + SSL</cell><cell>(3,5)</cell><cell cols="4">76.07 84.31 89.44 92.94 85.69</cell></row><row><cell>(a) DSA-ULD*</cell><cell>(3,5)</cell><cell cols="4">77.38 84.06 89.28 92.44 85.79</cell></row><row><cell>(b)+SSL</cell><cell>(3,5)</cell><cell cols="4">78.30 84.51 88.99 92.40 86.05</cell></row><row><cell cols="6">Table 1. Sensitivity (%) Comparison of DSA-ULD with previous</cell></row><row><cell cols="6">state-of-the-art methods on DeepLesion [9] test-set. (S, W) denote</cell></row><row><cell cols="4">no. of slices and HU windows used in each experiment.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies and average sensitivity (%) on introducing different no. of HU windows and attention based feature fusion in (DSA-ULD) on the DeepLesion test-set.</figDesc><table><row><cell>)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MSS U-Net: 3D segmentation of kidneys and tumors from ct images with a multi-scale supervised u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatics in Medicine Unlocked</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated pulmonary nodule detection: High sensitivity with few candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="759" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MULAN: multitask universal lesion analysis network for joint lesion detection, tagging, and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MVP-Net: Multi-view fpn with positionaware attention for deep universal lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Li</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">3D context enhanced region-based convolutional neural network for end-to-end lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MLANet: Multi-layer anchor-free network for generic lesion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Anchor-Free Lesion Detector on computed tomography scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 First International Conference on TransAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving RetinaNet for CT lesion detection with dense masks from weak recist labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zlocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Universal Lesion Detection by learning from multiple heterogeneously labeled datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13753</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quick guide on radiology image preprocessing for deep learning applications in prostate cancer research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Masoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Window classification of brain ct images in biomedical articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyun</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Other</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
