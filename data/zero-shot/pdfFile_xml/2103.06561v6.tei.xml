<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongzheng</forename><surname>Xi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwen</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichen</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Song</surname></persName>
							<email>rsong@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Cui</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Hou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhao</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
							<email>luzhiwu@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihua</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pretraining, which is the focus of the Chinese project 'Wen-Lan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a twotower pre-training model called BriVL within the crossmodal contrastive learning framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source imagetext dataset called RUC-CAS-WenLan for pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER and OpenAI CLIP on various downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, pre-training models have become topical in natural language processing (NLP). A number of pretraining language models such as BERT <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b19">19]</ref> and * Co-corresponding authors.</p><p>"There are several burning candles on a fruit cake."</p><p>"Happy Birthday! Make a wish."</p><p>Strong Correlation Weak Correlation <ref type="figure">Figure 1</ref>. Example of strong correlation assumption versus weak correlation assumption over image-text pairs. Note that the strong correlation assumption widely used in many multi-model pretraining models is often invalid in real-world scenarios.</p><p>GPT <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b2">3]</ref> have achieved significant improvements on various downstream NLP tasks. With the release of GPT-3 <ref type="bibr" target="#b2">[3]</ref> (i.e., the latest large-scale language model of OpenAI), pre-training language models <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b15">15]</ref> have now drawn the most attention of the NLP community.</p><p>Compared with text understanding in the single-modal scenario, understanding multiple modalities is more attractive and has a broader rang of application scenarios. In fact, with the success of pre-training models in NLP <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b30">30]</ref>, they have recently been extended to understand the text and the image simultaneously, that is, multi-modal pretraining models have been intensively explored to bridge vision and language in the last two years. Particularly, in January 2021, OpenAI released a multi-modal version of GPT-3 <ref type="bibr" target="#b2">[3]</ref> called DALL?E <ref type="bibr" target="#b0">[1]</ref>, demonstrating its excellent textto-image generation capability. This clearly declares the power of multi-modal pre-training, and also encourages researchers to explore the potential of large-scale multi-modal pre-training in the vision+language area.</p><p>Along this line of research, our team started a Chinese project called 'WenLan' on large-scale multi-modal pretraining since September 2020, and released the first version to demonstrate its understanding ability on the Chinese multi-modal data. At this moment, our released model presents the strong image-text retrieval ability as well as the impressive commonsense understanding ability.</p><p>As we have mentioned, with the considerable progress made by pre-training models, multi-modal pre-training has started to attract significant attention from machine learning, computer vision, and natural language processing in recent years, i.e., it has now been a hot interdisciplinary research topic. However, there are still three challenges in large-scale multi-modal pre-training: (1) Invalid Strong Assumption: Most existing models are designed by assuming that there exists strong semantic correlation between the input image-text pair (see <ref type="figure">Figure 1</ref>), but this strong correlation assumption is often invalid in practice. (2) Inefficiency of Pre-training: The pre-training process is often very expensive, and a large number of GPUs are needed for parallel pre-training. (3) Difficulty in Model Deployment: The pre-training models are typically too large to be deployed in real-world application scenarios, which is especially harder for those single-tower models (e.g., UNITER <ref type="bibr" target="#b5">[6]</ref>). In this project, to overcome the above three challenges, we propose a novel two-tower pre-training model called BriVL within the cross-modal contrastive learning framework (like OpenAI CLIP <ref type="bibr" target="#b27">[27]</ref>), instead of the single-tower architecture that is adopted by most multi-modal pre-training models. Importantly, unlike OpenAI CLIP <ref type="bibr" target="#b27">[27]</ref>, we devise a more advanced cross-modal contrastive learning algorithm based on the latest MoCo <ref type="bibr" target="#b16">[16]</ref> so that our BriVL can incorporate more negative samples in limited GPU resources. Our motivation for model design is detailed below.</p><p>Most existing multi-modal pre-training models, especially those with the single-tower architecture <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b14">14]</ref>, take an assumption that there exists strong semantic correlation between the input imagetext pair. With this strong assumption, the interaction between image-text pairs can thus be modeled with crossmodal transformers. However, in real-world application scenarios, the strong correlation assumption is often invalid. For example, there often exists only weak correlation between image-text pairs, as illustrated in <ref type="figure">Figure 1</ref>. Moreover, we also conduct extensive experiments and find that the performance of the two-tower models is significantly better than that of the single-tower models on the noisy image-text data (e.g., crawled from the Web). In this project, we thus choose the two-tower architecture to devise our large-scale multi-modal pre-training model. Specifically, given the web-crawled image-text data for pre-training, we need to design a multi-modal pre-training model based on the two-tower architecture. However, such  network architecture is too simple (without fine-grained cross-modal interaction like UNITER) and its representation ability must be enforced for multi-modal pre-training.</p><p>Thanks to the recent progress of self-supervised learning <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b1">2]</ref>, contrastive learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">35]</ref> has been found to significantly improve the representation ability of deep neural networks. Following this idea, we introduce comparative learning into our two-tower architecture. However, unlike OpenAI CLIP <ref type="bibr" target="#b27">[27]</ref> that adopts a simple contrastive learning method with the requirement of large batches, we devise a more advanced cross-modal contrastive learning algorithm. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, given a specific image-text pair, the image modality or the text modality can be used to construct absent samples of the image-text pair, and the number of negative samples is expanded based on the latest MoCo <ref type="bibr" target="#b16">[16]</ref> framework to improve the representation ability of the neural network. By building a large queue-based dictionary, our model can incorporate more negative samples in limited GPU resources, leading to even better results in image-text retrieval. Due to the usage of the two-tower architecture as well as the contrastive-learning based pre-training strategy, our proposed BriVL model has a high flexibility and can be readily deployed in real-world application scenarios. It mainly has three advantages: (i) With a two-tower architecture, the text encoder and the image encoder can be easily replaced with the latest larger single-modal pre-training models, further enforcing the representation ability of our BriVL model. (ii) Once our BriVL model is pre-trained, it can provide cloudaccessible APIs of the image and text feature embeddings  as well as the matching score of an image-text pair, which are very convenient to be deployed in various downstream tasks. Particularly, when a vector engine is used to speed up the inference stage, the efficiency of image-text retrieval can be significantly improved. (iii) It is convenient to add other pre-training tasks (e.g., image-to-text generation) into our BriVL model. Note that our image-to-text generation (i.e., image captioning) model achieves the new state-of-the-art on the AIC-ICC <ref type="bibr" target="#b37">[37]</ref> dataset.</p><p>Our main contributions are three-fold: (1) We have constructed a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for multi-modal pre-training. The first version of RUC-CAS-WenLan consists of 30 million image-text pairs, which come from the rich image-text content generated by web users, including news, sports, entertainment, culture, and other topics. In the near future, this pre-training dataset will be enlarged to 500 million image-text pairs. (2) We have proposed the first large-scale Chinese multi-modal pre-training model called BriVL. The first version of our BriVL model pre-trained on RUC-CAS-WenLan has 1 billion parameters. Importantly, our BriVL model outperforms both UNITER <ref type="bibr" target="#b5">[6]</ref> and OpenAI CLIP <ref type="bibr" target="#b27">[27]</ref> on the RUC-CAS-WenLan test set and AIC-ICC <ref type="bibr" target="#b37">[37]</ref> validation set. In the near future, our BriVL model will contain 10 billion parameters, which will be pre-trained with 500 million image-text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Our cross-modal pre-training model is defined based on the image-text retrieval task. Our main goal is thus to learn two encoders that can embed image and text samples into the same space for effective image-text retrieval. To enforce such cross-modal embedding learning, we introduce contrastive learning with the InfoNCE loss <ref type="bibr" target="#b24">[24]</ref> into our BriVL model, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Specifically, for a given text embedding, our learning objective is to find the best image embedding from a batch of image embeddings. Similarly, for a given image embedding, our learning objective is to find the best text embedding from a batch of text embeddings. In one word, our pre-training model learns a cross-modal embedding space by jointly training the image and text encoders to maximize the cosine similarity of the image and text embeddings of the true pair for each sample in the batch while minimizing the cosine similarity of the embeddings of the other incorrect pairs. This results in an InfoNCE loss over each batch of image-text pairs for pre-training our BriVL model. Note that our model can incorporate more negative samples in limited GPU resources comparing to OPENAI CLIP, leading to even better results in image-text retrieval (see Section 3.3).</p><p>Formally, for the image-text retrieval task, we denote the training set as D = (</p><formula xml:id="formula_0">x I i , x T i )|i = 1, ? ? ? , N , where (x I i , x T i )</formula><p>is a matched image-text pair from the RUC-CAS-WenLan dataset, and N is the size of D. Our image-text retrieval model leverages contrastive learning and expands the latest MoCo <ref type="bibr" target="#b16">[16]</ref> as the pre-training framework, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Each image x I i (or each text x T i ) is encoded by the image encoder f I (or text encoder f T ) to obtain its 1-D embedding z I i (or z T i ). The image encoder (see <ref type="figure" target="#fig_3">Figure 3</ref>(b)) contains a CNN backbone and a successive self-attention block. A sequence of object embeddings is obtained using a object detector to downsample the feature map from CNN and then encoded by the self-attention block. The text encoder is stacked by several self-attention blocks such as RoBERTa <ref type="bibr" target="#b21">[21]</ref>. A two-layer Muti-Layer Perception (MLP) block with a RELU activation function is used for mapping each encoder's representation to the joint cross-modal embedding space. The parameters of f I and f T are denoted as ? I and ? T , respectively.</p><p>Note that MoCo provides a mechanism of building dynamic dictionaries for contrastive learning, which can be used with various pretext tasks. In this work, we adopt a simple instance discrimination task: a query of an image matches a key of an augmented text if the image corresponds to the text, and vice versa. Further, the introduction of a queue decouples the dictionary size from the minibatch size. As a result, the dictionary size can be much larger than a typical mini-batch size, and we can set it as a hyper-parameter. Given the momentum parameter m, two momentum-updated encoders f I m (with the parameters ? I m ) and f T m (with the parameters ? T m ) are kept for the image and text modalities, respectively. Their update rule is given by:</p><formula xml:id="formula_1">? I m = m ? ? I m + (1 ? m) ? ? I (1) ? T m = m ? ? T m + (1 ? m) ? ? T<label>(2)</label></formula><p>Similar to MoCo, BriVL maintains two queues Q I and Q T , which contain K image negative keys and K text negative keys, respectively. Given batch size bs in the pretaining stage, after each iteration, all bs image negative keys and bs text negative keys are separately pushed into these two queues. In this way, keys in queues are updated in each iteration. Specifically, at iteration t, the image and text negative keys from the current data batch {B I t , B T t } are calculated by forwarding the momentum-updated encoders f I m and f T m :</p><formula xml:id="formula_2">N I t = {f I m (x I j )|x I j ? B I t }, and N T t = {f T m (x T j )|x T j ? B T t }.</formula><p>N I t and N T t are then updated to Q I and Q T , respectively. Moreover, the positive key is unique for each image query x I j (or text query x T j ), and it is obtained also by forwarding the momentum-updated encoders:</p><formula xml:id="formula_3">p T j = f T m (x T j ) (or p I j = f I m (x I j )</formula><p>). The loss function for each data batch is constructed as follows: for each image query x I j , we define the contrastive loss between its image embedding z I j and all positive/negative text keys in the queue Q T , and then obtain an InfoNCE loss:</p><formula xml:id="formula_4">L I2T =? j log exp(z I j ? p T j /? ) exp(z I j ? p T j /? ) + n T ?Q T exp(z I j ? n T /? )<label>(3)</label></formula><p>where n T denotes a text negative key for each image query and the hyper-parameter ? denotes the temperature. The similarity is measured by dot product here. Similarly, for each text query x T j , the InfoNCE loss is formulated as:</p><formula xml:id="formula_5">L T 2I =? j log exp(z T j ? p I j /? ) exp(z T j ? p I j /? ) + n I ?Q I exp(z T j ? n I /? )<label>(4)</label></formula><p>where n I denotes an image negative key of each text query.</p><p>The total loss function for BriVL is defined as:</p><formula xml:id="formula_6">L total = L I2T + L T 2I<label>(5)</label></formula><p>In the test/evaluation stage, the query image (or text) is also retrieved simply by the dot product defined over the output (i.e., embeddings) of the pre-trained encoders. Due to its high flexibility, our BriVL model can be readily deployed in a wide range of application scenarios. First, other pre-training tasks (e.g. image-to-text generation) can be added to our BriVL model by sharing the same text or image encoder. Second, the pre-trained text and image encoders can be directly applied to many downstream multimodal tasks such as image-to-text retrieval, text-to-image retrieval, text-to-image generation <ref type="bibr" target="#b31">[31]</ref> and visual dialog <ref type="bibr" target="#b23">[23]</ref>. This actually leads to several downstream applications developed based on our BriVL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Settings</head><p>Pre-Training Dataset Our BriVL model is pre-trained on a Web-crawled multi-source image-text dataset. This dataset is part of the WenLan project, called RUC-CAS-WenLan for short. RUC-CAS-WenLan collects image-text pairs from multiple information sources on the Web, including news, encyclopedia (i.e., Baidu Baike) and Weibo. Images from these data sources are selected to form image-text pairs together with their corresponding text descriptions. Since the obtained image-text pairs are crawled from the Web, there exist much noise in the original data. Thus, we then perform an elaborate cleaning process (e.g., duplicate and sensitive information detection) to filter out sensitive or lowquality pairs. For each data source, we also employ topic models to analyze the overall topic distribution and extract topic words, which help select and keep high-quality content information. Finally, our dataset has kept 30 million image-text pairs covering a variety of topics and content categories, including news, art, education, sports, entertainment, games, and culture. Out of them, 11,000 pairs are randomly selected to form the test set. Text Encoder As mentioned in Section 2, a text encoder consists of a textual backbone, a self-attention block, and a two-layer MLP. We choose the encoder of Chinese RoBERTa Large 1 as our textual backbone. Note that RoBERTa Large includes a total of 24 transformer layers with 1,024 hidden units and 16 heads. The self-attention block consists of 4 transformer layers, designed for capturing the relationships across the textual tokens. The twolayer MLP is used to project the textual embedding to the cross-modal embedding space.</p><p>Image Encoder Following UNITER <ref type="bibr" target="#b5">[6]</ref>, we first employ pre-trained Faster-RCNN <ref type="bibr" target="#b32">[32]</ref> to detect object boundingboxes from each image. We further utilize Efficient-Net B7 <ref type="bibr" target="#b34">[34]</ref> to extract the visual features of each image for computation efficiency. By applying RoI pooling <ref type="bibr" target="#b12">[12]</ref> on the output of EfficientNet B7, we obtain the features of multiple objects and then combine them with a selfattention block (of 4 transformer layers). The fused object features are fed into a two-layer MLP and projected to the cross-modal embedding space. Implementation Details We utilize the momentumupdated history queue as in MoCo <ref type="bibr" target="#b16">[16]</ref> for contrastive learning. We adopt clip-wise random crops, horizontal flips, Gaussian blur, graying, and color jittering for data augmentation over input images. A non-linear projection head is attached to the text/image encoder to obtain feature vectors in the same size 2,560. Our BriVL model is trained with 15 epochs. We select hyper-parameters heuristically due to computational constraint: the learnable temperature parameter ? = 0.05, momentum m = 0.99, and the queue size is 16,384. We adopt the Adam optimizer with decoupled weight decay regularization over all weights that are not gains or biases, and decay the learning rate using a cosine schedule. We use a mini-batch size of 128 for each of the 16 machines (each machine has 8 A100 GPUs), resulting in a total batch size of 2,048. The mixed-precision and half-precision Adam statistics are used to accelerate the pre-training process and save the memory. It takes 7 days to pre-train our BriVL model on 128 A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results on AIC-ICC</head><p>We select the AIC-ICC caption competition <ref type="bibr" target="#b37">[37]</ref> to evaluate our pre-trained BriVL model because it is the only publicly-available Chinese multi-modal dataset. This Chinese caption dataset (called as AIC-ICC) includes about 300,000 images, with 5 candidate Chinese caption texts per image. The validation split (with 30,000 images) of this dataset is used for performance evaluation on two downstream tasks (i.e., image-text retrieval and image captioning). To make a comparison with OpenAI CLIP on this dataset, we have to translate the Chinese captions in the validation split into the English ones (with Google Translation). It is noticeable that we can only obtain the inference code 2 (but not the training code) of CLIP from OpenAI, and thus are unable to pre-train CLIP on our own RUC-CAS-WenLan dataset. <ref type="table">Table 1</ref> presents the image-text retrieval results. We directly leverage the extracted features for nearest-neighbour (NN) retrieval without fine-tuning. We can observe that our BriVL significantly outperforms CLIP and UNITER on both the text-to-image and image-to-text retrieval subtasks, showing the effectiveness of the proposed BriVL in <ref type="table">Table 1</ref>. Evaluation results for the text-image retrieval downstream task on the AIC-ICC validation set.  multi-modal pre-training. Note that our BriVL runs about 20 times faster than UNITER (but as fast as CLIP). <ref type="table" target="#tab_0">Table 2</ref> presents the image captioning results. Finetuning is conducted on the training split of AIC-ICC. We adopt four widely-used evaluation metrics: BLEU, ME-TEOR, ROUGE-L, and CIDEr. It can be clearly seen that our BriVL performs better than the competitors in terms of three of the four metrics, i.e., our BriVL achieves the best overall performance on the AIC-ICC dataset. This means that our BriVL model also has a good generalization ability in the image captioning downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results on RUC-CAS-WenLan</head><p>We further make performance evaluation on the textimage retrieval task on the test split of RUC-CAS-WenLan, which includes 11,000 image-text pairs. <ref type="table" target="#tab_1">Table 3</ref> presents the text-image retrieval results on the RUC-CAS-WenLan test set. It is noticeable that our BriVL achieves significant improvements over UNITER and OpenAI CLIP 3 . Particularly, our BriVL leads to more than 45% performance gaps in terms of R@10 on both retrieval subtasks. This demonstrates the largest advantage of our BriVL in multi-modal pre-training. Furthermore, our BriVL is pre-trained by using 128 GPUs for about 7 days, comparing to OpenAI CLIP using 256 GPUs for 12 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">User Study Results</head><p>The user study is carried out over the text-image retrieval results obtained by the pre-training models (e.g., CMLC Prediction:  and CLIP <ref type="bibr" target="#b27">[27]</ref>). We select a group of image and text queries for testing. For each text (or image) query, we retrieve the first 30 results with the tested model from the specified candidate set, and manually score each of the 30 results by 3 ratings (i.e., 0, 1, and 2). Note that the higher the score is, the stronger the correlation between the image and text is.</p><formula xml:id="formula_7">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (A</formula><p>Since three human annotators are involved independently, the final score for each of the 30 results is obtained with 7 ratings (0-6). The scores of each text (or image) query are thus formed into a 30-length score sequence. The NDCG and MAP metrics are used to evaluate the human retrieval quality. Note that these metrics are widely used for evaluating retrieval quality. Particularly, during computing MAP, the text-image pair is considered to be relevant if the corresponding score is higher than 2. The obtained comparative results are presented in <ref type="table" target="#tab_2">Table 4</ref>. As expected, the user study does validate that our BriVL out-Prediction: '????', '??', '???????', ' ?????', '??', '?', '??', '?', ' ??', '??', '??', '??' ('gray scarf', 'roof', 'hands in pockets', 'cloudy sky', 'coat', 'bird', 'scarf', 'cloud', 'sky', 'long sleeve', 'smile ', 'long hair' ) Prediction: '???', '??', '??', '??', '? ?', '??', '??', '??', '??', ' ?', '??' ('wooden fence', 'town', 'street', 'house', 'road', 'sunset', 'building', 'landscape', 'window', 'cloud', 'sky' )  performs OpenAI CLIP <ref type="bibr" target="#b27">[27]</ref>. When the candidate set (per query) of UNITER is obtained using our BriVL, UNITER is shown to lead to further improvements over our BriVL (see BriVL+UNITER vs. BriVL). <ref type="figure" target="#fig_5">Figure 4</ref> presents the visualization examples obtained by our image captioning model. Note that two data sources (caption and web) are used in the top and bottom rows, respectively. We can observe that the generated captions by our model are fluent, vivid, and accurate to express the semantic meanings of the input pictures. This suggests that multi-model pre-training indeed brings benefits to the image captioning downstream task. <ref type="figure" target="#fig_7">Figure 5</ref> presents the visualization examples obtained by our image tagging model. Note that our image tagging model is almost the same as our image captioning model. The anime images are used in this downstream task. We can see that our image tagging model is able to predict accurate tags for each anime image. This provides evidence that multi-model pre-training indeed brings benefits to the image tagging downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Visual Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Downstream Applications</head><p>Although 'WenLan' can be applied to a variety of crossmodal downstream tasks, we have only developed two web applications, MatchSoul and Soul-Music, at this moment. Our main goal is to directly demonstrate the power of multimodal pre-training in real-world scenarios. We will develop more applications in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MatchSoul</head><p>MatchSoul is developed based on our pre-trained BriVL model. Note that we directly deploy our pre-trained model without any fine-tuning. This application is devised as follows: given a picture uploaded by user, it returns an 'golden' sentence that is the most relevant to this picture.</p><p>Unlike the general image generation, this application does not generate a descriptive sentence for the input picture. In contrast, it chooses to match the picture with the 'golden' sentence (from a candidate set of 300,000 'golden' sentences) according to the characteristics of the picture, as illustrated in <ref type="figure" target="#fig_8">Figure 6</ref>(a). The chosen 'golden' sentences are humor, literary, and philosophical thinking. We look forward to giving users a sense of surprise and playing the finishing touch to the picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Soul-Music</head><p>Similar to MatchSoul, Soul-Music is also developed based on our pre-trained BriVL model. Specifically, given a picture uploaded by user, Soul-music returns a song lyric that well fits the artistic conception of this picture. As illustrated in <ref type="figure" target="#fig_8">Figure 6(b)</ref>, Soul-Music matches the input picture with the most relevant song lyric, and even accurately localizes a part of the song lyric which best matches the characteristics of this picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>This paper presents the first large-scale Chinese multimodal pre-training model called BriVL. The first version of our BriVL model has 1 billion parameters, which is pretrained on the RUC-CAS-WenLan dataset with 30 million image-text pairs. As a part of this project, RUC-CAS-WenLan is a large Chinese multi-source image-text dataset constructed by ourselves for multi-modal pre-training. It is noticeable that our BriVL model significantly outperforms both UNITER and OpenAI CLIP on the RUC-CAS-WenLan test set and AIC-ICC validation set. With the pretrained BriVL model, we have also developed two web applications called MatchSoul and Soul-Music. In the near future, our BriVL model will be enlarged to 10 billion parameters, which will be pre-trained with 500 million imagetext pairs. Moreover, we will also exploit the text-to-image generation pretext task for multi-modal pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A schematic illustration of our BriVL model within the cross-modal contrastive learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) A schematic illustration of the proposed BriVL model for large-scale multi-model pre-training. (b) The architecture of the image encoder f I used for BriVL. Notation: SA -self-attention based on transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>woman with a slight smile is picking fruits in the fruitful orchard) Prediction: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (An aeroplane flies across the sky in a sunny day) Prediction: ???????????? ?????????? (A man in a costume is staying with a girl in a costume) Prediction: ?????????????? ??????? (On a city street, there is a traffic light with a sign on it)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualization examples obtained by our image captioning model. Note that two data sources (caption and web) are used in the top and bottom rows, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Prediction: '???', '???', '?????', '?? ?', '?', '??', '???', '??' ('curled up', 'open book', 'black dress', 'luminous', 'book', 'barefoot', 'yellow eyes', 'long hair' ) Prediction: '???', '???', '???', '?', ' ??', '???', '??', '???' ('broom ride', 'long nails', 'witch hat', 'star', 'boots', 'yellow eyes', 'long hair', 'dress' )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Visualization examples obtained by our image tagging model. Note that our image tagging model is almost the same as our image captioning model. The anime images are used in this downstream task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Demonstration of our downstream application. (a) MatchSoul: matching pictures with 'golden' sentences. (b) Soul-Music: matching pictures with 'golden' lyrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results for the image captioning downstream task on the AIC-ICC validation set. * denotes the result obtained on the test set.</figDesc><table><row><cell>Tasks</cell><cell cols="3">Image-to-Text Retrieval</cell><cell cols="3">Text-to-Image Retrieval</cell></row><row><cell>Metrics</cell><cell>R@1</cell><cell>R@5</cell><cell cols="2">R@10 R@1</cell><cell>R@5</cell><cell>R@10</cell></row><row><cell>CLIP [27]</cell><cell>13.4</cell><cell>27.3</cell><cell>35.1</cell><cell>7.8</cell><cell>18.5</cell><cell>25.0</cell></row><row><cell>UNITER [6]</cell><cell>14.8</cell><cell>29.8</cell><cell>37.9</cell><cell>9.8</cell><cell>23.3</cell><cell>31.4</cell></row><row><cell>BriVL (ours)</cell><cell>20.3</cell><cell>37.0</cell><cell>45.6</cell><cell>14.4</cell><cell>30.4</cell><cell>39.1</cell></row><row><cell>Metrics</cell><cell></cell><cell>BLEU</cell><cell>METEOR</cell><cell cols="2">ROUGE-L</cell><cell>CIDEr</cell></row><row><cell cols="2">CHAMPION'17  *</cell><cell>62.8</cell><cell>43.0</cell><cell></cell><cell>-</cell><cell>210.4</cell></row><row><cell>UNITER [6]</cell><cell></cell><cell>62.8</cell><cell>38.7</cell><cell></cell><cell>69.2</cell><cell>199.7</cell></row><row><cell>BriVL (ours)</cell><cell></cell><cell>66.1</cell><cell>41.1</cell><cell></cell><cell>71.9</cell><cell>220.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results for the text-image retrieval downstream task on the RUC-CAS-WenLan test set.</figDesc><table><row><cell>Tasks</cell><cell cols="3">Image-to-Text Retrieval</cell><cell cols="3">Text-to-Image Retrieval</cell></row><row><cell>Metrics</cell><cell>R@1</cell><cell>R@5</cell><cell cols="2">R@10 R@1</cell><cell>R@5</cell><cell>R@10</cell></row><row><cell>CLIP [27]</cell><cell>7.3</cell><cell>15.0</cell><cell>19.0</cell><cell>7.8</cell><cell>15.9</cell><cell>19.9</cell></row><row><cell>UNITER [6]</cell><cell>5.3</cell><cell>16.9</cell><cell>24.6</cell><cell>5.7</cell><cell>16.7</cell><cell>24.3</cell></row><row><cell>BriVL (ours)</cell><cell>36.1</cell><cell>55.5</cell><cell>62.2</cell><cell>36.0</cell><cell>55.4</cell><cell>62.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>User study results for the text-image retrieval downstream task. Three human annotators are involved in such user study.</figDesc><table><row><cell>Tasks</cell><cell></cell><cell cols="2">Image-to-Text Retrieval</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="4">NDCG@5 NDCG@10 NDCG@20 MAP</cell></row><row><cell>CLIP [27]</cell><cell>32.9</cell><cell>38.8</cell><cell>53.0</cell><cell>30.3</cell></row><row><cell>BriVL</cell><cell>37.5</cell><cell>42.8</cell><cell>55.5</cell><cell>38.3</cell></row><row><cell>BriVL+UNITER</cell><cell>37.0</cell><cell>43.5</cell><cell>56.3</cell><cell>37.6</cell></row><row><cell>Tasks</cell><cell></cell><cell cols="2">Text-to-Image Retrieval</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="4">NDCG@5 NDCG@10 NDCG@20 MAP</cell></row><row><cell>CLIP [27]</cell><cell>28.0</cell><cell>32.3</cell><cell>43.7</cell><cell>16.7</cell></row><row><cell>BriVL</cell><cell>46.9</cell><cell>51.5</cell><cell>61.6</cell><cell>47.2</cell></row><row><cell>BriVL+UNITER</cell><cell>49.9</cell><cell>55.0</cell><cell>65.1</cell><cell>52.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/brightmart/roberta zh</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/openai/CLIP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The inference code of OpenAI CLIP is directly implemented on the translated test split with Google Translation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natural Science Foundation of China (61976220 and 61832017), Beijing Outstanding Young Scientist Program (BJJWZYJH012019100020098), and Large-Scale Pre-Training Program of Beijing Academy of Artificial Intelligence (BAAI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlov</forename><surname>Mikhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goh</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gray</forename><surname>Scott</surname></persName>
		</author>
		<title level="m">Creating images from text. Ope-nAI Blog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566,2020.2</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UNITER: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">X-LXMERT: Paint, caption and answer questions with multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11278</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06666</idno>
		<title level="m">VirTex: Learning visual representations from textual annotations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selfsupervised relationship probing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">REALM: Retrievalaugmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AL-BERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fine-grained visual textual alignment for cross-modal retrieval using transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05231</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive visual attention in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6679" to="6688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ImageBERT: Cross-modal pretraining with large-scale weak-supervised image-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08910</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contrastive learning, multi-view redundancy, and linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Tosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1179" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Emerging trends of multimodal research in vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagun</forename><surname>Uppal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09522</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">AI challenger: A largescale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">XGPT: Cross-modal generative pre-training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01473</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie-Vil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<title level="m">Knowledge enhanced vision-language representations through scene graph</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
