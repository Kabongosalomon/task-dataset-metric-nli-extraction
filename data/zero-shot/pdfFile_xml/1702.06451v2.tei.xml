<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sochor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Information Technology</orgName>
								<orgName type="department" key="dep2">Centre of Excellence IT4Innovations</orgName>
								<orgName type="institution">Brno University of Technology</orgName>
								<address>
									<addrLine>Bo?et?chova 2</addrLine>
									<postCode>612 66</postCode>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Jur?nek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Information Technology</orgName>
								<orgName type="department" key="dep2">Centre of Excellence IT4Innovations</orgName>
								<orgName type="institution">Brno University of Technology</orgName>
								<address>
									<addrLine>Bo?et?chova 2</addrLine>
									<postCode>612 66</postCode>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Herout</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Information Technology</orgName>
								<orgName type="department" key="dep2">Centre of Excellence IT4Innovations</orgName>
								<orgName type="institution">Brno University of Technology</orgName>
								<address>
									<addrLine>Bo?et?chova 2</addrLine>
									<postCode>612 66</postCode>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.cviu.2017.05.015</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speed measurement</term>
					<term>camera calibration</term>
					<term>fully automatic</term>
					<term>traffic surveillance</term>
					<term>bounding box alignment</term>
					<term>vanishing point detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on fully automatic traffic surveillance camera calibration, which we use for speed measurement of passing vehicles. We improve over a recent state-ofthe-art camera calibration method for traffic surveillance based on two detected vanishing points. More importantly, we propose a novel automatic scene scale inference method. The method is based on matching bounding boxes of rendered 3D models of vehicles with detected bounding boxes in the image. The proposed method can be used from arbitrary viewpoints, since it has no constraints on camera placement. We evaluate our method on the recent comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show that our automatic camera calibration method by detection of two vanishing points reduces error by 50 % (mean distance ratio error reduced from 0.18 to 0.09) compared to the previous state-of-the-art method. We also show that our scene scale inference method is more precise, outperforming both state-of-the-art automatic calibration method for speed measurement (error reduction by 86 % -7.98 km/h to 1.10 km/h) and manual calibration (error reduction by 19 % -1.35 km/h to 1.10 km/h). We also present qualitative results of the proposed automatic camera calibration method on video sequences obtained from real surveillance cameras in various places, and under different lighting conditions (night, dawn, day).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Surveillance systems pose specific requirements on camera calibration. Their cameras are typically placed in hardly accessible locations and optics are focused at longer distances, making the common pattern-based calibration approaches unusable (such as classical <ref type="bibr" target="#b48">Zhang (2000)</ref>). That is why many solutions place markers to the observed scene and/or measure existing geometric features <ref type="bibr" target="#b5">Do et al., 2015;</ref><ref type="bibr" target="#b46">You and Zheng, 2016;</ref><ref type="bibr" target="#b33">Luvizon et al., 2016)</ref>. These approaches are laborious and inconvenient both in terms of camera setup (manually clicking on the measured features in the image) and in terms of physically visiting the scene and measuring the distances.</p><p>In our paper, we focus on precise and at the same time fully automatic traffic surveillance camera calibration including scene scale for speed measurement. The proposed speed measurement method needs to be able to deal with significant viewpoint variation, different zoom factors, various roads and densities of traffic. If the method should be applicable for large-scale deployment, it needs to run fully automatically without the necessity to stop traffic for installation or for performing calibration measurements. Our solution uses camera calibration obtained from two detected vanishing points and it is built on our previous work <ref type="bibr" target="#b7">(Dubsk? et al., 2014;</ref><ref type="bibr" target="#b9">Dubsk? et al., 2015)</ref>. However, this calibration procedure only allows reconstruction of the rotation matrix and the intrinsic parameters from vanishing points, and it is still necessary to obtain the scene scale. We propose to detect vehicles on the road by Faster-RCNN <ref type="bibr" target="#b37">(Ren et al., 2015)</ref>, classify them into a few common fine-grained types by a CNN <ref type="bibr" target="#b23">(Krizhevsky et al., 2012)</ref> and use bounding boxes of 3D models for the known classes to align the detected vehi-cles. The vanishing point-based calibration allows for full reconstruction of the viewpoint on the vehicle and the only free parameter in the alignment is therefore the scene scale. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of the 3D model and the aligned images. Our experiments show that our method (mean speed measurement error 1.10 km/h) significantly outperforms existing automatic camera calibration method by <ref type="bibr" target="#b7">Dubsk? et al. (2014)</ref> (error reduction by 86 % -mean error 7.98 km/h) and also calibration obtained from manual measurements on the road (error reduction by 19 % -mean error 1.35 km/h). This is important because in previous approaches, automation always compromised accuracy, forcing a trade off by the system developer. Our work shows that fully automatic calibration methods may produce better results than manual calibration (which was performed thoroughly and according to state-of-the-art approaches).</p><p>Existing solutions for traffic surveillance camera calibration <ref type="bibr" target="#b3">(Dailey et al., 2000;</ref><ref type="bibr" target="#b38">Schoepflin and Dailey, 2003;</ref><ref type="bibr" target="#b0">Cathey and Dailey, 2005;</ref><ref type="bibr" target="#b15">Grammatikopoulos et al., 2005;</ref><ref type="bibr" target="#b17">He and Yung, 2007b;</ref><ref type="bibr" target="#b34">Maduro et al., 2008;</ref><ref type="bibr" target="#b41">Sina et al., 2013;</ref><ref type="bibr" target="#b35">Nurhadiyatna et al., 2013;</ref><ref type="bibr" target="#b7">Dubsk? et al., 2014;</ref><ref type="bibr" target="#b24">Lan et al., 2014;</ref><ref type="bibr" target="#b32">Luvizon et al., 2014;</ref><ref type="bibr" target="#b9">Dubsk? et al., 2015;</ref><ref type="bibr" target="#b5">Do et al., 2015;</ref><ref type="bibr" target="#b33">Luvizon et al., 2016;</ref><ref type="bibr" target="#b46">You and Zheng, 2016</ref>) (see Section 2 for detailed analysis) usually have limitations for real world applications. They are either limited to some viewpoints (zero pan, second vanishing point at infinity), or they require some per-installed-camera manual work. To our knowledge, there is only one work <ref type="bibr" target="#b7">(Dubsk? et al., 2014)</ref> which does not have these limitations, and therefore we compare our results with this solution. For a brief description of the method, see Section 2; a more comprehensive review can be found in a recent dataset paper BrnoCompSpeed by <ref type="bibr" target="#b43">Sochor et al. (2016b)</ref>.</p><p>The key contributions of this paper are:</p><p>? An improved camera calibration method by detection of two vanishing points. The camera calibration error is reduced by 50 % -0.18 to 0.09 mean distance ratio error.</p><p>? A novel method for scene scale inference, which significantly outperforms automatic traffic camera calibration methods (error reduced by 86 % -7.98 km/h to 1.10 km/h) and also manual calibration (error reduced by 19 % -1.35 km/h to 1.10 km/h) in automatic speed measurement from a monocular camera.</p><p>? Results show that when used for the speed measurement task, the automatic (zero human input) method can perform better than the laborious manual calibration, which is generally considered accurate and treated as the ground truth. This finding can be important also in other fields beyond traffic surveillance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The camera calibration algorithm (obtaining intrinsic and extrinsic parameters of the surveillance camera) is critical for the accuracy of vehicle speed measurement by a single monocular camera, as it directly influences the speed measurement accuracy. There is a very recent comprehensive review of the traffic surveillance calibration methods <ref type="bibr" target="#b43">(Sochor et al., 2016b)</ref>, so for detailed information we refer to this review and we include only a brief description of the methods.</p><p>Several methods <ref type="bibr" target="#b17">(He and Yung, 2007b;</ref><ref type="bibr" target="#b0">Cathey and Dailey, 2005;</ref><ref type="bibr" target="#b15">Grammatikopoulos et al., 2005)</ref> are based on the detection of vanishing points as an intersection of road markings (lane dividing lines). Other methods <ref type="bibr" target="#b7">(Dubsk? et al., 2014;</ref><ref type="bibr" target="#b9">Dubsk? et al., 2015;</ref><ref type="bibr" target="#b38">Schoepflin and Dailey, 2003;</ref><ref type="bibr" target="#b3">Dailey et al., 2000)</ref> use vehicle motion to calibrate the camera. There is also a set of methods which use some form of manually measured dimensions on the road plane <ref type="bibr" target="#b34">(Maduro et al., 2008;</ref><ref type="bibr" target="#b35">Nurhadiyatna et al., 2013;</ref><ref type="bibr" target="#b41">Sina et al., 2013;</ref><ref type="bibr" target="#b32">Luvizon et al., 2014</ref><ref type="bibr" target="#b33">Luvizon et al., , 2016</ref><ref type="bibr" target="#b5">Do et al., 2015;</ref><ref type="bibr" target="#b24">Lan et al., 2014)</ref>.</p><p>An important attribute of calibration methods is whether they are able to work automatically without any manual per-camera calibration input. Only two methods <ref type="bibr" target="#b3">(Dailey et al., 2000;</ref><ref type="bibr" target="#b7">Dubsk? et al., 2014)</ref> are fully automatic and both of them use mean vehicle dimensions for camera calibration. Another important requirement for real-world deployment is whether the camera can be placed in an arbitrary position above the road, which is not true for some methods as they assume to have zero pan or other constraints.</p><p>Regarding fine-grained vehicle classification, there are several approaches. The first one is based on detected parts of vehicles <ref type="bibr" target="#b21">(Krause et al., 2015;</ref><ref type="bibr" target="#b40">Simon and Rodner, 2015;</ref><ref type="bibr" target="#b10">Fang et al., 2016)</ref>, another approach is based on bilinear pooling <ref type="bibr" target="#b26">(Lin et al., 2015;</ref><ref type="bibr" target="#b13">Gao et al., 2016)</ref>. There is also an approach based on Convolutional Neural Networks (CNN) and input modification <ref type="bibr" target="#b42">(Sochor et al., 2016a)</ref>. For object detection, it is possible to use boosted cascades <ref type="bibr" target="#b6">(Doll?r et al., 2014)</ref>, HOG detectors <ref type="bibr" target="#b4">(Dalal and Triggs, 2005)</ref>, or Deformable Parts Models (DPMs) <ref type="bibr" target="#b11">(Felzenszwalb et al., 2010)</ref>. There are also recent advances in object detection based on CNNs <ref type="bibr" target="#b14">(Girshick et al., 2014;</ref><ref type="bibr" target="#b37">Ren et al., 2015;</ref><ref type="bibr" target="#b29">Liu et al., 2016)</ref>.</p><p>Several authors deal with alignment of 3D models and vehicles and use this technique for gathering data in the context of traffic surveillance. <ref type="bibr" target="#b27">Lin et al. (2014)</ref> propose to jointly optimize 3D model fitting and fine-grained classification, and <ref type="bibr" target="#b18">Hsiao et al. (2014)</ref> align edges formulated as an Active Shape Model <ref type="bibr" target="#b2">(Cootes et al., 1995;</ref><ref type="bibr" target="#b25">Li et al., 2009)</ref>. <ref type="bibr" target="#b22">Krause et al. (2013)</ref> and propose the use of synthetic data to train geometry and viewpoint classifiers for 3D model and 2D image alignment. <ref type="bibr" target="#b36">Prokaj and Medioni (2009)</ref> use detected SIFT features <ref type="bibr" target="#b31">(Lowe, 1999)</ref> to align 3D vehicle models and the vehicle's observation. They use the alignment mainly to overcome vehicle appearance variation under different viewpoints. However, in our case, as the precise viewpoint on the vehicle is known (Section 4.3), such alignment does not have to be performed. Hence, we adopt a simpler and more efficient method based on 2D bounding boxessimplifying the procedure considerably without sacrificing the accuracy.</p><p>When it comes to camera calibration in general, various approaches exist. The widely used method by <ref type="bibr" target="#b48">Zhang (2000)</ref> uses a calibration checkerboard to obtain intrinsic and extrinsic camera parameters (relative to the checkerboard). <ref type="bibr" target="#b28">Liu et al. (2012)</ref> use controlled panning or tilting with stereo matching to calibrate the camera. Correspondences of lines and points are used by <ref type="bibr" target="#b1">Chaperon et al. (2011)</ref>. <ref type="bibr" target="#b47">Yu et al. (2009)</ref> focus on automatic camera calibration for tennis videos from detected tennis court lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Traffic Camera Model</head><p>The main goal of camera calibration in the application of speed measurement is to be able to measure distances on the road plane between two arbitrary points in meters (or other distance units), therefore we only focus on a camera model which enables the measurement of distance between two points on the road plane. For convenience and better comparison of the methods, we adopt the traffic camera model and notation proposed in previous papers <ref type="bibr" target="#b7">(Dubsk? et al., 2014;</ref><ref type="bibr" target="#b9">Dubsk? et al., 2015)</ref>; however, to make the paper self-contained, we briefly describe the model and notation. For intrinsic parameters of our camera model, we assume to have zero pixel skew, and the principal point c in the center of the image. The method also assumes the road section to be flat and straight; the experiments reported in the previous work and our experiments as well show that this requirement is not very strict, because most roads that are not sharply curved locally meet this assumption for practical purposes.</p><p>Homogeneous 2D image coordinates are referenced by bold small letters p = [p x , p y , 1] T , points on the image plane p = [p x , p y , f ] T in 3D, where f is the focal length, are denoted by small bold letters with overline. Finally, other 3D points (on the road plane) are denoted by bold capital letters P = [P x , P y , P z ] T . <ref type="figure" target="#fig_1">Figure 2</ref> shows the camera model and its notation. For convenience, we assume that the origin of the image coordinate system is at the center of the image; therefore, the principal point c has 2D homogeneous coordinates [0, 0, 1] T (3D coordinates of the center of camera projection are [0, 0, 0] T ). As it is shown, the road plane is denoted by ?. We encode vanishing points in the following way. The first one (in the direction of vehicle flow) is referenced as u; the second vanishing point (whose direction is perpendicular to the first one and which is parallel to the road plane) is denoted by v; and the third one (direction perpendicular to the road plane) is w.</p><p>Using the first two vanishing points u, v and the principal point c, it is possible to compute the focal length f , the third vanishing point w, the road plane normalized normal vector n, and the road plane ?. However, the road plane is computed only up to scale (as it is not possible to recover the distance to the road plane only from the vanishing points) and therefore, we add an arbitrary value ? = 1 as the constant term in Equation <ref type="formula" target="#formula_0">(6)</ref>.</p><formula xml:id="formula_0">f = ?u T ? v (1) u = [u x , u y , f ] T (2) v = [v x , v y , f ] T (3) w = u ? v (4) n = w w (5) ? = n T , ? T<label>(6)</label></formula><p>With known road plane ?, it is possible to compute 3D coordinates P = [P x , P y , P z ] T of an arbitrary point p = [p x , p y , 1] T by projecting it onto the road plane using the following equations:</p><formula xml:id="formula_1">p = [p x , p y , f ] T (7) P = ? ? p T , 0 ? ? p<label>(8)</label></formula><p>It is possible to measure distances on the road plane directly with 3D coordinates P; however, as the road plane is shifted to a predefined distance by a constant term, the distance P 1 ? P 2 between points P 1 and P 2 is not directly expressed in meters (or other real-world units of distance). Therefore, it is necessary to introduce another calibration parameter, referred to as the scene scale ?, which converts the distance P 1 ? P 2 from pseudo-units on the road plane to meters by scaling the distance to ? P 1 ? P 2 .</p><p>Under the assumptions that the principal point is in the center of the image and zero pixel skew, it is necessary for the calibration method to compute two vanishing points (u and v in our case) together with the scene scale ?, yielding 5 degrees of freedom. Methods to convert these camera parameters to the standard intrinsic and extrinsic camera model K [R T] have been discussed before in several papers <ref type="bibr" target="#b49">(Zhang et al., 2013;</ref><ref type="bibr" target="#b12">Fung et al., 2003;</ref><ref type="bibr" target="#b50">Zheng and Peng, 2014)</ref>, therefore we refer to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Camera Calibration and Vehicle Tracking</head><p>We adopted the calibration method of <ref type="bibr" target="#b7">Dubsk? et al. (2014)</ref>, which gives the image coordinates of the vanishing points and scene scale information. We improved the method with more precise detection of the vanishing points, and we infer the scene scale by using 3D models of frequently passing cars.</p><p>Our method measures the speed of passing cars detected by Faster-RCNN <ref type="bibr" target="#b37">(Ren et al., 2015)</ref> and tracked by a combination of background subtraction and Kalman filter <ref type="bibr" target="#b20">(Kalman, 1960)</ref> assisted by the detector. This method, more sophisticated than the previous method <ref type="bibr" target="#b7">(Dubsk? et al., 2014)</ref>, gives fewer false positives and a comparable recall rate. In the case of very dense flow when vehicles overlap each other in the camera image (which does rarely occur even in real conditions), our method would miss some of the cars as we target free-flow conditions. In the following text, we describe the components of the method in detail, and evaluate it in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Vanishing Point Estimation from Edgelets</head><p>We adopted the algorithm proposed by <ref type="bibr" target="#b9">Dubsk? et al. (2015)</ref> (based on the detection of two orthogonal vanishing points) for the detection of the first vanishing point and propose to use a similar algorithm for detecting the second vanishing point. However, we improved the detection of the second vanishing point by using edgelets instead of image gradients used in the previous paper . This change, although subtle, improves the calibration and speed measurement considerably, as the results in Section 5.3 show.</p><p>We start with the detection of vanishing points from which the camera rotation with respect to the road can be estimated. The first vanishing point u is estimated from the movement of the vehicles by a form of cascaded Hough Transform  of lines formed by tracking points of interest on the moving vehicles. This is a more stable approach than finding the closest point to the lines in an algebraic way, because it is more robust to tracking noise and it is not influenced by vehicles that change lane (and therefore, the vanishing point of their movement is different from the rest of the vehicles). Similarly to <ref type="bibr" target="#b9">Dubsk? et al. (2015)</ref>, we use the Min-eigenvalue point detector <ref type="bibr" target="#b39">(Shi and Tomasi, 1994)</ref> and the KLT tracker <ref type="bibr" target="#b44">(Tomasi and Kanade, 1991)</ref>.</p><p>To detect the second vanishing point v we use edges on passing vehicles as many lines formed by the edges coincide with v. This step heavily relies on the correct estimation of the orientation of the edges. The angle can be easily computed from gradients, but angles close to k?/2 are almost impossible to accurately recover on small neighborhoods. We estimate edge orientation from a larger neighborhood by analysis of the shape of image gradient magnitude (edgelets). The detection process is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Edgelets are detected by the following algorithm. Given an image I, first, we find seed points s i as local maxima of gradient magnitude of the image E = ?I , keeping only the strong ones with magnitudes above a threshold. From the 9 ? 9 neighborhood </p><formula xml:id="formula_2">i = [x i , y i , 1] T , matrix X i is formed: X i = ? ? ? ? ? w 1 (m 1 ? x i ) w 1 (n 1 ? y i ) w 2 (m 2 ? x i ) w 2 (n 2 ? y i ) . . . . . . w k (m k ? x i ) w k (n k ? y i ) ? ? ? ? ?<label>(9)</label></formula><p>where [m k , n k , 1] T are coordinates of the neighboring pixels (k = 1 . . . 81) and w k is their gradient magnitude from E, i.e. for a 9 ? 9 neighborhood, the size of X i is 81 ? 2. Then, singular vectors and values of X i can be computed as:</p><formula xml:id="formula_3">W i ? 2 i W T i = SVD X T i X i , (10) where W i = [a 1 , a 2 ]<label>(11)</label></formula><formula xml:id="formula_4">? i = ? 1 0 0 ? 2 .<label>(12)</label></formula><p>Vectors a 1 and a 2 represent the eigenvectors of X i , while ? 1 and ? 2 denote the corresponding eigenvalues. Edge orientation is then the first singular column vector d i = a 1 from (11) and the edge quality is the ratio of singular values q i = ?1 ?2 from (12). Each edgelet is then represented as a triplet E i = (s i , d i , q i ).</p><p>We gather the edgelets from the input video (see <ref type="figure" target="#fig_3">Figure 4</ref>), keeping only the strong ones which do not coincide with the already estimated u, and accumulate them to the Diamond Space accumulator <ref type="bibr" target="#b8">(Dubsk? and Herout, 2013)</ref>. The position of the global maximum in the accumulator is taken as the second vanishing point v. It should be noted that in this step, additional filtering can be applied -e.g. masking the Diamond Space to find only plausible solutions (i.e. avoid imaginary focal length from Equation <ref type="formula">(1)</ref>), or to find solutions within a certain range of focal lengths or horizon inclinations (when known in advance). This may improve the robustness of the second vanishing point estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Vehicle Detection and Tracking</head><p>During speed measurement, passing cars are detected in each frame by the Faster-RCNN (FRCN) detector <ref type="bibr" target="#b37">(Ren et al., 2015)</ref> but any detector can be used as well (e.g. ACF, LDCF <ref type="bibr" target="#b6">(Doll?r et al., 2014)</ref>). We trained the detector on the COD20K dataset <ref type="bibr" target="#b19">(Jur?nek et al., 2015)</ref>, which contains approximately 20 k car instances for training from views of surveillance nature. The detection rate of the detector is 96 % with 0.02 false positive detections per image on the test part of the COD20K dataset. The detector yields a coarse information about locations of cars in the image (bounding boxes are not precisely aligned). We use a simple heuristic to remove detections that would lead to imprecise tracking and ultimately to wrong speed estimation -those that are slightly occluded by other detections and that are farther from the camera. Therefore we track only cars that are fully visible.</p><p>For the tracking itself, we use a simple background model that builds a background reference image by moving average. In the foreground image, compact blobs are detected and the FRCN detections are used to group those blobs that correspond to one car. From each group of blobs, the convex hull and its 2D bounding box are extracted. Finally, we track the 2D bounding box of the convex hull using a Kalman filter to get the movement of the car. For an example, see <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>For each tracked car, we extract a reference point for speed measurement. The convex hull is used to construct the 3D bounding box <ref type="bibr" target="#b7">(Dubsk? et al., 2014)</ref> and we take the center of the bottom-front edge -the reference point located in the ground/road plane. Each track is represented by a sequence of bounding boxes and reference points both constructed from the convex hull. Our method inherits all the advantages and limita-tions of similar approaches based on the extraction of the vehicle's foreground mask. We rely on the extractor to do its job properly, and we can take advantage of works dealing with different issues related to for example lighting and weather (for example contour extractors such as <ref type="bibr" target="#b45">Yang et al. (2016)</ref>, or semantic segmentation methods such as <ref type="bibr" target="#b30">Long et al. (2015)</ref>). In Section 5.6, we show a number of examples of real-world surveillance cameras under bad conditions, where the calibration algorithm nonetheless works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Scale Inference using 3D Model Bounding Box Alignment</head><p>The previous state-of-the-art automatic method for scale inference in traffic surveillance by <ref type="bibr" target="#b7">Dubsk? et al. (2014)</ref> used three-dimensional bounding boxes built around the vehicle and mean dimensions of vehicles to compute the scale. However, this approach has two main drawbacks. The obvious one is in the usage of mean dimensions of vehicles. However, the more important one is less obvious: the constructed bounding box is too tight around the vehicle and the tightness is largely influenced by the particular viewpoint direction. This causes systematic errors in the calibration depending on the camera location with respect to the road, leading to high sensitivity to viewpoint change.</p><p>We propose to use a different approach for scale inference, overcoming the mentioned imprecisions. We use fine-grained types of vehicles (i.e. make, model, variant, model year) and for a few (two in our experiments) common types we obtained 3D models which are rendered to the image and we align them to the real observed vehicles in order to obtain the proper scale.</p><p>As it is necessary to know the precise vehicle classes (up to model year) for our scale inference method, we used the BoxCars dataset <ref type="bibr" target="#b42">(Sochor et al., 2016a)</ref> and we also collected some other training data from videos related to papers by <ref type="bibr" target="#b7">Dubsk? et al. (2014)</ref>; <ref type="bibr" target="#b9">Dubsk? et al. (2015)</ref>. The classification of vehicles is done only into a few most common fine-grained vehicle types on roads in the area plus one class for all the others vehicles. The full training dataset contained ?23 k tracks and ?92 k images of vehicles. We used a CNN <ref type="bibr" target="#b23">(Krizhevsky et al., 2012)</ref> for the classification itself. The classification accuracy on the validation set (?7 k of images) was 0.97. As only single instances of vehicles are classified by the CNN, we use mean probability over all of the detections belonging to one vehicle track to improve the recognition rates.</p><p>For each vehicle, we also build a 3D bounding box around it <ref type="bibr" target="#b7">(Dubsk? et al., 2014</ref>) to obtain the center b of the vehicle's base in image coordinates. To obtain the viewpoint vector ?, we first compute the rotation matrix R, which has columns equal to normalized u, v, and w. It is then possible to compute the 3D viewpoint vector as ? = ?R T b. The minus sign is necessary as we need the viewpoint vector going from the vehicle to the camera, not the opposite one.</p><p>Once the viewpoint vector to the vehicle, the vehicle's class, and its position on the screen are determined, we render the appropriate 3D model given the parameters. The only open variable is the scale of the vehicle to be rendered (i.e. the distance between the vehicle and the camera). Examples of the two used 3D models are shown in <ref type="figure">Figure 6</ref>. Therefore, we render images of the vehicle in multiple different scales and match the bounding boxes of the rendered vehicles with the bounding box detected in the video by using the Intersection-over-Union (IoU) metric. Examples of such matches can be found in <ref type="figure">Figure 7</ref>. The figure also shows two interesting points related to the vehicle in red: points on the base of the 3D models representing front f and rear r of the vehicle. Finally, for all vehicle instances i and scales j, these points are projected on the road plane, yielding F ij and R ij . They are used to compute the scale ? ij (Eq. (13), where l ti is the real world length of the type t i ). For all considered combinations of i and j, the IoU matching metric m ij is computed.</p><formula xml:id="formula_5">? ij = l ti F ij ? R ij<label>(13)</label></formula><p>To obtain the final camera's scale ? * , all the scales ? ij are taken into account together with metrics m ij . We consider only cases with m ij larger then a predefined threshold (we used 0.85 in our experiments) to eliminate poor matches. Finally, we compute ? * according to Equation <ref type="formula" target="#formula_6">(14)</ref>. The probability p (? | (? ij , m ij )) is computed by kernel density estimation with a discretized space:</p><formula xml:id="formula_6">? * = arg max ? p (? | (? ij , m ij ))<label>(14)</label></formula><p>In order to further improve the scale inference, we use several training videos from BrnoCompSpeed dataset <ref type="bibr" target="#b43">(Sochor et al., 2016b)</ref>. We train the scale-correcting linear regression ? * reg = ?? * + ?, using manually obtained scales as the ground truth. Even though this step is not necessary, it improves the scale acquisition further by correcting the imprecise geometry of the obtained 3D models.</p><p>We also experimented with an alignment metric based on matching of edges on the rendered and detected vehicles (based on distance transform). However, the speed measurement did not improve further. The biggest problem with this method is that most of the edges on vehicles are blurry and therefore not detected at all. However, the vehicle detector <ref type="bibr" target="#b37">(Ren et al., 2015)</ref> is able to detect the vehicles properly and in most cases accurately. Also, the proposed algorithm using just the bounding boxes is much more efficient in terms of storage (it is possible to store just the bounding boxes, not the images) and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Speed Measurement of Tracked Cars</head><p>The speed measurement itself is done by following the methodology proposed by <ref type="bibr" target="#b43">Sochor et al. (2016b)</ref>. Given a tracked car with reference points p i and timestamps t i for each reference point, where i = 1 . . . N , the speed v is calculated from Equation <ref type="formula" target="#formula_7">(15)</ref> by projecting the reference points p i to the ground plane P i (see Equation <ref type="formula" target="#formula_1">(8))</ref>.</p><formula xml:id="formula_7">v = median i=1...N ?? ? * reg P i+? ? P i t i+? ? t i<label>(15)</label></formula><p>The speed is computed as the median value of speeds between consecutive time positions. However, for stability of the measurement, it is better not to use the next frame, but the time position several video frames apart. This is controlled by the constant ? , and for all our experiments, we use ? = 5 (the time difference is usually 0.2 s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>To evaluate our proposed methods for camera calibration and scene scale inference, we use the very recent BrnoCompSpeed dataset <ref type="bibr" target="#b43">(Sochor et al., 2016b)</ref> which contains over 20 k vehicles with precise ground truth speed from multiple locations. The dataset also contains markers on the road with known dimensions between them. For an example of such road markers, see <ref type="figure" target="#fig_6">Figure 8</ref>. The ground truth distances can be used for either calibration or evaluation of distance measurements on the road plane. It is also possible to evaluate the accuracy of vanishing point estimation by using the markings <ref type="bibr" target="#b43">(Sochor et al., 2016b)</ref>. In the following text we will refer to various methods for camera calibration which are defined as:</p><p>? ITS15 -Automatic camera calibration method as described by <ref type="bibr" target="#b9">Dubsk? et al. (2015)</ref>. Brief outline of the method is in Sections 2 and 4.  ? Edgelets -Camera calibration method proposed in this paper, Section 4.1.</p><p>? ManualCalib -We use known distances <ref type="figure" target="#fig_6">(Figure 8</ref>) on the road for manual calibration of the camera. In agreement with the previous papers <ref type="bibr" target="#b0">(Cathey and Dailey, 2005;</ref><ref type="bibr" target="#b15">Grammatikopoulos et al., 2005;</ref><ref type="bibr" target="#b16">He and Yung, 2007a)</ref> we use intersection lanes dividing lines (blue dashed lines in <ref type="figure" target="#fig_6">Figure 8</ref>) for estimation of the first vanishing point u. As there are usually more than just two lane dividing lines, we use least squares minimization to obtain the intersection of multiple lines. Formally, given lines l i with normalized normal vectors, we compute the vanishing point u by solving Au = ?b in a least squares manner, where rows of A contain transposed normal vectors of the lines, and rows of b contain constant terms of the lines.</p><p>The second vanishing point v can be obtained in the same manner (as the intersection of yellow dashed lines in <ref type="figure" target="#fig_6">Figure 8</ref>, since they are perpendicular to the vehicle flow on the road). However, we found out that it is more accurate and robust to use the intersection only as a first guess, and then use measured distances on the road to optimize the vanishing point position using Equation <ref type="formula" target="#formula_0">(16)</ref>.</p><formula xml:id="formula_8">v * = arg min v ? ? (p1,p2,d)?D2 |? P 1 ? P 2 ? d| ? ? ,<label>(16)</label></formula><p>where set D 2 contains image endpoints and distances measured on the road towards the second vanishing point (green line segments in <ref type="figure" target="#fig_6">Figure 8</ref>) and scale ? is computed for the given vanishing points u, v by Equation <ref type="formula" target="#formula_9">(17)</ref>. It should be noted that the computation of 3D coordinates P i of image point p i depends on the vanishing points (see Equation <ref type="formula" target="#formula_1">(8)</ref> for details). The optimization itself is done by grid search (we loop over discretized feasible positions of v corresponding to reasonable focal lengths and evaluate the optimization objective <ref type="formula" target="#formula_0">(16)</ref>).</p><p>The usage of standard manual methods based on calibration patterns (e.g checkerboards) proposed by Zhang <ref type="formula">(2000)</ref> is impractical, as it would require a large checkerboard (more than 10 m 2 ) placed on the road.</p><p>We also define method names for different approaches for scale inference:</p><p>? BMVC14 -Scale inference method proposed by <ref type="bibr" target="#b7">Dubsk? et al. (2014)</ref>. Brief outline of the method is in Section 2.</p><p>? BBScale + reg -Our method for scale calibration using bounding box matching (Section 4.3) with scale correction regression.</p><p>? ManualScale -Scale computed from manually measured distances between markers towards the first vanishing point on the road. The scale is computed as the mean value of Equation <ref type="formula" target="#formula_9">(17)</ref> from a set of endpoints and distances (p i,1 , p i,2 , d i ) towards the first vanishing point (red line segments in <ref type="figure" target="#fig_6">Figure 8</ref>).</p><formula xml:id="formula_9">? = E d i P i,1 ? P i,2<label>(17)</label></formula><p>? SpeedScale -Scale is computed from ground truth speed measurements and minimizes the speed measurement error for given camera calibration. It can be understood as the lower error bound for the given camera calibration method. The scale is computed as the mean value of Equation <ref type="formula" target="#formula_1">(18)</ref> where, the set M contains pairs of ground truth speedv i and measured speed v i . It is assumed that scale ? = 1 was used for computation of speeds v i .</p><formula xml:id="formula_10">? = E v i v i<label>(18)</label></formula><p>If not stated otherwise, the evaluation was done on BrnoCompSpeed -Split C (contains more than 10 k of vehicle tracks for evaluation), because our method requires parameter tuning for the scale correction regression and split C provides a sufficient amount of data for training and testing. For each metric, we report mean, median, and 99 percentile error for both absolute units (err = |r ? r|) and relative units (err = |r ? r|/r ? 100%), wherer denotes the ground truth measurement, and r represents the measured value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation of Vanishing Point Estimation -Camera Calibration Error</head><p>To evaluate the camera calibration itself (the obtained vanishing points), we follow the evaluation metric proposed with the BrnoCompSpeed dataset <ref type="bibr" target="#b43">(Sochor et al., 2016b)</ref>. The evaluation measures the difference between ratios of distances between markings towards the first vanishing point (red lines in <ref type="figure" target="#fig_6">Figure 8</ref>) and the distances between markers towards the second vanishing point (green lines in <ref type="figure" target="#fig_6">Figure 8</ref>). As the ratio does not depend on scale, this metric considers only the camera calibration in the form of two detected vanishing points. Since we do not require any parameter tuning for the camera calibration method, we report the results on all videos in the BrnoCompSpeed dataset (including the extra session0). The results (reported in <ref type="table" target="#tab_1">Table 1)</ref> show that our automatic calibration method Edgelets outperforms calibration method ITS15 almost twice on mean error. It should be noted that the same distances that were used to obtain the manual calibration were evaluated by the calibration error metric based on distance ratios; this gives the manual calibration an unfair advantage in the comparison.</p><p>The significant improvement of our method is caused by more precise acquisition of v; position of u stays the same for our method as for the ITS15 calibration method. There are two reasons why vanishing points play an important role. The first one is that the vanishing points are directly used for estimating the focal length; the second one is that they are used for computation of the viewpoint on the vehicle for scale estimation. Therefore, if the viewpoint is computed imprecisely, the alignment of the rendered 3D model is also imprecise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation of Distance Measurement in the Road Plane</head><p>The next step is to evaluate the camera calibration together with the obtained scale. We use manual annotations of distances on the road plane which are directed towards the first or the second vanishing point, respectively (red and green in <ref type="figure" target="#fig_6">Figure 8</ref>).</p><p>First, we evaluated the distance measurement only towards the first vanishing point as it is the direction in which the vehicles are going and it is more important for speed measurement. The results are shown in <ref type="table">Table 2</ref> for different combinations of calibrations and scale estimations. First, our fully automatic method for camera calibration (Edgelets) and scale inference (BBScale + reg) significantly outperforms the previous automatic method ITS15 + BMVC14. Second, when we use our automatically computed calibration and scale obtained with manual annotations, we achieve almost the same results as ManualCalib + ManualScale, which required much more manual effort than our automatic system. <ref type="table">Table 2</ref>: Distance measurement errors on the road plane for different calibrations. Only distances towards the first vanishing point (red in <ref type="figure" target="#fig_6">Figure 8</ref>) were used for this evaluation. The first row for each calibration method contains absolute errors in meters; the relative errors in percents are in the second row.  When we evaluated the same metric with all the distances, the results are similar (see <ref type="table" target="#tab_3">Table 3</ref>). Again, our method significantly outperforms the previous automatic method. Considering the calibrations with manually obtained scale, our system has a slightly higher error then the manual calibration. However, this is caused by the fact that the manual calibration is optimized directly to the evaluation metric by Equation (16) and thus gets an unfair and unrealistic advantage.</p><p>To summarize the distance measurement results: our method significantly outperforms previous automatic state-of-the-art for speed measurement -the mean error for distance measurement in the direction of vehicles' flow (which is important for speed measurement) was reduced by 79 % (1.23 m to 0.26 m). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of Speed Measurement</head><p>The most important part of the evaluation is the speed measurement itself. We used the same vehicle detection and tracking system (see Section 5) in all experiments so that the results for different calibrations and scales are directly comparable.</p><p>We show both quantitative results in the form of <ref type="table" target="#tab_4">Table 4</ref> and plots with cumulative error histograms in <ref type="figure" target="#fig_7">Figure 9</ref>. The table and the figures are divided into several parts where we compare similar levels of supervision.</p><p>The first level of supervision is fully automatic; in the second level, known ground truth dimensions on the road plane are used. In the third and final level of supervision, we use known ground truth speeds to form the lower error bound for different calibration methods.</p><p>Regarding the first level of supervision, our system Edgelets + BBScale + reg significantly outperforms the previous automatic method ITS15 + BMVC14 and we reduce the mean speed measurement error by 86 % (7.98 km/h to 1.10 km/h) . Another important fact is that our fully automatic method for camera calibration and scale inference also outperforms manual calibration and scale inference (1.35 km/h mean error) while the error is reduced by 19 % (1.35 km/h to 1.10 km/h). This improvement is important because in previous approaches, the automation always compromised accuracy, forcing the system developer to trade off between them. Our work shows that fully automatic calibration methods may produce better results than manual calibration. calibrations with scale obtained by minimizing the speed measurement error, thus forming a lower bound error for speed measurement with given camera calibration and tracking algorithm, bottom right: analysis of influence of different aspects of used 3D car models evaluated on speed measurement, see Section 5.4. The cumulative histogram is suitable for directly obtaining the "success rate" for a given error tolerance.</p><p>When it comes to the second and third level of supervision, the results follow the same trend with our calibration outperforming all of them (manual and automatic). The fact that manual calibration is better on the calibration metric (Section 5.1) and distance measurement (Section 5.2), while our method outperforms the manual calibration at the speed measurement task, is caused by the fact that manual calibration uses the same data which are then used for the evaluation of the calibration metric and distance measurement. The achieved accuracy is very close to meeting the standards for speed measurements accuracy required for enforcement (typically 3 % in many European countries). The accuracy is definitely comparable to measurements achievable by radars <ref type="bibr" target="#b43">(Sochor et al., 2016b)</ref>, while being considerably cheaper, more flexible, and passive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Sensitivity to Selection of the 3D Model</head><p>We also evaluated how using different 3D models of vehicles influences the speed measurement results. The results are shown in <ref type="table" target="#tab_5">Table 5</ref> and <ref type="figure" target="#fig_7">Figure 9</ref> (bottom right). We tested several combinations of used vehicles: use of only one of the models (Combi, Sedan) or both of them together (Combi + Sedan), forming the first segment of the table. It shows that using both models significantly improves the results, as the errors in geometry of the 3D models cancel out. We consider that using only a few (as few as two) fine-grained models is beneficial because it is not necessary to obtain more 3D models and training data for fine-grained recognition. The experiments show that having two models is sufficient for obtaining usable results; using more than two models in practice would follow the same principles and could increase the robustness further.</p><p>The second segment of the table shows the performance of the system with scale correction regression to overcome the inaccuracies of the 3D models. The results show that for model Combi, the error significantly decreases. However, for the Sedan model, the results stay more or less the same. This paradox is caused by the smaller number of training data for Sedan version as for some training videos, no Sedan vehicle was detected. The results also show that if we use both models, the performance drop is not that significant (1.10 km/h to 1.38 km/h) and therefore, it is possible to use the scale inference without the scale correction regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Vehicle Detection and Tracking Evaluation</head><p>Since we use a different vehicle detection and tracking method from <ref type="bibr" target="#b7">Dubsk? et al. (2014)</ref>, we also evaluate this part of the solution. We compare the methods on all videos of BrnoCompSpeed (including extra session0) with exactly the same calibration (ManualCalib + ManualScale) to isolate the influence of vehicle detection and tracking. We report the number of False Positives Per Minute and mean recall in vehicle counting. The results can be found in <ref type="table" target="#tab_6">Table 6</ref>, and as the table shows, our method considerably reduces the number of false positives with essentially the same recall.</p><p>A tracked vehicle is matched to the ground truth if it passes through the correct lane and the time difference of pass through the measurement line (yellow line in <ref type="figure" target="#fig_6">Figure 8</ref> which is closest to the camera) compared to the ground truth is less than 0.2 s. This threshold is used by <ref type="bibr" target="#b43">Sochor et al. (2016b)</ref> to correctly match the vehicles, as a higher threshold could lead to mismatches between the detected track and ground truth.</p><p>As we use the same calibration, we can also compare directly the speed measurement error which is influenced (with the same calibration) only by the tracking. As the table shows, our tracking method yields slightly reduced speed measurement error for the same scale and camera calibration.</p><p>For the tracking and speed measurement, we use the point at the front of the vehicle on the road plane (using the 3D bounding box), which is geometrically correct, as the point is on the road plane. We evaluated how the choice of the tracking point influences the measurement error, comparing to a naive solution which takes the center of the bottom edge of the 2D bounding box for the tracking, and we found out that the difference to the correct solution was negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Camera Calibration on Real Surveillance Cameras</head><p>The automatic calibration from vehicle movement can be justifiably suspected of requiring idealized conditions and to be sensitive to bad lighting, etc. In order to verify the usability of our camera calibration method in real-world conditions, we obtained data from surveillance cameras in production use at 9 different locations. The videos were captured both at day and night conditions. The data are of rather poor quality (704 ? 576 px or 704 ? 288 px) with 6 frames per second and a mean length of 40s. As the ground truth calibration is not available for the data, we report only qualitative results in the form of equilateral grid projected on the road plane. Despite the challenging character of the sequences (poor video quality and lighting conditions), we were able to correctly detect the vanishing points, as can be seen in <ref type="figure" target="#fig_0">Figure 10</ref> on a few examples, and thus find the camera parameters and its orientation, which is important in many real-world surveillance applications (e.g estimation of vehicle viewpoints or image rectification). <ref type="figure" target="#fig_0">Figure 10</ref>: Example of camera calibration (two vanishing points) for real world surveillance cameras. The first row shows different locations, while the second one show the same locations at night, dawn, and during daylight. The yellow line denotes the detected horizon (if present inside the frames) and red-green grid is formed by lines going to the first vanishing point (red) and to the second one (green). In an ideal case the grid is perpendicular in the real world and the lines are parallel to the features which define the vanishing points on the ground (e.g. line marking). It should also be noted that the method is able to work even on an intersection (top center).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose a fully automatic method for traffic surveillance camera calibration. It does not have any constraints on camera placement and does not require any manual input whatsoever. The results show that our system decreases the mean speed measurement error by 86 % (7.98 km/h to 1.10 km/h) compared to the previous automatic state-of-the-art method and by 19 % (1.35 km/h to 1.10 km/h) compared to the manual calibration method. This improvement is important, as in the previous approaches, automation always compromised accuracy, forcing the system developer to trade off between them. Our work shows that fully automatic calibration methods may produce better results than manual calibration. This result can be important beyond the field of traffic surveillance, since different forms of manual camera calibration are often considered the "ground truth", but our work shows that automatic calibration from statistics of repeated inaccurate measurements can be more precise, despite requiring no user input. Our method removes the necessity of per-camera setting or calibration, but it still requires some human annotations per coarse geographic region (e.g. Euro-pean Union or the USA) and per time period when the car models get vastly replaced (e.g. per decade).</p><p>In the experiments, we also showed that our method is able to calibrate real world traffic surveillance cameras and our proposed method for vehicle detection and tracking significantly reduces the number of false positives compared to the previous method. In future work, we would like to simplify the system and remove the necessity to render the vehicles by approximation of the bounding box size with a function parametrized by viewpoint and image location.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of detected vehicles and 3D model bounding box aligned to the vehicle detection bounding box. top: detected vehicle and corresponding 3D model (edges only), bottom: examples of aligned bounding boxes with shown 3D model edges (green), its bounding box (yellow) and vehicle detection (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Camera model and coordinates. Points denoted by small letters represent points in image space while points in the world space on the road plane ? are represented by capital letters. The representation stays the same for both finite and ideal points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of edgelet detection. From left to right -Seed points s i as local maxima of image gradient (foreground mask was used to filter interesting areas); Patches gatherded around the seed points from which the edge orientation is computed; Detail of an edgelet and its orientation superimposed on the gradient image; Top 25 % of edgelets detected in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of edges gathered from a video -(red) edges that pass close to the first vanishing point, (blue and green) edges accumulated to the Diamond Space, and (green) edges supporting the detected second vanishing point. The corresponding Diamond Space is shown in bottom-right corner. of each seed point s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Car detection and tracking. From left to right: Car detected by FRCN (blue), its foreground mask and convex hull (green); 3D bounding box constructed around the convex hull and tracking point on the bottom front edge; Car bounding box (from the convex hull) tracked by Kalman filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Examples of used 3D models (showing only edges) rendered under the same viewpoint as the corresponding real vehicle on the road. The left image shows the model which we will refer as Combi and the other two images show the 3D model Sedan. Both models are for Skoda Octavia mk1 which is common on the observed streets. Development of IoU (yellow boxes) metric for different scales (left to right), vehicle types and viewpoints (top to bottom). The left two images show larger rendered vehicles, the middle one shows the best match, and the right two images show smaller rendered vehicles. The rendered vehicle is shown only in a form of edges with the yellow rectangle bounding box of the rendered model and blue rectangle denoting the detected vehicle bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>An example of manually measured distances between markers on the road plane. Other examples can be found in the original BrnoCompSpeed publication<ref type="bibr" target="#b43">(Sochor et al., 2016b)</ref>. Blue lines denote the lane dividing lines, lines perpendicular to the vehicles direction are shown in yellow. Finally, measured distances between two points towards the first (second) vanishing point are shown by red (green) color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Evaluation of speed measurement -cumulative histograms of errors. The gray dashed vertical lines represent 3 km/h error. top left: comparison of automatic methods and a manual method for camera calibration, top right: calibrations obtained with known ground truth distances on the road plane, bottom left:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Errors of distance measurement ratios (see Section 5.1 for details). The first row for each calibration method contains absolute errors; the relative errors in percents are in the second row.</figDesc><table><row><cell>system</cell><cell cols="2">mean median 99 %</cell></row><row><cell>Edgelets (ours)</cell><cell>0.09 6.45</cell><cell>0.04 3.38 39.08 0.49</cell></row><row><cell>ITS15</cell><cell>0.18 11.74</cell><cell>0.05 5.25 61.03 1.36</cell></row><row><cell>ManualCalib</cell><cell>0.02 1.80</cell><cell>0.01 1.26 10.98 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Distance measurement errors on the road plane for different calibrations. Each segment of the table represents a different level of supervision in the calibration. The first row for each calibration method contains absolute errors in meters and the relative errors in percents are in the second row.</figDesc><table><row><cell>system</cell><cell cols="3">mean median 99 %</cell></row><row><cell>Edgelets + BBScale + reg (ours)</cell><cell>0.34 3.47</cell><cell cols="2">0.18 2.28 30.49 2.29</cell></row><row><cell>ITS15 + BMVC14</cell><cell>1.17 9.79</cell><cell cols="2">0.72 9.00 55.89 5.82</cell></row><row><cell>Edgelets + ManualScale (ours)</cell><cell>0.24 2.66</cell><cell cols="2">0.10 1.00 34.75 2.60</cell></row><row><cell>ITS15 + ManualScale</cell><cell>0.57 5.84</cell><cell cols="2">0.20 2.07 52.19 5.43</cell></row><row><cell>ManualCalib + ManualScale</cell><cell>0.07 0.84</cell><cell>0.04 0.50</cell><cell>0.30 3.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of speed measurement errors; all systems differ only in the calibration and scale inference, with the same tracking of vehicles. Each segment represents one level of supervision in the calibration (automatic, known ground truth distances on road, known ground truth speeds). The first row for each calibration method contains absolute errors in km/h; the relative errors in percents are in the second row.</figDesc><table><row><cell>system</cell><cell cols="3">mean median 99 %</cell></row><row><cell>Edgelets + BBScale + reg (ours)</cell><cell>1.10 1.39</cell><cell>0.97 1.22</cell><cell>3.05 4.13</cell></row><row><cell>ITS15 + BMVC14</cell><cell>7.98 10.15</cell><cell cols="2">8.18 18.58 11.45 19.22</cell></row><row><cell>Edgelets + ManualScale (ours)</cell><cell>1.04 1.31</cell><cell>0.83 1.04</cell><cell>3.48 4.61</cell></row><row><cell>ITS15 + ManualScale</cell><cell>1.44 1.76</cell><cell>1.17 1.50</cell><cell>5.43 6.16</cell></row><row><cell>ManualCalib + ManualScale</cell><cell>1.35 1.64</cell><cell>0.95 1.18</cell><cell>4.84 5.40</cell></row><row><cell>Edgelets + SpeedScale (ours)</cell><cell>0.52 0.66</cell><cell>0.35 0.44</cell><cell>2.57 3.71</cell></row><row><cell>ITS15 + SpeedScale</cell><cell>0.80 0.99</cell><cell>0.57 0.72</cell><cell>3.70 4.68</cell></row><row><cell>ManualCalib + SpeedScale</cell><cell>0.56 0.71</cell><cell>0.38 0.48</cell><cell>2.73 3.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Analysis of influence of different aspects of used 3D car models. It shows that it is best to use both models. The second segment of the table also shows that it is useful to use scale correction regression as described in Section 4.3. The first row for each 3D model combination method contains absolute errors in km/h; the relative errors in percents are in the second row.</figDesc><table><row><cell>system</cell><cell cols="2">mean median</cell><cell>99 %</cell></row><row><cell>Sedan</cell><cell>2.39 2.82</cell><cell>1.74 2.14</cell><cell>8.67 7.74</cell></row><row><cell>Combi</cell><cell>2.03 2.48</cell><cell>1.72 2.14</cell><cell>6.51 5.94</cell></row><row><cell>Combi + Sedan</cell><cell>1.38 1.70</cell><cell>0.99 1.23</cell><cell>5.18 4.94</cell></row><row><cell>Sedan + reg</cell><cell>2.43 2.97</cell><cell>2.49 3.17</cell><cell>7.26 6.56</cell></row><row><cell>Combi + reg</cell><cell>1.03 1.33</cell><cell>0.82 1.04</cell><cell>3.29 4.49</cell></row><row><cell>Combi + Sedan + reg</cell><cell>1.10 1.39</cell><cell>0.97 1.22</cell><cell>3.05 4.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of differences between vehicle detection and tracking proposed by<ref type="bibr" target="#b7">Dubsk? et al. (2014)</ref> and our detection and tracking method. FPPM denotes the number of False Positives Per Minute, recall was computed as mean recall across all videos and speed error denotes mean speed measurement error.</figDesc><table><row><cell>method</cell><cell cols="2">FPPM recall speed error [km/h]</cell></row><row><cell>Dubsk? et al. (2014)</cell><cell>9.77 0.885</cell><cell>1.46</cell></row><row><cell>ours</cell><cell>1.91 0.863</cell><cell>1.21</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by The Ministry of Education, Youth and Sports of the Czech Republic from the National Programme of Sustainability (NPU II); project IT4Innovations excellence in science -LQ1602.</p><p>We would also like to thank to company CAMEA for providing us data from industrial surveillance cameras.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel technique to dynamically measure vehicle speed using uncalibrated roadway cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cathey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="777" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reliable camera pose and calibration from a small set of point and line correspondences: A probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droulez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thibault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special issue on 3D Imaging and Modelling</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="576" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An algorithm to estimate mean traffic speed using uncalibrated cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cathey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pumrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="107" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple camera calibration method for vehicle velocity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Nghiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Thi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Ngoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology</title>
		<imprint>
			<publisher>ECTI-CON</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>2015 12th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1577" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Automatic camera calibration for traffic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dubsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real projective plane mapping for detection of orthogonal vanishing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dubsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully automatic roadside camera calibration for traffic surveillance. Intelligent Transportation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dubsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Juranek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1162" to="71" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fine-grained vehicle model recognition using a coarseto-fine convolutional neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1672" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Camera calibration from road lane markings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gsk</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhc</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gkh</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2967" to="77" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic estimation of vehicle speed from uncalibrated video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grammatikopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Petsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Modern Technologies, Educationand Profeesional Practice in Geodesy and Related Fields</title>
		<meeting>International Symposium on Modern Technologies, Educationand Profeesional Practice in Geodesy and Related Fields</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="332" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">New method for overcoming ill-conditioning in vanishing-point-based camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel algorithm for estimating vehicle speed from two consecutive images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhc</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Computer Vision, WACV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Car make and model recognition using 3D curve alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time pose estimation piggybacked on object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jur?nek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dubsk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zem??k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ASME -Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="issue">82</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop 3dRR-13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<editor>Pereira F, Burges C, Bottou L, Weinberger K</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vehicle speed measurement based on gray constraint optical flow algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik -International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="95" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A robust parametric method for bias field estimation and segmentation of mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="218" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Jointly optimizing 3D model fitting and finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dog breed classification using part localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">7572</biblScope>
			<biblScope unit="page" from="172" to="85" />
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features. In: Computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The proceedings of the seventh IEEE international conference on. Ieee</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vehicle speed estimation by license plate detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nassu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6563" to="6570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A video-based system for vehicle speed measurement in urban roadways</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Nassu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of vehicle velocity and traffic intensity using rectified images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maduro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="777" to="80" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved vehicle speed estimation using gaussian mixture model and hole filling algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nurhadiyatna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hardjono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wibisono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jatmiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma&amp;apos;sum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mursanto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Computer Science and Information Systems (ICACSIS), 2013 International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="451" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">3-D model based vehicle recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prokaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic camera calibration of roadside traffic management cameras for vehicle speed estimation. Intelligent Transportation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schoepflin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vehicle counting and speed measurement using headlight detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wibisono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nurhadiyatna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hardjono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jatmiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mursanto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Computer Science and Information Systems (ICACSIS), 2013 International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="149" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BoxCars: 3D boxes as CNN input for improved finegrained vehicle recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Havel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">BrnoComp-Speed: Review of traffic camera calibration and a comprehensive dataset for monocular speed measurement. Intelligent Transportation Systems (under review)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sochor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Juranek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spanhel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marsik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siroky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detection and Tracking of Point Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An accurate and practical calibration method for roadside camera using two vanishing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic camera calibration of broadcast tennis video with applications to 3D virtual content insertion and ball detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="643" to="52" />
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Based Analysis in Sport Environments</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Practical camera calibration from moving objects for traffic scene surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="518" to="551" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A practical roadside camera calibration method based on least squares optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="831" to="874" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
