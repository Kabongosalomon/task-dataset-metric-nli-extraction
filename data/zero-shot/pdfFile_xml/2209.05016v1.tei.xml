<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FiBiNet++:Improving FiBiNet by Greatly Reducing Model Size for CTR Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sina Weibo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FiBiNet++:Improving FiBiNet by Greatly Reducing Model Size for CTR Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Information systems ? Recommender systems Keywords: Recommender System</term>
					<term>Click-Through Rate</term>
					<term>Feature Importance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-Through Rate(CTR) estimation has become one of the most fundamental tasks in many real-world applications and various deep models have been proposed to resolve this problem. Some research has proved that FiBiNet is one of the best performance models and outperforms all other models on Avazu dataset.However, the large model size of FiBiNet hinders its wider applications.In this paper, we propose a novel FiBiNet++ model to redesign FiBiNet's model structure ,which greatly reducess model size while further improves its performance.Extensive experiments on three public datasets show that FiBiNet++ effectively reduces non-embedding model parameters of FiBiNet by 12x to 16x on three datasets and has comparable model size with DNN model which is the smallest one among deep CTR models.On the other hand, FiBiNet++ leads to significant performance improvements compared to state-of-theart CTR methods,including FiBiNet. The source code is in https://github.com/recommendation-algorithm/FiBiNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. The Framework of FiBiNet++ DeepFM <ref type="bibr" target="#b1">[2]</ref>,DCN <ref type="bibr" target="#b7">[8]</ref>,xDeepFM <ref type="bibr" target="#b3">[4]</ref> AutoInt <ref type="bibr" target="#b6">[7]</ref>,DCN v2 <ref type="bibr" target="#b8">[9]</ref> and FiBiNet <ref type="bibr" target="#b2">[3]</ref>.Specifically,Wide &amp; Deep Learning <ref type="bibr" target="#b10">[11]</ref> jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommender systems. DeepFM <ref type="bibr" target="#b1">[2]</ref> replaces the wide part of Wide &amp; Deep model with FM and shares the feature embedding between the FM and deep component.Some works explicitly introduce high-order feature interactions by subnetwork.For example,Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b7">[8]</ref> and DCN v2 <ref type="bibr" target="#b8">[9]</ref>efficiently capture feature interactions of bounded degrees in an explicit fashion. The eXtreme Deep Factorization Machine (xDeepFM) <ref type="bibr" target="#b3">[4]</ref> also models the low-order and high-order feature interactions in an explicit way by proposing a novel Compressed Interaction Network (CIN) part. Similarly, AutoInt <ref type="bibr" target="#b6">[7]</ref> proposes a multi-head self-attentive neural network with residual connections to explicitly model the feature interactions in the low-dimensional space.FiBiNet <ref type="bibr" target="#b2">[3]</ref> dynamically learns the importance of features via the Squeeze-Excitation network (SENET) and feature interactions via bi-linear function. Though many models have been proposed,seldom works fairly compares these model's performance. FuxiCTR <ref type="bibr" target="#b12">[13]</ref> performs open benchmarking for CTR prediction and presents a rigorous comparison of various models in a reproducible manner. This works runs over 7,000 experiments for more than 12,000 GPU hours in total to reevaluate 24 existing models on multiple datasets.Experimental results <ref type="bibr" target="#b12">[13]</ref>show that FiBiNet is one of the best performance models and outperforms all other 23 models on Avazu dataset, which is one of the most popular real-world benchmarks for CTR model evaluation.</p><p>However,we argue in this paper that FiBiNet has too much model parameters , which mainly come from the design shortcoming of bi-linear module and hinder its wider applications in many real life CTR scenes. Our works aims to redesign the model structure to greatly reduce model size while maintaining or even improving its performance.In this paper,we propose a novel FiBiNet++ model to address these challenges as shown in <ref type="figure">Figure 1</ref>. First, we reconstruct the model structure by removing the bi-linear module on SENet and linear part in FiBiNet,which directly reduces large amount of parameters. Second,we upgrade bi-linear function into bi-linear+ module by changing the hadamard product to inner product and bringing a compression MLP layer into it,which further reduces model size.Finally,we introduce feature normalization and the upgraded SENet+ module to enhance model performance. Extensive experiments are conducted on three public real-world datasets and experimental results show that FiBiNet++ provides orders of magnitude improvement in model size while improving the quality of the model compared with FiBiNet. We summarize our major contributions as below:</p><p>1. The proposed FiBiNet++ greatly reduces non-embedding model size of FiBiNet by 12x to 16x on three datasets and has comparable model size with DNN model which is the smallest one among deep CTR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FiBiNet++ yields remarkable improvements compared</head><p>to state-of-the-art models on three real-world datasets, including FiBiNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Deep learning models are widely used in industrial recommendation systems, such as DNN <ref type="bibr" target="#b11">[12]</ref>, DCN <ref type="bibr" target="#b7">[8]</ref>,DeepFM <ref type="bibr" target="#b1">[2]</ref>and FiBiNet <ref type="bibr" target="#b2">[3]</ref>. Among them, DNN model <ref type="bibr" target="#b11">[12]</ref>is almost the simplest one and is always used as a sub-component in most current DNN ranking systems <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b7">8]</ref>. It contains three components:feature embedding,MLP and prediction layer.In this section, we introduce the DNN model and the optimization objective of CTR prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Embeddding</head><p>As all we know, features in CTR tasks usually can be segregated into the following two groups:</p><p>1. Categorical features. This type of feature is common and the one-hot representation may produce very sparse features. We map one-hot representation to dense, lowdimensional embedding vectors suitable for complex transformation. We can obtain feature embedding v for one-hot vector x via:</p><formula xml:id="formula_0">v = W x ? R 1?<label>(1)</label></formula><p>where W ? R ? is the embedding matrix of features and is the dimension of field embedding.</p><p>2. Numerical features. We map the feature field into an embedding vector as follows:</p><formula xml:id="formula_1">v = e x ? R 1? (2)</formula><p>where e ? R 1? is an embedding vector for field with size , and x is a scalar value which means the actual value of that numerical feature. Some big numerical values will dominate the parameter updating procedure during model training and we normalized x into [0,1] via min-max normalization technique before multiplying it into e to avoid this:</p><formula xml:id="formula_2">x = x ? x x ? x<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MLP and Prediction Layer</head><p>To learn high-order feature interactions, multiple feed-forward layers are stacked on the concatenation of dense features rep-</p><formula xml:id="formula_3">resented as H 0 = [v 1 , v 2 , ..., v ]</formula><p>,here denotes fields number. Then,the feed forward process of MLP is:</p><formula xml:id="formula_4">H = (W H ?1 + )<label>(4)</label></formula><p>where is the depth and ReLU is the activation function. W , , H are weighting matrix, bias and output of the -th layer. The prediction layer is put on the last layer of multiple feed-forward networks,and the model' s output is:</p><formula xml:id="formula_5">= (w 0 + ?? =1 w x )<label>(5)</label></formula><p>where?? (0, 1) is the predicted value of CTR, (?) is the sigmoid function, is the size of feed-forward layer, x is the bit value of feed-forward layer and w is the learned weight for each bit value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization Objective</head><p>For binary classifications, the loss function of CTR prediction is the log loss:</p><formula xml:id="formula_6">L = ? 1 ?? =1 log(?) + (1 ? ) log(1 ??)<label>(6)</label></formula><p>where is the total number of training instances, is the ground truth of -th instance and?is the predicted CTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Model</head><p>The architecture of the proposed FiBiNet++ is shown in <ref type="figure">Figure 1</ref>.We argue that current structure of FiBiNet ,especially bi-linear function,leads to large amount of unnecessary parameters.Therefore,the bi-linear function on SENet module in original FiBiNet is removed and only left branch of bi-linear function is kept.In addition, the linear part of FiBi-Net helps its good performance while we find it's beneficial to remove it from FiBiNet++ model.These two changes on network structure directly decrease model size and we will discuss this in detail in section 3.5. <ref type="figure">Figure 1</ref>, the original feature embedding is first normalized before be sent to the following components.Then,bi-linear+ module models feature interactions and SENet+ module computes bit-wise feature importance. The outputs of two branches are concatenated as input of the following MLP layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Normalization</head><p>Normalization techniques have been recognized as very effective component in deep learning and works <ref type="bibr" target="#b9">[10]</ref> conducts a systematic study on the effect of widely used normalization schema to both feature embedding and MLP part in deep CTR model.Inspired by this work,we introduce feature normalization into FiBiNet++ to enhance model's training stability and performance as follows:</p><formula xml:id="formula_7">N(V) = [N(v 1 ), N(v 2 ), ..., N(v )] ? R 1?<label>(7)</label></formula><p>where N (?) is layer normalization for numerical feature and batch normalization operation for categorical feature:</p><formula xml:id="formula_8">N(v ) = ? ? ? ? ? ? ? ? ? (v ) x ? S (v ) x ? S<label>(8)</label></formula><p>here S is a set containing categorical features and S is a set of numerical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bi-Linear+ Module</head><p>FiBiNet models interaction between feature x and feature x by bi-linear function which introduces an extra learned matrix W as follows:</p><formula xml:id="formula_9">p , = v ? W ? v ? R 1?<label>(9)</label></formula><p>where ? and ? denote inner product and element-wise hadamard product,respectively.The matrix W can be parameters to learn following one of the three options:'field all type', 'field each type' or 'field interaction type'. Though it's effective to model feature interactions via bi-linear function, we argue it is the hadamard product that brings large amount of unnecessary parameters.In order to effectively reduce model size, we upgrade bi-linear function into bi-linear+ module by following two methods.First,the hadamard product is replaced by another inner product as follows:</p><formula xml:id="formula_10">p , = v ? W ? v ? R 1?1<label>(10)</label></formula><p>It's easy to see the parameters of p , decrease greatly from d dimensional vector to 1 bit for each feature interaction.Suppose the input instance has fields and we have the following vector after bi-linear feature interactions:</p><formula xml:id="formula_11">P = [p 1,2 , p 1,3 , ......, p ?1, ] ? R 1? ?( ?1) 2<label>(11)</label></formula><p>To further reduce parameter number, we introduce a compression MLP layer stacking on vector P as follows:</p><formula xml:id="formula_12">H = 1 (W 1 P) ? R 1?<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">W 1 ? R ? ?( ?1) 2</formula><p>is a learning matrix of thin MLP layer with small size . 1 (?) is an identity function without nonlinear transformation because we find model performance decreases when non-linear function is adopted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SENet+ Module</head><p>SENet module is comprised of three steps:squeeze, excitation and re-weight step and it is first proposed by FiBiNet in CTR prediction domain to dynamically compute the feature importance. We upgrade it into SENet+ to boost model performance in this paper.SENet+ module consists of four phases:squeeze, excitation, re-weight and fuse.Though our proposed SENet+ module has similar three phases just as original SENet does,each step is improved in order to enhance model performance.</p><p>Squeeze. SENet collects one bit information by mean pooling from each feature embedding as 'summary statistics'. However,we think more input information will benefit model performance.Therefore, we improve the original squeeze step by providing more useful information.Specifically,we first segment each normalized feature embedding v ? R 1? into g groups,which is a hyper-parameter, as follows:</p><formula xml:id="formula_14">v = [v ,1 , v ,2 , ......, v , ]<label>(13)</label></formula><p>where v , ? R 1? means information in the j-th group of the i-th feature.Let k = denotes size of each group.Then, we select the max value z , and average pooling value z ,</p><p>in v , as representative information of the group as follows:</p><formula xml:id="formula_15">z , = max v , =1 z , = 1 ?? =1 v ,<label>(14)</label></formula><p>The concatenated representative information of each group forms the 'summary statistic' Z of feature embedding v :</p><formula xml:id="formula_16">Z = [z ,1 , z ,1 , z ,2 , z ,2 , ......, z , , z , ] ? R 1?2<label>(15)</label></formula><p>Finally,we can concatenate each feature's summary statistic as the input of SENet+ module:</p><formula xml:id="formula_17">Z = [Z 1 , Z 2 , ......, Z ] ? R 1?2<label>(16)</label></formula><p>Excitation. The excitation step in SENet computes each feature's weight according to statistic vector Z,which is a field-wise attention. However, we improve this step by changing the field-wise attention into a more fine-grained bit-wise attention.Similarly,we also use two full connected (FC) layers to learn the weights as follows:</p><formula xml:id="formula_18">A = 3 (W 3 2 (W 2 Z)) ? R 1?<label>(17)</label></formula><p>where</p><formula xml:id="formula_19">W 2 ? R 2 ?2</formula><p>denotes learning parameters of first FC layer,which is a thin layer and is reduction ratio.W 3 ? R ? 2 means learning parameters of second FC layer,which is a wider layer with size of . Here 2 (?) is (?) and 3 (?) is an identity function without non-linear transformation.In this way,each bit in input embedding can dynamically learns corresponding attention score provided by A.</p><p>Re-Weight. Re-weight step does element-wise multiplication between the original field embedding and the learned attention scores as follows:</p><formula xml:id="formula_20">V = A ? N(V) ? R 1? (18)</formula><p>where ? is an element-wise multiplication between two vectors and N(V) denotes original embedding after normalization.</p><p>Fuse. An extra "fuse" step is introduced in order to better fuse the information contained both in original feature embedding and weighted embedding.Specifically,we first use skip-connection to merge two embedding as follows:</p><formula xml:id="formula_21">v = v ? v<label>(19)</label></formula><p>where v donates the i-th normalized feature embedding, v denotes embedding after re-weight step,? is an element-wise addition operation.Then, another feature normalization is applied on feature embedding v for a better representation:</p><formula xml:id="formula_22">v = LN(v )<label>(20)</label></formula><p>Note we adopt layer normalization here no matter what type of feature it belongs to,numerical or categorical feature.Finally,we concatenate all the fused embeddings as output of the SENet+ module:</p><formula xml:id="formula_23">V + = [v 1 , v 2 , ..., v ] ? R 1? (21)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Concatenation Layer</head><p>Let H denotes output of the compression MLP layer in bi-linear+ module and V + denotes the weighted feature embedding of the SENet+ module,we can concatenate them to form the input of the following MLP layers:</p><formula xml:id="formula_24">H 0 = [H , V + ]<label>(22)</label></formula><p>The following procedure is same with what we describe in section 2.2 and section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>In this section,we discuss the model size difference between FiBiNet and FiBiNet++.Note only non-embedding parameter is considered,which really demonstrates model complexity.</p><p>The major parameter of FiBiNet comes from two components:one is the connection between the first MLP layer and output of two bi-linear modules,and the other is the linear part.Suppose we denote ? = 400 as size of first MLP layer, = 50 as fields number, = 10 as feature embedding size and = 1 as feature number.Therefore, the parameter number in these two parts is nearly 10.8 millions:</p><formula xml:id="formula_25">T = f ? (f ? 1) ? d ? h ? + = 10.8 (23)</formula><p>For FiBiNet++, majority of model parameter comes from following three components:connection between the first MLP layer and embedding produced by SENet+ module(1-th part),connection between the first MLP layer and compression MLP layer(2-th part),and parameters between compression MLP layer and bi-linear feature interaction results(3-th part).Let = 50 denote size of compression MLP layer.We have parameter number of these components as follows:</p><formula xml:id="formula_26">T ++ = f ? d ? h 1? ? + m ? h 2? ? + f ? (f ? 1) 2 ? m 3? ? = 0.28</formula><p>(24) We can see that the above-mentioned methods to reduce model size greatly decrease model size from 10.8 millions to 0.28 millions,which is nearly 39x model compression.In addiction,the larger the fields number is, the larger model compression ratio we can achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We conduct extensive experiments on three public datasets to validate the effectiveness of FiBiNet++. First,we describe the experimental settings and compare performance of FiBi-Net++ with other state-of-the-art CTR models.Then,we compare the model size of FiBiNet++ with FiBiNet .Finally, we provide the hyper-parameter analyses of FiBiNet++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. The following three datasets are used in our experiments:  chronologically. For each click data, there are 24 fields which indicate elements of a single ad impression. 3. KDD12 3 Dataset: KDD12 dataset aims to predict the click-through rate of ads and the instances are derived from session logs of the Tencent proprietary search engine.There are 13 fields spanning from user id to ad position for a clicked data. We randomly split instances by 8:1:1 for training , validation and test while <ref type="table" target="#tab_0">Table 1</ref> lists the statistics of the evaluation datasets.</p><p>Evaluation Metrics. AUC (Area Under ROC) is used as the evaluation metric in our experiments. This metric is very popular for binary classification tasks.Logloss (binary crossentropy loss) shows similar trend with AUC and we didn't present it because of the space limit.</p><p>Models for Comparisons. We compare the performance of the FM <ref type="bibr" target="#b5">[6]</ref>, DNN <ref type="bibr" target="#b11">[12]</ref>, DeepFM <ref type="bibr" target="#b1">[2]</ref>,DCN <ref type="bibr" target="#b7">[8]</ref>,AutoInt <ref type="bibr" target="#b6">[7]</ref>,DCN V2 <ref type="bibr" target="#b8">[9]</ref>,xDeepFM <ref type="bibr" target="#b3">[4]</ref> and FiBiNet <ref type="bibr" target="#b2">[3]</ref> models as baseline and all of which are discussed in Section 1.Results of some models such as LR <ref type="bibr" target="#b4">[5]</ref>, Wid&amp;Deep <ref type="bibr" target="#b0">[1]</ref> and AFM <ref type="bibr" target="#b10">[11]</ref> are not presented in this paper, because more recent models like FiBiNet <ref type="bibr" target="#b2">[3]</ref> and DCN-V2 <ref type="bibr" target="#b8">[9]</ref> have outperformed these methods significantly as experiments in FuxiCTR <ref type="bibr" target="#b12">[13]</ref> shows. Implementation Details. We implement all the models with Tensorflow in our experiments. For optimization method, we use the Adam with a mini-batch size of 1024.We find different learning rate has great influence on some baseline's performance.Therefore, both 0.0001 and 0.001 learning rate are verified and the best results are reported as final performance for all baselines. We make the dimension of field embedding for all models to be a fixed value of 10 for Criteo dataset, 50 for Avazu dataset and 10 for KDD12 dataset. For models with DNN part, the depth of hidden layers is set to 3, the number of neurons per layer is 400, all activation 3 KDD12 https://www.kaggle.com/c/kddcup2012-track2 function are ReLU. In SENet+, the reduction ratio is set to 3 and group number is 2 as default settings.In Bi-linear+ module, we set the size of compression MLP layer as 50.For other models, we take the optimal settings from the original papers. <ref type="table" target="#tab_1">Table 2</ref> shows the performance of different SOTA baselines and FiBiNet++. The experiments for FiBiNet++ and the baseline models are repeated 5 times by changing the random seeds and the averaged results are reported for fair comparison.The best results are in bold, and the best baseline results are underlined. From the experimental results, we mainly have the following observations: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Size Comparison</head><p>We compare the model size of different methods in <ref type="table" target="#tab_1">Table  2</ref>.Note that the presented model parameters only contain the non-embedding parts of various models,which truly demonstrate the model complexity.We make the following observations from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyper-Parameters of FiBiNet++</head><p>In this section, we study hyper-parameter sensitivity of FiB-iNet++. There are three hyper-parameters influencing the model performance: group number,reduction ratio in SENet+ Group Number. <ref type="figure">Figure 2a</ref> shows the impact of group number of feature embedding on model performance. We can observe a slightly performance increase with the increase of group number,which indicates that more group number benefits model performance because we can input more useful information in feature embedding into SENet+ module. However,bigger group number will bring a wider input for SENET++ module and it will slow down the training speed.</p><p>Reduction Ratio. We conduct some experiments to adjust the reduction ratio in SENet+ module from 1 to 9 and <ref type="figure">Figure 2b</ref> shows the result. It can be seen that the performance is better if we set the reduction ratio to 3 or 9.</p><p>Size of Compression MLP Layer. The results in <ref type="figure">Figure  2c</ref> show the impact when we adjust the size of compression MLP layer in bi-linear+ module. We can observe that the performance begin to decrease when the size is set greater than 150,which demonstrates the thin layer is better and wider layer may bring more noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Some research shows that FiBiNet is one of the best performance CTR models.However,FiBiNet has too much model parameters that hinder its wider applications.In this paper, we propose FiBiNet++ model in order to greatly reduce the model size while improving the model performance. Experimental results show that FiBiNet++ provides orders of magnitude improvement in model size while improving the quality of the model compared with FiBiNet. It also outperforms other state-of-the-art models on three real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Effect of Hyper-Parameters Comparison of Model Size module and size of compression MLP layer in bi-linear+ module. The experiments are conducted on Criteo and Avazu datasets via changing one hyper-parameter while holding the other settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the Evaluation Datasets</figDesc><table><row><cell cols="4">Datasets #Instances #fields #features</cell></row><row><cell>Criteo</cell><cell>45M</cell><cell>39</cell><cell>30M</cell></row><row><cell>Avazu</cell><cell>40.4M</cell><cell>24</cell><cell>9.5M</cell></row><row><cell>KDD12</cell><cell>194.6M</cell><cell>13</cell><cell>6M</cell></row><row><cell cols="4">1. Criteo 1 Dataset: As a very famous public real world</cell></row><row><cell cols="4">display ad dataset with each ad display information</cell></row><row><cell cols="4">and corresponding user click feedback, Criteo data set</cell></row><row><cell cols="4">is widely used in many CTR model evaluation. There</cell></row><row><cell cols="4">are 26 anonymous categorical fields and 13 continuous</cell></row><row><cell cols="3">feature fields in Criteo data set.</cell><cell></cell></row></table><note>2. Avazu 2 Dataset: The Avazu dataset consists of sev- eral days of ad click-through data which is ordered</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Overall performance (AUC) of different models on three datasets</figDesc><table><row><cell></cell><cell cols="2">Avazu</cell><cell cols="2">Criteo</cell><cell cols="2">KDD12</cell></row><row><cell>Model</cell><cell cols="6">AUC(%) Paras. AUC(%) Paras. AUC(%) Paras.</cell></row><row><cell>FM</cell><cell>78.17</cell><cell>1.54M</cell><cell>78.97</cell><cell>1.0M</cell><cell>77.65</cell><cell>5.46M</cell></row><row><cell>DNN</cell><cell>78.67</cell><cell>0.74M</cell><cell>80.73</cell><cell>0.48M</cell><cell>79.54</cell><cell>0.37M</cell></row><row><cell>DeepFM</cell><cell>78.64</cell><cell>2.29M</cell><cell>80.58</cell><cell>1.48M</cell><cell>79.40</cell><cell>5.84M</cell></row><row><cell>xDeepFM</cell><cell>78.88</cell><cell>4.06M</cell><cell>80.64</cell><cell>4.90M</cell><cell>79.51</cell><cell>6.91M</cell></row><row><cell>DCN</cell><cell>78.68</cell><cell>0.75M</cell><cell>80.73</cell><cell>0.48M</cell><cell>79.58</cell><cell>0.37M</cell></row><row><cell>AutoInt+</cell><cell>78.62</cell><cell>0.77M</cell><cell>80.78</cell><cell>0.48M</cell><cell>79.69</cell><cell>0.38M</cell></row><row><cell>DCN v2</cell><cell>78.98</cell><cell>4.05M</cell><cell>80.88</cell><cell>0.65M</cell><cell>79.66</cell><cell>0.39M</cell></row><row><cell>FiBiNet</cell><cell>79.12</cell><cell>10.27M</cell><cell>80.73</cell><cell>7.25M</cell><cell>79.52</cell><cell>6.41M</cell></row><row><cell cols="2">FiBiNet++ 79.15</cell><cell>0.81M</cell><cell cols="4">81.10 0.56M 79.98 0.40M</cell></row><row><cell>Improv.</cell><cell>+0.03</cell><cell>12.7x</cell><cell>+0.37</cell><cell>12.9x</cell><cell>+0.46</cell><cell>16x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : 1 .</head><label>21</label><figDesc>Among all the baseline models,DNN is the smallest one while FiBiNet and xDeepFM are relatively complex models.FM and DeepFM also have much bigger model size compared with DNN because of the existence of linear part as original paper described.2. As can be seen inTable 2 and Figure 3, FiBiNet++ provides orders of magnitude improvement in model size while improving the quality of the model compared with FiBiNet. Specifically,FiBiNet++ reduce model size of FiBiNet by 12.7x, 12.9x and 16x in terms of the number of parameters on three datasets, respectively, which demonstrates that our proposed methods to reduce model parameter in this paper are effective.Now FiBiNet++ has comparable model size with DNN model while outperforms all other models on three datasets at the same time.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Criteo http://labs.criteo.com/downloads/download-terabyte-click-logs/ 2 Avazu http://www.kaggle.com/c/avazu-ctr-prediction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FiBiNET: combining feature importance and bilinear feature interaction for clickthrough rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3298689.3347043</idno>
		<ptr target="https://doi.org/10.1145/3298689.3347043" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems, RecSys 2019</title>
		<meeting>the 13th ACM Conference on Recommender Systems, RecSys 2019<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-09-16" />
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: A View from the Trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2488200</idno>
		<ptr target="https://doi.org/10.1145/2487575.2488200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Chicago, Illinois, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
	<note>KDD &apos;13)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoint: Automatic feature interaction learning via self-attentive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Shivanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021, Virtual Event / Ljubljana</title>
		<meeting><address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19" />
			<biblScope unit="page" from="1785" to="1797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12753</idno>
		<ptr target="https://arxiv.org/abs/2006.12753" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FuxiCTR: An Open Benchmark for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2009.05794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Qi Zhang, and Xiuqiang He</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
