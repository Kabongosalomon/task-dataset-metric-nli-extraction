<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Underwater Imagery using Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-15">January 15, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Fabbri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Jahidul</forename><surname>Islam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaed</forename><surname>Sattar</surname></persName>
							<email>junaed3@umn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Underwater Imagery using Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-15">January 15, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous underwater vehicles (AUVs) rely on a variety of sensors -acoustic, inertial and visual -for intelligent decision making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face difficult challenges, and consequently exhibit poor performance on visiondriven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that effect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Underwater robotics has been a steadily growing subfield of autonomous field robotics, assisted by the advent of novel platforms, sensors and propulsion mechanisms. While autonomous underwater vehicles are often equipped with a variety of sensors, visual sensing is an attractive option because of its non-intrusive, passive, and energy efficient nature. The monitoring of coral reefs <ref type="bibr" target="#b27">[28]</ref>, deep ocean exploration <ref type="bibr" target="#b31">[32]</ref>, and mapping of the seabed <ref type="bibr" target="#b4">[5]</ref> are a number of tasks where visually-guided AUVs and ROVs (Remotely Operated Vehicles) have seen widespread use. Use of these robots ensures humans are not exposed to the hazards of underwater exploration, as they no longer need to venture to the depths (which was how such tasks were carried out in the past). Despite the advantages of using vision, underwater environments pose unique challenges to visual sensing, as light refraction, absorption and scattering from suspended particles can greatly affect optics. For example, because red wavelengths are quickly absorbed by water, images tend to have a green or blue hue to them. As one goes deeper, this effect worsens, as more and more red hue is absorbed. This distortion is extremely non-linear in nature, and is affected by a large number of factors, such as the amount of light present (overcast versus sunny, operational depth), amount of particles in the water, time of day, and the camera being used. This may cause difficulty in tasks such as segmentation, tracking, or classification due to their indirect or direct use of color.</p><p>As color and illumination begin to change with depth, vision-based algorithms need to be generalizable in order to work within the depth ranges a robot may operate in. Because of the high cost and difficulty of acquiring a variety of underwater data to train a visual system on, as well as the high amount of noise introduced, algorithms may (and do) perform poorly in these different domains. <ref type="figure" target="#fig_0">Figure 2</ref> shows the high variability in visual scenes that may occur in underwater environments. A step towards a solution to this issue is to be able to restore the images such that they appear to be above water, i.e., with colors corrected and suspended particles removed from the scene. By performing a many-to-one mapping of these domains from underwater to not underwater (what the image would look like above water), algorithms that have difficulty performing across multiple forms of noise may be able to focus only one clean domain.</p><p>Deep neural networks have been shown to be powerful non-linear function approximators, especially in the field of vision <ref type="bibr" target="#b16">[17]</ref>. Often times, these networks require large amounts of data, either labeled or paired with ground truth. For the problem of automatically colorizing grayscale images <ref type="bibr" target="#b32">[33]</ref>, paired training data is readily available due to the fact that any color image can be converted to black and white. However, underwater images distorted by either color or some other phenomenon lack ground truth, which is a major hindrance towards adopting a similar approach for correction. This paper proposes a technique based on Generative Adversarial Networks (GANs) to improve the quality of visual underwater scenes with the goal of improving the performance of vision-driven behaviors for autonomous underwater robots. We use the recently proposed CycleGAN <ref type="bibr" target="#b34">[35]</ref> approach, which learns to translate an image from any arbitrary domain X to another arbitrary domain Y without image pairs, as a way to generate a paired dataset. By letting X be a set of undistorted underwater images, and Y be a set of distorted underwater images, we can generate an image that appears to be underwater while retaining ground truth. <ref type="figure">Figure 1</ref>: Sample underwater images with natural and man-made artifacts (which in this case is our underwater robot) displaying the diversity of distortions that can occur. With the varying camera-to-object distances in the images, the distortion and loss of color varies between the different images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While there have been a number of successful recent approaches towards automatic colorization <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b10">11]</ref>, most are focused on the task of converting grayscale images to color. Quite a few approaches use a physics-based technique to directly model light refraction <ref type="bibr" target="#b14">[15]</ref>. Specifically for restoring color in underwater images, the work of <ref type="bibr" target="#b28">[29]</ref> uses an energy minimization formulation using a Markov Random Field. Most similar to the work proposed in this paper is the recently proposed WaterGAN <ref type="bibr" target="#b19">[20]</ref>, which uses an adversarial approach towards generating realistic underwater images. Their generator model can be broken down into three stages: 1) Attenuation, which accounts for range-dependent attenuation of light. 2) Scattering, which models the haze effect caused by photons scattering back towards the image sensor and 3) Vignetting, which produces a shading effect on the image corners that can be caused by certain camera lenses. Differentiating from our work, they use a GAN for generating the underwater images and use strictly Euclidean loss for color correction, whereas we use a GAN for both. Furthermore, they require depth information during the training of Water-GAN, which can be often difficult to attain particularly for underwater autonomous robotic applications. Our work only requires images of ob-jects in two separate domains (e.g., underwater and terrestrial) throughout the entire process.</p><p>Recent work in generative models, specifically GANs, have shown great success in areas such as inpainting <ref type="bibr" target="#b23">[24]</ref>, style transfer <ref type="bibr" target="#b7">[8]</ref>, and imageto-image translation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>. This is primarily due to their ability to provide a more meaningful loss than simply the Euclidean distance, which has been shown to produce blurry results. In our work, we structure the problem of estimating the true appearance of underwater imagery as a paired image-to-image translation problem, using Generative Adversarial Networks (GANs) as our generative model (see Section 3.2 for details). Much like the work of <ref type="bibr" target="#b13">[14]</ref>, we use image pairs from two domains as input and ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Underwater images distorted by color or other circumstances lack ground truth, which is a necessity for previous colorization approaches. Furthermore, the distortion present in an underwater image is highly nonlinear; simple methods such as adding a hue to an image do not capture all of the dependencies. We propose to use Cycle-GAN as a distortion model in order to generate paired images for training. Given a domain of underwater images with no distortion, and a domain of underwater images with distortion, Cy-cleGAN is able to perform style transfer. Given an undistorted image, CycleGAN distorts it such that it appears to have come from the domain of distorted images. These pairs are then used in our algorithm for image reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Generation</head><p>Depth, lighting conditions, camera model, and physical location in the underwater environment are all factors that affect the amount of distortion an image will be subjected to. Under certain conditions, it is possible that an underwater image may have very little distortion, or none at all. We let I C be an underwater image with no distortion, and I D be the same image with distortion. Our goal is to learn the function f : I D ? I C . Becasue of the difficulty of collecting underwater data, more often than not only I D or I C exist, but never both.</p><p>To circumvent the problem of insufficient image pairs, we use CycleGAN to generate I D from I C , which gives us a paired dataset of images. Given two datasets X and Y , where I C ? X and I D ? Y , CycleGAN learns a mapping F : X ? Y . <ref type="figure" target="#fig_0">Figure 2</ref> shows paired samples generated from CycleGAN. From this paired dataset we train a generator G to learn the function f :</p><formula xml:id="formula_0">I D ? I C .</formula><p>It should be noted that during the training process of CycleGAN, it simultaneously learns a mapping G : Y ? X, which is similar to f . In Section 4, we compare images generated by CycleGAN with images generated through our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Networks</head><p>In machine learning literature, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">[9]</ref> represent a class of generative models based on game theory in which a generator network competes against an adversary. From a classification perspective, the generator network G produces instances which actively attempt to 'fool' the discriminator network D. The goal is for the discriminator network to be able to distinguish between 'true' instances coming from the dataset and 'false' instances produced by the generator network. In our case, conditioned on an image I D , the generator is trained to produce an image to try and fool the discriminator, which is trained to distinguish between distorted and non-distorted underwater images. In the original GAN formulation, our goal is to solve the minimax problem:</p><formula xml:id="formula_1">min G max D E I C ?p train (I C ) [logD(I C )]+ E I D ?pgen(I D ) [log(1 ? D(G(I D )))]</formula><p>(1) Note for simplicity in notation, we will further omit I C ? p train (I C ) and I D ? p gen (I D ). In this formulation, the discriminator is hypothesized as a classifier with a sigmoid cross-entropy loss function, which in practice may lead to issues such as the vanish gradient and mode collapse. As shown by <ref type="bibr" target="#b1">[2]</ref>, as the discriminator improves, the gradient of the generator vanishes, making it difficult or impossible to train. Mode collapse occurs when the generator "collapses" onto a single point, fooling the discriminator with only one instance. To illustrate the effect of mode collapse, imagine a GAN is being used to generate digits from the MNIST <ref type="bibr" target="#b17">[18]</ref> dataset, but it only generated the same digit. In reality, the desired outcome would be to generate a diverse collection of all the digits. To this end, there have been a number of recent methods which hypothesize a different loss function for the discriminator <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34]</ref>. We focus on the Wasserstein GAN (WGAN) <ref type="bibr" target="#b2">[3]</ref> formulation, which proposes to use the Earth-Mover or Wasserstein-1 distance W by constructing a value function using the Kantorovich-Rubinstein duality <ref type="bibr" target="#b30">[31]</ref>. In this formulation, W is approximated given a set of k-Lipschitz functions f modeled as neural networks. To ensure f is k-Lipschitz, the weights of the discriminator are clipped to some range [?c, c]. In our work, we adopt the Wasserstein GAN with gradient penalty (WGAN-GP) <ref type="bibr" target="#b9">[10]</ref>, which instead of clipping network weights like in <ref type="bibr" target="#b2">[3]</ref>, ensures the Lipschitz constraint by enforcing a soft constraint on the gradient norm of the discriminator's output with respect to its input. Following <ref type="bibr" target="#b9">[10]</ref>, our new objective then becomes</p><formula xml:id="formula_2">L W GAN (G, D) = E[D(I C )] ? E[D(G(I D ))]+ ? GP Ex ?Px [(||?xD(x)|| 2 ? 1) 2 ]<label>(2)</label></formula><p>where Px is defined as samples along straight lines between pairs of points coming from the true data distribution and the generator distribution, and ? GP is a weighing factor. In order to give G some sense of ground truth, as well as capture low level frequencies in the image, we also consider the L1 loss</p><formula xml:id="formula_3">L L1 = E[||I C ? G(I D )|| 1 ]<label>(3)</label></formula><p>Combining these, we get our final objective function for our network, which we call Underwater GAN (UGAN),</p><formula xml:id="formula_4">L * U GAN = min G max D L W GAN (G, D) + ? 1 L L1 (G)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Gradient Difference Loss</head><p>Often times generative models produce blurry images. We explore a strategy to sharpen these predictions by directly penalizing the differences of image gradient predictions in the generator, as proposed by <ref type="bibr" target="#b21">[22]</ref>. Given a ground truth image I C , predicted image I P = G(I D ), and ? which is an integer greater than or equal to 1, the Gradient Difference Loss (GDL) is given by</p><formula xml:id="formula_5">L GDL (I C , I P ) = i,j ||I C i,j ? I C i?1,j | ? |I P i,j ? I P i?1,j || ? + ||I C i,j?1 ? I C i,j | ? |I P i,j?1 ? I P i,j || ?<label>(5)</label></formula><p>In our experiments, we denote our network as UGAN-P when considering the GDL, which can be expressed as</p><formula xml:id="formula_6">L * U GAN ?P = min G max D L W GAN (G, D)+ ? 1 L L1 (G)+? 2 L GDL<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture</head><p>Our generator network is a fully convolutional encoder-decoder, similar to the work of <ref type="bibr" target="#b13">[14]</ref>, which is designed as a "U-Net" <ref type="bibr" target="#b25">[26]</ref> due to the structural similarity between input and output. Encoder-decoder networks downsample (encode) the input via convolutions to a lower dimensional embedding, in which this embedding is then upsampled (decode) via transpose convolutions to reconstruct an image. The advantage of using a "U-Net" comes from explicitly preserving spatial dependencies produced by the encoder, as opposed to relying on the embedding to contain all of the information. This is done by the addition of "skip connections", which concatenate the activations produced from a convolution layer i in the encoder to the input of a transpose convolution layer n ? i + 1 in the decoder, where n is the total number of layers in the network. Each convolutional layer in our generator uses kernel size 4 ? 4 with stride 2. Convolutions in the encoder portion of the network are followed by batch normalization <ref type="bibr" target="#b11">[12]</ref> and a leaky ReLU activation with slope 0.2, while transpose convolutions in the decoder are followed by a ReLU activation <ref type="bibr" target="#b22">[23]</ref> (no batch norm in the decoder). Exempt from this is the last layer of the decoder, which uses a TanH nonlinearity to match the input distribution of [?1, 1]. Recent work has proposed Instance Normalization <ref type="bibr" target="#b29">[30]</ref> to improve quality in image-toimage translation tasks, however we observed no added benefit. Our fully convolutional discriminator is modeled after that of <ref type="bibr" target="#b24">[25]</ref>, except no batch normalization is used. This is due to the fact that WGAN-GP penalizes the norm of the discrim-inator's gradient with respect to each input individually, which batch normalization would invalidate. The authors of <ref type="bibr" target="#b9">[10]</ref> recommend layer normalization <ref type="bibr" target="#b3">[4]</ref>, but we found no significant improvements. Our discriminator is modeled as a PatchGAN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, which discriminates at the level of image patches. As opposed to a regular discriminator, which outputs a scalar value corresponding to real or fake, our PatchGAN discriminator outputs a 32 ? 32 ? 1 feature matrix, which provides a metric for high level frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We used several subsets of Imagenet <ref type="bibr" target="#b6">[7]</ref> for training and evaluation of our methods. We also evaluate a frequency-and spatial-domain divertracking algorithm on a video of scuba divers taken from YouTube TM 1 . Subsets of Imagenet containing underwater images were selected for the training of CycleGAN, and manually separated into two classes based on visual inspection. We let X be the set of underwater images with no distortion, and Y be the set of underwater images with distortion. X contained 6143 images, and Y contained 1817 images. We then trained CycleGAN to learn the mapping F : X ? Y , such that images from X appeared to have come from Y . Finally, our image pairs for training data were generated by distorting all images in X with F . <ref type="figure" target="#fig_0">Figure 2</ref> shows sample training pairs. When comparing with CycleGAN, we used a test set of 56 images acquired from Flickr TM .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>CycleGAN UGAN UGAN-P </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We train UGAN and UGAN-P on the image pairs generated by CycleGAN, and evaluate on the images from the test set, Y . Note that these images do not contain any ground truth, as they are original distorted images from Imagenet. Images for training and testing are of size 256 ? 256 ? 3 and normalized between [?1, 1]. <ref type="figure" target="#fig_1">Figure 3</ref> shows samples from the test set. Notably, these images contain varying amounts of noise. Both UGAN and UGAN-P are able to recover lost color information, as well as correct any color information this is present. While many of the distorted images contain a blue or green hue over the entire image space, that is not always the case. In certain environments, it is possible that objects close to the camera are undistorted with correct colors, while the background of the image contains distortion. In these cases, we would like the network to only correct parts of the image that appear distorted. The last row in <ref type="figure" target="#fig_1">Figure 3</ref> shows a sample of such an image. The orange of the clownfish is left unchanged while the distorted sea anemone in the background has its color corrected.</p><p>For a quantitative evaluation we compare to CycleGAN, as it inherently learns an inverse mapping during the training of G : Y ? X. We first use the Canny edge detector <ref type="bibr" target="#b5">[6]</ref>, as this provides a color agnostic evaluation of the images in comparison to ground truth. Second, we compare local image patches to provide sharpness metrics on our images. Lastly, we show how an existing tracking algorithm for an underwater robot improves performance with generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to CycleGAN</head><p>It is important to note that during the process of learning a mapping F : X ? Y , CycleGAN also learns a mapping G : Y ? X. Here we give a comparison to our methods. We use the Canny edge detector <ref type="bibr" target="#b5">[6]</ref> to provide a color agnostic evaluation of the images, as the original contain distorted colors and cannot be compared back to as ground truth. Due to the fact that restor-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original CycleGAN</head><p>UGAN UGAN-P <ref type="figure">Figure 5</ref>: Local image patches extracted for quantitative comparisons, shown in <ref type="table" target="#tab_1">Tables 2 and 3</ref>.</p><p>Each patch was resized to 64 ? 64, but shown enlarged for viewing ability.</p><p>ing color information should not alter the overall structure of the image, we measure the distance in the image space between the edges found in the original and generated images. <ref type="figure" target="#fig_2">Figure 4</ref> shows the original images and results from edge detection. <ref type="table" target="#tab_0">Table 1</ref> provides the measurements from <ref type="figure" target="#fig_2">Figure 4</ref>, as well as the average over our entire Flickr TM dataset. Both UGAN and UGAN-P are consistently closer in the image space to the original than that of CycleGAN, suggesting noise due to blur. Next, we evaluate this noise explicitly.</p><p>We explore the artifacts of content loss, as seen in <ref type="figure">Figure 5</ref>. In particular, we compare local statistics of the highlighted image patches, where each image patch is resized to 64 ? 64. We use the GDL <ref type="bibr" target="#b21">[22]</ref> from (5) as a sharpness measure. A lower GDL measure implies a smoother transition between pixels, as a noisy image would have large jumps in the image's gradient, leading to a higher score. As seen in <ref type="table" target="#tab_1">Table 2</ref>, the GDL is lower for both UGAN and UGAN-P. Interestingly, UGAN consistently has a lower score than UGAN-P, despite UGAN-P explicitly accounting for this metric in the objective function. Reasoning for this is left for our future work. Another metric we use to compare image patches are the mean and standard deviation of a patch. The standard deviation gives us a sense of blurriness because it defines how far the data deviates from the mean. In the case of images, this would suggest a blurring effect due to the data being more clustered toward one pixel value. <ref type="table" target="#tab_2">Table 3</ref> shows the mean and standard deviations of the RGB values for the local image patches seen in <ref type="figure">Figure 5</ref>. Despite qualitative evaluation showing our methods are much sharper, quantitatively they show only slight improvement. Other metrics such as entropy are left as future work.  <ref type="figure">6</ref> illustrates the improved performance of MDPM tracker on generated images compared to the real ones. Underwater images often fail to capture the true contrast in intensity values between foreground and background due to low visibility. The generated images seem to restore these eroded intensity variations to some extent, causing much improved positive detection (a 350% increase in correct detections) for the MDPM tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training Details and Inference Performance</head><p>In all of our experiments, we use ? 1 = 100, ? GP = 10, batch size of 32, and the Adam Optimizer <ref type="bibr" target="#b15">[16]</ref> with learning rate 1e ? 4. Following WGAN-GP, the discriminator is updated n times for every update of the generator, where n = 5. For UGAN-P, we set ? 2 = 1.0 and ? = 1. Our implementation was done using the Tensor-flow library <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr" target="#b1">2</ref> All networks were trained from scratch on a GTX 1080 for 100 epochs. Inference on the GPU takes on average 0.0138s, which is about 72 Frames Per Second (FPS). On a CPU (Intel Core i7-5930K), inference takes on average 0.1244s, which is about 8 FPS. In both cases, the input images have dimensions 256 ? 256 ? 3. We find both of these measures acceptable for underwater tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents an approach for enhancing underwater color images through the use of generative adversarial networks. We demonstrate the use of CycleGAN to generate dataset of paired images to provide a training set for the proposed restoration model. Quantitative and qualitative results demonstrate the effectiveness of this method, and using a diver tracking algorithm on corrected images of scuba divers show higher accuracy compared to the uncorrected image sequence. Future work will focus on creating a larger and more diverse dataset from underwater objects, thus making the network more generalizable. Augmenting the data generated by Cycle-GAN with noise such as particle and lighting effects would improve the diversity of the dataset. We also intend to investigate a number of different quantitative performance metrics to evaluate our method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Paired samples of ground truth and distorted images generated by CycleGAN. Top row: Ground truth. Bottom row: Generated samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Samples from our ImageNet testing set. The network can both recover color and also correct color if a small amount is present.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Running the Canny Edge Detector on sample images. Both variants of UGAN contain less noise than CycleGAN, and are closer in the image space to the original. For each pair, the top row is the input image, and bottom row the result of the edge detector. The figure depicts four different sets of images, successively labeled A to D from top to bottom. SeeTable 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Distances in image space</figDesc><table><row><cell cols="4">Row/Method CycleGAN UGAN UGAN-P</cell></row><row><cell>A</cell><cell>116.45</cell><cell>85.71</cell><cell>86.15</cell></row><row><cell>B</cell><cell>114.49</cell><cell>97.92</cell><cell>101.01</cell></row><row><cell>C</cell><cell>120.84</cell><cell>96.53</cell><cell>97.57</cell></row><row><cell>D</cell><cell>129.27</cell><cell>108.90</cell><cell>110.50</cell></row><row><cell>Mean</cell><cell>111.60</cell><cell>94.91</cell><cell>96.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Gradient Difference Loss Metrics</figDesc><table><row><cell cols="4">Method/Patch CycleGAN UGAN UGAN-P</cell></row><row><cell>Red</cell><cell>11.53</cell><cell>9.39</cell><cell>10.93</cell></row><row><cell>Blue</cell><cell>7.52</cell><cell>4.83</cell><cell>5.50</cell></row><row><cell>Green</cell><cell>4.15</cell><cell>3.18</cell><cell>3.25</cell></row><row><cell>Orange</cell><cell>6.72</cell><cell>5.65</cell><cell>5.79</cell></row><row><cell cols="4">4.4 Diver Tracking using Frequency-</cell></row><row><cell cols="2">Domain Detection</cell><cell></cell><cell></cell></row><row><cell cols="4">We investigate the frequency-domain character-</cell></row><row><cell cols="4">istics of the restored images through a case-study</cell></row><row><cell cols="4">of periodic motion tracking in sequence of im-</cell></row><row><cell cols="4">ages. Particularly, we compared the performance</cell></row><row><cell cols="4">of Mixed Domain Periodic Motion (MDPM)-</cell></row><row><cell cols="4">tracker [13] on a sequence of images of a</cell></row><row><cell cols="4">diver swimming in arbitrary directions. MDPM</cell></row><row><cell cols="4">tracker is designed for underwater robots to fol-</cell></row><row><cell cols="4">low scuba divers by tracking distinct frequency-</cell></row><row><cell cols="4">domain signatures (high-amplitude spectra at 1-</cell></row><row><cell cols="4">2Hz) pertaining to human swimming. Ampli-</cell></row><row><cell cols="4">tude spectra in frequency-domain correspond to</cell></row><row><cell cols="4">the periodic intensity variations in image-space</cell></row><row><cell cols="4">over time, which is often eroded in noisy under-</cell></row><row><cell>water images [27].</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean and Standard Deviation Metrics Method/Patch Original CycleGAN UGAN UGAN-P Red 0.43 ? 0.23 0.42 ? 0.22 0.44 ? 0.23 0.45 ? 0.25 Blue 0.51 ? 0.18 0.57 ? 0.17 0.57 ? 0.17 0.57 ? 0.17 Green 0.36 ? 0.17 0.36 ? 0.14 0.37 ? 0.17 0.36 ? 0.17 Orange 0.3 ? 0.15 0.25 ? 0.12 0.26 ? 0.13 0.27 ? 0.14 Performance of MDPM tracker [13] on both real (top row) and generated (second row) images; the Table compares the detection performance for both sets of images over a sequence of 500 frames.</figDesc><table><row><cell></cell><cell cols="4">Correct detection Wrong detection Missed detection Total # of frames</cell></row><row><cell>Real</cell><cell>42</cell><cell>14</cell><cell>444</cell><cell>500</cell></row><row><cell>Generated</cell><cell>147</cell><cell>24</cell><cell>329</cell><cell>500</cell></row><row><cell>Figure 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.youtube.com/watch?v=QmRFmhILd5o</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code is available at https://github.com/ cameronfabbri/Underwater-Color-Correction</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors are grateful to Oliver Hennigh for his implementation of the Gradient Difference Loss measure.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and L?on Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robotic tools for deep water archaeology: Surveying an ancient shipwreck with an autonomous underwater vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanumant</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Camilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Delaporta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Mallios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Sakellariou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="702" to="717" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mixed-domain biological motion tracking for underwater human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahidul</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaed</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4457" to="4464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Underwater 3D reconstruction based on physical models for refraction and underwater light propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Jordt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Univ. Kiel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Watergan: Unsupervised generative network to enable real-time color correction of monocular underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07392</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<idno>ArXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Underwater multi-robot convoying using visual tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Shkurti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Di</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md Jahidul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan Camilo Gamboa</forename><surname>Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Manderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Dudek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaed</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-domain monitoring of marine environments using a heterogeneous robot team</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Shkurti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malika</forename><surname>Meghjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan Camilo Gamboa</forename><surname>Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Giguere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Bir Bikram Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kalmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prahacs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1747" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Color correction of underwater images for aquatic robot inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luz Abril Torres-M?ndez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMM-CVPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="60" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Advances in underwater robot vehicles for deep ocean exploration: Navigation, control, and survey operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Whitcomb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanumant</forename><surname>Dana R Yoerger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
