<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiliang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<email>cwfu@cse.cuhk.edu.hktangwl123@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Self-Ensembling Single-Stage object Detector (SE-SSD) for accurate and efficient 3D object detection in outdoor point clouds. Our key focus is on exploiting both soft and hard targets with our formulated constraints to jointly optimize the model, without introducing extra computation in the inference. Specifically, SE-SSD contains a pair of teacher and student SSDs, in which we design an effective IoU-based matching strategy to filter soft targets from the teacher and formulate a consistency loss to align student predictions with them. Also, to maximize the distilled knowledge for ensembling the teacher, we design a new augmentation scheme to produce shape-aware augmented samples to train the student, aiming to encourage it to infer complete object shapes. Lastly, to better exploit hard targets, we design an ODIoU loss to supervise the student with constraints on the predicted box centers and orientations. Our SE-SSD attains top performance compared with all prior published works. Also, it attains top precisions for car detection in the KITTI benchmark (ranked 1 st and 2 nd on the BEV and 3D leaderboards 1 , respectively) with an ultra-high inference speed. The code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>To support autonomous driving, 3D point clouds from LiDAR sensors are often adopted to detect objects near the vehicle. This is a robust approach, since point clouds are readily available regardless of the weather (fog vs. sunny) and time of the day (day vs. night). Hence, various pointcloud-based 3D detectors have been proposed recently.</p><p>To boost the detection precision, an important factor is the quality of the extracted features. This applies to both single-stage and two-stage detectors. For example, the series of works <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b23">23]</ref> focus on improving the region-proposal-aligned features for a better refinement with a second-stage network. Also, many methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b19">19]</ref> try to extract more discrimina- <ref type="bibr" target="#b0">1</ref> On the date of CVPR deadline, i.e., <ref type="bibr">Nov 16, 2020</ref>   <ref type="figure">Figure 1</ref>. Our SE-SSD attains top precisions on both 3D and BEV car detection in KITTI benchmark <ref type="bibr" target="#b6">[6]</ref> with real-time speed (30.56 ms), clearly outperforming all state-of-the-art detectors. Please refer to <ref type="table" target="#tab_0">Table 1</ref> for a detailed comparison with more methods.</p><p>tive multi-modality features by fusing RGB images and 3D point clouds. For single-stage detectors, Point-GNN <ref type="bibr" target="#b26">[26]</ref> adapts a graph neural network to obtain a more compact representation of point cloud, while TANet <ref type="bibr" target="#b17">[17]</ref> designs a delicate triple attention module to consider the feature-wise relation. Though these approaches give significant insights, the delicate designs are often complex and could slow down the inference, especially for the two-stage detectors.</p><p>To meet the practical need, especially in autonomous driving, 3D object detection demands high efficiency on top of high precision. Hence, another stream of works, e.g., SA-SSD <ref type="bibr" target="#b8">[8]</ref> and Associate-3Ddet <ref type="bibr" target="#b4">[5]</ref>, aim to exploit auxiliary tasks or further constraints to improve the feature representation, without introducing additional computational overhead during the inference. Following this stream of works, we formulate the Self-Ensembling Single-Stage object Detector (SE-SSD) to address the challenging 3D detection task based only on LiDAR point clouds.</p><p>To boost the detection precision, while striving for high efficiency, we design the SE-SSD framework with a pair of teacher SSD and student SSD, as inspired by <ref type="bibr" target="#b27">[27]</ref>. The teacher SSD is ensembled from the student. It produces relatively more precise bounding boxes and confidence, which serve as soft targets to supervise the student. Compared with manually-annotated hard targets (labels), soft targets from the teacher often have higher entropy, thus offering more information <ref type="bibr" target="#b9">[9]</ref> for the student to learn from. Hence, we exploit both soft and hard targets with our formulated constraints to jointly optimize the model, while incurring no extra inference time. To encourage the bounding boxes and confidence predicted by the student to better align with the soft targets, we design an effective IoU-based matching strategy to filter soft targets and pair them with student predictions, and further formulate a consistency loss to reduce the misalignment between them.</p><p>On the other hand, to enable the student SSD to effectively explore a larger data space, we design a new augmentation scheme on top of conventional augmentation strategies to produce augmented object samples in a shape-aware manner. By this scheme, we can encourage the model to infer the complete object shape from incomplete information. It is also a plug-and-play and general module for 3D detectors. Furthermore, hard targets are still essential in the supervised training, as they are the final targets for the model convergence. To better exploit them, we formulate a new orientation-aware distance-IoU (ODIoU) loss to supervise the student with constraints on both the center and orientation of the predicted bounding boxes. Overall, our SE-SSD is trained in a fully-supervised manner to best boost the detection performance, in which all the designed modules are needed only in the training, so there are no extra computation during the inference.</p><p>In summary, our contributions include (i) the Self-Ensembling Single-Stage object Detector (SE-SSD) framework, optimized by our formulated consistency constraint to better align predictions with the soft targets; (ii) a new augmentation scheme to produce shape-aware augmented ground-truth objects; and (iii) an Orientation-aware Distance-IoU (ODIoU) loss to supervise the detector using hard targets. Our SE-SSD attains state-of-the-art performance on both 3D and BEV car detection in the KITTI benchmark <ref type="bibr" target="#b6">[6]</ref> and demonstrates ultra-high inference speed (32 FPS) on commodity CPU-GPU, clearly outperforming all prior published works, as presented in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In general, 3D detectors are categorized into two types: (i) single-stage detectors regress bounding box and confidence directly from features learned from the input, and (ii) two-stage detectors use a second stage to refine the firststage predictions with region-proposal-aligned features. So, two-stage detectors often attain higher precisions benefited from the extra stage, whereas single-stage detectors usually run faster due to simpler network structures. Recent trend (see <ref type="figure">Figure 1</ref> and <ref type="table" target="#tab_0">Table 1</ref>) reveals that the precisions of single-stage detectors <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b31">31]</ref> gradually approach those of the two-stage detectors <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b32">32]</ref>. This motivates us to focus this work on developing a single-stage detector and aim for both high precision and high speed.</p><p>Two-stage Object Detectors Among these two-stage detectors, PointRCNN <ref type="bibr" target="#b24">[24]</ref> uses PointNet <ref type="bibr" target="#b21">[21]</ref> to fuse semantic features and raw points from region proposals for a second-stage refinement. Part-A 2 <ref type="bibr" target="#b25">[25]</ref> exploits a voxelbased network to extract region proposal features to avoid ambiguity and further improve the feature representation. Similarly, STD <ref type="bibr" target="#b32">[32]</ref> converts region-proposal semantic features to a compact representation with voxelization and reduce the number of anchors to improve the performance. PV-RCNN <ref type="bibr" target="#b23">[23]</ref> utilizes both point-based and voxel-based networks to extract features from the voxels and raw points inside the region proposal. 3D-CVF <ref type="bibr" target="#b33">[33]</ref> obtains semantics from multi-view images and fuses them with point features in both stages, whereas CLOCs PVCas <ref type="bibr" target="#b19">[19]</ref> fuses semantic features from images and points to refine the predicted confidence.</p><p>Single-stage Object Detectors VoxelNet <ref type="bibr" target="#b38">[38]</ref> proposes the voxel feature encoding layer to extract features from point clouds. PointPillar <ref type="bibr" target="#b11">[11]</ref> divides a point cloud into pillars for efficient feature learning. SECOND <ref type="bibr" target="#b30">[30]</ref> modifies the sparse convolution <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b15">15]</ref> to efficiently extract features from sparse voxels. TANet <ref type="bibr" target="#b17">[17]</ref> proposes the triple attention module to consider feature-wise relation in the feature extraction. Point-GNN <ref type="bibr" target="#b26">[26]</ref> exploits a graph neural network to learn point features. 3DSSD <ref type="bibr" target="#b31">[31]</ref> combines feature-and point-based sampling to improve the classification. Associate-3Ddet <ref type="bibr" target="#b4">[5]</ref> extracts feature from complete point clouds to supervise the features learned from incomplete point clouds, encouraging the model to infer from incomplete points. SA-SSD <ref type="bibr" target="#b8">[8]</ref> adopts an auxiliary network in parallel with the backbone to regress box centers and semantic classes to enrich the features. CIA-SSD <ref type="bibr" target="#b8">[8]</ref> adopts a lightweight BEV network for extraction of robust spatialsemantic features, combined with an IoU-aware confidence rectification and DI-NMS for better post processing. Inspired by <ref type="bibr" target="#b27">[27]</ref>, SESS <ref type="bibr" target="#b34">[34]</ref> addresses the detection in indoor scenes with a semi-supervised strategy to reduce the dependence on manual annotations. Different from prior works, we aim to exploit both soft and hard targets to refine features in a fully-supervised manner through our novel constraints and augmentation scheme. Also, compared with all prior single-and two-stage detectors, our SE-SSD attains the highest average precisions for both 3D and BEV car detection in the KITTI benchmark <ref type="bibr" target="#b6">[6]</ref> and exhibits very high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orientation-Aware DIoU Loss</head><p>Teacher SSD Student SSD reg cls reg cls Shape-Aware Data Augmentation Global Transformations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IoU-based Matching</head><p>TODO: colors TODO: colors TODO: colors TODO: colors TODO: colors <ref type="figure">Figure 2</ref>. The framework of our Self-Ensembling Single-Stage object Detector (SE-SSD) with a teacher SSD and a student SSD. To start, we feed the input point cloud to the teacher to produce relatively precise bounding boxes and confidence, and take these predictions (after global transformations) as soft targets to supervise the student with our consistency loss (Section 3.2). On the top branch, we apply the same global transformations to the input, then perform our shape-aware data augmentation (Section 3.4) to generate augmented samples as inputs to the student. Further, we formulate the Orientation-aware Distance-IoU loss (Section 3.3) to supervise the student with hard targets, and update the teacher parameters based on the student parameters with the exponential moving average (EMA) strategy. In this way, the framework can boost the precisions of the detector significantly without incurring extra computation during the inference. <ref type="figure">Figure 2</ref> shows the framework of our self-ensembling single-stage object detector (SE-SSD), which has a teacher SSD (left) and a student SSD (right). Different from prior works on outdoor 3D object detection, we simultaneously employ and train two SSDs (of same architecture), such that the student can explore a larger data space via the augmented samples and be better optimized with the associated soft targets predicted by the teacher. To train the whole SE-SSD, we first initialize both the teacher and student with a pre-trained SSD model. Then, started from an input point cloud, our framework has two processing paths:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-Ensembling Single Stage Detector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Framework</head><p>? In the first path (blue arrows in <ref type="figure">Figure 2</ref>), the teacher produces relatively precise predictions from the raw input point cloud. Then, we apply a set of global transformations on the prediction results and take them as soft targets to supervise the student SSD.</p><p>? In the second path (green arrows in <ref type="figure">Figure 2</ref>), we perturb the same input by the same global transformations as in the first path plus our shape-aware data augmentation (Section 3.4). Then, we feed the augmented input to the student, and train it with (i) our consistency loss (Section 3.2) to align the student predictions with the soft targets; and (ii) when we augment the input, we bring along its hard targets ( <ref type="figure">Figure 2</ref> (top right)) to supervise the student with our orientationaware distance-IoU loss (Section 3.3).</p><p>In the training, we iteratively update the two SSD models: optimize the student with the above two losses and update the teacher using only the student parameters by a standard exponential moving average (EMA). Thus, the teacher can obtain distilled knowledge from student and produce soft targets to supervise student. So, we call the final trained student a self-ensembling single-stage object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of Teacher &amp; Student SSD</head><p>The model has the same structure as <ref type="bibr" target="#b35">[35]</ref> to efficiently encode point clouds, but we remove the Confidence Function and DI-NMS. It has a sparse convolution network (SPConvNet), a BEV convolution network (BEVConvNet), and a multi-task head (MT-Head). BEV means bird's eye view. After point cloud voxelization, we find mean 3D coordinates and point density per voxel as the initial feature, then extract features using SPConvNet, which has four blocks ({2, 2, 3, 3} submanifold sparse convolution <ref type="bibr" target="#b7">[7]</ref> layers) with a sparse convolution <ref type="bibr" target="#b15">[15]</ref> layer at the end. Next, we concatenate the sparse 3D feature along z into 2D dense ones for feature extraction with the BEVConvNet. Lastly, we use MTHead to regress bounding boxes and perform classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Consistency Loss</head><p>In 3D object detection, the patterns of point clouds in pre-defined anchors may vary significantly due to distances and different forms of object occlusion. Hence, samples of the same hard target may have very different point patterns and features. In contrast, soft targets can be more informative per training sample, and helps to reveal the difference between data samples of the same class <ref type="bibr" target="#b9">[9]</ref>. This motivates us to treat the relatively more precise teacher predictions as soft targets and employ them to jointly optimize the student with hard targets. Accordingly, we formulate a consistency loss to optimize the student network with soft targets.</p><p>We first design an effective IoU-based matching strategy before calculating the consistency loss, aiming to pair the non-axis-aligned teacher and student boxes predicted from the very sparse outdoor point clouds. To obtain high-quality soft targets from the teacher, we first filter out those predicted bounding boxes (for both teacher and student) with confidence less than threshold ? c , which helps reduce the calculation of the consistency loss. Next, we calculate the IoU between every pair of remaining student and teacher bounding boxes, and filter out the pairs with IoUs less than threshold ? I , thus avoiding to mislead the student with unrelated soft targets; We denote N and N as the initial and final number of box pairs, respectively. Thus, we keep only the highly-overlapping student-teacher pairs. Lastly, for each student box, we pair it with the teacher bounding box that has the largest IoU with it, aiming to increase the confidence of the soft targets. Compared with hard targets, the filtered soft targets are often closer to the student predictions, as they are predicted based on similar features. So, soft targets can better guide the student to fine-tune the predictions and reduce the gradient variance for better training.</p><p>Different from the IoU loss, Smooth-L 1 loss <ref type="bibr" target="#b16">[16]</ref> can evenly treat all dimensions in the predictions, without biasing toward any specific one, so the features corresponding to different dimensions can also be evenly optimized. Hence, we adopt it to formulate our consistency loss for bounding boxes (L c box ) to minimize the misalignment errors between each pair of teacher and student bounding boxes:</p><formula xml:id="formula_0">L c box = 1 N N i=1 1(IoU i &gt; ? I ) e 1 7 L c ?e and ? e = |e s ? e t | if e ? {x, y, z, w, l, h} |sin(e s ? e t )| if e ? {r}<label>(1)</label></formula><p>where {x, y, z}, {w, l, h}, and r denote the center position, sizes, and orientation of a bounding box, respectively, predicted by teacher (subscript t) or student (subscript s), L c ?e denotes the Smooth-L 1 loss of ? e , and IoU i denotes the largest IoU of the i-th student bounding box with all teacher bounding boxes. Next, we formulate the consistency loss for classification score (L c cls ) to minimize the difference in predicted confidence of student and teacher:</p><formula xml:id="formula_1">L c cls = 1 N N i=1 1(IoU i &gt; ? I )L c ?c and ? c = |?(c s ) ? ?(c t )|<label>(2)</label></formula><p>where L c ?c denotes the Smooth-L 1 loss of ? c , and ?(c s ) and ?(c t ) denote the sigmoid classification scores of student and teacher, respectively. Here, we adopt the sigmoid function to normalize the two predicted confidences, such that the deviation between the normalized values can be kept inside a small range. Combining Eqs <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>, we can obtain the overall consistency loss as</p><formula xml:id="formula_2">L cons = L c cls + L c box<label>(3)</label></formula><p>where we empirically set the same weight for both terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Orientation-Aware Distance-IoU Loss</head><p>In supervised training with hard targets, Smooth-L 1 loss <ref type="bibr" target="#b16">[16]</ref> is often adopted to constrain the bounding box regression. However, due to long distances and occlusion in outdoor scenes, it is hard to acquire sufficient information from the sparse points to precisely predict all dimensions of the bounding boxes. To better exploit hard targets for regressing bounding boxes, we design the Orientation-aware Distance-IoU loss (ODIoU) to focus more attention on the alignment of box centers and orientations between the predicted and ground-truth bounding boxes; see <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Inspired by <ref type="bibr" target="#b36">[36]</ref>, we impose a constraint on the distance between the 3D centers of the predicted and ground-truth bounding boxes to minimize the center misalignment. More importantly, we design a novel orientation constraint on the predicted BEV angle, aiming to further minimize the orientation difference between the predicted and ground-truth boxes. In 3D object detection, such a constraint is significant for the precise alignment between the non-axis-aligned boxes in the bird's eye view (BEV). Also, we empirically <ref type="figure">Figure 4</ref>. We formulate the orientation constraint (1 ? |cos( r)|) in the ODIoU loss to precisely minimize the orientation difference between bounding boxes; note the gradient of the term.</p><p>find that this constraint is an important means to further boost the detection precision. Compared with Smooth-L 1 loss, our ODIoU loss enhances the alignment of box centers and orientations, which are easy to infer from the points distributed on the object surface, thus leading to a better performance. Overall, our ODIoU loss is formulated as</p><formula xml:id="formula_3">L s box = 1 ? IoU (B p , B g ) + c 2 d 2 + ?(1 ? |cos( r)|) (4)</formula><p>where B p and B g denote the predicted and ground-truth bounding boxes, respectively, c denotes the distance between the 3D centers of the two bounding boxes (see <ref type="figure" target="#fig_1">Figure 3</ref>), d denotes the diagonal length |AC| of the minimum cuboid that encloses both bounding boxes; r denotes the BEV orientation difference between B p and B g ; and ? is a hyper-parameter weight.</p><formula xml:id="formula_4">|O 1 O 2 | in</formula><p>In our ODIoU loss formula, (1 ? |cos( r)|) is an important term we designed specifically to encourage the predicted bounding box to rotate to the nearest direction that is parallel to the ground-truth orientation. When r equals ? 2 or ? ? 2 , i.e., the orientations of the two boxes are perpendicular to each other, so the term attains its maxima. When r equals 0, ?, or ??, the term attains its minima, which is zero. As shown in <ref type="figure">Figure 4</ref>, we can further look at the gradient of (1 ? |cos( r)|). When the training process minimizes the term, its gradient will help to bring r to 0, ?, or ??, which is the nearest location to minimize the loss. It is because the gradient magnitude is positively correlated to the angle difference, thus promoting fast convergence and smooth fine-tuning in different training stages.</p><p>Besides, we use the Focal loss <ref type="bibr" target="#b14">[14]</ref> and cross-entropy loss for the bounding box classification (L s cls ) and direction classification (L s dir ), respectively. Hence, the overall loss to train the student SSD is</p><formula xml:id="formula_5">L student = L s cls + ? 1 L s box + ? 2 L s dir + ? t (L c cls + L c box ) (5)</formula><p>where L s box is the ODIoU loss for regressing the boxes, and the loss weights ? 1 , ? 2 , and ? t are hyperparameters. Futher, our SSD can be pre-trained with the same settings as SE-SSD but without the consistency loss and teacher SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Shape-Aware Data Augmentation</head><p>Data augmentation is important to improve a model's generalizability. To enable the student SSD to explore a dropout swap sparsify Input Object Another Input Object <ref type="figure">Figure 5</ref>. Illustrating how the shape-aware data augmentation scheme works on a ground-truth object. We divide the object's point cloud into six pyramidal subsets (one for each face of the object's bounding box), and then independently augment each subset by random dropout, swap, and/or sparsify operations. larger data space, we design a new shape-aware data augmentation scheme to boost the performance of our detector. Our insight comes from the observation that the point cloud patterns of ground-truth objects could vary significantly due to occlusions, changes in distance, and diversity of object shapes in practice. So, we design the shape-aware data augmentation scheme to mimic how point clouds are affected by these factors when augmenting the data samples.</p><p>By design, our shape-aware data augmentation scheme is a plug-and-play module. To start, for each object in a point cloud, we find its ground-truth bounding box centroid and connect the centroid with the box faces to form pyramidal volumes that divide the object points into six subsets. Observing that LiDAR points are distributed mainly on object surfaces, the division is like an object disassembly, and our augmentation scheme efficiently augments each object's point cloud by manipulating these divided point subsets like disassembled parts.</p><p>In details, our scheme performs the following three operations with randomized probabilities p 1 , p 2 , and p 3 , respectively: (i) random dropout removes all points (blue) in a randomly-chosen pyramid ( <ref type="figure">Figure 5 (top-left)</ref>), mimicking a partial object occlusion to help the network to infer a complete shape from the remained points. (ii) random swap randomly selects another input object in the current scene and swap a point subset (green) with the point subset (yellow) in the same pyramid of the other input object ( <ref type="figure">Figure 5</ref> (middle)), thus increasing the diversity of object samples by exploiting the surface similarity across objects. (iii) random sparsifying subsamples points in a randomly-chosen pyramid using farthest point sampling <ref type="bibr" target="#b22">[22]</ref>, mimicking the sparsity variation of points due to changes in distance from LiDAR camera; see the sparsified points (red) in <ref type="figure">Figure 5</ref>.</p><p>Furthermore, before the shape-aware augmentation, we perform a set of global transformations on the input point cloud, including a random translation, flipping, and scaling; see "global transformations" in <ref type="figure">Figure 2</ref>. <ref type="figure">Figure 6</ref>. Snapshots of our 3D and BEV detection results on the KITTI validation set. We render the predicted and ground-truth bounding boxes in green and red, respectively, and project them back onto the color images for visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our SE-SSD on the KITTI 3D and BEV object detection benchmark <ref type="bibr" target="#b6">[6]</ref>. This widely-used dataset contains 7,481 training samples and 7,518 test samples. Following the common protocol, we further divide the training samples into a training set (3,712 samples) and a validation set (3,769 samples). Our experiments are mainly conducted on the most commonly-used car category and evaluated by the average precision with an IoU threshold 0.7. Also, the benchmark has three difficulty levels in the evaluation: easy, moderate, and hard, based on the object size, occlusion, and truncation levels, in which the moderate average precision is the official ranking metric for both 3D and BEV detection on the KITTI website. We will release our code on GitHub upon the publication of this work. <ref type="figure">Figure 6</ref> shows 3D bounding boxes (2nd &amp; 5th rows) and BEV bounding boxes (3rd &amp; 6th rows) predicted by our SE-SSD model for six different inputs, demonstrating its highquality prediction results. Also, for a better visualization of the results, we project and overlay the 3D predictions onto the corresponding images (1st &amp; 4th rows). Please refer to the supplemental material for more experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Data preprocessing We only use LiDAR point clouds as input and voxelize all points in ranges [0, 70.4], <ref type="bibr">[-40, 40]</ref>, and [-3, 1] meters into a grid of resolution [0.05, 0.05, 0.1] along x, y, and z, respectively. We empirically set hyperparameters p 1 =0.25, p 2 =0.05, and p 3 =0.1 (see Section 3.4). Besides shape-aware data augmentation, we adopt three types of common data augmentation: (i) mix-up <ref type="bibr" target="#b30">[30]</ref>, which randomly samples ground-truth objects from other scenes and add them into the current scene; (ii) local augmentation on points of individual ground-truth object, e.g., random rotation and translation; and (iii) global augmentation on the whole scene, including random rotation, translation, and flipping. The former two are for preprocessing the inputs to both teacher and student SSDs.</p><p>Training details We adopt the ADAM optimizer and cosine annealing learning rate <ref type="bibr" target="#b18">[18]</ref> with a batch size of four for 60 epochs. We follow <ref type="bibr" target="#b27">[27]</ref> to ramp up ? t (Eq. (5)) from 0 to 1 in the first 15 epoches using a sigmoid-shaped function e ?5(1?x) <ref type="bibr" target="#b1">2</ref> . We set ? c and ? I (Section 3.2) as 0.3 and 0.7, respectively, ? 1 and ? 2 (Eq. (5)) as 2.0 and 0.2, respectively, the EMA decay weight as 0.999, and ? (Eq. (4)) as 1.25.  <ref type="table">Table 2</ref>. Comparison with latest best two single-and two-stage detectors on KITTI val split for car detection, in which "R40" and "R11" mean 40 and 11 sampling recall points for AP, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Arts</head><p>By submitting our prediction results to the KITTI server for evaluation, we obtain the 3D and BEV average precisions of our model on the KITTI test set and compare them with the state-of-the-art methods listed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>As shown in the table, our model ranks the 1 st place among all state-of-the-art methods for both 3D and BEV detections in all three difficulty levels. Also, the inference speed of our model ranks the 2 nd place among all methods, about 2.6 times faster than the latest best two-stage detector Deformable PV-RCNN <ref type="bibr" target="#b0">[1]</ref>. In 3D detection, our one-stage model attains a significant improvement of 1.1 points on moderate AP compared with PV-RCNN <ref type="bibr" target="#b0">[1]</ref> and Deformable PV-RCNN <ref type="bibr" target="#b23">[23]</ref>. For single-stage detectors, our model also outperforms all prior works by a large margin, outperforming the previous one-stage detector SA-SSD <ref type="bibr" target="#b8">[8]</ref> by an average of 2.8 points for all three difficulty levels and with shorter inference time (reduced by ?25%). Our large improvement in APs comes mainly from a better model optimization by exploiting both soft and hard targets, and the high efficiency of our model is mainly due to the nature of our proposed methods, i.e., we refine features in SSD without incurring extra computation in the inference.</p><p>In BEV detection, our model also leads the best singleand two-stage detectors by around 0.8 points on average. Besides, we calculate the mean average precision (mAP) of three difficulty levels for comparison. Our higher mAPs indicate that SE-SSD attains a more balanced performance compared with others, so our method is more promising to address various cases more consistently in practice. Further, we compare our SE-SSD with latest best two single-and two-stage methods on KITTI val split. As shown in <ref type="table">Table 2</ref>, our 3D and BEV moderate APs with 11 or 40 recall points both outperform these prior methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Next, we present ablation studies to analyze the effectiveness of our proposed modules in SE-SSD on KITTI val split. <ref type="table">Table 3</ref> summarizes the ablation results on our consistency loss ("Cons loss"), ODIoU loss ("ODIoU"), and shape-aware data augmentation ("SA-DA"). For ODIoU loss, we replace it with the Smooth-L 1 loss in this ablation  <ref type="table">Table 3</ref>. Ablation study on our designed modules. We report the 3D average precisions of 40 sampling recall points on KITTI val. split for car detection. Cons Loss and SA-DA denote our consistency loss and shape-aware data augmentation, respectively. study, since we cannot simply remove it like Cons loss and SA-DA. All reported APs are with 40 recall points.</p><p>Effect of consistency loss As first and fourth rows in <ref type="table">Table 3</ref> show, our consistency loss boosts the moderate AP by about 0.9 point. This large improvement shows that using the more informative soft targets can contribute to a better model optimization. For the slight increase in easy AP, we think that the predictions of the baseline on the easy subset are already very precise and thus are very close to the hard targets already. Importantly, by combining hard labels with the ODIoU loss in the optimization, our SE-SSD further boosts the moderate and hard APs by about 2.6 points, as shown in the fifth row in <ref type="table">Table 3</ref>. This demonstrates the effectiveness of jointly optimizing the model by leveraging both hard and soft targets with our designed constraints.</p><p>Further, we analyze the effect of the consistency loss for bounding boxes ("reg") and confidence ("cls") separately to show the effectiveness of the loss on both terms. As <ref type="table">Table 4</ref> shows, the gain in AP from the confidence term is larger, we argue that the confidence optimization may be more effective to alleviate the misalignment between the localization accuracy and classification confidence. Also, we evaluate the box and confidence constraints <ref type="bibr" target="#b34">[34]</ref> designed on the box-centers distance matching strategy and obtain a much lower AP ("dist"), we think that the underlying reason is related to their design, which is to address axis-aligned boxes and so is not suitable for our task.</p><p>Effect of ODIoU loss As first and third rows in <ref type="table">Table 3</ref> show, our ODIoU loss improves the moderate AP by about 0.6 points compared with the Smooth-L 1 loss. This gain in AP is larger than the one with the SA-DA module, thus showing the effectiveness of the constraints on both the boxcenters distance and orientation difference in the ODIoU loss. Also, the gain in AP on the hard subset is larger than others, which is consistent with our expectation that even sparse points on the object surface could provide sufficient information to regress the box centers and orientations.</p><p>Effect of shape-aware data augmentation In <ref type="table">Table 3</ref>, the first two rows indicate that our shape-aware data augmentation (SA-DA) scheme brings an average improvement of about 0.5 points on the baseline model. Based on the pre-trained SSD, SA-DA further improves the moderate and  <ref type="table">Table 4</ref>. Ablation study on our consistency loss, in which "cls" and "reg" mean our consistency loss on confidence and bounding boxes, respectively, and "dist" means the box and confidence constraints based on a box-centers distance matching strategy.  <ref type="table">Table 5</ref>. Ablation study on our IoU-based matching strategy, in which "nms", "gt", and "stu" mean that we filter soft targets with NMS, ground truths, and student predictions, respectively.</p><p>hard APs of SE-SSD by about 0.3 points, as indicated in the last two rows in <ref type="table">Table 3</ref>. These gains in AP show the effectiveness of our SA-DA on boosting the performance by enhancing the object diversity and model generalizability.</p><p>IoU-Based Matching Strategy Also, we compare different ways of filtering soft targets, i.e., removing soft targets that (i) overlap with each other using NMS ("nms filter"), (ii) do not overlap with any ground truth ("gt filter"), and (iii) do not overlap with student boxes for less than an IoU threshold ("stu filter"). We can see from <ref type="table">Table 5</ref> that our proposed "stu filter" attains the largest gain in AP, as it keeps the most related and informative soft targets for the student predictions, compared with other strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Runtime Analysis</head><p>The overall inference time of SE-SSD is only 30.56ms, including 2.84ms for data preprocessing, 24.33ms for network forwarding, and 3.39ms for post-processing and producing the final predictions. All evaluations were done on an Intel Xeon Silver CPU and a single TITAN Xp GPU. Our method attains a faster inference speed compared with SA-SSD <ref type="bibr" target="#b8">[8]</ref>, as our BEVConvNet structure is simpler and we further optimized our voxelization code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a novel self-ensembling single-stage object detector for outdoor 3D point clouds. The main contributions include the SE-SSD framework optimized by our formulated consistency constraint with soft targets, the ODIoU loss to supervise the network with hard targets, and the shape-aware data augmentation scheme to enlarge the diversity of training samples. The series of experiments demonstrate the effectiveness of SE-SSD and each proposed module, and show the advantage of high efficiency. Overall, our SE-SSD outperforms all state-of-the-art methods for both 3D and BEV car detection in the KITTI benchmark and attains an ultra-high inference speed.</p><p>This supplementary material contains the following three sections. Section A shows screenshots of the KITTI 3D and BEV leaderboard taken on the date of CVPR submission, showing the rank and time performance of our SE-SSD. Section B presents further ablation studies to analyze our ODIoU loss and shape-aware data augmentation on KITTI car dataset. Section C shows the 3D and BEV detection results of our baseline SSD and SE-SSD on the KITTI cyclist and pedestrian benchmarks. All our results on the KITTI val split are averaged from multiple runs and evaluated with the average precision of 40 sampling recall points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. KITTI Car Detection Leaderboards</head><p>As shown in <ref type="figure" target="#fig_2">Figures 7 and 8</ref>, our SE-SSD ranks 1st and 2nd on the KITTI BEV and 3D leaderboards of car detection 2 , respectively, comparing with not only prior published works but also unpublished works submitted to the leaderboard. Also, our SE-SSD runs the fastest among the top submissions and achieved a balanced performance for the three difficulty levels, especially in 3D detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Ablation Studies</head><p>Shape-aware data augmentation We analyze the effect of random dropout, swap, and sparsifying in our shape-aware <ref type="bibr" target="#b1">2</ref> On the date of CVPR deadline, i.e., <ref type="bibr">Nov 16, 2020</ref>   <ref type="table">Table 9</ref>. Comparison of 3D and BEV APs between our baseline SSD and SE-SSD on KITTI val split for "pedestrian" detection.</p><p>data augmentation on KITTI val split for car detection, respectively. As <ref type="table">Table 6</ref> shows, all these operators (random dropout, swap, and sparsifying) boost the 3D moderate AP effectively, thus demonstrating the effectiveness of our proposed augmentation operators to enrich the object diversity.</p><p>ODIoU Loss Next, we try different values of ? in the ODIoU loss on KITTI val split for car detection. As <ref type="table">Table 7</ref> shows, the orientation constraint is an important factor to further boost the precision, so we finally set ? as 1.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on KITTI Cyclist&amp;Pedestrian</head><p>To validate the effectiveness of our SE-SSD framework, we further conduct experiments on the Cyclist and Pedestrian datasets in KITTI benchmark. In <ref type="table" target="#tab_4">Tables 8 and 9</ref>, we compare the 3D and BEV average precisions between the baseline SSD and our SE-SSD on KITTI val split.</p><p>Cyclist &amp; Pedestrian Results As <ref type="table" target="#tab_4">Table 8</ref> shows, our SE-SSD outperforms the baseline SSD by a large margin for both 3D and BEV cyclist detection, especially on the 3D moderate and hard subsets with an improvement of about 15 points. As <ref type="table">Table 9</ref> shows, our SE-SSD also outperforms the baseline SSD on both the 3D and BEV pedestrian detection by a large margin. These large improvements clearly show the effectiveness of our proposed SE-SSD framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustrating axis-aligned bounding boxes (left) and nonaxis-aligned bounding boxes (right) in Bird's Eye View (BEV), where the red and green boxes represent the ground truth and predicted box, respectively. To handle non-axis-aligned bounding boxes (right), our ODIoU loss impose constraints on both the box center distance |O1O2| and the orientation difference r in BEV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>KITTI BEV (Bird's Eye View) car detection leaderboard, in which our SE-SSD ranks the 1st place.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>KITTI 3D car detection leaderboard, in which our SE-SSD ranks the 2nd place (HRI-ADLab-HZ is unpublished).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>LiDAR+RGB 82.19 69.79 60.59 70.86 91.17 84.67 74.77 83.54 170 AVOD [10] IROS 2018 LiDAR+RGB 83.07 71.76 65.73 73.52 89.75 84.95 78.32 84.34 100 PointRCNN [24] CVPR 2019 LiDAR 86.96 75.64 70.70 77.77 92.13 87.39 82.72 87.Comparison with the state-of-the-art methods on the KITTI test set for car detection, with 3D and BEV average precisions of 40 sampling recall points evaluated on the KITTI server. Our SE-SSD attains the highest precisions for all difficulty levels with a very fast inference speed, outperforming all prior detectors. "*" means the runtime is cited from the submission on the KITTI website. RCNN [23] 92.57 84.83 82.69 95.76 91.11 88.93 83.90 SE-SSE (ours) 93.19 86.12 83.31 96.59 92.28 89.72 85.71</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reference</cell><cell></cell><cell>Modality</cell><cell>Easy</cell><cell>Mod</cell><cell>3D</cell><cell>Hard</cell><cell>mAP</cell><cell>Easy</cell><cell>BEV Mod Hard</cell><cell>mAP</cell><cell>Time (ms)</cell></row><row><cell></cell><cell cols="2">MV3D [3]</cell><cell></cell><cell cols="2">CVPR 2017</cell><cell cols="6">LiDAR+RGB 74.97 63.63 54.00</cell><cell>64.2</cell><cell>86.62 78.93 69.80 78.45</cell><cell>360</cell></row><row><cell></cell><cell cols="2">F-PointNet [20]</cell><cell></cell><cell cols="2">CVPR 2018</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41</cell><cell>100</cell></row><row><cell></cell><cell cols="2">F-ConvNet [28]</cell><cell></cell><cell cols="2">IROS 2019</cell><cell cols="6">LiDAR+RGB 87.36 76.39 66.69 76.81 91.51 85.84 76.11 84.49</cell><cell>470*</cell></row><row><cell>Two-stage</cell><cell cols="3">3D IoU Loss [37] Fast PointRCNN [4] UberATG-MMF [12] Part-A 2 [25]</cell><cell cols="2">3DV 2019 ICCV 2019 CVPR 2019 TPAMI 2020</cell><cell cols="6">LiDAR LiDAR LiDAR+RGB 88.40 77.43 70.22 78.68 93.67 88.21 81.99 87.96 86.16 76.50 71.39 78.02 91.36 86.22 81.20 86.26 85.29 77.40 70.24 77.64 90.87 87.84 80.52 86.41 LiDAR 87.81 78.49 73.51 79.94 91.70 87.79 84.61 88.03</cell><cell>80* 65 80 80</cell></row><row><cell></cell><cell cols="2">STD [32]</cell><cell></cell><cell cols="2">ICCV 2019</cell><cell></cell><cell>LiDAR</cell><cell cols="4">87.95 79.71 75.09 80.92 94.74 89.19 86.42 90.12</cell><cell>80</cell></row><row><cell></cell><cell cols="2">3D-CVF [33]</cell><cell></cell><cell cols="2">ECCV 2020</cell><cell cols="6">LiDAR+RGB 89.20 80.05 73.11 80.79 93.52 89.56 82.45 88.51</cell><cell>75</cell></row><row><cell></cell><cell cols="3">CLOCs PVCas [19]</cell><cell cols="2">IROS 2020</cell><cell cols="6">LiDAR+RGB 88.94 80.67 77.15 82.25 93.05 89.80 86.57 89.81</cell><cell>100*</cell></row><row><cell></cell><cell cols="2">PV-RCNN [23]</cell><cell></cell><cell cols="2">CVPR 2020</cell><cell></cell><cell>LiDAR</cell><cell cols="4">90.25 81.43 76.82 82.83 94.98 90.65 86.14 90.59</cell><cell>80*</cell></row><row><cell></cell><cell cols="2">De-PV-RCNN [1]</cell><cell cols="3">ECCVW 2020</cell><cell></cell><cell>LiDAR</cell><cell cols="4">88.25 81.46 76.96 82.22 92.42 90.13 85.93 89.49</cell><cell>80*</cell></row><row><cell></cell><cell cols="2">VoxelNet [38]</cell><cell></cell><cell cols="2">CVPR 2018</cell><cell></cell><cell>LiDAR</cell><cell cols="4">77.82 64.17 57.51</cell><cell>66.5</cell><cell>87.95 78.39 71.29 79.21</cell><cell>220</cell></row><row><cell></cell><cell cols="2">ContFuse [13]</cell><cell></cell><cell cols="2">ECCV 2018</cell><cell cols="6">LiDAR+RGB 83.68 68.78 61.67 71.38 94.07 85.35 75.88</cell><cell>85.1</cell><cell>60</cell></row><row><cell></cell><cell cols="2">SECOND [30]</cell><cell></cell><cell cols="2">Sensors 2018</cell><cell></cell><cell>LiDAR</cell><cell cols="4">83.34 72.55 65.82</cell><cell>73.9</cell><cell>89.39 83.77 78.59 83.92</cell><cell>50</cell></row><row><cell></cell><cell cols="2">PointPillars [11]</cell><cell></cell><cell cols="2">CVPR 2019</cell><cell></cell><cell>LiDAR</cell><cell cols="4">82.58 74.31 68.99 75.29 90.07 86.56 82.81 86.48</cell><cell>23.6</cell></row><row><cell>One-stage</cell><cell cols="3">TANet [17] Associate-3Ddet [5] HotSpotNet [2] Point-GNN [26]</cell><cell cols="2">AAAI 2020 CVPR 2020 ECCV 2020 CVPR 2020</cell><cell></cell><cell>LiDAR LiDAR LiDAR LiDAR</cell><cell cols="4">84.39 75.94 68.82 76.38 91.58 86.54 81.19 86.44 85.99 77.40 70.53 77.97 91.40 88.09 82.96 87.48 87.60 78.31 73.34 79.75 94.06 88.09 83.24 88.46 88.33 79.47 72.29 80.03 93.11 89.17 83.90 88.73</cell><cell>34.75 60 40* 643</cell></row><row><cell></cell><cell cols="2">3DSSD [31]</cell><cell></cell><cell cols="2">CVPR 2020</cell><cell></cell><cell>LiDAR</cell><cell cols="4">88.36 79.57 74.55 80.83 92.66 89.02 85.86 89.18</cell><cell>38</cell></row><row><cell></cell><cell cols="2">SA-SSD [8]</cell><cell></cell><cell cols="2">CVPR 2020</cell><cell></cell><cell>LiDAR</cell><cell cols="4">88.75 79.79 74.16 80.90 95.03 91.03 85.96 90.67</cell><cell>40.1</cell></row><row><cell></cell><cell cols="2">CIA-SSD [35]</cell><cell></cell><cell cols="2">AAAI 2021</cell><cell></cell><cell>LiDAR</cell><cell cols="4">89.59 80.28 72.87 80.91 93.74 89.84 82.39 88.66</cell><cell>30.76</cell></row><row><cell></cell><cell cols="2">SE-SSD (ours)</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>LiDAR</cell><cell cols="4">91.49 82.54 77.15 83.73 95.68 91.84 86.72 91.41</cell><cell>30.56</cell></row><row><cell cols="2">Method</cell><cell cols="7">3D R40 Easy Moderate Hard Easy Moderate Hard Moderate BEV R40 3D R11</cell><cell></cell><cell></cell></row><row><cell cols="2">3DSSD [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.45</cell><cell></cell><cell></cell></row><row><cell cols="2">SA-SSD [8]</cell><cell cols="3">92.23 84.30 81.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.91</cell><cell></cell><cell></cell></row><row><cell cols="2">De-PV-RCNN [1]</cell><cell>-</cell><cell>84.71</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.30</cell><cell></cell><cell></cell></row><row><cell>PV-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Type baseline dist reg only cls only cls + reg Moderate AP 83.22 80.38 83.65 83.83 84.15</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Type baseline dropout swap sparsify Full SA-DA Moderate AP 83.22 83.46 83.48 83.43 83.70 Table 6. Ablation study on the operators (random dropout, swap, and sparsifying) in our shape-aware data augmentation (SA-DA). ? 0.25 0.5 0.75 1.0 1.25 1.5 1.75 Moderate AP 83.47 83.65 83.73 83.78 83.85 83.58 83.52Table 7. Ablation study on our ODIoU loss, in which we compare the 3D moderate AP of different settings of ?. Comparison of 3D and BEV APs between our baseline SSD and SE-SSD on KITTI val split for "cyclist" detection.</figDesc><table><row><cell></cell><cell>Cyclist</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>3D</cell><cell cols="2">SSD our SE-SSD 80.07 75.73</cell><cell>55.86 70.43</cell><cell>51.97 66.45</cell></row><row><cell>BEV</cell><cell cols="2">SSD our SE-SSD 91.83 83.71</cell><cell>59.02 72.62</cell><cell>55.05 68.24</cell></row><row><cell></cell><cell>Pedestrian</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>3D</cell><cell cols="2">SSD our SE-SSD 63.27 59.64</cell><cell>52.63 57.32</cell><cell>46.59 50.82</cell></row><row><cell>BEV</cell><cell cols="2">SSD our SE-SSD 67.47 63.53</cell><cell>57.29 61.88</cell><cell>51.36 55.94</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported by the Hong Kong Centre for Logistics Robotics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deformable PV-RCNN: Improving 3D object detection with learned deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prarthana</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08766</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Object as hotspots: An anchor-free 3D object detection approach via firing of hotspots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast point R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Associate-3Ddet</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceptual-to-conceptual association for 3D point cloud object detection</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13329" to="13338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3D object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11873" to="11882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Joint 3D proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Pensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TANet: Robust 3D object detection from point clouds with triple attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruolan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11677" to="11684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CLOCs: Camera-LiDAR object candidates fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayder</forename><surname>Radha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00784</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3D object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PV-RCNN: Pointvoxel feature set abstraction for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PointR-CNN: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From points to parts: 3D object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Point-GNN: Graph neural network for 3D object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frustum ConvNet: Sliding frustums to aggregate local point-wise features for amodal 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PI-RCNN: An efficient multi-sensor 3D object detector with point-based attentive cont-conv fusion module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12460" to="12467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Point-based 3D single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">STD: Sparse-to-dense 3D object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D-CVF: Generating joint camera and LiDAR features using cross-view spatial feature fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Hyeok</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeocheol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Song</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Won</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SESS: Selfensembling semi-supervised 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="11079" to="11087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CIA-SSD: Confident IoU-aware single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distance-IoU loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongguang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12993" to="13000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">IoU loss for 2D/3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
