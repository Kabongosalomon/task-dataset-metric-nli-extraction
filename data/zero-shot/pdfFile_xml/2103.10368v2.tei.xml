<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>G?mez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Meoni</surname></persName>
						</author>
						<title level="a" type="main">MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>multispectral image classifica- tion</term>
					<term>scene classification</term>
					<term>semi-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning techniques are at the center of many tasks in remote sensing. Unfortunately, these methods, especially recent deep learning methods, often require large amounts of labeled data for training. Even though satellites acquire large amounts of data, labeling the data is often tedious, expensive and requires expert knowledge. Hence, improved methods that require fewer labeled samples are needed. We present MSMatch, the first semi-supervised learning approach competitive with supervised methods on scene classification on the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and multispectral images of EuroSAT and perform various ablation studies to identify the critical parts of the model. The trained neural network achieves state-of-the-art results on EuroSAT with an accuracy that is up to 19.76% better than previous methods depending on the number of labeled training examples. With just five labeled examples per class, we reach 94.53% and 95.86% accuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC Merced Land Use dataset, we outperform previous works by up to 5.59% and reach 90.71% with five labeled examples. Our results show that MSMatch is capable of greatly reducing the requirements for labeled data. It translates well to multispectral data and should enable various applications that are currently infeasible due to a lack of labeled data. We provide the source code of MSMatch online to enable easy reproduction and quick adoption.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE last decade has seen a momentous increase in the availability of remote sensing data, thus enhancing the need for efficient image processing and analysis methods using deep learning <ref type="bibr" target="#b0">[1]</ref>. The former is driven by continuously decreasing launch costs, especially for so-called Smallsats (&lt; 500kg). As Wekerle et al. <ref type="bibr" target="#b1">[2]</ref> describe, less than 40 Smallsats were launched per year between 2000 and 2012 but over a hundred in 2013 and almost 200 in 2014. Since then the numbers have been increasing with over 300 launches in 2018 and 2017 <ref type="bibr" target="#b2">[3]</ref>. Many of these are imaging satellites serving either commercial purposes <ref type="bibr" target="#b3">[4]</ref> or related to earth observation programs, such as the European Space Agency's Copernicus program <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. This has led to an increase in the availability of large datasets. Concurrently, image processing and analysis have improved dramatically with the advent of deep learning methods <ref type="bibr" target="#b5">[6]</ref>. As a consequence, there is a large corpus of research describing successful applications of deep learning methods to remote P. <ref type="bibr">G?mez</ref>  sensing data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>. However, training deep neural networks usually requires large amounts of labeled samples, where the expected solution has been manually annotated by experts <ref type="bibr" target="#b10">[11]</ref>. This is in particular tedious for imaging modalities such as radar data or multispectral (MS) imaging data, which is not as easily labeled by humans as, e.g., RGB imaging data. One way to alleviate these issues is the application of socalled semi-supervised learning (SSL) techniques. These aim to train machine learning methods, e.g. neural networks, while providing only a small set of labeled training samples and a typically larger corpus of unlabeled training samples. This has recently garnered a lot of attention in the remote sensing community <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b15">[16]</ref>. In the last two years, the state-of-theart in SSL has advanced significantly 1 to a point, where the proposed methods are virtually competitive with fully supervised approaches <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. The recent advances in SSL approaches bear the promise to save large amounts of time and cost that would be required for manual labeling.</p><p>In this work, we propose MSMatch, a novel approach that builds on recent advances <ref type="bibr" target="#b18">[19]</ref> together with recent neural network architectures (so-called EfficientNets <ref type="bibr" target="#b19">[20]</ref>) to tackle the problem of land scene classification, i.e. correctly identifying land use or land cover of satellite or airborne images. This is an active research problem with a broad range of research focusing on it <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. We compare with previous methods on two datasets, the EuroSAT benchmark dataset <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> collected by the Sentinel-2A satellite and the aerial UC Merced Land Use (UCM) dataset <ref type="bibr" target="#b25">[26]</ref>. The EuroSAT dataset also includes MS data, which is commonly used for tasks related to vegetation mapping <ref type="bibr" target="#b7">[8]</ref>.</p><p>In summary, the main contributions of this work are:</p><p>? First SSL approach that is competitive with supervised methods on scene classification on EuroSAT and UCM ? MSMatch can reach 94.53% and 95.86% accuracy on EuroSAT RGB and MS and 90.71 % on UCM, respectively, with only five labels per class. This greatly reduces the need for labeled data. ? Extending the applicability of recent advances in neural network architectures and SSL approaches to MS data and remote sensing ? Analyses of critical components of the proposed pipeline by performing ablation studies and identification of heterogeneous properties in the classes in EuroSAT and UCM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The need for a large amount of labeled training data is one of the most significant bottlenecks in bringing deep learning approaches to practical applications <ref type="bibr" target="#b8">[9]</ref>. For satellite imaging data this problem is particularly aggravated as satellites have greatly varying sensors and applications, which makes a transfer between a model trained on data from one application or sensor to another challenging <ref type="bibr" target="#b26">[27]</ref>. Hence, SSL is of particular interest in remote sensing. In the following, we describe relevant works that applied SSL to scene classification problems. Further, we point out the works that led to significant improvements in SSL in the broader machine learning community in the last years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semi-supervised learning for scene classification</head><p>There are several datasets that have been established as benchmark datasets for scene classification. Some of the most commonly used ones are EuroSAT, UCM and the Aerial Image Dataset (AID) <ref type="bibr" target="#b27">[28]</ref>. Both, UCM and AID, use aerial imaging data and provide, respectively, 2100 and 10000 images for a classification of 21 and 30 classes. EuroSAT provides 27000 images of 10 classes. Aside from the SSL works mentioned here, there is also a multitude of studies using supervised methods for these datasets (e.g. <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>). In terms of SSL approaches, there have been several interesting approaches: Guo et al. <ref type="bibr" target="#b31">[32]</ref> trained a generative adversarial network (GAN) that performs particularly well with few labels on UCM and on EuroSAT. They achieved between 57% and 90% accuracy (4.76 to 80 labels per class) on UCM and between 77% to 94% on EuroSAT (10 to 216 labels per class). Han et al. <ref type="bibr" target="#b32">[33]</ref> used self-labeling to achieve even better results on UCM reaching 91% to 95% using comparatively more labels. They report similarly good results with comparatively large label counts (10% of the whole data) on AID. Dai et al. <ref type="bibr" target="#b33">[34]</ref> used ensemble learning and residual networks with much fewer labels on UCM and on AID reaching 85% on UCM and between 72% and 85% on AID. Similiarly, Gu &amp; Angelov proposed a deep rule-based classifier using just one to ten labels per class and achieving between 57% and 80% on UCM <ref type="bibr" target="#b34">[35]</ref>. A self-supervised learning paradigm was suggested by Tao et al. <ref type="bibr" target="#b15">[16]</ref> for AID and EuroSAT which obtained between 77% to 81% on AID and 76% to 85% on EuroSAT. Another GANbased approach has been suggested in the work of Roy et al. <ref type="bibr" target="#b14">[15]</ref>, who applied it to EuroSAT, but other approaches have already accomplished better results, such as the work by Zhang and Yang <ref type="bibr" target="#b13">[14]</ref>, who utilized the EuroSAT MS data (97% accuracy) -however with 300 labels per class. Yamashkin et al. <ref type="bibr" target="#b35">[36]</ref> also suggested an SSL approach where they extended dataset, but their results are not competitive. Thus, reaching high accuracy over 90% on any of the datasets usually still requires a larger amount of labels (80 per class or more). Low label regimes with e.g. five labels per class typically reach only about 70% to 80% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recent advances in semi-supervised learning</head><p>Semi-supervised learning is a field that has been gaining a lot of attention in recent years. In particular, the last two years have seen several methods being published that led to unprecedented results and, for the first time, achieved results that are competitive with supervised methods trained on significantly more data. For example, the accuracy on the popular CIFAR-10 dataset <ref type="bibr" target="#b36">[37]</ref> for training with just 250 labels has improved from 47% to 95% 2 from 2016 to 2020. Many of these improvements rely on smart data augmentation strategies such as RandAugment <ref type="bibr" target="#b37">[38]</ref> or AutoAugment <ref type="bibr" target="#b38">[39]</ref>. The most significant improvements were made in 2019 in two works describing the so-called MixMatch <ref type="bibr" target="#b16">[17]</ref> method and Unsupervised Data Augmentation <ref type="bibr" target="#b39">[40]</ref>. The former improved the state-of-the-art by over 20% and the latter pushed the accuracy above 90%. In a series of follow-up works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b40">[41]</ref> results were further improved until the current state-ofart method was introduced in 2020. Utilizing the ideas of pseudo-labeling and consistency regularization by Bachman et al. <ref type="bibr" target="#b41">[42]</ref>, FixMatch <ref type="bibr" target="#b18">[19]</ref> achieved state-of-the-art result on four benchmark datasets including almost 95% accuracy on CIFAR-10 with 250 labels. This is comparable to the performance of a supervised approach for the utilized network architecture. Furthermore, using just four labels per class, they still achieved 89% accuracy. To the authors' knowledge, none of the mentioned works have found their way into the remote sensing community yet. Hence, this work aims to build on these recent advances to achieve state-of-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>This section will introduce the network architecture, SSL method and setup of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EfficientNet</head><p>EfficientNets <ref type="bibr" target="#b19">[20]</ref> have become the go-to neural network architecture for many applications. They achieve state-ofart results, especially in terms of efficiency as EfficientNets use comparatively few parameters in relation to achieved performance. The architecture was conceived using neural architecture search <ref type="bibr" target="#b42">[43]</ref>, a method where the neural network architecture itself is optimized. Tan and Le propose several versions of EfficientNets called B0 to B7 with increasing numbers of parameters and performance. Thereby, it is possible to choose a suitable trade-off that keeps memory and computational requirements manageable at a sufficient model complexity. Note, however, that EfficientNets have not seen broad adoption in the remote sensing community, yet. None of the mentioned prior SSL works utilized them. We utilize them for MSMatch given their excellent performance and low memory footprint. We relied on an open-source implementation utilizing PyTorch. <ref type="bibr" target="#b2">3</ref> For MS images the first network layer received all available bands as normalized input channels individually. Thus, the number of network parameters is almost identical for MS and RGB images and performance differences ought to be largely due to contained information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FixMatch</head><p>There are two central ideas behind the effectiveness of the FixMatch approach, pseudo-labeling and consistency regularization <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Pseudo-labeling refers to practice of using the model (or another model) to automatically label otherwise unlabeled data. The second idea, consistency regularization, refers to the concept that the model should predict the same output for similar inputs. FixMatch training consists of a supervised and unsupervised loss. While the supervised loss is a common cross-entropy loss, the unsupervised loss incorporates both pseudo-labeling and consistency regularization. This is achieved by creating two different augmentations of the same image, a so-called weak and a strong one. As depicted in <ref type="figure">Figure 1</ref>, the weakly augmented image is used to create a pseudo-label for the image. Consistency regularization is then employed by computing a cross-entropy loss between a pseudo-label on the weakly augmented image and the model's classification of the strongly augmented image. Thus, the supervised loss is</p><formula xml:id="formula_0">L s = 1 N N i H(y i ,? i ),<label>(1)</label></formula><p>where H is a cross-entropy loss, y i the ground-truth label and y i the network's prediction on the weakly augmented Image. The unsupervised loss is</p><formula xml:id="formula_1">L u = 1 N N i ? i H(? i ,? i )<label>(2)</label></formula><p>where? i is the pseudo-label on an unlabeled, weakly augmented image and ? i is 1 if? i ? 0.95 and 0 otherwise. The total loss from the supervised loss L s and unsupervised loss L u is then obtained as L = L s + L u . For the implementation of FixMatch we adapted an opensource PyTorch implementation 4 . To the authors' knowledge this is also the first work applying FixMatch to MS images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Augmentation</head><p>Data augmentation is frequently used to help neural networks generalize better to unseen data and to increase the richness of the utilized training data <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. During the FixMatch training process the augmentation is critical to ensure that the little amount of available labeled data is exploited optimally using the weak augmentations, and to aid generalization to unseen data using the strong augmentations.</p><p>For the weak augmentation, we only utilized horizontal flips and image translations by up to 12.5%. Note, that Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> describe in their work that, e.g., they tried to harness stronger augmentations for the labeled data but experienced training divergence. We encountered similar issues utilizing, e.g., image crops. For the strong augmentation of the RGB images, several methods from the Python library Pillow were applied. For the strong augmentation of the MS data, a slightly reduced set (due to a lack of implementations for more than three image channels) were applied utilizing the albumentations Python module <ref type="bibr" target="#b43">[44]</ref>. Exemplary weak and strong augmentations of RGB EuroSAT images are depicted in <ref type="figure">Figure 2</ref>. A full overview of the applied augmentations can be seen in <ref type="table" target="#tab_1">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training</head><p>All models were trained for three different random seeds on NVIDIA RTX 2080 TI graphics cards using PyTorch 1.7. The training utilized a stochastic gradient descent optimizer with a Nesterov momentum of 0.9 and different weight decay amounts. A learning rate of 0.03 was used and reduced with cosine annealing. Training batch size was 32 for EuroSAT datasets and 16 for UCM, with one batch containing that many images and additionally four and seven times as many unlabeled ones for UCM and EuroSAT, respectively. The training was run for a total of 500 and 1000 epochs with 1000 iterations each for EuroSAT and UCM, respectively, after which all investigated models had converged. For UCM, the number of epochs was doubled to compensate the smaller batch size. All images were normalized to the mean and standard deviation of the datasets. The train and test sets were stratified. The test sets for each seed contained 10% of the data for EuroSAT (2700 images) and 20% of the data for UCM (420 images). To speed up the training and to reduce the memory footprint, UCM images were downscaled to 224 ? 224. For a supervised baseline the unsupervised loss L u was set to 0 to allow a fair and direct comparison. Overall, with this setup, training of one model requires up to 48 hours on a single GPU for EuroSAT. A single run for UCM requires 131 hours on two GPUs. We provide the code for this work open source online 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>We investigate results on two datasets. We report results for both, the RGB and MS, versions of EuroSAT as well as the UCM dataset. Aside from a detailed comparison with previous research depicted in <ref type="table" target="#tab_1">Table II and Table III</ref>, we also investigated the impact of the weight decay strength in MSMatch as well as the number of parameters of the utilized EfficientNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Some of the most commonly utilized benchmarks for SSL in remote sensing are UCM and EuroSAT. They allow for a detailed comparison to previous works.</p><p>1) EuroSAT: Given the comparatively large images, the computational burden of the heavy image augmentation and training the model is larger for datasets like UCM and AID. Further, they provide fewer and only RGB images. Hence, we intensively tested on the EuroSAT dataset, which consists of 27 000 64 ? 64 pixel images in RGB and 13-band MS format. The MS bands are between 443 nm and 2190 nm, the spatial resolution is up to 10 meters per pixel depending on the band. Note that especially the infrared bands are well-suited to vegetation identification. The data stem from the Sentinel-2A satellite and are split into ten classes, such as river, forest, permanent crop and similar ones. Some examples of images from the EuroSAT dataset can be seen in <ref type="figure">Figure 2</ref>. The data were obtained from the authors' GitHub respository. <ref type="bibr" target="#b5">6</ref> 2) UCM: The UCM dataset is arguably the most established land scene classification dataset. It consists of 2100 images of areas in the USA classified into 21 classes, such as beach, forest or storage tanks. The original images were taken using aerieal orthoimagery and processed into slices of 256 ? 256 pixels. Each class is represented with a 100 images. We display several example images from UCM with the associated labels and predicted classes in <ref type="figure" target="#fig_1">Figure 3</ref>. The data were obtained from the authors' website. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Number of Labels</head><p>The main factor for comparing SSL approaches is the number of labeled training examples used for training. For EuroSAT, we tested a large range of different amounts ranging from 50 (five per class) to 3000 (300 per class) labels to ensure comparability with prior research. For UCM, training datasets with 105, 210, 441 and 1680 labels (respectively 5, 10, 21 and 80 labels per class) were investigated. The rest of the (unlabeled) data are used for the unsupervised part of the training. As seen in <ref type="table" target="#tab_1">Table II and Table III</ref>, MSMatch outperforms all prior research by large margins for all tested amounts of available labels. The greatly enhanced accuracy per labeled training sample is especially prominent for the cases with just 50 and 100 labels (five and ten per class). In these cases MSMatch improves on previous methods between 16% and 20% on EuroSAT and 2.71% and 5.59% on UCM. Using 1000 (100 per class) labels, which was the most popular amount in prior research on EuroSAT, it improves the stateof-the-art by 7%. For EuroSAT, the last three rows in <ref type="table" target="#tab_1">Table II</ref> showcase the impact of using MS data and difference if, instead of using our SSL approach, we train an EfficientNet using only the labeled samples and no unlabeled samples. Note, that results on supervised baseline, i.e. training without any unlabeled data, are clearly worse. This demonstrates the effect of the proposed SSL framework. Even for 3000 labels, the SSL method performs significantly better than the baseline. Notably, the MS data improves results even further. The proposed method is hence successfully adapted to MS data. Due to the high computational demands of training a model UCM, this ablation was not performed on UCM. Additionally, <ref type="figure">Figure 4</ref> and <ref type="figure">Figure  5</ref> show F1 scores for all classes and amount of training labels. Notably, some classes, such as PermanentCrop for EuroSAT or denseresidential for UCM, seem to require more samples to reach optimal results. )6FRUH</p><formula xml:id="formula_2">$QQXDO&amp;URS )RUHVW +HUEDFHRXV9HJHWDWLRQ +LJKZD\ ,QGXVWULDO 3DVWXUH 3HUPDQHQW&amp;URS</formula><p>5HVLGHQWLDO 5LYHU 6HD/DNH &amp;ODVV RIODEHOV SHUFODVV <ref type="figure">Fig. 4</ref>. Classification F1 scores for models trained with a different amount of labeled samples. RGB data was used. Notably, some classes, such as PermanentCrop, seem to require more samples than others, such as SeaLake, to reach good scores. Results are averaged over three seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyperparameters</head><p>Two further factors were investigated in detail. One factor that was found to be particularly critical by <ref type="bibr">Kurakin</ref>   weights in the neural network. They found 5.0 ? 10 ?4 to be optimal in most cases. The other one is model size, which is a decisive factor for model performance and can be varied using the different versions (B0 to B7) of the EfficientNet architecture. Note that only B0 to B3 fit into the available GPU memory with the utilized training settings and, thus, no larger models were compared. These runs were only done on EuroSAT due to the large memory requirements of the larger images in UCM. Further, note that results in <ref type="table" target="#tab_1">Tables II and V</ref> were run on different machines, which led to slightly different random seeds and mean values. Results for different weight decay values are displayed in <ref type="table" target="#tab_1">Table IV</ref>. In our experiments we found that slightly larger weight decay values were beneficial than originally proposed by Kurakin et al. <ref type="bibr" target="#b18">[19]</ref>. The highest accuracy was obtained with a weight decay of 7.5 ? 10 ?4 , which led to an accuracy of 96.63%.</p><p>Detailed results for the model size are given in <ref type="table" target="#tab_6">Table V</ref>.  We found the EfficientNet-B2 with 9.2 million parameters to provide the best trade-off of performance and model size with an accuracy of 96.85%. However, performance gains from a larger number of parameters are limited and smaller than standard deviation among random seeds. Thus, choosing a smaller model when optimizing for efficiency can also be reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Overall, MSMatch outperforms previously published SSL works tested on EuroSAT and UCM. Even with a low number of labels (five per class), it is able to achieve a high accuracy at 94.53% or 95.86% for EuroSAT RGB and MS data and 90.71% for UCM, respectively. This makes the approach applicable in scenarios where only very limited labeled data are available. The superior performance using MS data both highlights the potential of utilizing such data as well as the suitability of the proposed method for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison to FixMatch</head><p>Compared to the original FixMatch approach, we find that slightly higher weight decay values benefit the train-    <ref type="table" target="#tab_6">Table V</ref>) it is noteworthy that improvements from a larger number of parameters are only marginal. Possibly, the reason is that the EuroSAT dataset features just ten classes. The larger models may be overparameterized for the problem. However, it is also conceivable that the proposed training procedure performs better on smaller models. This will require further investigation in the future. Another element that may warrant more detailed examination in the future is the type of augmentations utilized for the strong augmentation. Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> described some performance impact of the utilized augmentation method. Due to the computational demands of running a large number of test runs the authors were unable to investigate this in more detail. However, it would warrant further investigation, especially in regards to the interplay with MS data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Class-dependent Performance</head><p>Another interesting factor is the varying performance depending on the classes. This tendency is observed in both, UCM and EuroSAT and persists across multiple seeds, i.e. random splits of the train and test sets. A detailed overview of the per class performance for EuroSAT can be seen in <ref type="table" target="#tab_1">Table VI</ref>. Clearly, some classes require more labeled training data than others, which hints at a possible improvement to the described method. In particular, the number of labeled samples could be adjusted for each class in relation to the model's performance on it. For example, after observing the worse performance on the classes PermanentCrop and HerbaceousVegation in EuroSAT when training with 50 labels (five per class) in an operational scenario, this insight could be used to selectively label more data from such underperforming classes. Note that <ref type="figure">Figure 7</ref> highlights that the models tend to confuse these two classes with each other, respectively, and especially the recall of the model on these classes is impacted. In practice, the lower performance for these classes might also hint at an underrepresentation of some necessary features in the supplied training data. This is also evident in <ref type="figure">Figure 4</ref> as the problem is remedied when additional labeled examples for these classes are added. Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> also observed a strong impacted of the selected samples on the performance. In <ref type="figure" target="#fig_2">Figure 6</ref> we display some saliency maps (using guided backpropagation <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>), where a model trained on 50 labels (five per class) misclassified the examples whereas a 3000-label model succeeded. Note that the 3000-label model clearly relies on specific contour features in the image that the 50-label model does not recognize.</p><p>For UCM, the tendency of underperforming in some of the classes was also observed when using 105 labeled images (five images per class). Indeed, as shown in <ref type="table" target="#tab_1">Table VII</ref>, the F1score value obtained by averaging an EfficientNet-B2 model over three seeds is higher than 80% for all the classes except for mediumresidential (74.89%), denseresidential (44.52%), and buildings (79,43%). In particular, the confusion matrix in <ref type="figure" target="#fig_3">Figure 8</ref> shows that misclassified denseresidential images are often misidentifed as mediumresidential (28%) and mo-bilehomepark (28%), and as buildings (10%). This may also indicate that these classes do not feature sufficient distinctive properties for the network to pick up. Similarly, 7% and 13% of buildings images are predicted as denseresidential and storagetanks, respectively. This leads to comparatively lower precision on, e.g., storagetanks and mobilehomepark and low recall on denseresidential and buildings. Overall, these results highlight that by using five images per class the trained model is not fully capable to distinguish among mediumresidential, denseresidential, and buildings images, where as it can attain robust performance in predicting the other classes. As shown in in <ref type="figure">Figure 5</ref> a F1-score higher than 80% is reached for the buildings, mediumresidential, and denseresidential classes when training with, respectively, 5, 10, and 21 images per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This works presents MSMatch, a novel SSL approach that is able to vastly improve the state-of-the-art on the EuroSAT and UCM datasets compared to previous works. Depending on the number of labeled training samples, it improves accuracy by between 1.47% and 18.43% on EuroSAT and 2.71% and 7.85% on UCM compared to previous works. More importantly, it showcases that an accuracy of 95.86% and 90.71% on EuroSAT and UCM, respectively, is obtainable with just five labels per class. This bears the promise to make MSMatch applicable to scenarios where a lack of labeled data previously inhibited training neural networks for the task. The method also translates well to MS data, which is, however, harder to process given a lack of GPU-based data augmentation frameworks. Future research will aim to test datasets with even higher resolutions, such as AID, which are computationally more demanding, especially in terms of GPU memory. Adapting MSMatch to a segmentation problem is also conceivable given suitable augmentation methods and might be of interest to broaden the range of possible applications even further.</p><p>Gabriele Meoni , PhD, received the Laurea degree in electronic engineering from the University of Pisa in 2016 and the Ph.D. degree in information engineering in 2020. During his Ph.D., he developed skills in digital and embedded systems design, digital signal processing, and artificial intelligence. Since 2020 he is a research fellow in the ESA Advanced Concepts Team. His research interests include machine learning, embedded systems and edge computing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Overview of the processing pipeline of MSMatch. Examples of weak and strong augmentation applied to different EuroSAT images. In this picture, strong augmentation effects applied to the class River are AutoContrast, Color, and Solarize; the class Permanent Crop is augmented through Sharpness, ShearX,and Equalize; finally, the class Industrial is augmented through Posterize, Rotate, and Brightness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of UCM images with ground-truth labels (GT) and predicted classes (PR). Prediction was performed with an EfficientNet-B2 model trained with five images per class on one seed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Guided backpropagation saliency maps for a model trained on 50 and 3000 labels, respectively. RGB data was used. Images are examples where the 50-label model misclassified the images while the 3000-label model succeeded. The saliency map clearly displays the features that the 3000-label model utilized, which were missed by the 50-label one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Average confusion matrix over three seeds for the UCM dataset obtained through an EfficientNet-B2 model trained with five labeled images per class. Cells containing 0s are not shown. Results are expressed as percentage of the support. Due to rounding numbers may not add up to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Gabriele Meoni are with the Advanced Concepts Team, European Space Agency, Noordwijk, The Netherlands e-mail: pablo.gomez@esa.int, gabriele.meoni@esa.int Manuscript received ...; revised ....</figDesc><table /><note>* Equal contribution</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I OVERVIEW</head><label>I</label><figDesc>OF APPLIED STRONG AUGMENTATIONS. RGB IMAGES WERE AUGMENTED USING THE PYTHON LIBRARY Pillow, MS IMAGES WITH THE PYTHON LIBRARY albumentations. DETAILED PARAMETER OVERVIEWS ARE GIVEN IN THE LIBRARIES' RESPECTIVE DOCUMENTATION.</figDesc><table><row><cell>Augmentation</cell><cell>RGB</cell><cell>MS</cell><cell>Parameterrange</cell></row><row><cell>AutoContrast</cell><cell>x</cell><cell></cell><cell>-</cell></row><row><cell>Brightness</cell><cell>x</cell><cell></cell><cell>[0.05, 0.95]</cell></row><row><cell>Color</cell><cell>x</cell><cell></cell><cell>[0.05, 0.95]</cell></row><row><cell>Contrast</cell><cell>x</cell><cell>x</cell><cell>[0.05, 0.95]</cell></row><row><cell>Equalize</cell><cell>x</cell><cell>x</cell><cell>-</cell></row><row><cell>Posterize</cell><cell>x</cell><cell>x</cell><cell>[4, 8]</cell></row><row><cell>Rotate</cell><cell>x</cell><cell>x</cell><cell>[-30, 30]</cell></row><row><cell>Sharpness</cell><cell>x</cell><cell>x</cell><cell>[0.05, 0.95]</cell></row><row><cell>ShearX</cell><cell>x</cell><cell>x</cell><cell>[-0.3, 0.3]</cell></row><row><cell>ShearY</cell><cell>x</cell><cell>x</cell><cell>[-0.3, 0.3]</cell></row><row><cell>Solarize</cell><cell>x</cell><cell>x</cell><cell>[0, 256]</cell></row><row><cell>TranslateX</cell><cell>x</cell><cell>x</cell><cell>[-0.3, 0.3]</cell></row><row><cell>TranslateY</cell><cell>x</cell><cell>x</cell><cell>[-0.3, 0.3]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>et al.<ref type="bibr" target="#b18">[19]</ref> is the strength of the weight decay, which penalizes largeFig. 5. Classification F1 scores for models trained with a different amount of labeled samples. Notably, some classes, such as denseresidential, seem to require more samples than others, such as airplane, to reach good scores.</figDesc><table><row><cell>agricultural airplane baseballdiamond beach buildings chaparral denseresidential forest freeway golfcourse harbor intersection mediumresidential mobilehomepark overpass tenniscourt storagetanks sparseresidential runway river parkinglot Class</cell><cell>0.50</cell><cell>0.75 F1 Score</cell><cell>1.00</cell><cell># of labels (per class) 105 (5) 210 (10) 441 (21) 1680 (80)</cell></row></table><note>Results are averaged over three seeds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II ACCURACY</head><label>II</label><figDesc>RESULTS COMPARISON ON EUROSAT IN PERCENT. WORKS USING MS DATA ARE MARKED WITH AN ASTERISK. MSMATCH OUTPERFORMS ALL OTHER METHODS ON EUROSAT. THE BEST RESULTS PER LABEL AMOUNT ARE BOLD.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Number of labels (per class)</cell><cell></cell></row><row><cell>Work</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell></row><row><cell></cell><cell>(5)</cell><cell>(10)</cell><cell>(50)</cell><cell>(100)</cell><cell>(200)</cell><cell>(300)</cell></row><row><cell>Guo et al. 2020 [32]</cell><cell>-</cell><cell cols="2">76.79 -</cell><cell cols="3">88.72 90.66 -</cell></row><row><cell>Roy et al. 2018 [15]</cell><cell>-</cell><cell cols="2">68.60 -</cell><cell cols="3">86.10 89.00 -</cell></row><row><cell>Tao et al. 2020 [16]</cell><cell cols="2">76.10 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Zhang &amp; Yang 2020* [14] -</cell><cell cols="4">80.12 89.01 91.11 -</cell><cell>96.67</cell></row><row><cell>Supervised Baseline</cell><cell cols="6">40.75 54.63 77.99 87.41 91.74 93.94</cell></row><row><cell>Ours (RGB)</cell><cell cols="6">94.53 96.04 97.62 97.63 98.07 98.14</cell></row><row><cell>Ours (MS)*</cell><cell cols="6">95.86 96.63 98.23 98.33 98.47 98.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III ACCURACY</head><label>III</label><figDesc>RESULTS COMPARISON ON UCM IN PERCENT. MSMATCH OUTPERFORMS ALL OTHER METHODS ON UCM. THE BEST RESULTS PER LABEL AMOUNT ARE BOLD.</figDesc><table><row><cell></cell><cell cols="4">Number of labels (per class)</cell></row><row><cell>Work</cell><cell>100 -105</cell><cell>200 -210</cell><cell>400 -441</cell><cell>1680</cell></row><row><cell></cell><cell>(? 5)</cell><cell>(? 10)</cell><cell>(20 ? 21)</cell><cell>(80)</cell></row><row><cell>Dai et al. 2019 [34]</cell><cell>85.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Gu &amp; Angelov 2018 [35] 71.86</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Guo et al. 2020 [32]</cell><cell>57.10</cell><cell>75.69</cell><cell>83.33</cell><cell>90.48</cell></row><row><cell>Han et al. 2018 [33]</cell><cell>-</cell><cell>91.42</cell><cell>92.68</cell><cell>-</cell></row><row><cell>Ours</cell><cell>90.71</cell><cell>94.13</cell><cell>95.71</cell><cell>98.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF DIFFERENT WEIGHT DECAY VALUES IN TERMS OF ACCURACY IN PERCENT. ALL RUNS ARE ON EUROSAT AND USED AN EFFICIENTNET-B2 AND 250 LABELS. THE BEST RESULT IS BOLD.</figDesc><table><row><cell>Weight</cell><cell>5.0 ?</cell><cell>7.5 ?</cell><cell>1.0 ?</cell><cell>2.5 ?</cell><cell>5.0 ?</cell><cell>7.5 ?</cell><cell>1.0 ?</cell></row><row><cell>decay</cell><cell>10 ?5</cell><cell>10 ?5</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?3</cell></row><row><cell>Accuracy</cell><cell>94.63 ?0.79</cell><cell>93.49 ?1.95</cell><cell>95.53 ?1.78</cell><cell>95.21 ?1.88</cell><cell>95.26 ?3.24</cell><cell>96.63 ?0.34</cell><cell>96.53 ?0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF EFFICIENTNETS IN TERMS OF ACCURACY AND PARAMETERS. ALL MODELS WERE TRAINED ON EUROSAT WITH 250 LABELS AND A WEIGHT DECAY OF 7.5 ? 10 ?4 . THE BEST RESULT IS BOLD. 78?0.04 96.85?0.46 96.85?0.56 96.91?0.27</figDesc><table><row><cell>Original Image</cell><cell>Guided Backprop 50 Labels</cell><cell>Guided Backprop 3000 Labels</cell><cell></cell></row><row><cell></cell><cell cols="2">Model # of Model Parameters Accuracy [%] 96.Text 5.3M B0</cell><cell>B1 7.8M</cell><cell>B2 9.2M</cell><cell>B3 12M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI PRECISION</head><label>VI</label><figDesc>, RECALL AND F1-SCORE METRICS FOR EUROSAT RGB OBTAINED BY USING AN EFFICIENTNET-B2 MODEL TRAINED WITH FIVE LABELED IMAGES PER CLASS. RESULTS ARE AVERAGED OVER THREE SEEDS. Kurakin et al. [19] described this for the CIFAR-100 dataset, which features 100 classes. However, EuroSAT, as the other datasets investigated by Kurakin et al., only features ten classes. The benefit of the higher weight decay in our experiments may also be related to the different model choices, as Kurakin et al. relied on a different network architecture. In terms of the comparison of model parameters in the network architecture (see</figDesc><table><row><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>Support</cell></row><row><cell>AnnualCrop</cell><cell>94.52</cell><cell>97.66</cell><cell>96.06</cell><cell>300</cell></row><row><cell>Forest</cell><cell>99.55</cell><cell>98.22</cell><cell>98.88</cell><cell>300</cell></row><row><cell>HerbaceousVegetation</cell><cell>88.66</cell><cell>82.22</cell><cell>83.23</cell><cell>300</cell></row><row><cell>Highway</cell><cell>96.39</cell><cell>99.73</cell><cell>98.03</cell><cell>250</cell></row><row><cell>Industrial</cell><cell>97.39</cell><cell>98.13</cell><cell>97.74</cell><cell>250</cell></row><row><cell>Pasture</cell><cell>97.21</cell><cell>92.83</cell><cell>94.97</cell><cell>200</cell></row><row><cell>PermanentCrop</cell><cell>87.25</cell><cell>82.40</cell><cell>83.62</cell><cell>250</cell></row><row><cell>Residential</cell><cell>91.41</cell><cell>99.44</cell><cell>95.14</cell><cell>300</cell></row><row><cell>River</cell><cell>98.49</cell><cell>95.46</cell><cell>96.95</cell><cell>250</cell></row><row><cell>SeaLake</cell><cell>99.32</cell><cell>98.22</cell><cell>98.77</cell><cell>300</cell></row><row><cell>ing.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII PRECISION</head><label>VII</label><figDesc>, RECALL AND F1-SCORE METRICS FOR UCM OBTAINED BY USING AN EFFICIENTNET-B2 MODEL TRAINED WITH 5 LABELED IMAGES PER CLASS. THE SUPPORT IS 20 IMAGES FOR EACH CLASS. RESULTS ARE AVERAGED OVER THREE SEEDS. Average confusion matrix over three seeds for the EuroSAT RGB dataset obtained through an EfficientNet-B2 model trained with five labeled images per class. Cells containing 0s are not shown. Results are expressed as percentage of the support. Due to rounding numbers may not add up to 1.</figDesc><table><row><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell></row><row><cell>agricultural</cell><cell>94.63</cell><cell>88.33</cell><cell>91.32</cell></row><row><cell>airplane</cell><cell>98.41</cell><cell>100.00</cell><cell>99.19</cell></row><row><cell>baseballdiamond</cell><cell>93.80</cell><cell>100.00</cell><cell>96.79</cell></row><row><cell>beach</cell><cell>94.06</cell><cell>100.00</cell><cell>96.86</cell></row><row><cell>buildings</cell><cell>87.06</cell><cell>75.00</cell><cell>79.43</cell></row><row><cell>chaparral</cell><cell>93.57</cell><cell>95.00</cell><cell>94.27</cell></row><row><cell>denseresidential</cell><cell>69.48</cell><cell>33.33</cell><cell>44.52</cell></row><row><cell>forest</cell><cell>98.41</cell><cell>95.00</cell><cell>96.48</cell></row><row><cell>freeway</cell><cell>89.29</cell><cell>96.67</cell><cell>92.75</cell></row><row><cell>golfcourse</cell><cell>87.86</cell><cell>96.67</cell><cell>92.02</cell></row><row><cell>harbor</cell><cell>95.45</cell><cell>98.33</cell><cell>96.83</cell></row><row><cell>intersection</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>mediumresidential</cell><cell>72.05</cell><cell>78.33</cell><cell>74.89</cell></row><row><cell>mobilehomepark</cell><cell>75.09</cell><cell>93.33</cell><cell>83.14</cell></row><row><cell>overpass</cell><cell>95.30</cell><cell>98.33</cell><cell>96.75</cell></row><row><cell>parkinglot</cell><cell>96.97</cell><cell>98.33</cell><cell>97.56</cell></row><row><cell>river</cell><cell>98.33</cell><cell>86.67</cell><cell>91.60</cell></row><row><cell>runway</cell><cell>93.31</cell><cell>91.67</cell><cell>92.39</cell></row><row><cell>sparseresidential</cell><cell>96.67</cell><cell>93.33</cell><cell>94.91</cell></row><row><cell>storagetanks</cell><cell>82.03</cell><cell>90.00</cell><cell>85.15</cell></row><row><cell>tenniscourt</cell><cell>98.33</cell><cell>96.67</cell><cell>97.48</cell></row><row><cell>Fig. 7.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">see https://paperswithcode.com/task/semi-supervised-image-classification (Accessed 17.03.2021)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">see https://paperswithcode.com/sota/semi-supervised-image-classification-on-cifar-6 (Accessed 11.03.2021) 3 https://github.com/lukemelas/EfficientNet-PyTorch (Accessed 11.03.2021)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/LeeDoYup/FixMatch-pytorch (Accessed 11.03.2021)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/gomezzz/MSMatch 6 https://github.com/phelber/EuroSAT (Accessed 11.03.2021) 7 http://weegee.vision.ucmerced.edu/datasets/landuse.html (Accessed 13.07.2021)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Status and trends of smallsats and their launch vehicles-an up-to-date review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pessoa Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E V L D</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Trabasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Aeros. Tech. Manag</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="286" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Are smallsats entering the maturity stage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Puteaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najjar</surname></persName>
		</author>
		<ptr target="https://spacenews.com/analysis-are-smallsats-entering-the-maturity-stage/" />
	</analytic>
	<monogr>
		<title level="m">SpaceNews</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Earth-observing companies push for more-advanced science satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Popkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">545</biblScope>
			<biblScope unit="issue">7655</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Esa&apos;s earth observation strategy and copernicus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aschbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Satellite earth observations and their impact on society and policy</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sidike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nasrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Van Esesn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A S</forename><surname>Awwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01164</idno>
		<title level="m">The history began from alexnet: A comprehensive survey on deep learning approaches</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">42609</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperspectral images classification with gabor filtering and convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2355" to="2359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A semi-supervised convolutional neural network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="839" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning using pseudo labels for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1259" to="1270" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-spectral land cover classification with multi-attention and adaptive kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1881" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic-fusion gans for semi-supervised satellite image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification with self-supervised paradigm under limited labeled samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neur. Inf. Proc. Sys. (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neur. Inf. Proc. Sys. (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2110</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems</title>
		<meeting>the 18th SIGSPATIAL international conference on advances in geographic information systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards the use of artificial intelligence on the edge in space systems: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Furano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moloney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferlet-Cavrois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavoularis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Psarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-O</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Aerosp. Electron. Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="44" to="56" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aid: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A full convolutional network based on densenet for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Biosci. Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3345" to="3367" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale-free convolutional neural network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6916" to="6928" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural network for remote-sensing scene classification: Transfer learning analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pires De Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marfurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gan-based semisupervised scene classification of remote sensing image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semisupervised scene classification for remote sensing images: A method based on convolutional neural networks and ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="869" to="873" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised deep rule-based approach for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Angelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving the efficiency of deep learning methods in remote sensing data analysis: Geosystem approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Yamashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Yamashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Zanozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Barmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="179" to="516" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Realmix: Towards realistic semi-supervised deep learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beltramelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08766</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning with pseudoensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neur. Inf. Proc. Sys. (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pytorch cnn visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ozbulak</surname></persName>
		</author>
		<ptr target="https://github.com/utkuozbulak/pytorch-cnn-visualizations" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">He received his M.Sc. in computer science from the Technical University Munich in 2015. Research topics of interest to him range from machine learning and inverse problems to numerical methods and high performance computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESA&apos;s</title>
		<imprint/>
	</monogr>
	<note>Advanced Concepts Team at ESTEC, Noordwijk. He received his PhD from the Friedrich-Alexander-Universit?t Erlangen-N?rnberg in 2019 (supervisor Prof. D?llinger)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
