<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Channelized Axial Attention - Considering Channel Relation within Spatial Attention for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI LAB</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Channelized Axial Attention - Considering Channel Relation within Spatial Attention for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial and channel attentions, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation. However, computing spatial and channel attentions separately sometimes causes errors, especially for those difficult cases. In this paper, we propose Channelized Axial Attention (CAA) to seamlessly integrate channel attention and spatial attention into a single operation with negligible computation overhead. Specifically, we break down the dot-product operation of the spatial attention into two parts and insert channel relation in between, allowing for independently optimized channel attention on each spatial location. We further develop grouped vectorization, which allows our model to run with very little memory consumption without slowing down the running speed. Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PAS-CAL Context, and COCO-Stuff, demonstrate that our CAA outperforms many state-of-the-art segmentation models (including dual attention) on all tested datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Spatial and channel attentions, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation. However, computing spatial and channel attentions separately sometimes causes errors, especially for those difficult cases. In this paper, we propose Channelized Axial Attention (CAA) to seamlessly integrate channel attention and spatial attention into a single operation with negligible computation overhead. Specifically, we break down the dot-product operation of the spatial attention into two parts and insert channel relation in between, allowing for independently optimized channel attention on each spatial location. We further develop grouped vectorization, which allows our model to run with very little memory consumption without slowing down the running speed. Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PAS-CAL Context, and COCO-Stuff, demonstrate that our CAA outperforms many state-of-the-art segmentation models (including dual attention) on all tested datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a fundamental task in many computer vision applications, which assigns a class label to each pixel in the image. Most of the existing approaches <ref type="bibr" target="#b33">(Yuan, Chen, and Wang 2020;</ref><ref type="bibr" target="#b30">Yang et al. 2018;</ref><ref type="bibr">Fu et al. 2019;</ref><ref type="bibr" target="#b15">Li et al. 2019)</ref> have adopted a pipeline similar to the one that is defined by Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b18">(Long, Shelhamer, and Darrell 2015)</ref> using fully convolutional layers to output the pixel-level segmentation results of input images. These approaches have achieved state-of-the-art performance. After the FCNs, there have been many approaches dedicated to extracting enhanced pixel representations from the backbone. Earlier approaches, including PSPNet <ref type="bibr" target="#b37">(Zhao et al. 2017)</ref> and DeepLab , used a Pyramid Pooling Module or an Atrous Spatial Pyramid Pooling module to expand the receptive field to enhance the representation capabilities. Recently, many works focus on using the attention mechanisms to enhance pixel representations. The first attempts in this direction included Squeeze and Excitation Networks (SENets) <ref type="bibr" target="#b12">(Hu, Shen, and Sun 2018)</ref> that introduced a simple yet effective channel attention module to explicitly model the interdependencies between channels.  <ref type="figure">Figure 1</ref>: Different dual attention designs: (a) Parallel dual attention sums the results from spatial and channel attentions directly, which may cause conflicts because spatial and channel attentions are focusing on different aspects. (b) Sequential dual attention performs spatial attention after channel attention, where the spatial attention may override correct features extracted by the channel attention. (c) Our channelized attention seamlessly merges the spatial and channel attentions into a single operation (see <ref type="bibr">Sect. 4.2)</ref>, removing the potential conflicting issue caused by a or b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Under Review</head><p>Meanwhile, spatial attention relied on self-attention proposed in <ref type="bibr" target="#b27">Vaswani et al. 2017)</ref> to model long-range dependencies in spatial domain, so as to pro-duce more correct pixel representations. For each pixel in the feature maps, spatial attention "corrects" its representation with the representations of other pixels depending on their similarity. In contrast, channel attention identifies important channels based on all spatial locations and reweights the extracted features.</p><p>Parallel dual attention (e.g., <ref type="bibr">(Fu et al. 2019)</ref>) was proposed to gain the advantages of both spatial attention and channel attention. This approach directly fused their results with an element-wise addition (see <ref type="figure">Fig. 1(a)</ref>). Although they have achieved improved performance, the relationship between the contributions of spatial and channel attentions to the final results is unclear. Moreover, calculating the two attentions separately not only increases the computational complexity, but may also result in conflicting importance of feature representations. For example, some channels may appear to be important in spatial attention for a pixel that belongs to a partial region in the feature maps. However, channel attention may have its own perspective, which is calculated by summing up the similarities over the entire feature maps, and weakens the impact of spatial attention.</p><p>Sequential dual attention, which combines channel attention and spatial attention in a sequential manner ( <ref type="figure">Fig. 1(a)</ref>) has similar issues. For example, channel attention can ignore the partial region representation obtained from the overall perspective. However, this partial region representation may be required by spatial attention. Thus, directly fusing the spatial and channel attention results may yield incorrect importance weights for pixel representations. In Sect. 5, we develop an approach to visualize the impact of the conflicting feature representation on the final segmentation results.</p><p>In order to overcome the aforementioned issues, we propose Channelized Axial Attention (CAA), which breaks down the axial attention into more basic parts and inserts channel attention into them, combining spatial attention and channel attention together seamlessly and efficiently. Specifically, when applying the axial attention maps to the input signal , we capture the intermediate results of the dot product before they are summed up along the corresponding axes. Capturing these intermediate results allows channel attention to be integrated for each column and each row, instead of computing on the mean or sum of the features in the entire feature maps. We also develop a novel grouped vectorization approach to maximize the computation speed in limited GPU memory.</p><p>In summary, our contributions in this paper include:</p><p>? We are the first to explicitly identify the potential conflicts between spatial and channel attention in existing dual attention designs by visualizing the effects of each attention on the final result. ? We propose a novel Channelized Axial Attention, which breaks down the axial attention into more basic parts and inserts channel attention in between, integrating spatial attention and channel attention together seamlessly and efficiently, with only a minor computation overhead compared to the original axial attention. ? To balance the computation speed and GPU memory usage, a grouped vectorization approach for computing the channelized attentions is proposed. This is particularly advantageous when processing large images. ? Experiments on three challenging benchmark datasets, including PASCAL Context <ref type="bibr" target="#b6">(Everingham et al. 2009</ref>), COCO-Stuff <ref type="bibr" target="#b0">(Caesar, Uijlings, and Ferrari 2018)</ref> and Cityscapes <ref type="bibr" target="#b19">(Marius et al. 2016)</ref>, demonstrate the superiority of our approach over the state-of-the-art approaches.</p><p>2 Related Work Spatial attention. Non-local networks  and Transformer <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref> introduced the selfattention mechanism to examine the pixel relationship in the spatial domain. It usually calculates dot-product similarity or cosine similarity to obtain the similarity measurement between every two pixels in feature maps, and recalculates the feature representation of each pixel according to its similarity with others. Self-attention has successfully addressed the feature map coverage issue of multiple fixed-range approaches <ref type="bibr" target="#b1">(Chen et al. 2017;</ref><ref type="bibr" target="#b37">Zhao et al. 2017)</ref>, but it has also introduced huge computation costs for computing the complete feature map. This means that, for each pixel in the feature maps, its attention similarity affects all other pixels.</p><p>Recently, many approaches <ref type="bibr" target="#b13">(Huang et al. 2020;</ref><ref type="bibr" target="#b38">Zhu et al. 2019)</ref> have been developed to optimize the GPU memory costs of spatial self-attention.</p><p>Channel Attention. Channel attention <ref type="bibr" target="#b12">(Hu, Shen, and Sun 2018)</ref> examined the relationships between channels, and enhanced the important channels so as to improve performance. SENets <ref type="bibr" target="#b12">(Hu, Shen, and Sun 2018)</ref> conducted a global average pooling to get mean feature representations, and then went through two fully connected layers, where the first one reduced channels and the second one recovered the original channels, resulting in channel-wise weights according to the importance of channels. In DANet <ref type="bibr">(Fu et al. 2019)</ref>, channel-wise relationships were modelled by a 2D attention matrix, similar to the self-attention used in the spatial domain, except that it computed the attention with a dimension of C ? C rather than (H ? W ) ? (H ? W ) (here, C represents the number of channels, and H and W represent the height and width of the feature maps, respectively).</p><p>Spatial Attention + Channel Attention. Combining spatial attention and channel attention can provide fully optimized pixel representations in a feature map. However, it is not easy to enjoy both advantages seamlessly. In DANet <ref type="bibr">(Fu et al. 2019)</ref>, the results of the channel attention and spatial attention are directly added together. Supposing that there is a pixel belonging to a semantic class that has a tiny region in the feature maps, spatial-attention can find its similar pixels. However, channel representation of the semantic class with a partial region of the feature maps may not be important in the perspective of entire feature maps, so it may be ignored when conducting channel attention computations. Computing self-attention and channel attention separately (as illustrated in <ref type="figure">Fig. 1(a)</ref>) can cause conflicting results, and thus   weaken their performance when both results are summarized together. Similarly, in the cascaded model (see <ref type="figure">Fig. 1(b)</ref>), the spatial attention module after the channel attention module may pick up an incorrect pixel representation enhanced by channel attention, because channel attention computes channel importance according to the entire feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Exploring Conflicting Features</head><p>As we have analyzed earlier in Sect. 2, computing spatial and channel attentions separately can cause conflicting features. In our experiments, to illustrate this feature conflicting issue faced by existing dual attention approaches, we designed a simple way to visualize the effects of spatial attention and channel attentions on pixel representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visualizing Conflicts</head><p>For a parallel dual attention design such as DANet <ref type="bibr">(Fu et al. 2019)</ref>, since it has two auxiliary losses for each of spatial attention and channel attention, we directly use their logits during inference to generate corresponding segmentation results and compare them with the result generated by the main logits. For a sequential dual attention design, we add an extra branch that directly uses the pixel representation obtained from channel attention to perform the segmentation Image Ground Truth Channel Attention Spatial Attention &amp;Prediction <ref type="figure">Figure 4</ref>: In sequential dual attention designs, the spatial attention representation (the 4th column) ignores the correct channel attention representation (the 3rd column).</p><p>logits. Note that, since the original sequential design does not have independent logits after the channel attention module, we stop the gradient from back-propagating to the main branch, to ensure that our newly added branch has no effect on the main branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Examples of Conflicting Features</head><p>To visualize the impact of the feature conflicting issue in the existing dual attention designs (see Sect. 2), we present examples of the segmentation results obtained with the conflicting features in the parallel dual attention design (see <ref type="figure" target="#fig_3">Fig. 3</ref>) and the sequential dual attention design (see <ref type="figure">Fig. 4</ref>).</p><p>As observed from <ref type="figure" target="#fig_3">Fig. 3</ref>, the parallel design of dual attention directly sums up the pixel representations obtained from spatial attention and channel attention. With this approach, the advantages of the pixel representations obtained from one can be weakened by the other. The sequential way of combining the dual attentions avoids taking their average but still has its own problem. As shown in <ref type="figure">Fig. 4</ref>, the pixel representation obtained from the spatial attention ignores the correct representation obtained from the channel attention, and worsens the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries</head><p>Formulation of the Spatial Self-attention Following Non Local  and Stand Alone Self Attention <ref type="bibr" target="#b22">(Ramachandran et al. 2019)</ref>, a 2D self-attention operation in spatial domain can be defined by:</p><formula xml:id="formula_0">yi,j = ?m,n f (xi,j , xm,n)g(xm,n).<label>(1)</label></formula><p>Here, a pairwise function f computes the similarity between the pixel representation x i,j (query) at position (i, j) and the pixel representation x m,n (key) at all other possible positions (m, n). The unary function g maps the original representation at position (m, n) to a new domain (value). In our work, we use the similarity function </p><formula xml:id="formula_1">) as f , i.e., f (xi,j , xm,n) = softmaxm,n(?(xi,j ) T ?(xm,n)),<label>(2)</label></formula><p>where ? is a 1 ? 1 convolution layer transforming the feature maps x to a new domain to calculate dot-product similarity ) between every two pixels. Note . We present the way to apply channel attention seamlessly in (b). We mark the independent spatial dimension in bold style. This allows channel attention to also consider spatial unique information. Note that, in our design, the "value" for row attention is obtained from the result of column attention. See Eq. 11 for details, and the Appendix for the full architecture. that, following a common practice , we use the same 1 ? 1 convolution weights for both query and key. Then, these similarities are used as the weights (Eq. (1)) to aggregate features of all pixels, producing an enhanced pixel representation y i,j at position (i, j).</p><p>Formulation of the Axial Attention From the above equations, we can see the computational complexity of the self-attention module is O(H 2 W 2 ), which requires large computation resources and prevents real-time applications, such as autopilot. Several subsequent works <ref type="bibr" target="#b13">(Huang et al. 2020;</ref><ref type="bibr">Ho et al. 2019</ref>) focused on reducing the computational complexity while maintaining high accuracy. In this work, we adopt axial-attention to perform spatial attention. In axial attention, the attention map is calculated for the column and row that cover the current pixel, reducing the computational complexity to O(HW 2 + H 2 W ).</p><p>For convenience, we call the attention values calculated along the Y axis "column attention", and the attention values calculated along the X axis "row attention". Similar to Eq. 2, we define axial similarity functions by:</p><formula xml:id="formula_2">A col (xi,j , xm,j ) = softmaxm ?(xi,j ) T ?(xm,j ) , m ? [H] 1 ,<label>(3)</label></formula><p>and</p><formula xml:id="formula_3">Arow(xi,j , xi,n) = softmaxn ?(xi,j ) T ?(xi,n) , n ? [W ].<label>(4)</label></formula><p>Note that we use different feature transformations (?, ?) for the row and column attention calculations.</p><p>With the column and row attention maps A col and A row , the final value weighted by the column and row attention maps can be represented by:</p><p>yi,j = ?n Arow(xi,j , xi,n)( ?m A col (xi,j , xm,j )g(xm,n)) . (5) <ref type="bibr">1</ref> We use i ? [n] to denote that i is generated from [n] = {1, 2, ..., n}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Channelized Axial Attention</head><p>In order to address the feature conflicting issue of the existing dual attention designs, we propose a novel Channelized Axial Attention (CAA), which seamlessly combines the advantages of spatial attention and channel attention.</p><p>As mentioned in the above sections, feature conflicts may be caused by the different interests of spatial and channel attentions. Ideally, channel attention should not ignore the regional features that are interesting to spatial attention. Conversely, spatial attention should consider channel relation as well.</p><p>Thus, we propose to compute channel attention within spatial attention. Specifically, we firstly break down spatial attention into more basic parts (Sect. 4.2). Then, spatially varying channel attention is generated with ? i,j,m,n and ? i,j,n . In this way, channel attention is incorporated into spatial attention and spatial attention will not be ignored when small objects exist, seamlessly and effectively combining spatial and channel attention together.</p><p>Breaking Down Axial Attention. For convenience, we firstly define two variables ? i,j,m,n and ? i,j,n to represent the intermediate weighted features as follows:</p><p>?i,j,m,n = A col (xi,j , xm,j )g(xm,n)</p><p>and ?i,j,n = Arow(xi,j , xi,n) ?m ?i,j,m,n.</p><p>Thus, Eq. (5) can be rewritten as:</p><p>yi,j = ?n ?i,j,n = ?n Arow(xi,j , xi,n) ?m ?i,j,m,n .</p><p>Eqs. (6), <ref type="formula" target="#formula_5">(7)</ref> and <ref type="formula" target="#formula_6">(8)</ref> show that the computation of the dot product is composed of two steps: 1) Reweighting: reweighting features on selected locations by column attention as in Eq. (6) and row attention as in Eq. (7), and 2) Summation: summing the elements along row and column axes in Eq. (8). Note that, this breakdown is also applicable to regular self-attention (see <ref type="table" target="#tab_3">Table 3</ref> and Appendix).</p><p>Spatially Varying Channel Attention. With the intermediate results ? i,j,m,n and ? i,j,n in Eqs. <ref type="formula" target="#formula_4">(6)</ref> and <ref type="formula" target="#formula_5">(7)</ref>, channel relation can be applied inside spatial attention, seamlessly combining them into one operation. In addition, channel attention is now independently conducted on each column or row (on each pixel in regular self-attention) and provides spatial perspective for the channel relation modeling, resulting in our spatially varying channel attention. Enhanced with spatially varying channel attentions, now C col and C row are written as:</p><formula xml:id="formula_7">C col (?i,j,m,n) = Sigmod ReLU( ?m,j (?i,j,m,n) H ? W ?c1)?c2 ?i,j,m,n,<label>(9)</label></formula><p>and Crow(?i,j,n) = Sigmod ReLU( ?i,n (?i,j,n) H ? W ?r1)?r2 ?i,j,n, <ref type="formula" target="#formula_0">(10)</ref> where Sigmod(?) is the learned channel attention, and ? c1 , ? c2 , ? r1 and ? r2 are the learned relationships between different channels according to ? i,j,m,n and ? i,j,n . Thus, instead of directly using ? i,j,m,n and ? i,j,n as in Eq. <ref type="formula" target="#formula_6">(8)</ref>, for each column and row, we obtain the channelized axial attention features, where the intermediate results ? i,j,m,n and ? i,j,n are weighted by the spatially varying channel attention defined in Eqs. <ref type="formula" target="#formula_7">(9)</ref> and <ref type="formula" target="#formula_0">(10)</ref> as:</p><formula xml:id="formula_8">yi,j = ?n Crow Arow(xi,j , xi,n)( ?m C col (?i,j,m,n)) .<label>(11)</label></formula><p>Note that the spatially varying channel attention keeps a W dimension after averaging H ? W during the channel attention ( <ref type="figure" target="#fig_4">Fig. 5</ref>). Now each row has its own channel attention thanks to the breaking down of spatial axial attention.</p><p>Going Deeper in Channel Attention. Similar to the work in <ref type="bibr" target="#b12">(Hu, Shen, and Sun 2018)</ref>, we use two fully connected layers, followed by ReLU and sigmoid activations respectively, to first reduce the channel number and then increase it to the original channel number.</p><p>To further boost performance, we explore the design of more powerful channel attention modules for our channelization since our attention module keeps the spatial dimension, and thus contains more information than a regular SE module (1 ? 1 ? C ? W orH vs 1 ? 1 ? C, see <ref type="figure" target="#fig_4">Fig. 5</ref>).</p><p>We experimented with increased depth and/or width of hidden layers to enhance the capacity of spatial varying channel attention. We find that deeper hidden layers allow channel attention to find a better relationship between channels for our spatially varying channel attention. Moreover, increasing layer width is not as effective as adding layer depth (see <ref type="table">Table 1</ref>).</p><p>Furthermore, in spatial domain, each channel of a pixel contains unique information that can lead to a unique semantic representation. We find that using Leaky ReLU <ref type="bibr" target="#b20">(Mass, Hannun, and Ng 2013)</ref> is more effective than ReLU in preventing the loss of information along deeper activations <ref type="bibr" target="#b24">(Sandler et al. 2018)</ref>. Apparently, this replacement only works in spatially varying channel attention.</p><p>Grouped Vectorization. Computing spatial attention row by row and column by column can save computation but it is still too slow (about 2.5 times slower on a single V100 with feature map size = 33 ? 33) even with parallelization. Full vectorization can achieve a very high speed but it has a high requirement on GPU memory (about 2 times larger GPU memory usage than no vectorization on a single V100 with feature map size = 33 ? 33) for storing the intermediate partial axial attention results ? (which has a dimension of H ? H ? W ? C) and ? (which has a dimension of W ? H ? W ? C) in Eqs. <ref type="formula" target="#formula_4">(6)</ref> and <ref type="formula" target="#formula_5">(7)</ref>. To enjoy the high speed benefit of vectorization with limited GPU memory usage, in our implementation we propose grouped vectorization to dynamically batch rows and columns into multiple groups, and then perform vectorization for each group individually. The technical details and impact of our group vectorization are detailed in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To demonstrate the effectiveness for accuracy of the proposed CAA, comprehensive experimental results are compared with the state-of-the-art methods on three benchmark datasets, i.e., PASCAL Context <ref type="bibr" target="#b6">(Everingham et al. 2009</ref>), COCO-Stuff <ref type="bibr" target="#b0">(Caesar, Uijlings, and Ferrari 2018)</ref> and Cityscapes <ref type="bibr" target="#b19">(Marius et al. 2016</ref>).</p><p>Using similar settings as in other existing works, we measure the segmentation accuracy using mean intersection over union <ref type="bibr">(mIOU)</ref>. Moreover, to demonstrate the efficiency of our CAA, we also compare the floating point operations per second (FLOPS) of different approaches. Experimental results show that our CAA outperforms the state-of-the-art methods on all tested datasets. Due to page limitations, we focus on ResNet-101 (with naive upsampling) results in the main paper for the fairest comparison. Results obtained with EfficientNet or results on other extra datasets are presented in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Backbone Our network is built on ResNet-101 <ref type="bibr" target="#b9">(He et al. 2016</ref>) pre-trained on ImageNet. The original ResNet results in a feature map of 1/32 of the input size. Following other works <ref type="bibr" target="#b15">Li et al. 2019)</ref>, we apply dilated convolution at the output stride = 16 for ablation experiments if not specified. We conduct experiments with the output stride = 8 to compare with the state-of-the-art methods.</p><p>Naive Upsampling Unless otherwise specified, we directly bi-linearly upsampled the logits to the input size without refining using any low-level and high resolution features.</p><p>Training Settings We employ stochastic gradient descent (SGD) for optimization, where the polynomial decay learning rate policy (1 ? iter maxiter ) 0.9 is applied with an initial learning rate = 0.01. We use synchronized batch normalization with batch size = 16 (8 for Cityscapes) during training. For data augmentation, we only apply the most basic data augmentation strategies as in , including random flip, random scale and random crop.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on PASCAL Context</head><p>The PASCAL Context <ref type="bibr" target="#b21">(Mottaghi et al. 2014</ref>) dataset has 59 classes with 4,998 images for training and 5,105 images for testing. We train our CAA on the training set for 40k iterations. In the following, we first present a series of ablative experiments to show the effectiveness of our method. Then, quantitative and qualitative comparisons with other state-ofthe-art methods are presented. Note that, in ablation studies below, we report mean result with standard deviation (numbers in parentheses) calculated with 5 repeated experiments. (See Deterministic in the Appendix for technical details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the Proposed Channelization</head><p>We first report the impact of adding channelized axial attention and with different depth and width in <ref type="table">Table 1</ref>, where '-' for the baseline result indicates no channelization is performed.</p><p>As can be seen from this table, our proposed channelization improves the mIOU over the baseline regardless of the layer counts and the number of channels used. In particular, the best performance is achieved when the Layer Counts = 5 and the number of Channels = 128.</p><p>We also compare our model with the sequential design of "Axial Attention + SE", as shown in <ref type="table" target="#tab_1">Table 2</ref>. We find the sequential design brings only marginal contributions to performance, showing that our proposed channelization method can combine the advantages of both spatial attention and channel attention more effectively. In <ref type="table" target="#tab_6">Table 5</ref>, results obtained with other backbones are provided to demonstrate the effectiveness and robustness of CAA.</p><p>Channelized Self-Attention In this section, we conduct experiments on the PASCAL Context by applying channelization to the original self-attention. We report its singlescale performance in <ref type="table" target="#tab_3">Table 3</ref> with ResNet-101. Our channelized method can also further improve the performance of   self-attention by 0.67% (51.09% vs 50.42%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of the Testing Strategies</head><p>We compare the performance and computation cost of our proposed model against the baseline and DANet <ref type="bibr">(Fu et al. 2019)</ref> with different testing strategies in <ref type="table" target="#tab_4">Table 4</ref>. Using the same settings as in other works <ref type="bibr">(Fu et al. 2019)</ref>, we add multi-scale, left-right flip and auxiliary loss during inference. The accuracies of CAA are further boosted with output stride = 8 since the channel attention can learn and optimize three times more pixels.</p><p>Comparison with the State-of-the-art Finally, in Table 6, we compare our approach with the state-of-the-art approaches. Like other similar works, we apply multi-scale and left-right flip during inference. For a fair comparison, we only compare with the methods that use ResNet-101 and naive upsampling in the main paper. More results using alternative backbones are included in <ref type="table" target="#tab_6">Table 5</ref>. As shown in this table, our proposed CAA outperforms all listed state-of-the-art models that are trained with an output stride = 8. Our CAA also performs better than EMANet and SPYGR that are trained with output stride = 16. Note that,   <ref type="bibr" target="#b38">(Zhu et al. 2019)</ref> ResNet-101 52.8 EMANet <ref type="bibr" target="#b15">(Li et al. 2019)</ref> ResNet-101 53.1 SPYGR  ResNet-101 52.8 CPN <ref type="bibr">(Yu et al. 2020)</ref> ResNet-101 53.9 CFNet <ref type="bibr" target="#b36">(Zhang et al. 2019)</ref> ResNet-101 54.0</p><p>DANet <ref type="formula">(</ref>  in this and the following tables, we report the best results of our approach obtained in experiments. In <ref type="figure">Fig. 6</ref>, we show some results obtained by our CAA model, FCN and Dual attention. Our model is able to handle previous failure cases better, especially when a class A covering a smaller region is surrounded by another class B covering a much larger region (see the boxed regions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on the COCO-Stuff 10K</head><p>Following the other works <ref type="bibr">(Fu et al. 2019)</ref>, we evaluate our CAA on COCO-Stuff 10K dataset <ref type="bibr" target="#b0">(Caesar, Uijlings, and Ferrari 2018)</ref>, which contains 9,000 training images and 1,000 testing images with 172 classes. Our model is trained for 40k iterations. As shown in <ref type="table" target="#tab_9">Table 7</ref>, our proposed CAA outperforms all other state-of-the-art approaches by a large margin of 1.3%, demonstrating that our model can better handle complex images with a large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on the Cityscapes</head><p>The Cityscapes dataset <ref type="bibr" target="#b19">(Marius et al. 2016</ref>) has 19 classes. Its fine set contains high quality pixel-level annotations of 2,975, 500 and 1,525 images in the training, validation, and test sets, respectively. Following previous works <ref type="bibr">(Fu et al. 2019)</ref>, we use only the fine set with a crop size of 769?769 during training, and our training iteration is set to 80k. We report our results on the test set in <ref type="table" target="#tab_10">Table 8</ref>. Results show our CAA is also working well on high-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth FCN Dual Attention Ours <ref type="figure">Figure 6</ref>: Examples of the segmentation results obtained on the PASCAL Context dataset using FCN, DANet and CAA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Backbone mIOU (%) DSSPN <ref type="bibr" target="#b17">(Liang, Zhou, and Xing 2018)</ref> ResNet-101 38.9 SVCNet <ref type="bibr" target="#b4">(Ding et al. 2019)</ref> ResNet-101 39.6 EMANet <ref type="bibr" target="#b15">(Li et al. 2019)</ref> ResNet-101 39.9 SPYGR  ResNet-101 39.9 OCR <ref type="bibr" target="#b33">(Yuan, Chen, and Wang 2020)</ref> ResNet-101 39.5</p><p>DANet <ref type="bibr">(Fu et al. 2019)</ref> ResNet-101 39.7</p><p>Our CAA ResNet-101 41.2  <ref type="bibr" target="#b38">(Zhu et al. 2019)</ref> ResNet-101 81.3 CCNet <ref type="bibr" target="#b13">(Huang et al. 2020)</ref> ResNet-101 81.4 CPN <ref type="bibr">(Yu et al. 2020)</ref> ResNet-101 81.3 SPYGR  ResNet-101 81.6 OCR <ref type="bibr" target="#b33">(Yuan, Chen, and Wang 2020)</ref> ResNet-101 81.8</p><p>DANet <ref type="bibr">(Fu et al. 2019)</ref> ResNet-101 81.5</p><p>Our CAA ResNet-101 82.6 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed a novel and effective Channelized Axial Attention, effectively combining the advantages of the popular spatial-attention and channel attention. Specifically, we first break down spatial attention into two steps and insert channel attention in between, enabling different spatial positions to have their own channel attentions. Experiments on the three popular benchmark datasets have demonstrated the effectiveness of our proposed channelized axial attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank TensorFlow Research Cloud (TFRC) for TPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mIOU (%)</head><p>CTNet <ref type="bibr" target="#b16">(Li, Sun, and Tang 2021</ref>) + JPU 55.5 SETR-MLA <ref type="bibr" target="#b25">(Sixiao et al. 2021)</ref> 55.83 HRNetV2 + OCR <ref type="bibr" target="#b4">(Ding et al. 2019)</ref> 56.2 ResNeSt-269 <ref type="bibr" target="#b35">(Zhang et al. 2020</ref>) + DeepLab V3+ 58.9 HRNetV2 + OCR + RMI 59.6 DPT-Hybrid <ref type="bibr" target="#b23">(Ranftl, Bochkovskiy, and Koltun 2021)</ref> 60.46</p><p>Our CAA (EfficientNet-B7, w/o decoder) 60.12 Our CAA (EfficientNet-B7 + simple decoder) 60.50 <ref type="table">Table 9</ref>: Result comparison with the state-of-the-art approaches on the PASCAL Context test set for multi-scale prediction. Note that, the listed methods were not trained under the same settings, or using same backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Extra Experiments Stronger Backbone in PASCAL Context As mentioned in the main paper, our CAA outperforms the SOTA methods <ref type="bibr" target="#b36">(Zhang et al. 2019;</ref><ref type="bibr" target="#b15">Li et al. 2019)</ref> with the same settings (ResNet-101 + naive upsampling). Furthermore, we show our proposed CAA is suitable for different backbones.</p><p>In this section, we report our CAA's performance with Ef-ficientNet <ref type="bibr" target="#b26">(Tan and Le 2019)</ref> in <ref type="table">Table 9</ref>. Note that, this is not a fair comparison, since the listed methods were not trained under the same settings, or using the same backbone. The results show that our method can outperform the state-ofthe-art Transformer <ref type="bibr" target="#b5">(Dosovitskiy et al. 2021)</ref> based hybrid models such as SETR <ref type="bibr" target="#b25">(Sixiao et al. 2021</ref>) and DPT (Ranftl, Bochkovskiy, and Koltun 2021) with the CNN backbone EfficientNet-B7. The simple decoder merges the low level features from output stride = 4, during the final upsampling (see  for details).</p><p>Stronger Backbone in COCOStuff-10K We also report our CAA's results using Efficientnet-b7 <ref type="bibr" target="#b26">(Tan and Le 2019)</ref> as the backbone in <ref type="table" target="#tab_12">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in COCOStuff-164k</head><p>The recent method Segformer <ref type="bibr" target="#b29">(Xie et al. 2021)</ref> used COCOStuff-164k (164,000 images), i.e., the full set of COCOStuff-10k to validate its performance for the first time. Since Segformer is a strong backbone, in this section, we also use EfficientNet-B5 + CAA to verify the robustness of our CAA on COCOStuff-164k. Table 11 shows that our method outperforms the recent strong baselines Segformer and SETR <ref type="bibr" target="#b25">(Sixiao et al. 2021</ref>) by a large margin, indicating our CAA keeps the superior performance with large training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Extra Visualizations</head><p>COCOStuff-10k <ref type="figure" target="#fig_5">Fig. 7</ref> shows some examples of the segmentation results obtained on the COCO-Stuff 10K with our proposed CAA in comparison to the results of FCNs <ref type="bibr" target="#b18">(Long, Shelhamer, and Darrell 2015)</ref>, DANet <ref type="bibr">(Fu et al. 2019)</ref>, and the ground truth (output stride = 8, ResNet-101). As it can be seen, our CAA can segment common objects such as building, human, or sea very well.   Extra PASCAL Context In the main paper, due to the page limit, only 3 images from PASCAL Context are presented. In this section, we show more examples of the segmentation results obtained on the PASCAL Context in <ref type="figure">Fig. 8</ref>. Results show that the failure cases in FCN and DANet are segmented much better by our CAA, especially hard cases (see the 2nd row).</p><p>Cityscapes In <ref type="figure">Fig. 9</ref>, we compare the segmentation results obtained on Cityscapes validation set with DANet and our CAA. Key areas of difference are highlighted with white boxes. Results show that many errors produced by DANet no longer exist in our CAA results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Group Vectorization</head><p>Algorithm 1 presents the pseudo code of implementing the proposed grouped vectorization.</p><p>Effectiveness of Our Grouped Vectorization In our main paper, we introduced the grouped vectorization to split tensors into multiple groups so as to reduce the GPU memory usage when preforming channel attention inside spatial attention. As we use more groups in group vectorization, the proportionally less GPU memory is needed for the computation. However, longer running time is required. In this section, we conduct experiments to show the variation of the inference time (seconds/image) when different numbers of groups are used. <ref type="figure">Fig. 10</ref> shows the results of three different input resolutions. As shown in this graph, when splitting the vectorization into smaller numbers of groups, e.g., 2 or 4, our grouped vectorization can achieve similar inference speed using one half or one quarter of the original spatial complexity. For example, separating into 4 groups has similar inference speed with no separation (1 group).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Extra Technical Details</head><p>Deterministic In our paper, all experiments are conducted with enabled deterministic and fixed seed value to reduce the effect of randomness. However, in current deep learning frameworks (e.g., PyTorch or TensorFlow), completely reproducible results are not guaranteed since not all the operations have a deterministic implementation. To show the robustness of our proposed method, in the ablation studies of the main paper, we repeat each experiment 5 times and report their mean results and standard deviation. To learn more about deterministic, please check "https://pytorch.org/docs/stable/notes/randomness.html" and "https://github.com/NVIDIA/framework-determinism".</p><p>No Extra Tricks In our work, we strictly follow the training settings and implementation of DANet <ref type="bibr">(Fu et al. 2019)</ref> when comparing with other methods. Recently, many settings such as cosine decay learning rate or layer normalization has been widely used in computer vision to boost performance. In our experiments, we found they also worked well for our CAA, but we did not include them in this paper to conduct comparisons as fair as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground truth FCN Dual Attention Ours <ref type="figure">Figure 8</ref>: Examples of the results obtained on the PASCAL Context dataset with our proposed CAA in comparison to the results obtained with FCN, DANet and the ground truth.</p><p>A.5 The Detailed Architecture of CAA Due to the page limits, we are only able to present a partial architecture (row attention) of our CAA in the main paper.</p><p>In this section, we show the complete CAA architecture in <ref type="figure">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground truth Dual Attention Ours </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Our designs for visualizing the effects of dual attentions in parallel and sequential.ImageGround Truth Spatial Attention Channel Attention Prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Conflicting features in parallel dual attention designs. Top: The bad spatial attention representation negatively influences the good channel attention representation. Bottom: The bad channel attention representation negatively influences the good spatial attention representation. See the boxed areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The detailed architecture of the proposed CAA (Row Attention)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Examples of the results obtained on the COCO-Stuff 10K dataset with our proposed CAA in comparison to the results obtained with FCN, DANet and the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Extra examples of the segmentation results obtained on the Cityscapes validation set<ref type="bibr" target="#b19">(Marius et al. 2016</ref>) with our proposed CAA in comparison to the results obtained with DANet(Fu et al. 2019)  and the ground truth.Algorithm 1: Our proposed grouped vectorization algorithmRequire: G: Group Number, A: Attention Map [N, H, H, W ], X: Feature Map [N, C, H, W ] 1: padding ? H % G 2: A ? Transpose A into [H, N, H, W ] 3: H + ? H + padding 4: A ? padding zero to A into [H + , N, H, W ] 5: A ? Reshape A into [G, H + // G, N, H, W ] 6: for g ? G do 7: Yg ? Channelization (X, Ag), Yg ? [H + //G, N, C, W ] 8: end for 9: Y ? Concat(Y0,1,...G), Y ? [G, H + // G, N, C, W ] 10: Y ? Reshape Y into [H + , N, C, W ] 11: Y ? Remove padding from Y into [H, N, C, W ] 12: Y ? Transpose Y into [N, C, H, W ]return Y Inference time (seconds/image) when applying different numbers of groups in grouped vectorization. The detailed architecture of our CAA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Result comparison between axial attention, axial attention + SE and channelized axial attention.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of applying our Channelized Attention on self-attention with ResNet-101. Eval OS: Output strides) during evaluation.</figDesc><table><row><cell cols="3">Methods OS MF Aux</cell><cell>mIOU (%)</cell><cell>FLOPs</cell></row><row><cell>ResNet</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>59.85G</cell></row><row><cell>-101</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>190.70G</cell></row><row><cell>DANet</cell><cell>8</cell><cell></cell><cell></cell><cell>+101.25G</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>52.60</cell><cell>-</cell></row><row><cell>Axial</cell><cell>16</cell><cell></cell><cell>50.27(?0.2)</cell><cell>+8.85G</cell></row><row><cell cols="2">Attention 16</cell><cell></cell><cell>52.01(?0.2)</cell><cell>-</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>51.24(?0.2)</cell><cell>+34.33G</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>52.51(?0.2)</cell><cell>-</cell></row><row><cell>Our</cell><cell>16</cell><cell></cell><cell>51.06(?0.2)</cell><cell>+8.85G</cell></row><row><cell>CAA</cell><cell>16</cell><cell></cell><cell>53.09(?0.3)</cell><cell>-</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>52.73(?0.1)</cell><cell>+34.33G</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>54.05(?0.1)</cell><cell>-</cell></row><row><cell>Our</cell><cell>16</cell><cell></cell><cell>51.80(?0.2)</cell><cell>+8.85G</cell></row><row><cell>CAA</cell><cell>16</cell><cell></cell><cell>53.52(?0.2)</cell><cell>-</cell></row><row><cell>+</cell><cell>8</cell><cell></cell><cell>53.48(?0.3)</cell><cell>+34.33G</cell></row><row><cell>Aux loss</cell><cell>8</cell><cell></cell><cell>54.65(?0.4)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison results with different testing strategies. OS: Output stride in training and inference. MF: Apply multi-scale and left-right flipping during inference. Aux: Add auxiliary loss during training. "+" refers to the extra FLOPS over the baseline FLOPS of ResNet-101.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of other backbones. All results are obtained in single scale without flipping. OS: Output strides during evaluation. AA: Axial Attention. C: Channelized.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>mIOU (%)</cell></row><row><cell cols="2">ENCNet (Zhang et al. 2018) ResNet-101</cell><cell>51.7</cell></row><row><cell>ANNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparisons with other state-of-the-art approaches on the PASCAL Context test set. For a fair comparison, all compared methods used ResNet-101 and naive upsampling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparisons with other state-of-the-art approaches on the COCO-Stuff 10K test set. For a fair comparison, all compared methods use ResNet-101 and naive upsampling.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>mIOU (%)</cell></row><row><cell>CFNet (Zhang et al. 2019)</cell><cell>ResNet-101</cell><cell>79.6</cell></row><row><cell>ANNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparisons with other state-of-the-art approaches on the Cityscapes Test set. For a fair comparison, all compared methods use ResNet-101 and naive upsampling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Result comparison with the state-of-the-art approaches on the COCO-Stuff-10K test set for multi-scale prediction. Note that, the listed methods were not trained under the same settings, or using same backbone.</figDesc><table><row><cell>Methods</cell><cell>mIOU (%)</cell></row><row><cell>ResNet-50 + DeepLabV3+ (Chen et al. 2018)</cell><cell>38.4</cell></row><row><cell>HRNetV2 + OCR</cell><cell>42.3</cell></row><row><cell>SETR (Sixiao et al. 2021)</cell><cell>45.8</cell></row><row><cell>Segformer-B5 (Xie et al. 2021)</cell><cell>46.7</cell></row><row><cell>Our CAA (EfficientNet-B5)</cell><cell>47.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Result comparison with the state-of-the-art approaches on the COCO-Stuff-164K test set for multi-scale prediction. Note that, the listed methods were not trained under same settings, or using same backbone. Methods other than CAA and Segformer are reproduced in Segformer paper.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">COCO-Stuff: Thing and Stuff Classes in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep Learning With Depthwise Separable Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic Correlation Promoted Shape-Variant Context for Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wiliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual Attention Network for Scene Segmentation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Axial Attention in Multidimensional Transformers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CCNet: Criss-Cross Attention for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Based Graph Reasoning for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Expectation-Maximization Attention Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CTNet: Contextbased Tandem Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09805</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic-structured Semantic Propagation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Role of Context for Object Detection and Semantic Segmentation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stand-Alone Self-Attention in Vision Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision Transformers for Dense Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sixiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiachen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hengshuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zekun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yabiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yanwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">; H S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonlocal Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DenseASPP for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context Prior for Scene Segmentation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object-Contextual Representations for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context Encoding for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">ResNeSt: Split-Attention Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic Correlation Promoted Shape-Variant Context for Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Asymmetric Non-local Neural Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
