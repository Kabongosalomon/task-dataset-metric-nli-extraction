<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Question Generation</term>
					<term>Knowledge Graphs</term>
					<term>Natu- ral Language Processing</term>
					<term>Graph Neural Networks</term>
					<term>Deep Learn- ing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph (KG) question generation (QG) aims to generate natural language questions from KGs and target answers. Previous works mostly focus on a simple setting which is to generate questions from a single KG triple. In this work, we focus on a more realistic setting where we aim to generate questions from a KG subgraph and target answers. In addition, most of previous works built on either RNN-based or Transformer-based models to encode a linearized KG sugraph, which totally discards the explicit structure information of a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we enhance our RNN decoder with node-level copying mechanism to allow directly copying node attributes from the KG subgraph to the output question. Both automatic and human evaluation results demonstrate that our model achieves new state-of-the-art scores, outperforming existing methods by a significant margin on two QG benchmarks. Experimental results also show that our QG model can consistently benefit the Question Answering (QA) task as a mean of data augmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recent years have seen a surge of interests in Question Generation (QG) in machine learning and natural language processing. The goal of QG is to generate a natural language (NL) question for a given form of data such as text <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, images <ref type="bibr" target="#b4">[5]</ref>, tables <ref type="bibr" target="#b5">[6]</ref>, and knowledge graphs (KGs) <ref type="bibr" target="#b6">[7]</ref>. In this work, we focus on QG from KGs.</p><p>One of the biggest applications of QG is to provide training data for question answering (QA) systems <ref type="bibr" target="#b7">[8]</ref>. KGs have drawn a large amount of research attention in recent years, partially due to their huge potential for an accessible, natural way of retrieving information without a need for learning complex query languages such as SPARQL. In order to train a large Knowledge Base Question Answering (KBQA) system <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, a large number of labeled question-answer pairs are often needed, which can be a severe bottleneck in practice because human annotation is usually expensive and time-consuming. Developing effective approaches to generate high-quality QA pairs can significantly address the data scarcity issue for KBQA. In addition, QG can be applied for educational purposes by producing practice assessments <ref type="bibr" target="#b10">[11]</ref>. Moreover, QG can help dialog systems have more engaging conversations <ref type="bibr" target="#b12">[12]</ref>. <ref type="bibr">Y</ref>  In the past decade, the research on QG from KGs has gained increasing interest and can be categorized into two classes. The first line of research heavily relies on handcrafted question templates <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b6">[7]</ref>. They typically first construct a structured query (e.g., SPARQL query) and then apply a template-based method to verbalize it to a natural language question. Using a set of pre-designed templates not only requires a significant amount of human effort, thus leading to low generalizability and scalability, but also limits the diversity and fluency of the generated questions. The other line of research adopts a purely data-driven end-to-end approach without resort to any handcrafted templates. Those are mostly neural network based approaches which employ an RNN or Transformer <ref type="bibr" target="#b15">[15]</ref> decoder to generate a natural language question as a sequence of tokens. However, most of them <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref> focus only on generating simple questions, which limits their usage in benefiting complex KBQA systems often requiring multi-hop reasoning. The main reason they can only generate simple questions is due to their incapability of encoding a KG subgraph containing a rich set of interlinked triples. Instead, they can only take a keyword list or a single KG triple (i.e., subject-predicate-object) as the input because they adopt a Sequence-to-Sequence (Seq2Seq) <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref> architecture which can only encode sequential data via a sequence encoder.</p><p>More recently, <ref type="bibr" target="#b23">[23]</ref> presented a Transformer-based Seq2Seq model named MHQG+AE for generating multi-hop complex questions from a KG subgraph. To the best of our knowledge, MHQG+AE was the first neural network-based model focusing on QG from a KG subgraph. Because a Transformer cannot admit graph-structured input data like a KG subgraph, they proposed to represent a KG subgraph as a set of triples where the triple embeddings were computed based on the embeddings of the subject, predicate and object contained in the triple. They arXiv:2004.06015v3 [cs.CL] 12 Mar 2022 also removed the positional encoding in a regular Transformer in order to discard the position information of triples in a KG subgraph. Even though their approach was able to directly work on a KG subgraph for generating more complex questions compared to previous approaches, they failed to effectively utilize the rich structure information of a KG subgraph because they completely ignored the rich interactions among triples in a KG subgraph. In follow-up work, <ref type="bibr" target="#b24">[24]</ref> proposed to augment the input KG subgraph with external knowledge such as entity descriptions/domains, question word types and answer entity types. However, they still failed to respect the rich structure information of the KG subgraph as they simply regarded a KG subgraph as a sequence of subject, predicate and object embeddings, and applied a bidirectional LSTM <ref type="bibr" target="#b25">[25]</ref> to learn its representations. We believe capturing fine-grained structure information is critical for generating high-quality questions.</p><p>We summarize the three challenges of the task of multihop QG from KGs (denoted as KG-QG) as follows. The first one is how to learn a good representation of a KG subgraph. A KG subgraph has complex underlying structures such as node attributes and multi-relational edges. Each node and edge could have (long) associated text comprising multiple words. Previous approaches either only considered a keyword list or single triple for simple question generation or simply regarding the KG subgraph as a set of triples without fully utilizing its rich structure information when generating multi-hop questions. The second challenge is how to automatically learn a good mapping between a subgraph and a natural language question. For instance, it's common for a question to directly mention an entity name from the input KG subgraph. However, it's challenging for previous approaches to precisely generate such an entity name which often contains multiple tokens. The third challenge is how to effectively leverage the answer information. Given a KG subgraph containing many triples, one can generate completely different questions without knowing the exact target answer. Therefore, effectively utilizing the answer information is crucial for generating more relevant questions.</p><p>In order to address the above challenges, we present a subgraph guided knowledge graph question generation approach with Graph Neural Networks (GNNs). To this end, we introduce for the first time the Graph-to-Sequence (Graph2Seq) architecture with a novel node-level copying mechanism for the KG-QG task to address the second challenge. We extend the regular GNN-based encoder to allow processing directed and multi-relational KG subgraphs to solve the first challenge. In addition, we propose a simple yet elegant way to leverage the context information from the answers to effectively handle the third challenge. Extensive experimental results demonstrate that our model significantly outperforms the state-of-the-art baselines by a large margin on two benchmarks and consistently benefits the KBQA task. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the main ideas of various QG from KGs learning paradigms.</p><p>We highlight our main contributions as follows: ? We propose a novel Graph2Seq model for subgraph guided KG-QG. The proposed Graph2Seq model employs bidirectional graph embedding and we design two different GNN encoders to effectively encode KG subgraphs with directed and multi-relational edges.</p><p>? We extend the RNN decoder with a novel copying mechanism that allows the entire node attribute to be borrowed from the input KG subgraph when generating natural language questions. ? We investigate two different ways of initializing node/edge embeddings when applying a GNN encoder to process KG subgraphs. In addition, we study the impact of edge direction on the GNN encoder. ? Experimental results show that our model improves the state-of-the-art BLEU-4 score from 11.57 to 29.40 and from 25.99 to 59.59 on WebQuestions (WQ) and PathQuestions (PQ) benchmarks, respectively. A human evaluation study corroborates that the questions generated by our model are more natural (semantically and syntactically) and relevant compared to other baselines. Experiments also show that our QG model can consistently benefit the KBQA task as a mean of data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Question Generation from Knowledge Graphs</head><p>Early works <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b6">[7]</ref> on QG from KGs are mostly template-based approaches heavily relying on a set of predefined question templates to verbalize a structured query to a natural language question. However, they usually have low generalizability and scalability, and the diversity and fluency of the generated questions are limited due to the nature of template-based approaches. Recently, Seq2Seq-based neural architectures have been applied to this task without resort to manually-designed templates and are end-to-end trainable. However, these approaches <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref> only focus on generating simple questions from a keyword list or single triple as they typically employ an RNN or Transformer based encoder which cannot handle graph-structured data like a KG subgraph. Very recently, Seq2Seq-based approaches were also applied for generating a multi-hop complex question from a KG subgraph instead of just a single triple. However, they still failed to effectively utilize the rich structure information of the KG subgraph by simply regarding a KG subgraph as a set of triples <ref type="bibr" target="#b23">[23]</ref> or a sequence of subject, predicate and object embeddings <ref type="bibr" target="#b24">[24]</ref>. Unlike all previous approaches, in this work, we focus on generating multi-hop complex questions by effectively modeling the rich structures (e.g., edge directions, edge types) of KG subgraphs via a novel GNN-based graph encoder. To the best of our knowledge, we are the first to introduce the Graph2Seq architecture to the KG-QG task.</p><p>There was related work focusing on QG from text. In <ref type="bibr" target="#b2">[3]</ref>, we proposed a Reinforcement Learning based Graph2Seq model for the task of QG from text. Besides the difference in terms of problem settings, the major technical difference between this work and our previous work includes, in this work, i) we extend the GNN encoder to handle multi-relational graphs where in <ref type="bibr" target="#b2">[3]</ref> edge type information was not modeled, and ii) we extend the word-level copying mechanism in <ref type="bibr" target="#b2">[3]</ref> to the node-level copying mechanism. Some recent QG from text works explored leveraging the external knowledge for better performance. For instance, in <ref type="bibr" target="#b26">[26]</ref>, the authors proposed to augment the raw text with auxiliary knowledge retrieved from a KG using entities and keywords mentioned in the input text. Their approach then applies three different GNN-based encoders to encode three types of graphs constructed based on text and knowledge retrieved from a KG. Even though we both adopt a Graph2Seq architecture, we tackle very different problems and they utilize KG as external knowledge for better QG from text performance.</p><p>Our work is also related to recent research efforts on pretrained models for KG-to-text generation <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref> which used KG-to-text generation as one of the pre-training tasks. These large-scale pre-trained models could be used for many downstream KG-to-text applications (including QG from KGs) by finetuning them for a particular downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Neural Networks</head><p>Traditional Deep Learning approaches like Convolutional Neural Networks and Recurrent Neural Networks are designed for Euclidean data like images and text, and thus cannot directly handle non-Euclidean data like graphs. Over the past few years, Graph Neural Networks (GNNs) <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> have drawn increasing attention due to their ability to model graph-structured data and have successfully been applied in the NLP field <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>. Recently, in order to address the limitations of the widely used Seq2Seq architectures <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref> on encoding rich and complex graph-structured data, a number of works have applied the Graph2Seq architectures for various NLP tasks including machine translation <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b40">[40]</ref>, semantic parsing <ref type="bibr" target="#b41">[41]</ref>, code summarization <ref type="bibr" target="#b42">[42]</ref>, and graph-to-text generation (e.g., AMR, SQL and KG to text) <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>. Compared to existing Graph2Seq models, our proposed Graph2Seq model can better handle multi-relational graphs and employ node-level copying mechanism to enable generating more faithful text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH A. Problem Formulation</head><p>Our focus is on natural question generation from a KG subgraph, along with potential target answers; the overall architecture of our approach is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We assume that a KG subgraph is a collection of triples (i.e., subjectpredicate-object), that can also be represented as a graph G " pV, Eq, where V ? V denotes a set of entities (i.e., subjects or objects) and E ? E denotes all the predicates connecting these entities. We denote by V and E the complete entity set and predicate set of the KG, respectively. We also assume that all the answers from the target answer set V a are from the entity set V , which is the normal setting of the task of KBQA <ref type="bibr" target="#b8">[9]</ref>. The task of KG-QG is to generate the best natural language question consisting of a sequence of word tokens? " ty 1 , y 2 , ..., y T u which maximizes the conditional likelihood? " arg max Y P pY |G, V a q where T is the length of the question. We focus on the problem setting where we have a set of KG subgraphs (and answers) and target questions pairs, to learn the mapping; existing QG approaches <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b23">[23]</ref> make a similar assumption. Although the three main challenges we have discussed before are based on QG from KGs, other QG tasks from other data sources also share some or most of issues when dealing with these tasks. Therefore, our model could be generalized to cope with these tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoding Layer</head><p>Let us denote V as a set of nodes (i.e., entities) tv 1 , v 2 , ..., v n u in a KG subgraph G, where each node is associated with some attributes such as text or ID. Similarly, let us denote E as a set of edges (i.e., predicates) te 1 , e 2 , ..., e m u in G, where each edge has some attributes such as text or ID.</p><p>1) Encoding Nodes and Edges: Before applying the GNN encoder to process a KG subgraph, we need to map nodes and edges to an initial embedding space that encodes their attributes. There are two common ways of encoding nodes and edges in a KG. One solution is based on global KG embeddings that are pretrained on the whole KG by some KG representation learning algorithm such as TransE <ref type="bibr" target="#b47">[47]</ref>, while the other one is based on pretrained embeddings (e.g., GloVe <ref type="bibr" target="#b48">[48]</ref>) of the words making up the textual attributes. In this work, we choose to encode nodes and edges based on word embeddings of their textual attributes in our main model. We posit that it is relatively easier for a model to learn the mapping from the input KG subgraph to the output NL question with both sides based on word embeddings. We empirically compare and analyze the two encoding strategies in our experiments. In order to encode the nodes and edges in a KG subgraph, we apply two bidirectional LSTMs <ref type="bibr" target="#b25">[25]</ref> for nodes (i.e., one for nodes, and one for edges) to encode their associated text. The concatenation of the last forward and backward hidden states of the BiLSTM is used as the initial embeddings for nodes and edges.</p><p>2) Utilizing Target Answers: In the setting of KBQA <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b49">[49]</ref>, it is usually assumed that the answers to a question are entities in a KG subgraph. As a dual task of KBQA, in this QG work, we assume that utilizing the target answers along with the KG subgraph can help generate more relevant questions. To this end, we apply a simple yet effective strategy where we introduce an additional learnable markup vector associated with each node/edge to indicate whether it is an answer or not. Therefore, the initial vector representation of a node/edge will be the concatenation of the BiLSTM output and the answer markup vector. We denote X e " tx e 1 , x e 2 , ..., x e n u and X p " tx p 1 , x p 2 , ..., x p m u as the embeddings of the entity nodes and predicate edges, respectively. Both X e and X p have the same embedding dimension d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bidirectional Graph2Seq Generator with Copying</head><p>While RNNs are good at modeling sequential data, they cannot naturally handle graph-structured data. One might need to linearize a graph to a sequence so as to apply an RNN-based encoder, which will lose the rich structure information in the graph. Many previous works <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b40">[40]</ref> showed the superiority of GNNs compared to RNNs on modeling graph-structured data. <ref type="bibr" target="#b23">[23]</ref> proposed to encode a set of triples via a Transformer by removing positional encoding in the original architecture. Even though a Transformer-based encoder could learn the semantic relations among the triples through the all-to-all attention, the explicit graph structure is totally discarded. In this work, we introduce a bidirectional GNN-based encoder to encode the KG subgraph, and decode the output question via an RNN-based decoder equipped with node-level copying mechanism.</p><p>1) Bidirectional Graph Encoder: Many existing GNNs <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b50">[50]</ref> were not designed to process directed graphs such as a KG. Even though some GNN variants such as GGSNN <ref type="bibr" target="#b32">[32]</ref> and MPNN <ref type="bibr" target="#b30">[30]</ref> are able to handle directed graphs via message passing across graphs, they do not model the bidirectional information when aggregating information from neighboring nodes for each node. As a result, messages can only be passed across graphs in a unidirectional way.</p><p>In this work, we introduce the Bidirectional Gated Graph Neural Network (BiGGNN) which extends GGSNN by learning node embeddings from both incoming and outgoing directions in an interleaved fashion when processing a directed graph. A similar bidirectional approach has been exploited in <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b51">[51]</ref> to extend other GNN variants. While their methods simply learn the node embeddings of each direction independently and concatenate them at last step, BiGGNN fuses the intermediate node embeddings from both directions at every iteration.</p><p>The embedding h 0 v for node v is initialized to x v , namely, a concatenation of the BiLSTM output and the answer markup vector. BiGGNN then performs message passing across the graph for a fixed number of hops, with the same set of network parameters shared at each hop. At each hop of computation, for every node in the graph, we apply an aggregation function that takes as input a set of incoming (or outgoing) neighboring node vectors and outputs a backward (or forward) aggregation vector. In principle, many order-invariant operators such as max or attention <ref type="bibr" target="#b50">[50]</ref> can be employed to aggregate neighborhood information. Here we use a simple average aggregator:</p><formula xml:id="formula_0">h k N %pvq " AVGpth k?1 v u Y th k?1 u , @u P N %pvq uq h k N $pvq " AVGpth k?1 v u Y th k?1 u , @u P N $pvq uq<label>(1)</label></formula><p>where N %pvq and N $pvq denote the incoming and outgoing neighbors of node v. We then fuse the node embeddings aggregated from both directions,</p><formula xml:id="formula_1">h k N pvq " Fuseph k N %pvq , h k N $pvq q (2)</formula><p>The fusion function is computed as a gated sum of two information sources,</p><formula xml:id="formula_2">Fusepa, bq " z d a`p1?zq d b z " ?pW z ra; b; a d b; a?bs`b z q<label>(3)</label></formula><p>where d is the component-wise multiplication, ? is a sigmoid function, and z is a gating vector. The gate helps the model to determine how much of the information needs to be reserved from the two aggregated node embeddings. Finally, a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b22">[22]</ref> is used to update the node embeddings by incorporating the aggregation information.</p><formula xml:id="formula_3">h k v " GRUph k?1 v , h k N pvq q<label>(4)</label></formula><p>After n hops of GNN computation where n is a hyperparameter, we obtain the final state embedding h n v for node v. To compute the graph-level embedding, we first apply a linear projection to the node embeddings, and then apply max-pooling over all node embeddings to get a d-dim vector h G .</p><p>2) Handling Multi-relational Graphs: KGs are typically heterogeneous networks that contain a large number of edge types. However, many existing GNNs <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b50">[50]</ref> are not directly applicable to multi-relational graphs. In order to model both node and edge information with GNNs, researchers have extended them by either having separate learnable weights for different edge types or having explicit edge embeddings when performing message passing <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b52">[52]</ref>. While the former solution may have severe scalability issues when handling graphs with a large number of edge types, the later one requires major modifications to existing GNNs. In this work, we explore two solutions to adapt GNNs to multi-relational graphs. Levi graph transformation. We can directly apply regular GNNs to a multi-relational KG subgraph by converting it to a Levi graph <ref type="bibr" target="#b53">[53]</ref>. Specifically, we treat all edges in the original graph as new nodes and add new edges connecting original nodes and new nodes, which results in a bipartite graph. For instance, in a KG subgraph, a triple (Mario Siciliano, place of birth, Rome) will be converted to "Mario Siciliano ? place of birth ? Rome" where "place of birth" becomes a new node, and ? indicates a new edge connecting an entity and a predicate. Note that since most KG subgraphs are sparse, the number of newly added nodes (and edges as well) will at most be linear to the number of original nodes. Gated message passing with edge information. We also extend BiGGNN to explicitly incorporate edge embeddings when conducting message passing, calling the resultant variant as BiGGNN edge . Specifically, we rewrite the node aggregation function Eq. (1) as follows,</p><formula xml:id="formula_4">h k N %pvq " AVGpth k?1 v u Y tf prh k?1 u ; euvsq, @u P N %pvq uq h k N $pvq " AVGpth k?1 v u Y tf prh k?1 u ; euvs, @u P N $pvq uq<label>(5)</label></formula><p>where f is a nonlinear function (i.e., linear projection + ReLU <ref type="bibr" target="#b54">[54]</ref>) applied to the concatenation of h k?1 u and e uv which is the embedding of the edge connecting node u and v.</p><p>3) RNN Decoder with Node-level Copying: We adopt an attention-based <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b56">[56]</ref> LSTM decoder that generates the output sequence one word at a time. The decoder takes the graph-level embedding h G followed by two separate fullyconnected layers as initial hidden states (i.e., c 0 and s 0 ) and the node embeddings th n v , @v P Gu as the attention memory. The particular attention mechanism used in our decoder closely follows <ref type="bibr" target="#b57">[57]</ref>. Basically, at each decoding step t, an attention mechanism learns to attend to the most relevant nodes in the input graph, and computes a context vector ht based on the current decoding state s t and the attention memory.</p><p>We hypothesize that when generating NL questions from a KG subgraph, it is very likely to directly mention (i.e., copy) entity names that are from the input KG subgraph even without rephrasing them. When augmented with copying mechanism <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>, most RNN decoders are typically allowed to copy words from the input sequence. We extend the regular word-level copying mechanism to the node-level copying mechanism that allows copying node attributes (i.e., node text) from the input graph. Copying mechanism was used in some previous Graph2Seq papers <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b60">[60]</ref>. The most similar work is <ref type="bibr" target="#b60">[60]</ref> which proposed to copy both entities and predicates from the input graph. Unlike <ref type="bibr" target="#b60">[60]</ref>, we use masked copying mechanism to only copy entity nodes in the transformed Levy graph and do not copy predicate nodes. This is because we assume that for the KG-QG task, it is very likely for humans to directly mention entity names but not necessarily for predicate names that are from the KG.</p><p>At each decoding step, the generation probability p gen P r0, 1s is calculated from the context vector ht , the decoder state s t and the decoder input y t?1 . Next, p gen is used as a soft switch to choose between generating a word from the vocabulary or copying a node attribute from the input graph. We dynamically maintain an extended vocabulary which is the union of the usual vocabulary and all node names appearing in a batch of source examples (i.e., KG subgraphs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training and Testing</head><p>As customary for training sequential models, we minimize the following cross-entropy loss,</p><formula xml:id="formula_5">L " ? t?l og P pyt |X, y? t q<label>(6)</label></formula><p>where yt is the word at the t-th position of the gold output sequence. Scheduled teacher forcing <ref type="bibr" target="#b61">[61]</ref> is adopted to alleviate the exposure bias problem. During the testing phase, beam search is applied to generate the output. Two-stage training strategy.</p><p>Most prior works on QG employ cross-entropy based training objective, which is also a de facto choice for training sequential models in many other NLP tasks. However, cross-entropy based training strategy has some known limitations including exposure bias and evaluation discrepancy between training and testing <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b64">[64]</ref>. That is to say, during training, a model has access to the ground-truth previous token when decoding and is optimized toward cross-entropy loss, while during testing, no ground-truth previous token is provided and cross-entropy loss is not used for evaluation.</p><p>To tackle these issues, besides training our proposed model with the regular cross-entropy loss, we also explore a twostage training strategy where we first train the model with cross-entropy loss, and then finetune the model with a hybrid loss combining both the cross-entropy loss and Reinforcement Learning (RL) <ref type="bibr" target="#b65">[65]</ref> loss. The RL loss is defined based on evaluation metrics, enabling us to directly optimize the model towards the evaluation metrics. The reason we need the first stage training is because training models from scratch using RL is often challenging. The regular cross-entropy training can help us obtain a reasonably good performing model, and the RLbased finetuning can further improve the model performance.</p><p>In the first stage, the regular cross-entropy loss is used,</p><formula xml:id="formula_6">L lm " ? t?l og P pyt |X, y? t q<label>(7)</label></formula><p>as in Eq. (6). In the second stage, we further finetune the model by optimizing a hybrid objective function combining both cross-entropy loss and RL loss, defined as,</p><formula xml:id="formula_7">L " ?L rl`p 1??qL lm<label>(8)</label></formula><p>where ? is a scaling factor controlling the trade-off between the two losses. While our architecture is agnostic to the specific RL algorithm, in this work, we employ an efficient yet effective RL approach called self-critical sequence training (SCST) <ref type="bibr" target="#b66">[66]</ref> to directly optimize the discrete evaluation metrics. SCST is an efficient REINFORCE algorithm that utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. At each training iteration, the RL loss is defined by comparing the reward of the sampled output Y s with the reward of the baseline output? , </p><p>where Y s is produced by multinomial sampling, that is, each word y s t is sampled according to the likelihood P py t |X, y ?t q predicted by the generator, and? is obtained by greedy search, that is, by maximizing the output probability distribution at each decoding step. As we can see, minimizing the above loss is equivalent to maximizing the likelihood of some sampled output that has a higher reward than the corresponding baseline.</p><p>One of the key factors for RL is to pick the proper reward function. We define rpY q as the reward of an output sequence Y , computed by comparing it to the corresponding ground-truth sequence Y?with some reward metric which is a combination of our evaluation metrics (i.e., we used BLEU-4 and ROUGE-L scores in our experiments). This lets us directly optimize the model towards the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we conduct extensive experiments to evaluate the effectiveness of our proposed model for the QG task. We also conduct experiments to examine whether our QG model can help the QA task by providing more training data. Besides, we want to examine whether the introduced GNNbased encoder works better than an RNN-based or Transformerbased encoder when encoding a KG subgraph for the QG task. In addition, we explore and analyze two different ways of handling multi-relational graphs with GNNs. Moreover, we empirically compare two different ways of initializing node and edge embeddings before feeding them into a GNN-based encoder. An experimental comparison between bidirectional GNN-based encoder and unidirectional GNN-based encoder is also provided. The code and data will be released upon the paper acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Methods</head><p>We compare our model against the following baselines: i) L2A <ref type="bibr" target="#b0">[1]</ref>, ii) Transformer (w/ copy) <ref type="bibr" target="#b15">[15]</ref>, iii) MHQG+AE <ref type="bibr" target="#b23">[23]</ref>, iv) JointGT (T5) <ref type="bibr" target="#b28">[28]</ref>, and v) JointGT (BART) <ref type="bibr" target="#b28">[28]</ref>. To the best of our knowledge, MHQG+AE was probably the first neural network-based model that focused on QG from a KG subgraph. Their proposed model, called MHQG+AE, employs a Transformer-based encoder <ref type="bibr" target="#b15">[15]</ref> to encode a KG subgraph (i.e., a set of triples), and generates an output question with a Transformer-based decoder. L2A is a LSTM-based Seq2Seq model equipped with attention mechanism, which takes as input a linearized KG subgraph. It was included in <ref type="bibr" target="#b23">[23]</ref> as a baseline. The results of L2A reported here are taken from <ref type="bibr" target="#b23">[23]</ref>. We also include a Transformer-based encoderdecoder model <ref type="bibr" target="#b67">[67]</ref> with copying mechanism that takes as input a linearized KG subgraph, i.e., a sequence of triples where each triple is represented as a sequence of tokens containing the subject name, predicate name and object name. Hence, after the transformation, a KG subgraph becomes a sequence of tokens. Note that the Transformer baseline included in our experiments encodes the word sequence that is linearized from a KG subgraph, while the MHQG+AE model encodes the triple set contained in a KG subgraph by removing the positional encoding in a regular Transformer architecture. Unlike MHQG+AE that takes as input a set of triple embeddings that are pretrained by a knowledge-base representation learning framework called TransE <ref type="bibr" target="#b47">[47]</ref>, the Transformer baseline takes a sequence of word embeddings as input. We used the open-source implementation <ref type="bibr" target="#b67">[67]</ref> of the Transformer-based encoder-decoder model that is equipped with copying mechanism. Lastly, we also include two largescale pre-trained KG-to-text models JointGT (T5) and JointGT (BART) <ref type="bibr" target="#b28">[28]</ref> which were finetuned for the task of QG from KGs. We do not include <ref type="bibr" target="#b24">[24]</ref> as our baseline because their approach augmented the input KG subgraph with various types of external knowledge such as entity descriptions, entity domains, question word types and answer entity types, which makes it unfair to directly compare the performance of their approach with our approach. In their original paper, the authors reported that their ablated system without using auxiliary knowledge (i.e., but it still utilized the additional question word type information, see the results in their <ref type="table">Table 3</ref>) significantly underperformed our approach (denoted as BiGraph2Seq in their <ref type="table">Table 2</ref>) on two benchmarks (i.e., 3.11 absolute BLEU-4 gap and 0.64 absolute BLEU-4 gap).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data and Metrics</head><p>Following <ref type="bibr" target="#b23">[23]</ref>, we used WebQuestions (WQ) and PathQuestions (PQ) 1 as our benchmarks where both of them use Freebase <ref type="bibr" target="#b68">[68]</ref> as the underlying KG. The WQ dataset combines examples from WebQuestionsSP <ref type="bibr" target="#b69">[69]</ref> and ComplexWebQuestions <ref type="bibr" target="#b70">[70]</ref> where both of them are KBQA benchmarks that contain natural language questions, corresponding SPARQL queries and answer entities. For each instance in WQ, in order to construct the KG subgraph, <ref type="bibr" target="#b23">[23]</ref> converted its SPARQL query to return a subgraph instead of the answer entity, by changing it from a SELECT query to a CONSTRUCT query. The WQ dataset <ref type="bibr" target="#b23">[23]</ref> contains 18,989/2,000/2,000 (train/development/test) examples. The PQ dataset <ref type="bibr" target="#b71">[71]</ref> is similar to WQ except that the KG subgraph in PQ is a path between two entities that span two or three hops. The PQ dataset contains 9,793/1,000/1,000 (train/development/test) examples. Brief statistics of the two datasets are provided in <ref type="table" target="#tab_1">Table I.</ref> Following previous works, we use BLEU-4 <ref type="bibr" target="#b72">[72]</ref>, ME-TEOR <ref type="bibr" target="#b73">[73]</ref> and ROUGE-L <ref type="bibr" target="#b74">[74]</ref> as automatic evaluation metrics. Initially, BLEU-4 and METEOR were designed for evaluating machine translation systems and ROUGE-L was designed for evaluating text summarization systems. We also conduct a human evaluation study on WQ. Generated questions are rated (i.e., range 1-5) based on whether they are syntactically correct, semantically correct and relevant to the KG subgraph. More specially, we conducted a small-scale (i.e., 50 random examples per system) human evaluation study on the WQ test set. We asked 6 human evaluators to give feedback on the quality of questions generated by a set of anonymized competing systems. In each example, given a KG subgraph, target answers and an anonymized system output, they were asked to rate the quality of the output by answering the following three questions: i) is this generated question syntactically correct? ii) is this generated question semantically correct? and iii) is this generated question relevant to the KG subgraph and target answers? For each evaluation question, the rating scale is from 1 to 5 where a higher score means better quality (i.e., 1: Poor, 2: Marginal, 3: Acceptable, 4: Good, 5: Excellent). Responses from all evaluators were collected and averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Settings</head><p>We keep and fix the 300-dim GloVe <ref type="bibr" target="#b48">[48]</ref> vectors for those words that occur more than twice in the training set. The dimensions of answer markup embeddings are set to 32 and 24 for WQ and PQ, respectively. We set the hidden state size of BiLSTM to 150 so that the concatenated state size for both directions is 300. The size of all other hidden layers is set to 300. We apply a variational dropout <ref type="bibr" target="#b75">[75]</ref> rate of 0.4 after word embedding layers and 0.3 after RNN layers. The label smoothing ratio is set to 0.2. The number of GNN hops is set to 4. During training, in each epoch, we set the initial teacher forcing probability to 0.8 and exponentially increase it to 0.8?0.9999 i where i is the training step. In addition, partial teacher forcing is adopted, which means that when generating a sequence, some steps can be teacher forced and some not. We use Adam <ref type="bibr" target="#b76">[76]</ref> as the optimizer. The learning rate is set to 0.001. We reduce the learning rate by a factor of 0.5 if the validation BLEU-4 score stops improving for three epochs. We stop the training when no improvement is seen for 10 epochs. We clip the gradient at length 10. The batch size is set to 30. The beam search width is set to 5. In the RL fine-tuning experiments, we set ? in the mixed loss function Eq. (8) to 0.02 for WQ and 0.07 for PQ. And the ratios of BLEU-4 score and ROUGE-L score for computing the reward are set to 1 and 0.02, respectively. We set the learning rate to 0.00001 and 0.00002 for WQ and PQ, respectively. All hyperparameters are tuned on the development set. Experiments were conducted on a machine which has an Intel i7-2700K CPU and an Nvidia Titan Xp GPU with 16GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>Automatic evaluation results. <ref type="table" target="#tab_1">Table II</ref> shows the evaluation results comparing our proposed models against other state-ofthe-art baseline methods on WQ and PQ test sets. As we can see, our models outperform all QG baselines by a large margin on both benchmarks. This verifies the effectiveness of the proposed model. Besides, we can clearly see the advantages of GNNbased encoders for modeling KG subgraphs, by comparing our model with RNN-based (i.e., L2A) and Transformerbased (i.e., Transformer, MHQG+AE) baselines. Compared to our Graph2Seq model, both RNN-based and Transformerbased baselines ignore the explicit graph structure of a KG subgraph, which leads to degraded performance. Although RNNs are suitable for processing sequential data such as text, they are incapable of modeling graph-structured data such as a KG subgraph. To apply the RNN-based L2A model to a KG subgraph, <ref type="bibr" target="#b23">[23]</ref> linearized the graph to a sequence during preprocessing. However, this inevitably ignores the rich structure information in the graph. Recently, the Transformer <ref type="bibr" target="#b15">[15]</ref> has become a good alternative to the RNN when processing sequential data. Even though a Transformer might be able to learn the semantic relations among the sequence elements through all-to-all attention, the explicit graph structure of a KG subgraph is totally discarded by the model. Given these limitations, as shown in our experiments, both of the two Transformer-based Seq2Seq baselines significantly underperform our GNN-based Graph2Seq model. Interestingly, the Transformer baseline performs reasonably well on PQ, but dramatically fails on WQ. We speculate this is because PQ is more friendly to sequential models such as Transformer as the KG subgraph in PQ is more like path-structure while the one in WQ is more like tree-structure.</p><p>The comparisons with large-scale pre-trained KG-to-text models further demonstrated the superiority of our models. Without access to a large amount of pre-training data, our best performing model clearly outperforms the large-scale model JointGT (T5) and achieves competitive results compared to JointGT (BART). We also compare two variants of our model (i.e., G2S vs. G2S edge ) for handling multi-relational graphs. As shown in <ref type="table" target="#tab_1">Table II</ref>, directly applying the BiGGNN encoder to a Levi graph which is converted from a KG subgraph works quite well. The proposed BiGGNN edge model can directly handle multi-relational graphs without modifying the input graph. However, it performs slightly worse than the Levi graph solution. We can improve the modeling power of BiGGNN edge by updating edge embeddings in the message passing process and attending to edges in the attention mechanism. Currently, these two features are missing in BiGGNN edge . We leave these extensions as future work. Human evaluation results. We conduct a human evaluation study to assess the quality of the questions generated by our model, the Transformer baseline, and the ground-truth data in terms of syntax, semantics and relevance metrics. In addition, an overall score is computed for each example by taking the average of the three scores. As shown in <ref type="table" target="#tab_1">Table III</ref>, overall, we can see that our model achieves good results even compared to the ground-truth, and outperforms the Transformer baseline. Interestingly, we observe that the Transformer baseline gets high syntactic and semantic scores, but very poor relevant scores. After manually examining some generated questions, we noticed that it generates many fluent and meaningful questions that are by no means relevant to the given KG subgraph. However, our model is able to generate more relevant questions possibly by better capturing the KG semantics and the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>As shown in <ref type="table" target="#tab_1">Table IV</ref>, we perform an ablation study to assess the performance impacts of different model components. First of all, the node-level copying mechanism contributes a lot to the overall model performance. By turning it off, we observe significant performance drops on both benchmarks. This verifies our assumption that when generating questions from a KG subgraph, one usually directly copies named entities from the input KG subgraph to the output question. Besides, the answer information is also important for generating relevant questions. Even with the simple answer markup technique, we can see the performance boost on both benchmarks.   F. Model Analysis 1) Effect of Node/Edge Embedding Initialization: We empirically compare two different ways of initializing node/edge embeddings when applying the Graph2Seq model. As shown in <ref type="table" target="#tab_5">Table V</ref>, encoding nodes and edges based on word embeddings of their textual attributes works better than based on their KG embeddings. This might be because it is difficult for a NN-based model to learn the gap between KG embeddings on the encoder side and word embeddings on the decoder side. With the word embedding-based encoding strategy, it is relatively easier for a model to learn the mapping from the input KG subgraph to the output NL question. It also seems that modeling local dependency within the subgraph without utilizing the global KG information is enough for generating meaningful questions from a KG subgraph.      2) Impact of Directionality on GNN Encoder: As show in <ref type="table" target="#tab_1">Table VI</ref>, we compare the performance of bidirectional Graph2Seq with unidirectional (i.e., forward and backward) Graph2Seq. We observe that utilizing the edge direction information in the KG subgraph via bidirectional GNNs can significantly improve the model performance.</p><p>3) Results on the Two-stage Training Strategy: <ref type="table" target="#tab_1">Table VII and  Table VIII</ref> show the results of training our proposed G2S+AE  model with a hybrid objective combining both cross-entropy loss and RL loss following the two-stage training strategy. We denote this variant as G2S+AE+RL. While the RL-based training strategy boosts the model performance on WQ, it does not help the model training on PQ. We suspect this is because the PQ dataset is easier compared to the WQ dataset, therefore the benefit of RL-based finetuning on PQ is less significant. In order to study how the RL ratio ? affects the model performance, we report the test BLEU-4 scores on WQ corresponding to different values of ?, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. As we can see, compared to ? " 0 which means no RL-based finetuning is applied, increasing the value of ? can help the model performance until certain point. 4) Effect of the Number of GNN Hops: <ref type="figure" target="#fig_3">Fig. 3</ref> shows the impact of the number of GNN hops when applying a GNNbased encoder to encode the KG subgraph in WQ. It indicates that increasing the number of GNN hops can boost the model performance until some optimal value. 5) Effect of the Beam Search Size: <ref type="figure" target="#fig_5">Fig. 5</ref> shows the impact of the beam size when applying beam search decoding during the testing phase on PQ. It indicates that beam search decoding significantly outperforms greedy search decoding (i.e., beam size = 1) and increasing the beam size can boost the model performance until some optimal value. 6) Convergence Analysis: <ref type="figure" target="#fig_6">Fig. 6</ref> shows the changes of validation BLEU-4 scores over training epochs on PQ. As we can see, the model was able to converge quickly and achieved the best validation BLEU-4 score after epoch 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Case Study</head><p>As shown in <ref type="table" target="#tab_1">Table IX</ref>, we conducted a case study to examine the quality of generated questions using different ablated systems. First of all, by initializing node/edge embeddings with KG embeddings, the model fails to generate reasonable questions. As we discussed in Section IV-F1, this might be because of the semantic gap between KG embeddings on the encoder side and word embeddings on the decoder side. Besides, with the node-level copying mechanism, the model was able to directly copy the entity name "giza necropolis" from the input KG subgraph into the output question. Last, incorporating the answer information helps generate more relevant and specific questions. For instance, given the target answer "Egypt", the model was able to produce a more specific question which is specifically asking for "what country" instead of "where". KG subgraph: (martin luther king , jr ., speeches or presentations, /m/05r7ddy), (martin luther king , jr ., profession, writer), (martin luther king , jr ., profession, minister of religion), (martin luther king , jr ., profession, civil rights activist), (/m/05r7ddy, event, march on washington for jobs and freedom) Gold: who was the speaker at march on washington for jobs and freedom facts ? G2S+AE: who was the speaker in the march on washington for jobs and freedom ? KG subgraph: (family guy, theme song, family guy theme song), (family guy, regular cast, /m/02ntr0s), (/m/02ntr0s, actor, alex borstein), (/m/02ntr0s, character, lois griffin), (/m/02ntr0s, special performance type, voice) Gold: who ' s the voice of stewie griffin from the tv program , with the family guy theme song ? G2S+AE: who is the voice of the voice of the tv program with the family guy family guy theme song family guy theme song <ref type="table" target="#tab_10">Table X</ref> shows some failure cases of our proposed G2S+AE model on the WQ test set. One common syntactic error pattern we observed is repeated words (e.g., repeated "the voice of" in the second example) in generated questions. Another error pattern is missing important pieces of information. For instance, in the second example 2 , our model failed to utilize the tuple (/m/02ntr0s, character, lois griffin) when generating the question. The coverage mechanism <ref type="bibr" target="#b77">[77]</ref> is widely used in Seq2Seq models to encourage the full utilization of different tokens in the input text and penalize generating repetitive text. However, in our experiments, we found applying the coverage mechanism did not help improve the overall evaluation scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Error Analysis</head><p>We conjecture this might be because the coverage mechanism can also be too aggressive by encouraging the model to utilize irrelevant tuples in the input KG subgraph. <ref type="figure" target="#fig_7">Fig. 7</ref> and <ref type="figure" target="#fig_8">Fig. 8</ref> show the distributions of frequent trigram prefixes (i.e., frequency less than 5 not included) of the generated questions and golden questions on the WQ test set. As we can see, our G2S+AE model was able to generate diverse questions which have a similar distribution of trigram prefixes in comparison with the golden questions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Visualization of the Generated Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. QG-Driven Data Augmentation for QA</head><p>One of the most important applications of QG is to generate more training data for QA tasks. In this section, we use our proposed QG model to generate more questions for training KBQA methods. We use WQ as our KBQA benchmark, and randomly split it to 40%/20%/40% (train/dev/test) examples. As for the KBQA baseline, we use the state-of-the-art KBQA model called BAMnet <ref type="bibr" target="#b8">[9]</ref> which directly retrieves answers from a KG by mapping questions and candidate answers into a joint embedding space. In order to examine the effect of QG-driven data augmentation on the KBQA task, we compare the BAMnet baseline with its two data augmentation variants, namely, BAMnet w/ Transformer and BAMnet w/ G2S+AE. More specifically, the BAMnet baseline is trained only on the part (i.e., x% of the whole training data) where gold questions are available, while the other two variants are trained on the combination of the gold questions and the questions automatically generated by two QG models. Note that given x% training data, we randomly split it to 80%/20% (train/dev) for training a QG model.</p><p>We gradually increase the proportion (i.e., x%) of the training data used for training KBQA models, and report the F1 score performance of the above three KBQA model variants. Here F1 score measures the overlap between the predicted and ground-truth answer set. <ref type="figure" target="#fig_9">Fig. 9</ref> shows the results on improving the BAMnet baseline with automatically generated questions. Interestingly, both QG models consistently help improve the KBQA performance when varying x% training data, and the performance boost is the most significant when training data is scarce. Notably, our G2S+AE model consistently outperforms the Transformer model in improving the KBQA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we introduced a novel bidirectional Graph2Seq model for the KG-QG task. A novel node-level copying mechanism was proposed to allow directly copying node attributes from the KG subgraph to the output question. We explored different ways of initializing node/edge embeddings and handling multi-relational graphs. Our model outperforms existing methods by a significant margin on two benchmarks.</p><p>In our experiments, we observed that node/edge embedding initialization has a big impact on the overall model performance. We would like to explore more effective ways of initializing node/edge embeddings in the future. Besides, how to effectively utilize the answer information is critical for generating relevant and meaningful questions. In this work, we introduced simple markup vectors to indicate whether an entity is a target answer or not. We leave more effective ways of answer utilization as future work. It's also beneficial to design more effective mechanisms to penalize generating repetitive text and encourage fully utilizing important information in the input KG subgraph. Another interesting direction is to integrate the QG model with KG completion systems. We expect this can be extremely beneficial when the input KG is incomplete, and can potentially lead to generating more interesting and diverse questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Various QG from KGs learning paradigms. Left: template-based approaches. Middle: Seq2Seq-based approaches for simple QG from a single KG triple or multi-hop QG from a KG subgraph. Right (ours): Graph2Seq-based approaches for multi-hop QG from a KG subgraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overall architecture of our proposed model. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>rl " prp? q?rpY s qq ? t log P py s t |X, y s ?t q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Effect of number of GNN hops for G2S+AE on WQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Effect of RL ratio for G2S+AE+RL on WQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Effect of beam search size for G2S+AE on PQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Convergence analysis for G2S+AE on PQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Distribution of trigram prefixes of questions generated by G2S+AE on the WQ test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Distribution of trigram prefixes of golden questions in the WQ test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Performance of QG-driven KBQA baseline under different proportions of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Chen was with the Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180. Email: hugochan2013@gmail.com.</figDesc><table /><note>L. Wu was with IBM Research AI, Yorktown Heights, NY 10598. Email: lwu@email.wm.edu.M. J. Zaki is with the Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180. Email: zaki@cs.rpi.edu.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Data statistics. The min/max/avg statistics are reported on the queries and KG subgraph triples.</figDesc><table><row><cell cols="3">Data # examples # entities</cell><cell cols="3"># predicates # triples query length</cell></row><row><cell>WQ</cell><cell>22,989</cell><cell>25,703</cell><cell>672</cell><cell>2/99/5.8</cell><cell>5/36/15</cell></row><row><cell>PQ</cell><cell>9,731</cell><cell>7,250</cell><cell>378</cell><cell>2/3/2.7</cell><cell>8/25/14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Automatic evaluation results on WQ and PQ. The methods marked with : are large-scale pre-trained KG-to-text models finetuned on QG data while other methods do not have access to such pre-training data. The results marked in bold and with?indicate the best and second best results, respectively.</figDesc><table><row><cell>Method</cell><cell>BLEU-4</cell><cell cols="2">WQ METEOR ROUGE-L</cell><cell>BLEU-4</cell><cell cols="2">PQ METEOR ROUGE-L</cell></row><row><cell>L2A</cell><cell>6.01</cell><cell>25.24</cell><cell>26.95</cell><cell>17.00</cell><cell>19.72</cell><cell>50.38</cell></row><row><cell>Transformer</cell><cell>8.94</cell><cell>13.79</cell><cell>32.63</cell><cell>56.43</cell><cell>43.45</cell><cell>73.64</cell></row><row><cell>MHQG+AE</cell><cell>11.57</cell><cell>29.69</cell><cell>35.53</cell><cell>25.99</cell><cell>33.16</cell><cell>58.94</cell></row><row><cell>JointGT (T5) :</cell><cell>28.95</cell><cell cols="2">31.29?54.47</cell><cell>60.45</cell><cell cols="2">45.38?77.59</cell></row><row><cell>JointGT (BART) :</cell><cell>30.02</cell><cell>32.05</cell><cell>55.60</cell><cell>65.89</cell><cell>48.25</cell><cell>78.87</cell></row><row><cell>G2S+AE (ours)</cell><cell cols="2">29.45?30.96</cell><cell cols="3">55.45?61.48?44.57</cell><cell>77.72G</cell></row><row><cell>2S edge +AE (ours)</cell><cell>29.40</cell><cell>31.12</cell><cell>55.23</cell><cell>59.59</cell><cell>44.70</cell><cell>75.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Human evaluation results (?standard deviation) on the WQ test set. The rating scale is from 1 to 5 (higher scores indicate better results).</figDesc><table><row><cell>Method</cell><cell>Syntactic</cell><cell>Semantic</cell><cell>Relevant</cell><cell>Overall</cell></row><row><cell>Transformer</cell><cell cols="2">4.53 (0.18) 4.58 (0.22)</cell><cell>2.65 (0.57)</cell><cell>3.92 (0.24)</cell></row><row><cell>G2S+AE</cell><cell>4.18 (0.30)</cell><cell>4.30 (0.27)</cell><cell>4.26 (0.34)</cell><cell>4.25 (0.26)</cell></row><row><cell>Ground-truth</cell><cell>4.30 (0.15)</cell><cell>4.50 (0.18)</cell><cell cols="2">4.32 (0.32) 4.38 (0.19)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study on WQ and PQ.</figDesc><table><row><cell>Method</cell><cell cols="3">WQ BLEU-4 METEOR ROUGE-L</cell><cell cols="3">PQ BLEU-4 METEOR ROUGE-L</cell></row><row><cell>G2S+AE</cell><cell>29.45</cell><cell>30.96</cell><cell>55.45</cell><cell>61.48</cell><cell>44.57</cell><cell>77.72</cell></row><row><cell>G2S</cell><cell>28.43</cell><cell>30.13</cell><cell>54.44</cell><cell>60.68</cell><cell>44.07</cell><cell>75.94</cell></row><row><cell>G2S w/o copy</cell><cell>22.95</cell><cell>26.99</cell><cell>51.05</cell><cell>57.10</cell><cell>42.66</cell><cell>74.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Effect of node/edge init. embeddings on WQ.</figDesc><table><row><cell>Method</cell><cell cols="3">BLEU-4 METEOR ROUGE-L</cell></row><row><cell>w/ word emb.</cell><cell>28.43</cell><cell>30.13</cell><cell>54.44</cell></row><row><cell>w/ KG emb.</cell><cell>22.80</cell><cell>25.85</cell><cell>48.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Impact of directionality for G2S+AE on PQ.</figDesc><table><row><cell>Method</cell><cell cols="3">BLEU-4 METEOR ROUGE-L</cell></row><row><cell>Bidirectional</cell><cell>61.48</cell><cell>44.57</cell><cell>77.72</cell></row><row><cell>Forward</cell><cell>59.59</cell><cell>42.72</cell><cell>75.82</cell></row><row><cell>Backward</cell><cell>59.12</cell><cell>42.66</cell><cell>75.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Results of RL-based G2S+AE on WQ.</figDesc><table><row><cell>Method</cell><cell cols="3">BLEU-4 METEOR ROUGE-L</cell></row><row><cell>G2S+AE</cell><cell>29.45</cell><cell>30.96</cell><cell>55.45</cell></row><row><cell>G2S+AE+RL</cell><cell>29.80</cell><cell>31.29</cell><cell>55.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>Results of RL-based G2S+AE on PQ.</figDesc><table><row><cell>Method</cell><cell cols="3">BLEU-4 METEOR ROUGE-L</cell></row><row><cell>G2S+AE</cell><cell>61.48</cell><cell>44.57</cell><cell>77.72</cell></row><row><cell>G2S+AE+RL</cell><cell>59.21</cell><cell>44.47</cell><cell>77.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX :</head><label>IX</label><figDesc>Generated questions on WQ test set. Target answers are underlined. For the sake of brevity, we only display the lowest level of the predicate hierarchy.KG subgraph: (Egypt, administrative divisions, Cairo), (Giza Necropolis, contained by, Egypt) Gold: what country has the city of cairo and is home of giza necropolis ? G2S w/ KG emb.: what country that contains cairo has cairo as its province ? G2S w/o copy: where is the giza giza located in that has cairo ? G2S: where is the giza necropolis located in that contains cairo ? G2S+AE: what country that contains cairo is the location of giza necropolis ?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X :</head><label>X</label><figDesc>Error analysis on generated questions on WQ test set. Target answers are underlined. For the sake of brevity, we only display the lowest level of the predicate hierarchy.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/liyuanfang/mhqg</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this example, the ground-truth question refers to the entity "stewie griffin" which is not included in the given input KG subgraph.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging context information for natural question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="569" to="574" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reinforcement learning based graphto-sequence model for natural question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic graphs for generating deep questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12704</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual question generation as dual task of visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6116" to="6124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tableto-text: Describing table region with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge questions from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Question answering and question generation as dual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02027</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional attentive memory networks for question answering over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Personalized food recommendation as constrained question answering over a large-scale food knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subburathinam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2021, 2021</title>
		<imprint>
			<biblScope unit="page" from="544" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Good question! statistical ranking for question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06059</idno>
		<title level="m">Generating natural questions about an image</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating quiz questions from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="113" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Question generation from a knowledge base with web exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03807</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06807</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating natural language question-answer pairs from a knowledge graph using a rnn based question generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Zero-shot question generation from knowledge graphs for unseen predicates and entity types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laforest</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06842</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating questions for knowledge bases via incorporating diversified contexts and answer-aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="2431" to="2441" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating factoid questions with question type enhanced representation and attention-based copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1724" to="1734" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Difficulty-controllable multi-hop question generation from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge-enriched, typeconstrained and grammar-guided question generation over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2020, 2020</title>
		<imprint>
			<biblScope unit="page" from="2776" to="2786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diversified query generation guided by knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="897" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">KGPT: knowledge-grounded pre-training for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2020, 2020</title>
		<imprint>
			<biblScope unit="page" from="8635" to="8648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Jointgt: Graph-text joint representation learning for text generation from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2526" to="2538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in NeurIPS 2020, 2020</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Compact graph structure learning via mutual information compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in TheWebConf 2022, 2022</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04675</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A graph-to-sequence model for amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02473</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2020, 2020</title>
		<imprint>
			<biblScope unit="page" from="1230" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graph neural networks for natural language processing: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06090</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning on graphs for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2651" to="2653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09835</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploiting rich syntactic information for semantic parsing with graph-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sheinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07624</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for code summarization via hybrid GNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Siow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph2seq: Graph to sequence learning with attention-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sheinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00823</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sql-to-text generation with graph-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sheinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05255</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09995</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural wikipedian: Generating textual summaries from knowledge base triples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Kaffee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Foodkg: A semantics-driven knowledge graph for food recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ne&amp;apos;eman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Enhancing amr-to-text generation with dual graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00352</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Levi</surname></persName>
		</author>
		<title level="m">Finite geometrical systems: six public lectues delivered in February</title>
		<imprint>
			<date type="published" when="1940" />
		</imprint>
		<respStmt>
			<orgName>at the University of Calcutta. The University of Calcutta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02342</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Selfcritical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02810</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Freebase data dumps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://developers.google.com/freebase" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06643</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">An interpretable reasoning network for multi-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04811</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
