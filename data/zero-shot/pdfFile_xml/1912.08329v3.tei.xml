<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cost Volume Pyramid Based Depth Inference for Multi-View Stereo</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
							<email>josea@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
							<email>miaomiao.liu@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cost Volume Pyramid Based Depth Inference for Multi-View Stereo</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-MVSNet [43]</head><p>Point-MVSNet [5] Ours <ref type="figure">Figure 1</ref>: Point clouds reconstructed by state-of-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref> and our CVP-MVSNet. Best viewed on screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a cost volume-based neural network for depth inference from multi-view images. We demonstrate that building a cost volume pyramid in a coarse-to-fine manner instead of constructing a cost volume at a fixed resolution leads to a compact, lightweight network and allows us inferring high resolution depth maps to achieve better reconstruction results. To this end, we first build a cost volume based on uniform sampling of fronto-parallel planes across the entire depth range at the coarsest resolution of an image. Then, given current depth estimate, we construct new cost volumes iteratively on the pixelwise depth residual to perform depth map refinement. While sharing similar insight with Point-MVSNet as predicting and refining depth iteratively, we show that working on cost volume pyramid can lead to a more compact, yet efficient network structure compared with the Point-MVSNet on 3D points. We further provide detailed analyses of the relation between (residual) depth sampling and image resolution, which serves as a principle for building compact cost volume pyramid. Experimental results on benchmark datasets show that our model can perform 6x faster and has similar performance as state-of-the-art methods. Code is available at https: //github.com/JiayuYANG/CVP-MVSNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-view stereo (MVS) aims to reconstruct the 3D model of a scene from a set of images captured by a cam-era from multiple viewpoints. It is a fundamental problem for computer vision community and has been studied extensively for decades <ref type="bibr" target="#b34">[35]</ref>. While traditional methods before deep learning era have great achievements on the reconstruction of a scene with Lambertian surfaces, they still suffer from illumination changes, low-texture regions, and reflections resulting in unreliable matching correspondences for further reconstruction.</p><p>Recent learning-based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> adopt deep CNNs to infer the depth map for each view followed by a separate multiple-view fusion process for building 3D models. These methods allow the network to extract discriminative features encoding global and local information of a scene to obtain robust feature matching for MVS. In particular, Yao et al. propose MVSNet <ref type="bibr" target="#b41">[42]</ref> to infer a depth map for each view. An essential step in <ref type="bibr" target="#b41">[42]</ref> is to build a cost volume based on a plane sweep process followed by multiscale 3D CNNs for regularization. While effective in depth inference accuracy, its memory requirement is cubic to the image resolution. To allow handling high resolution images, they then adopt a recurrent cost volume regularization process <ref type="bibr" target="#b42">[43]</ref>. However, the reduction in memory requirements involves a longer run-time.</p><p>In order to achieve a computationally efficient network, Chen et al. <ref type="bibr" target="#b4">[5]</ref> work on 3D point clouds to iteratively predict the depth residual along visual rays using edge convolutions operating on the k nearest neighbors of each 3D point. While this approach is efficient, its run-time increases almost linearly with the number of iteration levels.</p><p>In this work, we propose a cost volume pyramid based Multi-View Stereo Network (CVP-MVSNet) for depth inference. In our approach, we first build an image pyramid for each input image. Then, for the coarsest resolution of the reference image, we build a compact cost volume by sampling the depth across the entire depth-range of a scene. After that, at the next pyramid level, we perform residual depth search from the neighbor of the current depth estimate to construct a partial cost volume using multi-scale 3D CNNs for regularization. As we build these cost volumes iteratively with a short search range at each level, it leads to a small and compact network. As a result, our network performs 6x faster than current state-of-the-art networks on benchmark datasets.</p><p>While it is noteworthy that we share the similar insight with <ref type="bibr" target="#b4">[5]</ref> as predicting and refining the depth map in a coarse-to-fine manner, our work differs from theirs in the following four main aspects. First, the approach in <ref type="bibr" target="#b4">[5]</ref> performs convolutions on 3D point cloud. Instead, we construct cost volumes on a regular grid defined on the image coordinates, which is shown to be faster in run-time. Second, we provide a principle for building a compact cost volume pyramid based on the correlation between depth sampling and image resolution. As third main difference, we use a multi-scale 3D-CNN regularization to cover large receptive field and encourage local smoothness on residual depth estimates which, as shown in <ref type="figure">Fig. 1</ref>, leads to a better accuracy. Finally, in contrast to <ref type="bibr" target="#b4">[5]</ref> and other related works, our approach can output depth of small resolution with small resolution image.</p><p>In summary, our main contributions are</p><p>? We propose a cost-volume based, compact, and computational efficient depth inference network for MVS.</p><p>? We build a cost volume pyramid in a coarse-to-fine manner based on a detailed analysis of the relation between the depth residual search range and the image resolution,</p><p>? Our framework can handle high resolution images with less memory requirement, is 6x faster than the current state-of-the-art framework, i.e. Point-MVSNet <ref type="bibr" target="#b4">[5]</ref>, and achieves a better accuracy on benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditional Multi-View Stereo. Multi-view stereo has been extensively studied for decades. We refer to algorithms before deep learning era as traditional MVS methods which represent the 3D geometry of objects or scene using voxels <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, level-sets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>, polygon meshes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> or depth maps <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>. In the following, we mainly focus on discussions about volumetric and depth-based MVS methods which have been integrated to learning-based framework recently. Volumetric representations can model most of the objects or scenes. Given a fixed volume of an object or scene, volumetric-based methods first divide the whole volume into small voxels and then use a photometric consistency metric to decide whether the voxel belongs to the surface or not. These methods do not impose constraints on the shape of the objects. However, the space discretisation is memory intensive. By contrast, depth-map based MVS methods have shown more flexibility in modeling the 3D geometry of scene <ref type="bibr" target="#b27">[28]</ref>. Readers are referred to <ref type="bibr" target="#b34">[35]</ref> for detailed discussions. Similar to other recent learning-based approaches, we adopt depth map representation in our framework.</p><p>Deep learning-based MVS. Deep CNNs have significantly advanced the progress of high-level vision tasks, such as image recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36]</ref>, object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>, and semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. As for 3D vision tasks, learning-based approaches have been widely adopted to solve stereo matching problems and have achieved very promising results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref>. However, these learningbased approaches cannot be easily generalized to solve MVS problems as rectifications are required for the multiple view scenario which may cause the loss of information <ref type="bibr" target="#b41">[42]</ref>.</p><p>More recently, a few approaches have proposed to directly solve MVS problems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>. For instance, Ji et al. <ref type="bibr" target="#b16">[17]</ref> introduce the first learning based pipeline for MVS. This approach learns the probability of voxels lying on the surface. Concurrently, Kar et al. <ref type="bibr" target="#b18">[19]</ref> present a learnable system to up-project pixel features to the 3D volume and classify whether a voxel is occupied or not by the surface. These systems provide promising results. However, they use volumetric representations that are memory expensive and therefore, these algorithms can not handle large-scale scenes.</p><p>Large-scale scene reconstruction has been approached by Yao et al. in <ref type="bibr" target="#b41">[42]</ref>. The authors propose to learn the depth map for each view by constructing a cost volume followed by 3D CNN regularization. Then, they obtain the 3D geometry by fusing the estimated depth maps from multiple views. The algorithm uses cost volume with memory requirements cubic to the image resolution. Thus, it can not leverage all the information available in high-resolution images. To circumvent this problem, the algorithm adopts GRUs <ref type="bibr" target="#b42">[43]</ref> to regularize the cost volume in a sequential manner. As a result, the algorithm reduces the memory requirement but leads to increased run-time.</p><p>Closely related work to ours is Point-MVSNet <ref type="bibr" target="#b4">[5]</ref>. Point-MVSNet is a framework to predict the depth in a coarse-to-fine manner working directly on point cloud. It allows the aggregation of information from its k nearest neighbors in 3D space. Our approach shares similar insight     <ref type="figure">Figure 2</ref>: Network Structure. Reference and source images are first downsampled to form an image pyramid. We apply feature extraction network to all levels and images to extract feature maps. We then build the cost volume pyramid in a coarse-to-fine manner. Specifically, we start with the construction of a cost volume corresponding to coarsest image resolution followed by building partial cost volumes iteratively for depth residual estimation in order to achieve depth map D = D 0 for I. Please refer to <ref type="figure" target="#fig_3">Fig. 3</ref> for details about re-projection, feature fetching and building cost volume.</p><formula xml:id="formula_0">V Z x F E G + + A A H A E T n I E W u A Y d 0 A U Y x O A R P I M X 7 U F 7 0 l 6 1 t 3 l r S S t m q u A X t P c v 6 h K U b w = = &lt; / l a t e x i t &gt; {I 0 i } N i=0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q c f A 6 W h J d e s p d v 4 N s + C o b f J Z Q z E = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u B l v B V U m r + F g I R T e 6 k Q q 2 F Z o 0 T K a T d u h k E m Y m Q g l Z u P F X 3 L h Q x K 0 f 4 c 6 / M U m D q P X A h c M 5 9 3 L v P U 7 A q F S G 8 a k V 5 u Y X F p e K y 6 W V 1 b X 1 D X 1 z q y P 9 U G D S x j 7 z x a 2 D J G G U k 7 a i i p H b Q B D k O Y x 0 n f F 5 6 n f v i J D U 5 z d q E h D L Q 0 N O X Y q R S i R b L 1 f N K D I d F 1 7 G N u 1 H R m z G d k R P j b h / V b X 1 i l E z M s B Z U</formula><formula xml:id="formula_1">Q = " &gt; A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x M 9 g K r k p S x c d C K L p R E K l g H 9 C k Y T K d t E M n D 2 Y m Q g n B j b / i x o U i b v 0 K d / 6 N S R p E r Q c u H M 6 5 l 3 v v s Q N G h d S 0 T 6 U w M z s 3 v 1 B c L C 0 t r 6 y u q e s b L e G H H J M m 9 p n P O z Y S h F G P N C W V j H Q C T p B r M 9 K 2 R + e p 3 7 4 j X F D f u 5 X j g J g u G n j U o R j J R L L U r Y o R R Y b t w M v Y o r 0 r I 7 Y i e q r F v e u K p Z a 1 q p Y B T h M 9 J 2 W Q o 2 G p H 0 b f x 6 F L P I k Z E q K r a 4 E 0 I 8 Q l x Y z E J S M U J E B 4 h A a k m 1 A P u U S Y U f Z C D H c T p Q 8 d n y f l S Z i p P y c i 5 A o x d u 2 k 0 0 V y K P 5 6 q f i f 1 w 2 l c 2 x G 1 A t C S T w 8 W e S E D E o f p n n A P u U E S z Z O C M K c J r d C P E Q c Y Z m k V s p C O E l x + P 3 y N G n V q v p + 9 e C m V q 6 f 5 X E U w T b Y A X t A B 0 e g D i 5 A A z Q B B v f g E T y D F + V B e V J e l b d J a 0 H J Z z b B L y j v X / T E l q M = &lt; / l a</formula><p>as predicting and refining depth maps iteratively. However, we differ from Point-MVSNet in a few key aspects: Instead of working on 3D, we build the cost volume on the regular image grid. Inspired by the idea of partial cost volume used in PWC-Net <ref type="bibr" target="#b36">[37]</ref> for optical flow estimation, we build partial cost volume to predict depth residuals. We compare the memory and computational efficiency with <ref type="bibr" target="#b4">[5]</ref>. It shows that our cost-volume pyramid based network leads to more compact and accurate models that run much faster for a given depth-map resolution.</p><p>Cost volume. Cost volume is widely used in traditional methods for dense depth estimation from unrectified images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41]</ref>. However, most recent learning-based works build cost volume at a fixed resolution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, which leads to high memory requirement for handling high resolution images. Recently, Sun et al. <ref type="bibr" target="#b36">[37]</ref> introduced the idea of partial cost volume for optical flow estimation. In short, given an estimate of the current optical flow, the partial cost volume is constructed by searching correspondences within a rectangle around its position locally in the warped source view image. Inspired by such strategy, in this paper, we propose cost volume pyramid as an algorithm to progressively estimate the depth residual for each pixel along its visual ray. As we will demonstrate in our experiments, constructing cost volumes at multiple levels leads to a more effective and efficient framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Let us now introduce our approach to depth inference for MVS. The overall system is depicted in <ref type="figure">Fig. 2</ref>. As existing works, we assume the reference image is denoted as I 0 ? R H?W , where H and W define its dimensions.</p><formula xml:id="formula_2">Let {I i } N i=1 be its N neighboring source images. Assume {K i , R i , t i } N i=0</formula><p>are the corresponding camera intrinsics, rotation matrix, and translation vector for all views. Our goal is to infer the depth map D for</p><formula xml:id="formula_3">I 0 from {I i } N i=0 .</formula><p>The key novelty of our approach is using a feed-forward deep network on cost volume pyramid constructed in a coarseto-fine manner. Below, we introduce our feature pyramid, the cost volume pyramid, depth map inference and finally provide details of the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Pyramid</head><p>As raw images vary with illumination changes, we adopt learnable features, which has been demonstrated to be crucial step for extracting dense feature correspondences <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38]</ref>. The general practice in existing works is to make use of high resolution images to extract multi-scale image features even for the output of a low resolution depth map. By contrast, we show that a low resolution image contains enough information useful for estimating a low resolution depth map.</p><p>Our feature extraction pipeline consists of two steps, see <ref type="figure">Fig. 2</ref>. First, we build the (L + 1)-level image pyramid {I j i } L j=0 for each input image, i ? {0, 1, ? ? ? , N }, where the bottom level of the pyramid corresponds to the input image, I 0 i = I i . Second, we obtain feature representations at the l-th level using a CNN, namely feature extraction network. Specifically, it consists of 9 convolutional layers, each of which is followed by a leaky rectified linear unit (Leaky-ReLU). We use the same CNN to extract features for all levels in all the images. We denote the feature maps for a given level l by {f l i } N i=0 , f l i ? R H/2 l ?W/2 l ?F , where F = 16 is the number of feature channels used in our experiments. We will show that, compared to existing works, our feature extraction pipeline leads to significant reduction in memory requirements and, at the same time, improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cost Volume Pyramid</head><p>Given the extracted features, the next step is to construct cost volumes for depth inference in the reference view. Common approaches usually build a single cost volume at a fixed resolution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, which incurs in large memory requirements and thus, limits the use of high-resolution images. Instead, we propose to build a cost volume pyramid, a process that iteratively estimates and refines depth maps to achieve high resolution depth inference. More precisely, we first build a cost volume for coarse depth map estimation based on images of the coarsest resolution in image pyramids and uniform sampling of the fronto-parallel planes in the scene. Then, we construct partial cost volumes based on coarse estimation and depth residual hypotheses iteratively to achieve depth maps with higher resolution and accuracy. We provide details about these two steps below. Cost Volume for Coarse Depth Map Inference. We start building a cost volume at the Lth level corresponding to the lowest image resolution (H/2 L , W/2 L ). Assume depth measured at the reference view of a scene ranges from d min to d max . We construct the cost volume for the reference view by sampling M fronto-parallel planes uniformly across entire depth range. A sampled depth d = d min + m(d max ? d min )/M, m ? {0, 1, 2, ? ? ? , M ? 1} represents a plane where its normal n 0 is the principal axis of the reference camera.</p><p>Similar to <ref type="bibr" target="#b41">[42]</ref>, we define the differentiable homography H i (d) between the ith source view and the reference view at depth d as</p><formula xml:id="formula_4">H i (d) = K L i R i (I ? (t 0 ? t i )n T 0 d )R ?1 0 (K L 0 ) ?1 ,<label>(1)</label></formula><p>where I is the identity matrix, and K L i and K L 0 are the scaled intrinsic matrices of K i and K 0 at level L.</p><p>Each homography H i (d) suggests a possible pixel correspondence betweenx i in source view i and a pixel x in the reference view. This correspondence is defined as ? ixi = H i (d)x, where ? i represents the depth ofx i in the source view i.</p><p>Givenx i and {f L i } N i=1 , we use differentiable bilinear interpolation to reconstruct the feature map warped to the ref-</p><formula xml:id="formula_5">erence view {f L i,d } N i=1 .</formula><p>The cost for all pixels at depth d is defined as its variance of features from N + 1 views,</p><formula xml:id="formula_6">C L d = 1 (N + 1) N i=0 (f L i,d ?f L d ) 2 ,<label>(2)</label></formula><p>wheref L 0,d = f L 0 is the feature map of the reference image andf L d is the average of feature volumes across all views</p><formula xml:id="formula_7">({f L i,d } N i=1 ? f L 0 )</formula><p>for each pixel. This metric encourages that the correct depth for each pixel has the smallest feature variance, which corresponds to the photometric consistency constraint. We compute the cost map for each depth hypothesis and concatenate those cost maps to a single cost volume</p><formula xml:id="formula_8">C L ? IR W/2 L ?H/2 L ?M ?F .</formula><p>A key parameter to obtain good depth estimation accuracy is the depth sampling resolution M . We will show in Section 3.3 how to determine the interval for depth sampling and coarse depth estimation. Cost Volume for Multi-scale Depth Residual Inference. Recall that our ultimate goal is to obtain D = D 0 for I 0 . We iterate starting from D l+1 , a given depth estimate for the (l + 1)th level, to obtain a refined depth map for the next level D l until reaching the bottom level. More precisely, we first upsample D l+1 to the next level D l+1 ? via bicubic interpolation and then, we build the partial cost volume to regress the residual depth map defined as ?D l to obtain a refined depth map D l = D l+1 ? + ?D l at the lth level. While we share the similar insight with <ref type="bibr" target="#b4">[5]</ref> to iteratively predict the depth residual, we argue that instead of performing convolutions on a point cloud <ref type="bibr" target="#b4">[5]</ref>, building the regular 3D cost volume on the depth residual followed by multiscale 3D convolution can lead to a more compact, faster, and higher accuracy depth inference. Our motivation is that depth displacements for neighboring pixels are correlated which indicates that regular multi-scale 3D convolution would provide useful contextual information for depth residual estimation. We therefore arrange the depth displacement hypotheses in a regular 3D space and compute the cost volume as follows.</p><p>Assume we are given camera parameters</p><formula xml:id="formula_9">{K l i , R i , t i } N i=0</formula><p>for all camera views and the upsam- pled depth estimate D l+1 ? . Current depth estimate for each pixel p = (u, v) is defined as d p = D l+1 ? (u, v). Let each depth residual hypothesis interval be ?d p = s p /M , where s p represents the depth search range at p and M denotes the number of sampled depth residual. We consider the projection of corresponding hypothesized 3D point with depth (D l+1 ? (u, v) + m?d p ) into view i as</p><formula xml:id="formula_10">? i x i = K l i (R i R ?1 0 ((K l 0 ) ?1 (u, v, 1) T (d p + m?d p ) ? t 0 ) + t i ),<label>(3)</label></formula><p>where ? i denotes the depth of corresponding pixel in view i, and m ? {?M/2, ? ? ? , M/2 ? 1} (see <ref type="figure" target="#fig_3">Fig. 3</ref>). Then, the cost for that pixel at each depth residual hypothesis is similarly defined based on Eq. 2, which leads to a partial cost volume C l ? IR H/2 l ?W/2 l ?M ?F .</p><p>In the next section, we introduce our solution to determine the depth search intervals and range for all pixels, s p , which is essential to obtain accurate depth estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth Map Inference</head><p>In this section, we first provide details to perform depth sampling at the coarsest image resolution and discretisation of the local depth search range at higher image resolution for building the cost volume. Then, we introduce depth map estimators on cost volumes to achieve the depth map inference. Depth Sampling for Cost Volume Pyramid We observe that the depth sampling for virtual depth planes is related to the image resolution. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, it is not necessary to sample depth planes densely as projections of those sampled 3D points in the image are too close to provide extra information for depth inference. In our experiments, to determine the number of virtual planes, we compute the mean depth sampling interval for a corresponding 0.5 pixel distance in the image.</p><p>For determining the local search range for depth residual around the current depth estimate for each pixel, we first project its 3D point into source views, find points that are two pixels away from the its projection along the epipolar line in both directions(see <ref type="figure" target="#fig_3">Fig. 3</ref> "2 pixel length"), and back project those two points into 3D rays. The intersection of these two rays with the visual ray in the reference view determines the search range for depth refinement on current level. Depth Map Estimator Similar to MVSNet <ref type="bibr" target="#b41">[42]</ref>, we apply 3D convolution to the constructed cost volume pyramid {C l } L l=0 to aggregate context information and output probability volumes {P l } L l=0 , where P l ? IR H/2 l ?W/2 l ?M . Detailed 3D convolution network design is in Supp. Mat. Note that P L and {P l } L?1 l=0 are generated on absolute and residual depth, respectively. We therefore first apply soft-argmax to P L to obtain the coarse depth map. Then, we iteratively refine the obtained depth map by applying soft-argmax to {P l } L?1 l=1 to obtain the depth residual for higher resolutions. Recall that sampled depth is d = d min + m(d max ? d min )/M, m ? {0, 1, 2, ? ? ? , M ? 1} at level L. Therefore, the depth estimate for each pixel p is computed as</p><formula xml:id="formula_11">D L (p) = M ?1 m=0 dP L p (d).<label>(4)</label></formula><p>To further refine the current estimate which is either the coarse depth map or a refined depth at (l+1)th level, we estimate the residual depth. Assume r p = m ? ?d l p denotes the depth residual hypothesis. We compute the updated depth at the next level as</p><formula xml:id="formula_12">D l (p) = D l+1 ? (p) + (M ?2)/2 m=?M/2 r p P l p (r p )<label>(5)</label></formula><p>where l ? {L ? 1, L ? 2, ? ? ? , 0}. In our experiments, we observe no depth map refinement after our pyramidal depth estimation is further required to obtain good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>We adopt a supervised learning strategy and construct the pyramid for ground truth depth {D l GT } L l=0 as supervisory signal. Similar to existing MVSNet framework <ref type="bibr" target="#b41">[42]</ref>, we make use of the l 1 norm measuring the absolute difference between the ground truth and the estimated depth. For each training sample, our loss is</p><formula xml:id="formula_13">Loss = L l=0 p?? D l GT (p) ? D l (p) 1 ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>DTU Dataset <ref type="bibr" target="#b0">[1]</ref> is a large-scale MVS dataset with 124 scenes scanned from 49 or 64 views under 7 different lighting conditions. DTU provides 3D point clouds acquired using structured-light sensors. Each view consists of an image and the calibrated camera parameters. To train our model, we generate a 160 ? 128 depth map for each view by using the method provided by MVSNet <ref type="bibr" target="#b41">[42]</ref>. We use the same training, validation and evaluation sets as defined in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. Tanks and Temples <ref type="bibr" target="#b20">[21]</ref> contains both indoor and outdoor scenes under realistic lighting conditions with large scale variations. For comparison with other approaches, we evaluate our results on the intermediate set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>Training We train our CVP-MVSNet on DTU training set. Unlike previous methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> that take high resolution image as input but estimate a depth map of smaller size, our method produces the same size depth map as the input image. For training, we match the ground-truth depth map by downsampling the high resolution image into a smaller one of size 160?128. Then, we build the image and ground truth depth pyramid with 2 levels. To construct the cost volume pyramid, we uniformly sample M = 48 depth hypotheses across entire depth range at the coarsest (2nd) level. Then, each pixel has M = 8 depth residual hypotheses at the next level for the refinement of the depth estimation. Following MVSNet <ref type="bibr" target="#b41">[42]</ref>, we adopt 3 views for training. We implemented our network using Pytorch <ref type="bibr" target="#b29">[30]</ref>, and we used ADAM <ref type="bibr" target="#b19">[20]</ref> to train our model. The batch size is set to 16 and the network is end-to-end trained on a NVIDIA TI-TAN RTX graphics card for 27 epochs. The learning rate is initially set to 0.001 and divided by 2 iteratively at the 10 th ,12 th ,14 th and 20 th epoch. Metrics. We follow the standard evaluation protocol as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref>. We report the accuracy, completeness and overall score of the reconstructed point clouds. Accuracy is measured as the distance from estimated point clouds to the ground truth ones in millimeter and completeness is defined as the distance from ground truth point clouds to the estimated ones <ref type="bibr" target="#b0">[1]</ref>. The overall score is the average of accuracy and completeness <ref type="bibr" target="#b41">[42]</ref>.</p><p>Evaluation As the parameters are shared across the cost volume pyramid, we can evaluate our model with different number of cost volumes and input views. For the evaluation, we set the number of depth sampling, M = 96 for the coarsest depth estimation (same as <ref type="bibr" target="#b4">[5]</ref>. We also provide results of M = 48 in the Supp. Mat.) and M = 8 for the following depth residual inference levels. Similar to previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, we use 5 views and apply the same depth map fusion method to obtain the point clouds. We Method Acc.</p><p>Comp. Overall (mm) Geometic Furu <ref type="bibr" target="#b10">[11]</ref> 0.613 0.941 0.777 Tola <ref type="bibr" target="#b38">[39]</ref> 0.342 1.190 0.766 Camp <ref type="bibr" target="#b2">[3]</ref> 0.835 0.554 0.695 Gipuma <ref type="bibr" target="#b11">[12]</ref> 0.283 0.873 0.578 Colmap <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> 0.400 0.664 0.532</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning</head><p>SurfaceNet <ref type="bibr" target="#b16">[17]</ref> 0.450 1.040 0.745 MVSNet <ref type="bibr" target="#b41">[42]</ref> 0.396 0.527 0.462 P-MVSNet <ref type="bibr" target="#b25">[26]</ref> 0.406 0.434 0.420 R-MVSNet <ref type="bibr" target="#b42">[43]</ref> 0.383 0.452 0.417 MVSCRF <ref type="bibr" target="#b39">[40]</ref> 0.371 0.426 0.398 Point-MVSNet <ref type="bibr" target="#b4">[5]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on DTU dataset</head><p>We first compare our results to those reported by traditional geometric-based methods and other learning-based baseline methods. As summarized in <ref type="table" target="#tab_2">Table 1</ref>, our method outperforms all current learning-based methods in terms of accuracy, completeness and overall score. Compared to geometric-based approaches, only the method proposed by Galliani et al. <ref type="bibr" target="#b11">[12]</ref> provides slightly better results in terms of mean accuracy.</p><p>We now compare our results to related learning based methods in terms of GPU memory usage and runtime for different input resolution. The summary of these results is listed in <ref type="table" target="#tab_4">Table 2</ref>. As shown, our network, with a similar memory usage (bottom row), is able to produce better point clouds with lower runtime. In addition, compared to Point-MVSNet <ref type="bibr" target="#b4">[5]</ref> on the same size of depth map output (top rows), our approach is six times faster and consumes six times less memory with similar accuracy. We can output high resolution depth map with better accuracy, less memory usage and shorter runtime than Point-MVSNet <ref type="bibr" target="#b4">[5]</ref>. <ref type="figure">Figures 5 and 6</ref> show some qualitative results. As shown, our method is able to reconstruct more details than Point-MVSNet <ref type="bibr" target="#b4">[5]</ref>, see for instance, the details highlighted in blue box of the roof behind the front building. Compared to R-MVSNet <ref type="bibr" target="#b42">[43]</ref> and Point-MVSNet <ref type="bibr" target="#b4">[5]</ref>, as we can see in the normal maps, our results are smoother on the surfaces while capturing more high-frequency details in edgy areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Tanks and Temples</head><p>We now evaluate the generalization ability of our method. To this end, we use the model trained on DTU R-MVSNet <ref type="bibr" target="#b42">[43]</ref> Point-MVSNet <ref type="bibr" target="#b4">[5]</ref> Ours Ground truth    <ref type="bibr" target="#b4">[5]</ref>, which is the strongest baseline on DTU dataset, and are competitive compared to P-MVSNet <ref type="bibr" target="#b25">[26]</ref>.</p><p>without any fine-tuning to reconstruct point clouds for scenes in Tanks and Temples dataset. For fair comparison, we use the same camera parameters, depth range and view selection of MVSNet <ref type="bibr" target="#b41">[42]</ref>. For comparison, we consider four baselines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> and evaluate the f-score on Tanks and Temples. <ref type="table" target="#tab_5">Table 3</ref> summarizes these results. As shown, our method yielded a mean f-score 5% higher than Point-MVSNet <ref type="bibr" target="#b4">[5]</ref>, which is the best baseline on DTU dataset, and only 1% lower than P-MVSNet <ref type="bibr" target="#b25">[26]</ref>. Note that P-MVSNet <ref type="bibr" target="#b25">[26]</ref> applies more depth filtering process for point cloud fusion than ours which just follows the simple fusion process provided by MVSNet <ref type="bibr" target="#b41">[42]</ref>. Qualitative re-sults of our point cloud reconstructions are shown in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>Training pyramid levels. We first analyze the effect of the number of pyramid levels on the quality of the reconstruction. To this end, we downsample the images to form pyramids with four different levels. Results of this analysis are summarized in <ref type="table" target="#tab_7">Table 4a</ref>. As shown, the proposed 2-level pyramid is the best. As the level of pyramid increases, the image resolution of the coarsest level decreases. For more than 2-levels, this resolution is too small to produce a good initial depth map to be refined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-MVSNet [43]</head><p>Point-MVSNet <ref type="bibr" target="#b4">[5]</ref> Ours Ground truth  <ref type="bibr" target="#b20">[21]</ref>. Best viewed on screen.  Evaluation pixel interval settings. We now analyze the effect of varying the pixel interval setting for depth refinement. As discussed in section 3.3, the depth sampling is determined by the corresponding pixel offset in source views, hence, it is important to set a suitable pixel interval. Table 4b summarizes the effect of varying the interval from depth ranges corresponding to 0.25 pixel to 2 pixels during evaluation. As shown, the performance drops when the interval is too small (0.25 pixel) or too large (2 pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed CVP-MVSNet, a cost volume pyramid based depth inference framework for MVS. CVP-MVSNet is compact, lightweight, fast in runtime and can handle high resolution images to obtain high quality depth map for 3D reconstruction. Our model achieves better performance than state-of-the-art methods by extensive evaluation on benchmark datasets. In the future, we want to explore the integration of our approach into a learning-based structure-from-motion framework to further reduce the memory requirements for different applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " H E J M m A u F l U t Z L F O a n l v R W Y Q T E o 0 = " &gt; A A A B 9 H i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k V H 7 u i L l x W s A 9 o Y 5 l M J + 3 Q y c O Z S a G E f I c b F 4 q 4 9 W P c + T d O 0 i B q P X D h c M 6 9 3 H u P E 3 I m l W l + G g u L S 8 s r q 4 W 1 4 v r G 5 t Z 2 a W e 3 J Y N I E N o k A Q 9 E x 8 G S c u b T p m K K 0 0 4 o K P Y c T t v O + C r 1 2 x M q J A v 8 O z U N q e 3 h o c 9 c R r D S k l 2 J e 4 6 L r p P 7 m C e V f q l s V s 0 M a J 5 Y O S l D j k a / 9 N E b B C T y q K 8 I x 1 J 2 L T N U d o y F Y o T T p N i L J A 0 x G e M h 7 W r q Y 4 9 K O 8 6 O T t C h V g b I D Y Q u X 6 F M / T k R Y 0 / K q e f o T g + r k f z r p e J / X j d S 7 r k d M z + M F P X J b J E b c a Q C l C a A B k x Q o v h U E 0 w E 0 7 c i M s I C E 6 V z K m Y h X K Q 4 / X 5 5 n r R q V e u 4 e n J b K 9 c v 8 z g K s A 8 H c A Q W n E E d b q A B T S D w A I / w D C / G x H g y X o 2 3 W e u C k c / s w S 8 Y 7 1 8 A E J G 8 &lt; / l a t e x i t &gt; D L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 6 T 5 r R f U k o e 7 G M i h Q c z T C B f k 0 i o = " &gt; A A A B 9 H i c b V D J S g N B E K 2 J W 4 x b 1 K O X x k T w F C Z R X G 5 B P X j w E M E s k I y h p 9 O T N O l Z 7 O 4 J h G G + w 4 s H R b z 6 M d 7 8 G 3 s m g 6 j x Q c H j v S q q 6 t k B Z 1 K Z 5 q e R W 1 h c W l 7 J r x b W 1 j c 2 t 4 r b O y 3 p h 4 L Q J v G 5 L z o 2 l p Q z j z Y V U 5 x 2 A k G x a 3 P a t s e X i d + e U C G Z 7 9 2 p a U A t F w 8 9 5 j C C l Z a s c t S z H X Q V 3 0 c 3 c b l f L J k V M w W a J 9 W M l C B D o 1 / 8 6 A 1 8E r r U U 4 R j K b t V M 1 B W h I V i h N O 4 0 A s l D T A Z 4 y H t a u p h l 0 o r S o + O 0 Y F W B s j x h S 5 P o V T 9 O R F h V 8 q p a + t O F 6 u R / O s l 4 n 9 e N 1 T O m R U x L w g V 9 c h s k R N y p H y U J I A G T F C i + F Q T T A T T t y I y w g I T p X M q p C G c J z j 5 f n m e t G q V 6 l H l + L Z W q l 9 k c e R h D / b h E K p w C n W 4 h g Y 0 g c A D P M I z v B g T 4 8 l 4 N d 5 m r T k j m 9 m F X z D e v w D P Q Z G c &lt; / l a t e x i t &gt;D l &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w q v H 2 B O 5 a F 9 D A T Z 2 n g O + k B 3 b 5 T c = " &gt; A A A B / H i c b V D L S s N A F J 3 U V 6 2 v a J d u B l v B V U m q + N g V 7 c J l B f u A J p b J d N I O n T y Y m Q g h x F 9 x 4 0 I R t 3 6 I O / / G S R p E r Q c u H M 6 5 l 3 v v c U J G h T S M T 6 2 0 t L y y u l Z e r 2 x s b m 3 v 6 L t 7 P R F E H J M u D l j A B w 4 S h F G f d C W V j A x C T p D n M N J 3 Z l e Z 3 7 8 n X N D A v 5 V x S G w P T X z q U o y k k k Z 6 t W 6 1 C Z M o s R w X t t O 7 h K X 1 k V 4 z G k Y O u E j M g t R A g c 5 I / 7 D G A Y 4 8 4 k v M k B B D 0 w i l n S A u K W Y k r V i R I C H C M z Q h Q 0 V 9 5 B F h J / n x K T x U y h i 6 A V f l S 5 i r P y c S 5 A k R e 4 7 q 9 J C c i r 9 e J v 7 n D S P p n t s J 9 c N I E h / P F 7 k R g z K A W R J w T D n B k s W K I M y p u h X i K e I I S 5 V X J Q / h I s P p 9 8 u L p N d s m M e N k 5 t m r X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>s 9 J</head><label>9</label><figDesc>B e R o 2 f q H O f B x 6 B G u M E N S 9 u p G o K w I C U U x I 3 H J D C U J E B 6 j I e k l l C O P S C v K n o j h b q I M o O u L p L i C m f p z I k K e l B P P S T o 9 p E b y r 5 e K / 3 m 9 U L n H V k R 5 E C r C 8 X S R G z K o f J g m A g d U E K z Y J C E I C 5 r c C v E I C Y R V k l s p C + E k x e H 3 y 7 O k 0 6 j V 9 2 s H 1 4 1 K 8 y y P o w j K Y A f s g T o 4 A k 1 w A V q g D T C 4 B 4 / g G b x o D 9 q T 9 q q 9 T V s L W j 6 z D X 5 B e / 8 C o K C X k w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " l + 9 p a b h k g a F C P O r / Y S w H A v P Z 7 6 w = " &gt; A A A C B n i c b V D L S s N A F J 3 U V 6 2 v q E s R B l v B j S W p 4 m M h F N 0 o i F S w D 2 j S M J l O 2 q G T B z M T o Y S s 3 P g r b l w o 4 t Z v c O f f m K R B f B 2 4 c D j n X u 6 9 x w 4 Y F V L T P p T C 1 P T M 7 F x x v r S w u L S 8 o q 6 u t Y Q f c k y a 2 G c + 7 9 h I E E Y 9 0 p R U M t I J O E G u z U j b H p 2 l f v u W c E F 9 7 0 a O A 2 K 6 a O B R h 2 I k E 8 l S N y t G F B m 2 A y 9 i i / a i y 1 0 9 N m I r o i d a 3 L u q W G p Z q 2 o Z 4 F + i 5 6 Q M c j Q s 9 d 3 o + z h 0 i S c x Q 0 J 0 d S 2 Q Z o S 4 p J i R u G S E g g Q I j 9 C A d B P q I Z c I M 8 r e i O F 2 o v S h 4 / O k P A k z 9 f t E h F w h x q 6 d d L p I D s V v L x X / 8 7 q h d I 7 M i H p B K I m H J 4 u c k E H p w z Q T 2 K e c Y M n G C U G Y 0 + R W i I e I I y y T 5 E p Z C M c p D r 5 e / k t a t a q + V 9 2 / r p X r p 3 k c R b A B t s A O 0 M E h q I N z 0 A B N g M E d e A B P 4 F m 5 V x 6 V F + V 1 0 l p Q 8 p l 1 8 A P K 2 y e 0 x p g h &lt; / l a t e x i t &gt; {I L i } N i=0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 8 P R p G w M 6 W d G q b T W 5 y 5 5 f e O / Z C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>, v) + m?d l p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u x W b n h S 7 / y F F j H a y / T g f 9 Z 6 S t 5 w = " &gt; A A A C G n i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 W o V E q i x c e u a B c u K 9 g H N G m Y T C f t 0 M m D m U m l h H y H G 3 / F j Q t F 3 I k b / 8 Y k L a L W A w O H c 8 7 l z j 1 2 w K i Q m v a p 5 B Y W l 5 Z X 8 q u F t f W N z S 1 1 e 6 c l / J B j 0 s Q + 8 3 n H R o I w 6 p G m p J K R T s A J c m 1 G 2 v b o K v X b Y 8 I F 9 b 1 b O Q m I 6 a K B R x 2 K k U w k S 9 U j w 3 Z g P e 5 F r K z H V m S E A e L c v 4 t L 4 d H 4 s O w a d c I k g n 0 r y w V x j 1 l q U a t o G e A 8 0 W e k C G Z o W O q 7 0 f d x 6 B J P Y o a E 6 O p a I M 0 I c U k x I 3 H B C A U J E B 6 h A e k m 1 E M u E W a U n R b D g 0 T p Q 8 f n y f M k z N S f E x F y h Z i 4 d p J 0 k R y K v 1 4 q / u d 1 Q + m c m x H 1 g l A S D 0 8 X O S G D 0 o d p T 7 B P O c G S T R K C M K f J X y E e I o 6 w T N o s Z C V c p D j 9 P n m e t I 4 r + k m l e l M t 1 i 5 n d e T B H t g H J a C D M 1 A D 1 6 A B m g C D e / A I n s G L 8 q A 8 K a / K 2 z S a U 2 Y z u + A X l I 8 v t J a g z Q = = &lt; / l a t e x i t &gt; M ?d l p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E r s w k L F t k 2 w Q + U m q c U Q s x s q C N B Q = " &gt; A A A B / H i c b V D L S s N A F J 3 U V 6 2 v a J d u B o v g q i R a f O y K u n A j V L A P a G K Y T C b t 0 M m D m Y k Q Q v 0 V N y 4 U c e u H u P N v n K R B 1 H r g w u G c e 7 n 3 H j d m V E j D + N Q q C 4 t L y y v V 1 d r a + s b m l r 6 9 0 x N R w j H p 4 o h F f O A i Q R g N S V d S y c g g 5 g Q F L i N 9 d 3 K R + / 1 7 w g W N w l u Z x s Q O 0 C i k P s V I K s n R 6 9 f W J W E S Q c / J L N e H 8 f S O O X r D a B o F 4 D w x S 9 I A J T q O / m F 5 E U 4 C E k r M k B B D 0 4 i l n S E u K W Z k W r M S Q W K E J 2 h E h o q G K C D C z o r j p 3 B f K R 7 0 I 6 4 q l L B Q f 0 5k K B A i D V z V G S A 5 F n + 9 X P z P G y b S P 7 U z G s a J J C G e L f I T B m U E 8 y S g R z n B k q W K I M y p u h X i M e I I S 5 V X r Q j h L M f x 9 8 v z p H f Y N I + a r Z t W o 3 1 e x l E F u 2 A P H A A T n I A 2 u A I d 0 A U Y p O A R P I M X 7 U F 7 0 l 6 1 t 1 l r R S t n 6 u A X t P c v F d a U j Q = = &lt; / l a t e x i t &gt; t e x i t s ha 1 _ b a s e 6 4 = " 5 1 R 4 Z X K s 3 c X T N r C 4 u l H o m N n A g x Y = " &gt; A A A C B n i c b V D L S s N A F J 3 U V 6 2 v q E s R B o t Q U U q i x c e u q A u X F e w D m h g m 0 0 k 7 d P J g Z l I p I S s 3 / o o b F 4 q 4 9 R v c + T c m a R C 1 H r h w O O d e 7 r 3 H D h g V U t M + l c L M 7 N z 8 Q n G x t L S 8 s r q m r m + 0 h B 9 y T J r Y Z z 7 v 2 E g Q R j 3 S l F Q y 0 g k 4 Q a 7 N S N s e X q R + e 0 S 4 o L 5 3 I 8 c B M V 3 U 9 6 h D M Z K J Z K n b k W E 7 8 D K + j d i + H l u R E Q a I c / 8 u r o Q H o z 1 L L W t V L Q O c J n p O y i B H w 1 I / j J 6 P Q 5 d 4 E j M k R F f X A m l G i E u K G Y l L R i h I g P A Q 9 U k 3 o R 5 y i T C j 7 I 0 Y 7 i Z K D z o + T 8 q T M F N / T k T I F W L s 2 k m n i + R A / P V S 8 T + v G 0 r n 1 I y o F 4 S S e H i y y A k Z l D 5 M M 4 E 9 y g m W b J w Q h D l N b o V 4 g D j C M k m u l I V w l u L 4 + + V p 0 j q s 6 k f V 2 n W t X D / P 4 y i C L b A D K k A H J 6 A O r k A D N A E G 9 + A R P I M X 5 U F 5 U l 6 V t 0 l r Q c l n N s E v K O 9 f w K a Y z g = = &lt; / l a t e x i t &gt; Reprojection, feature fetching and building cost volume. Left: We define M depth hypotheses for each pixel (u, v) in the reference view. By projecting them to each source view, we can fetch M corresponding features. Right: For each depth hypothesis, the matching cost is the variance of fetched features across source views and the reference view. The cost volume C l+1 defines matching costs for all depth hypotheses of all pixels in the reference view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Interpolation of two sampling points from four feature points in source view. (a) Densely sampled depth will result in very close (&lt; 0.5 pixel) locations which have similar feature. (b) Points projected using appropriate depth sampling carry distinguishable information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Additional results from DTU dataset. Best viewed on screen. Point cloud reconstruction of Tanks and Temples dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Quantitative results of reconstruction quality on</cell></row><row><cell>DTU dataset (lower is better). Our method outperforms all</cell></row><row><cell>methods on Mean Completeness and Overall reconstruction</cell></row><row><cell>quality and achieved second best on Mean Accuracy.</cell></row><row><cell>evaluate our model with images of different size and set the</cell></row><row><cell>pyramid levels accordingly to maintain a similar size as the</cell></row><row><cell>input image (80 ? 64) at coarsest level. For instance, for an input size of 1600 ? 1184, the pyramid has 5 levels and 4 levels for an input size of 800 ? 576 and 640 ? 480.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of scan 9 of DTU dataset. The upper row shows the point clouds and the bottom row shows the normal map corresponding to the orange rectangle. As highlighted in the blue rectangle, the completeness of our results is better than those provided by Point-MVSNet<ref type="bibr" target="#b4">[5]</ref>. The normal map (orange rectangle) further shows that our results are smoother on surfaces while maintaining more high-frequency details.</figDesc><table><row><cell>Method</cell><cell>Input Size</cell><cell cols="3">Depth Map Size Acc.(mm) Comp.(mm)</cell><cell>Overall(mm)</cell><cell cols="2">f-score(0.5mm) GPU Mem(MB)</cell><cell>Runtime(s)</cell></row><row><cell>Point-MVSNet[5]</cell><cell>1280x960</cell><cell>640x480</cell><cell>0.361</cell><cell>0.421</cell><cell>0.391</cell><cell>84.27</cell><cell>8989</cell><cell>2.03</cell></row><row><cell>Ours-640</cell><cell>640x480</cell><cell>640x480</cell><cell>0.372</cell><cell>0.434</cell><cell>0.403</cell><cell>82.44</cell><cell>1416</cell><cell>0.37</cell></row><row><cell cols="2">Point-MVSNet[5] 1600x1152</cell><cell>800x576</cell><cell>0.342</cell><cell>0.411</cell><cell>0.376</cell><cell>-</cell><cell>13081</cell><cell>3.04</cell></row><row><cell>Ours-800</cell><cell>800x576</cell><cell>800x576</cell><cell>0.340</cell><cell>0.418</cell><cell>0.379</cell><cell>86.82</cell><cell>2207</cell><cell>0.49</cell></row><row><cell>MVSNet[42]</cell><cell>1600x1152</cell><cell>400x288</cell><cell>0.396</cell><cell>0.527</cell><cell>0.462</cell><cell>78.10</cell><cell>22511</cell><cell>2.76</cell></row><row><cell>R-MVSNet[43]</cell><cell>1600x1152</cell><cell>400x288</cell><cell>0.383</cell><cell>0.452</cell><cell>0.417</cell><cell>83.96</cell><cell>6915</cell><cell>5.09</cell></row><row><cell cols="2">Point-MVSNet[5] 1600x1152</cell><cell>800x576</cell><cell>0.342</cell><cell>0.411</cell><cell>0.376</cell><cell>-</cell><cell>13081</cell><cell>3.04</cell></row><row><cell>Ours</cell><cell>1600x1152</cell><cell>1600x1152</cell><cell>0.296</cell><cell>0.406</cell><cell>0.351</cell><cell>88.61</cell><cell>8795</cell><cell>1.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of reconstruction quality, GPU memory usage and runtime on DTU dataset for different input sizes. GPU memory usage and runtime are obtained by running the official evaluation code of baselines on a same machine with a NVIDIA TITAN RTX graphics card. For the same size of depth maps (Ours-640, Ours-800) and a performance similar to Point-MVSNet<ref type="bibr" target="#b4">[5]</ref>, our method is 6 times faster and consumes 6 times smaller GPU memory. For the same size of input images (Ours), our method achieves the best reconstruction with the shortest time and a reasonable GPU memory usage.</figDesc><table><row><cell>Method</cell><cell>Rank</cell><cell cols="5">Mean Family Francis Horse Lighthouse</cell><cell>M60</cell><cell cols="2">Panther Playground</cell><cell>Train</cell></row><row><cell>P-MVSNet [26]</cell><cell>11.72</cell><cell>55.62</cell><cell>70.04</cell><cell>44.64</cell><cell>40.22</cell><cell>65.20</cell><cell>55.08</cell><cell>55.17</cell><cell>60.37</cell><cell>54.29</cell></row><row><cell>Ours</cell><cell>12.75</cell><cell>54.03</cell><cell>76.5</cell><cell>47.74</cell><cell>36.34</cell><cell>55.12</cell><cell>57.28</cell><cell>54.28</cell><cell>57.43</cell><cell>47.54</cell></row><row><cell cols="2">Point-MVSNet[5] 29.25</cell><cell>48.27</cell><cell>61.79</cell><cell>41.15</cell><cell>34.20</cell><cell>50.79</cell><cell>51.97</cell><cell>50.85</cell><cell>52.38</cell><cell>43.06</cell></row><row><cell>R-MVSNet[43]</cell><cell>31.75</cell><cell>48.40</cell><cell>69.96</cell><cell>46.65</cell><cell>32.59</cell><cell>42.95</cell><cell>51.88</cell><cell>48.80</cell><cell>52.00</cell><cell>42.38</cell></row><row><cell>MVSNet[42]</cell><cell>42.75</cell><cell>43.48</cell><cell>55.99</cell><cell>28.55</cell><cell>25.07</cell><cell>50.79</cell><cell>53.96</cell><cell>50.86</cell><cell>47.90</cell><cell>34.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance on Tanks and Temples [21] on November 12, 2019. Our results outperform Point-MVSNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Parameter sensitivity on DTU dataset. a) Accuracy as a function of the number of pyramid levels. b) Accuracy as a function of the interval setting.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">where ? is the set of valid pixels with ground truth measurements.4. ExperimentsIn this section, we demonstrate the performance of our framework for MVS with a comprehensive set of experiments in standard benchmarks. Below, we first describe the datasets and benchmarks and then analyze our results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by Australian Research Council grants (DE180100628, DP200102274).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsb?l Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogiatzis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CBMV: A coalesced bidirectional matching volume for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Batsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Using multiple hypotheses to improve depth-maps for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
		<editor>David Forsyth, Philip Torr, and Andrew Zisserman</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multiimage matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Poxels: Probabilistic voxelized volume reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy S De</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Silhouette and stereo fusion for 3d object modeling. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Schmitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Variational principles, surface evolution, PDE&apos;s, level set methods and the stereo problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Keriven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object-centered surface reconstruction: Combining multi-image stereo and shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leclerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMAPI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gipuma: Massively parallel multi-view stereo reconstruction. Publikationen der Deutschen Gesellschaft f?r Photogrammetrie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Fernerkundung und Geoinformation e. V</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learned multi-patch similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DPSNet: End-to-end deep plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handling occlusions in dense multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-camera scene reconstruction via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A theory of shape by space carving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiriakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shape reconstruction using volume sweeping and learned photoconsistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sebastien</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time visibility-based fusion of depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Merrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nist?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Despoina</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">umetric 3d reconstruction with ray potentials. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variational stereovision and 3d scene flow estimation with statistical similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-P</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermosillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BA-net: Dense bundle adjustment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient large-scale multi-view stereo for ultra high-resolution image sets. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mvscrf: Learning multi-view stereo with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youze</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-resolution realtime stereo on commodity graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent mvsnet for high-resolution multiview stereo depth inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
