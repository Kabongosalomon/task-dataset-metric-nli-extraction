<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-objective Optimisation of Multi-output Neural Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ojha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Reading Reading</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Nicosia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-objective Optimisation of Multi-output Neural Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Neural tree</term>
					<term>Multi-class classification</term>
					<term>multi- objective optimisation</term>
					<term>non-dominated sorting genetic algorithm; NSGA-III</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an algorithm and a new method to tackle the classification problems. We propose a multi-output neural tree (MONT) algorithm 1 , which is an evolutionary learning algorithm trained by the non-dominated sorting genetic algorithm (NSGA)-III. Since evolutionary learning is stochastic, a hypothesis found in the form of MONT is unique for each run of evolutionary learning, i.e., each hypothesis (tree) generated bears distinct properties compared to any other hypothesis both in topological space and parameter-space. This leads to a challenging optimisation problem where the aim is to minimise the tree-size and maximise the classification accuracy. Therefore, the Pareto-optimality concerns were met by hypervolume indicator analysis. We used nine benchmark classification learning problems to evaluate the performance of the MONT. As a result of our experiments, we obtained MONTs which are able to tackle the classification problems with high accuracy. The performance of MONT emerged better over a set of problems tackled in this study compared with a set of well-known classifiers: multilayer perceptron, reduced-error pruning tree, na?ve Bayes classifier, decision tree, and support vector machine. Moreover, the performances of three versions of MONT's training using genetic programming, NSGA-II, and NSGA-III suggests that the NSGA-III gives the best Pareto-optimal solution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Learning from data is essentially a search process by which we search for a hypothesis (a trained model) from a hypothesis space that maps (fits) the given input data to its target output as good as possible (the high accuracy on test data). A learning algorithm like multilayer perceptron's architecture and parameter tuning are the efforts to find a hypothesis that fits the data well <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Similarly, there is a variety of hypothesis selection possible. Such examples are decision tree <ref type="bibr" target="#b2">[3]</ref>, reduced error pruning tree <ref type="bibr" target="#b3">[4]</ref>, na?ve Bayes classifier <ref type="bibr" target="#b4">[5]</ref>, and support vector machine <ref type="bibr" target="#b5">[6]</ref>.</p><p>In this study, our effort is to take advantages of the evolutionary processes for designing a new method for searching a hypothesis (an evolutionary learning algorithm) that fits well on a variety of datasets. Therefore, we propose a multioutput neural tree (MONT) algorithm, which resembles a tree data structure whose nodes are neural nodes similar to the nodes of a multilayer perceptron. The tree exploits the genetic <ref type="bibr" target="#b0">1</ref> Source code: https://github.com/vojha-code/Multi-Output-Neural-Tree programming (GP) <ref type="bibr" target="#b6">[7]</ref> and non-dominated sorting genetic algorithm (NSGA) frameworks II <ref type="bibr" target="#b7">[8]</ref> and III <ref type="bibr" target="#b8">[9]</ref> to evolve from data, at independent instances.</p><p>The proposed algorithm MONT is an innovation from the early tree-based learning algorithms such as a flexible neural tree where a tree-like-structure was optimised by using probabilistic incremental program evolution <ref type="bibr" target="#b9">[10]</ref> and heterogeneous flexible neural tree (HFNT) <ref type="bibr" target="#b10">[11]</ref> where a treelike-structure was optimised by NSGA-II. Similar to these two approaches, in <ref type="bibr" target="#b11">[12]</ref>, a fuzzy inference system enabled hierarchical tree-based predictors was illustrated. In <ref type="bibr" target="#b12">[13]</ref>, a tree-based algorithm was evaluated on beta-basis function as a neural node. Among these algorithms, the proposed MONT algorithm closely linked to HFNT. Hence, a comparison of MONT with HFNT is presented in this research.</p><p>These early versions of the tree-based algorithms are limited to binary class classification since the root node reports the output. Hence, these algorithms worked in a multi-inputsingle-output fashion. For the multi-class classification, these algorithms need to be repeated for each class separately, which results in as many as trees as the number of classes.</p><p>Our proposed algorithm eliminates this limitation by using a single tree to learn for multiple classes. In MONT algorithm, each child of the tree's root-node is formulated as the class output. Hence, the proposed MONT works in multi-inputmulti-output fashion and treats binary classification as twoclass classification. The results show that the competitive nature of the evolutionary process improve performance. Moreover, our proposed method applies NSGA-III combined with hypervolume inductor analysis <ref type="bibr" target="#b13">[14]</ref> to obtain the best neural trees serving Pareto-optimality for an evolutionary learning process. The contributions of this study are as follows:</p><p>? A new algorithm called MONT is designed for classification tasks, specifically aims at adapting multi-class. ? A new method is proposed for neural trees generations. ? A Pareto-optimality of evolutionary learning processes was investigated using hypervolume indicator analysis. ? A comprehensive analysis of the MONT's (trained with NSGA-III) performance compared with other algorithms and with MONT's other two training version GP, NSGA-II is presented. The rest of the paper is organised as follows: Section II describes the multi-class classification problem and the basic architecture, principles, and properties of MONT algorithm. Section II-C describes the evolutionary learning processes and framework for constructing optimal neural trees. Section II-E describes the experimental setup designed for the induction of a varied range of hypothesis, including MONT and wellknown five algorithms over a diverse range of datasets. The results of the experiments are summarised in Section III and discussed in Section IV, followed by conclusions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MULTI-OBJECTIVE-MULTI-OUTPUT NEURAL TREES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement</head><p>Let X ? R d be an instance-space and let Y = {c 1 , . . . , c r } be a set of r labels such that label y ? Y can be assigned to an instance x ? X . Hence, for a training set of instancelabel pairs S = (x i , y i ) N i=1 , we face a multi-class learning problem. And, a hypothesis h from a set of hypothesis class H is induced that aims at reducing a cost function f (?). A typical cost function is the classification error-rate:</p><formula xml:id="formula_0">f = 1 N N i=1 (h(x i ) = y i )<label>(1)</label></formula><p>where h(x i ) is the predicted output for an input instance x i = x i 1 , x i 2 , . . . , x i d and y i ? {c 1 , . . . , c r } being its target class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Output Neural Trees</head><p>Multi-output neural tree (MONT) takes a tree-like structure where the root node takes as many as child nodes as the number of classes it is to be induced on. Each child node of the tree's root predicts a class, and each of them is a subtree of the MONT classifier.</p><p>Mathematically, MONT, G, is an m-ary rooted tree with one node designated as the root node and each node takes at least m ? 2 child nodes except for the leaf node that has no child node. Hence, for a tree depth p, MONT takes a maximum of (2 p+1 ?1) ? n ? (m p+1 ?1)/(m?1) nodes (including the number of internal nodes K = |V | and the leaf node L = |T |). A MONT, G, is denoted as:</p><formula xml:id="formula_1">G = V ? T = v j 1 , v j 2 , . . . , v j K ? {t 1 , t 2 , . . . , t L }<label>(2)</label></formula><p>where k-th node v j k ? V is an internal node and receives 2 ? j ? m inputs from its child nodes. The k-th leaf node t k ? T has no child and it contains an input</p><formula xml:id="formula_2">x i ? {x 1 , x 2 , . . . , x d }.</formula><p>An example of a MONT is shown in <ref type="figure">Fig. 1</ref>. <ref type="figure">Fig. 1</ref> is an example of three class problem, where the root node v 3 0 takes three child nodes v 2 1 , v 2 2 , and v 3 3 representing three classes c 1 , c 2 , and c 3 . Each child node of the root is a full m-ary subtree. For example, node v 2 1 takes two child nodes and the node v 3 1 takes three child nodes. The number of child nodes and the size of subtrees is governed by an evolutionary learning process. The leaf nodes of the MONT are the input nodes that takes an input feature x ? x.</p><p>The internal nodes of the MONT are neural nodes. Each internal node has an activation function (e.g., Gaussian, sigmoid, tangent hyperbolic) and behave similarly to a node in multilayer perceptron. <ref type="figure">Fig. 2</ref> is an example of the i-th <ref type="figure">Fig. 1</ref>. Representation of a multi-output neural tree for a three-class problem with classes c 1 , c 2 , and c 3 . The immediate child nodes v 1 , v 3 , and v 4 of the root node v 0 are the output class nodes. The other internal nodes or leaf nodes construct subtrees for each root child (the respective output class). This tree takes its input from the set {x 1 , x 2 , . . . , x 5 }. The link w v j i between nodes are neural weights.</p><formula xml:id="formula_3">x 1 x 3 x 3 x 4 x 2 x 2 v 3 0 v 2 1 v 3 4 v 3 5 x 1 x 3 x 3 v 3 2 c 1 c 3 root node neural node input node b v1 b v4 b v2 b v5 bias input output class v 2 3 c 2 x 5 x 0 b v3 w v3 2 w v3 1 w v1 2 w v1 1 w v2 1 w v2 3 w v2 2 w v4 1 w v4 3 w v4 2 w v5 1 w v5 3 w v5 2</formula><formula xml:id="formula_4">d (i) j=1 w i j z i j + b i z i 1 z i j z i d (i) z i 2 . . . . . . b i y i ? z k v i 1.0 w i 1 w i 2 w i j w i d (i) Fig. 2.</formula><p>Illustration of a computational (neural) node. The variable dv i indicates the number of inputs z i j and weights w i j received at the i-th node v i , the variable b i is the bias at the i-th node and the variable z k is the output of the i-th node squashed by an activation function ?(y i ).</p><p>MONT's neural node that receives the inputs from its child nodes (higher tree-depth) and produces an output for its parent node (e.g., k-th node in lower tree-depth).</p><p>A MONT resembles an expression tree, and the computation of MONT takes place as per depth-first-search in pre-order fashion ( <ref type="figure">Fig. 1)</ref>. Hence, the time complexity of MONT for computing its output is O(n), where n is the number of nodes in the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-Objective Optimisation of Neural Trees</head><p>In our designed evolutionary process, an evolutionary algorithm like NSGA-III searches through all possible combination of MONTs, which is roughly close to Catalan number C n = 1/[(m ? 1)(n + 1)]. m.n n , and for MONT, n is ? 7 and only 1 tree-structure (shape) possible for n = 7 since MONT takes at least two classes and each class takes at least two inputs. Hence, 1 root node, 2 child nodes and 4 leaf node, total 7 nodes can only be arranged in one unique structure as per MONT's definition (Section II-B). However, the parameters of the tree's edges and the node's function further increase the search-space size.</p><p>Felsenstein presented a theory for all possible combinations of tree-structure for an m-ary tree that has a total L = m p labelled leaf nodes <ref type="bibr" target="#b14">[15]</ref>, and, for L labelled leaf nodes, a total number of possible tree-structures arrangements (combinations) shown for an evolutionary process is (2L ? 3)!/(2 L?2 (L ? 2))! <ref type="bibr" target="#b14">[15]</ref>. Therefore, an evolutionary processes search through such a vast hypothesis search-space to obtain an optimal tree.</p><p>In our method, we used three training versions for MONT: NSGA-III <ref type="bibr" target="#b8">[9]</ref>, NSGA-II <ref type="bibr" target="#b7">[8]</ref>, and GP <ref type="bibr" target="#b6">[7]</ref>. MONT takes genetic operators such as crossover (of subtrees) and mutation defined in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, the following forms of mutation is defined for the tree's mutation: 1) deletion of a randomly selected leaf node, 2) replacement a randomly selected leaf node, 3) replacement of a randomly selected function node by a leaf node or a new subtree. Using the mentioned genetic operators, the MONT follows the typical evolutionary computation steps as per Algorithm 1 for its training <ref type="bibr" target="#b15">[16]</ref>. For MONT's multiobjective training, we supplied two objectives to be minimised: classification error-rate f 1 and tree-size f 2 . The error-rate f 1 is expressed as per (1). The objective tree-size f 2 is |G|, i.e., the total number of nodes in a tree G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Evolutionary Learning of MONT</head><p>Require: Initial population P 0 of randomly generated neural trees, objectives F = [f 1 , f 2 ] , data S, maximum evolutionary generations (termination criteria) g max . Ensure: Final population P gmax of Pareto-optimal trees 1: function TREE EVOLUTION(P 0 , F, S, g max .) <ref type="bibr">2:</ref> while number of generation g reached g max do <ref type="bibr">3:</ref> selection: parent trees for crossover and mutation <ref type="bibr">4:</ref> generation: a new population Q return P gmax 10: end function For MONT training, the algorithms GP, NSGA-II, and NSGA-III follow Algorithm 1 with some differences in a few steps. Especially, they differ in line numbers 6 and 7 of Algorithm 1. In GP, no non-dominated sorting is performed. Instead, the recombined population R in line no. 5 is sorted according to single objective f 1 (line no. 6 of Algorithm 1). Whereas, NSGA-II and NSGA-III both follows non-dominated sorting as per <ref type="bibr" target="#b7">[8]</ref>. However, NSGA-II and NSGA-III differ in line no. 7 of Algorithm 1. NSGA-II performs elitism based on crowding distance of the individuals computed based on each objective <ref type="bibr" target="#b7">[8]</ref>. On the other hand, NSGA-III performs niching. The niching operation takes advantage of a predefined set of reference points placed on a normalised hyperplane of an Mdimensional objective-space <ref type="bibr" target="#b16">[17]</ref>, where each individual in the population is associated to a reference point <ref type="bibr" target="#b8">[9]</ref>. Moreover, the total number of reference points depends on the division of each objective axis. Both, the crowding distance and niching in NSGA-II and NSGA-III respectively aims at preserving diversity in the population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hypervolume Analysis for Pareto-Optimality</head><p>An evolutionary process (e.g. NSGA-II or NSGA-III) for two objectives gives a non-dominated set of solutions. A nondominated solution is the one for which no one objective function can be improved without a simultaneous detriment to at least one of the other objectives <ref type="bibr" target="#b7">[8]</ref>. The non-dominated solution is also known as the Pareto-optimal solution. Moreover, a set of such solution creates a Pareto-optimal front.</p><p>In this study, we choose to compute the hypervolume indicator H i , which measures the dominance of Paret-front solutions on a geometric space (area for a 2D objective space) framed by the M -dimensional objective-space with respect of a positive semi-axle. Hence, H i measures the quality Paretooptimal solutions set <ref type="bibr" target="#b13">[14]</ref>, and it is an indicator of the quality of the solutions obtained by two algorithms with respect to the same reference frame. We want hypervolume indicator index H i to be maximised. A greater value indicates that the overall performance of the algorithm is better with respect to another algorithm associated with a smaller hypervolume value. Moreover, the greatest contributing point in a hypervolume indicator analysis is the point that covers the largest area and that can be considered as the best solution <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiment Set-Up</head><p>We designed our experiments to evaluate the performance of MONT algorithm on a set of different datasets each pertaining to distinct feature-space and a varied number of classes. The datasets used for the experiments were retrieved from the UCI machine learning repository <ref type="bibr" target="#b18">[19]</ref>. The details of the dataset are described in <ref type="table" target="#tab_0">Table I</ref>. These datasets were chosen because of their diversity in feature-space. For example, the dataset Australia and Heat have a mix of nominal and categorical attributes (features), the dataset Iris and Glass have real-values attributes, and the dataset Ionosphere has it every attribute within the range of -1.0 to 1.0. These make a single algorithm to perform equally well on each dataset difficult <ref type="bibr" target="#b19">[20]</ref>.</p><p>The performance of the proposed MONT (NSGA-III version) trained over these datasets was compared with the performance of five well-known algorithms bearing a differing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Name</head><p>Features <ref type="table" target="#tab_0">Samples Classes  aus  Australia  14  691  2  hrt  Heart  13  270  2  ion  Ionosphere  33  351  2  pma  Pima  8  768  2  wis  Wisconsin  30  569  2  irs  Iris  4  150  3  win  Wine  13  178  3  vhl  Vehicle  18  846  4  gls  Glass  9  214  7</ref> characteristic and a tree-based algorithm heterogeneous flexible neural tree (HFNT) <ref type="bibr" target="#b10">[11]</ref>. We used decision tree (DT) <ref type="bibr" target="#b2">[3]</ref>; multilayer perception (MLP) <ref type="bibr" target="#b0">[1]</ref>; reduced error pruning tree (REP-T) <ref type="bibr" target="#b3">[4]</ref>; na?ve Bayes classifier (NBC) <ref type="bibr" target="#b4">[5]</ref>; and support vector machine (SVM) <ref type="bibr" target="#b5">[6]</ref>. We chose the state-of-the-art implementations of these algorithms from the Weka tool <ref type="bibr" target="#b20">[21]</ref>. Each of these chosen algorithms differs in their nature. Therefore, we expect them to perform differently over a diverse range of the dataset. Hence, we look for the average performance of MONT over a set of datasets in comparisons to the average performance of the chosen algorithms, as well as; we look for the comparisons between three different versions of neural tree (MONT) training processes and settings. These versions are: single objective GP based training, MONT 1 , NSGA-II enabled multi-objective optimisation, MONT 2 , and NSGA-III enabled multi-objective optimisation, MONT 3 . Additionally, each of these training versions MONT 1 , MONT 2 , and MONT 3 also run for three different activation nodes: Gaussian, sigmoid, and tangent hyperbolic (tanh) functions. Hence, six training versions of MONT were evaluated.</p><p>The three training versions MONT 1 , MONT 2 , and MONT 3 respectively assume GP, NSGA-II, and NSGA-III and follows the evolutionary process outlined in Algorithm 1. For the comparison of solutions obtained by these three versions of MONT training, a reference point 1.0 and 100 respectively indicating the worst test error-rate and worst tree-size was chosen for computing hypervolume indicator index H i .</p><p>For the performance comparisons, we set-up a hold-out method of validation where datasets were randomly partitioned into 80% training and 20% test sets. For each dataset, 30 runs of algorithm training and testing results were collected. The parameter settings of the chosen algorithms were set to their default settings prescribed in Weka tool. A summary of the training parameters used for each algorithm are as follows:</p><p>? MONT: iterations -100; population -50; max child nodes -5; max tree height -10; optimisers -GP, NSGA-II, and NSGA-III; crossover probability 0.5; mutation probability 0.5; NSGA-III's reference point division -10. ? HFNT: iterations -100; population -50; max child nodes -5; max tree height -10; optimisers -NSGA-II; crossover probability 0.5; mutation probability 0.5. tree punning -no pruning. ? SVM: kernel type at the nodes -redial basis function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hypervolume Indicator Analysis and Tree Selection</head><p>Each solution in the MONT's population has an error-rate and a tree-size associated with it which are conflicting objectives. For MONT's population, the Pareto-optimal solutions set can be evaluated by hypervolume indicator index H i as discussed in Section II-D. The hypervolume indicator analysis allow us to select the best solution from the MONT 3 's population for each dataset. <ref type="figure" target="#fig_2">Fig. 3</ref> shows hypervolume indicator analysis of the dataset over two objectives: neural tree-size against training error-rate. In <ref type="figure" target="#fig_2">Fig. 3</ref>, the blue (upside) triangle indicates the greatest contributing point with respect to the reference point (maximum tree-size and maximum error-rate of the population with an offset 0.1) indicated with symbol "*." The Pareto-optimal solutions are indicated in red dots and other feasible solution are in gray. The best neural tree (MONT 3 ) for each dataset was obtained using the hypervolume indicated analysis. The greatest contributing point was considered as the best MONT satisfying both objectives: tree-size and error-rate. The average test errorrates of the best trees of the 30 runs are compared with other algorithms (Section III-B). <ref type="figure">Fig. 4</ref> shows the best trees obtained by using hypervolume analysis for Iris (marked T 1 and T 2 in <ref type="figure" target="#fig_2">Fig. 3</ref>) and Wine (marked T a and T b in <ref type="figure" target="#fig_2">Fig. 3</ref>). <ref type="figure" target="#fig_0">Fig. 5</ref> illustrate the example of MONT 3 learning ability over the 100 generations of the evolutionary optimisation process for datasets Iris and Wine. In <ref type="figure" target="#fig_0">Fig. 5</ref>, the MONT 3 performance is also indicated through training and test receiver operating characteristic (ROC) curve plot where solutions lying top-left corner indicate good performance of the classifier.</p><p>Moreover, since hypervolume inductor analysis is the performance quantifier of the multi-objective algorithm's solutions quality, we compared MONT 3 solutions with MONT 1 and MONT 2 with a reference point bearing test error-rate 1.0 and tree-size 100. The results of the analysis are shown in <ref type="table" target="#tab_0">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Tree Performance Against Other Algorithms</head><p>The collected results of 30 runs of MONT 3 and 30 runs of the mentioned algorithms HFNT, MLP, REP-T, NBC, DT, and SVM are shown in <ref type="table" target="#tab_0">Table III</ref>. <ref type="table" target="#tab_0">Table III</ref> shows the average test error and variance over the mentioned nine datasets. Since not a single algorithm's performance (measured as per the average test error-rate) outperform all other algorithms, the three lowest average test error obtained by the respective algorithms for each dataset are marked in bold <ref type="table" target="#tab_0">(Table III)</ref>.    We performed two-sided t-test statistics with setting the alpha value to 0.05 to compare the average error-rate of MONT 3 algorithm against other algorithms for each dataset. Hence, as a null hypothesis, we test "whether the average test error-rate of the MONT 3 is significantly lower than the average test error-rate of other algorithms?" The results of the statistical t-test are shown in <ref type="table" target="#tab_0">Table IV</ref>. <ref type="table" target="#tab_5">Table V</ref> shows the best test error of 30 runs of experiments over the mentioned nine datasets. Similar to the performance of algorithms shown in <ref type="table" target="#tab_0">Table III</ref>, the three lowest test error-  rate by respective algorithms for each dataset are marked in bold <ref type="table" target="#tab_5">(Table V)</ref>. Section IV presents a detailed discussion of the performance of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Neural Tree Training Versions Performances</head><p>Apart from comparing the MONT 3 's performance against other algorithms, the performances of the MONT's training versions were also evaluated and compared. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the performance of three training versions of MONT: MONT 1 , MONT 2 , and MONT 3 . <ref type="figure" target="#fig_5">Fig. 6(a)</ref> is a box plot of the errorrates collected for 30 runs of each of these training versions, i.e. optimisation of MONT using GP, NSGA-II, and NSGA-III respectively. Additionally, <ref type="figure" target="#fig_5">Fig. 6(b)</ref> shows the performance of MONT for three versions of nodes used: Gaussian, sigmoid, and tangent hyperbolic labelled 1, 2, and 3, respectively. Moreover, the average training error-rates and average treesize produced by MONT 1 , MONT 2 , and MONT 3 representing training by GP, NSGA-II, and NSGA-III respectively are shown in <ref type="table" target="#tab_0">Table VI</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>The MONT optimisation is an evolutionary process where a population of MONTs competes to yield the fittest solution. The computational properties of a MONT are similar to an MLP except for that the MONT has a tree-like structure compared to MLP that has fully connected network-like structure and MONT does a simultaneous feature selection ignoring insignificant features during its induction.</p><p>The MONT uses an evolutionary learning process for its tree structure evolution. The optimisation of tree-structure gives MONT the ability to find a hypothesis from a large hypothesis search-space as mentioned in Section II-C. Moreover, two multi-objective optimisers NSGA-II and NSGA-III both bearing differing mechanism for maintaining diversity in the population of an evolutionary process ensures exploration of this large hypothesis search space. This gives MONT the ability to induce a hypothesis from topological-space, featurespace, and parameter-space specific to each class of each problem. <ref type="table" target="#tab_0">Table III</ref> shows that the performance of the solution obtained by hypervolume inductor analysis from MONT 3 's population dominates other listed algorithms, which is evident from the MONT 3 being in the top three performing algorithms for the most selected datasets. The average test error-rate of the MONT 3 outperformed all algorithms for three datasets: Australia, Iris, and Pima <ref type="table" target="#tab_0">(Table III)</ref>. The performance of MONT 3 was closely second to other algorithms for the datasets Wine, Wisconsin, and Vehicle. For dataset Glass, the performance of all algorithms marginally differed from each other, and the best performing algorithms was REP-T.</p><p>The statistical significance test using two-sided t-test shown in <ref type="table" target="#tab_0">Table IV</ref> indicates that MONT 3 has statistically significant performance for dataset Australia, Iris, and Pima, and Wisconsin. For datasets, Heart, Ionosphere, Vehicle, can be said competitive since some other algorithm's average test error rates were better but not statistically significant. For the dataset, wine, however, REP-Tree and NBC appeared to be statistically significant.</p><p>Similarly, the best test error-rates of the algorithms over datasets shown in <ref type="table" target="#tab_5">Table V</ref> indicate that MONT 3 performed well on six datasets out of nine datasets. And, MONT 3 performed competitively over two datasets. This is evident from the statistical test where the average error-rate of MONT was found statistically significant and equivalent to 8 datasets. This performance of the MONT 3 is in the view that MONT 3 induce a hypothesis by simultaneous minimisation of the hypostasis complexity (model's parameter reduction) and the error rate minimisation. Whereas, other algorithms had a single objective (error rate) to minimise. However, one advantage with MONT 3 its nature of being population-based hypothesis induction compared with other mentioned algorithm. Consider this fact the MONT 3 was trained with small-scale training set with a minimum of 100 iterations and a population of 50 individuals. Hence, the performance of MONT 3 may improve for a higher number of iteration and population size.</p><p>Apart from comparing MONT 3 (trained by NSGA-III) performance against other algorithms, its performance was compared with two other training versions: MONT 2 (trained by NSGA-II) and single objective (optimisation of error-rates) version of MONT trained with GP. As per the obtained results of these three training versions shown in <ref type="figure" target="#fig_5">Fig. 6(a)</ref>, the NSGA-III based optimisation of MONT, i.e., version MONT 3 performed well in the cases of six datasets. As well as, the Pareto-optimality analysis shown in <ref type="table" target="#tab_0">Table II</ref> shows that on a hypervolume indicator analysis, the Pareto-optimal solutions set generated by NSGA-III is competitively better than NSGA-II and GP, which is attributed to its exploitation of non-dominated sorting and niching operator for population diversity maintenance. This is evident from average H i values 77.66 obtained for NSGA-III is higher than the NSGA-II (H i = 77.53) and GP (H i = 75.33). That is a solution in the NSGA-III tends to offer a better trade-off between treesize and test error-rates.</p><p>The MONT 3 performance was also compared for three different types of activation functions: Gaussian, Sigmoid, and tangent hyperbolic that can be used as activation nodes. The performance of Gaussian function was found better in the cases of seven datasets [ <ref type="figure" target="#fig_5">Fig. 6(b)</ref>]. However, the performance of the Sigmoid function was competitive and very close to the Gaussian function. The Gaussian function may have advantages of its ability to possess varies shapes during training, whereas the sigmoid function the advantages of the bias at the neural nodes. Moreover, this is evident from MONT's better performance compared to HFNT that uses a set of heterogeneous nodes (a variety of activation function set) without a bias input <ref type="table" target="#tab_0">(Table VI)</ref>. As well as, MONT 3 's NSGA-III based training results lower tree-size than HFNT.</p><p>In MONT algorithm, each class is represented by a subtree, and each subtree competes with another subtree to maximise its classification accuracy <ref type="figure" target="#fig_0">(Fig. 5</ref>). This comparative nature of subtrees of MONT results in maximisation of the MONT's classification accuracy. This is evident from the differences between the subtrees of each class obtained by the MONT algorithm, as shown in <ref type="figure">Fig. 4</ref> for Iris and Wine. Moreover, the learning process of each class during an evolutionary process shown in <ref type="figure" target="#fig_0">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes a new algorithm so-called multi-output neural tree (MONT) that is trained by using non-dominated sorting algorithm frameworks (NSGA) III. The evolutionary process yields a population of MONTs as the solutions to a problem. The population of MONTs was analysed using hypervolume indicator analysis for selection of a Paretooptimal set. The performance of the MONT was compared with five well-known algorithms: decision tree, multilayer perceptron, reduced error-pruning tree, na?ve Bayes, and support vector machine and heterogeneous flexible neural tree. The performance of MONT outperforms the mentioned algorithms on three datasets, and for another three datasets, it was competitive with other algorithms. In general, results show that the MONT performed competitively over a larger set of chosen data compared with the other algorithms. This performance of MONT attributed to its property where each class is represented as a subtree that competes against the other subtrees for improving the classification accuracy. In an additional set of experiments MONT's three training version GP, NSGA-II, and NSGA-III were compared, where NSGA-III emerged better in terms of minimising the trade-off between two objective tree-size and training accuracy. The use of activation function Guassun and sigmoid were found better choice compared to tangent-hyperbolic. Thus, simultaneously maximising overall classification accuracy and minimising tree-size (reducing parameter). These properties of MONT is crucial to achieving generalisation ability in searching a hypothesis from hypothesis-space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 :</head><label>5</label><figDesc>combined population: R = P g + Q6:    evaluation: NSGA-II/III non-dominated sorting(R)7:    survive: elitism/niching (P g+1 , size(P 0 ), R)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? MLP: iterations -500; number of hidden layer nodes -(features + classes)/2; learning rate -0.3; momentum rate -0.2; optimiser-backpropagation. ? REP-T: max tree height -unlimited; tree punning -no pruning; max feature at a node -2. ? NBC: distribution function -normal distribution. ? DT: attributes information quality measure -Gini index;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Hypervolume analysis of MONT 3 over datasets (Iris top) and on aus, hrt, ion, pim, wis, win, vhl, and gls, respectively from top left to bottom right. The reference point marked in "*", blue (upside) triangle is the greatest contributing point. Pareto-front solutions are indicated by the red dots on the left bottom corner and other feasible solutions are indicated in gray dots. Select trees are marked T i (see iris and wine plots) and are shown in Fig 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Best performing trees (as per Fig. 3) for datasets Iris (T 1 and T 2 ) and Wine (Ta and T b ). The shaded nodes are function nodes. Trees T 1 , T2, Ta, and T b of respective datasets gives test error-rate 0.00, 0.013, 0.00, 0.167. Performance of an evolutionary generation for the Iris-T 1 (top) and Wine-Ta (bottom) results shown in Fig. 4. The error-rate reduction is shown by the line, the ROC plot for train (top) and test (bottom) in each plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>aus1 aus2 aus3 gls1 gls2 gls3 hrt1 hrt2 hrt3 ion1 ion2 ion3 irs1 irs2 irs3 pma1 pma2 pma3 vhl1 vhl2 vhl3 win1 win2 win3 wis1 wis2 wis3 0gls1 gls2 gls3 hrt1 hrt2 hrt3 ion1 ion2 ion3 irs1 irs2 irs3 pma1 pma2 pma3 vhl1 vhl2 vhl3 win1 win2 win3 wis1 wis2 wis3 0Performance of neural tree training versions: (a) Performance of optimisers GP, NSGA-II, and NSGA-III respectively marked 1, 2, and 3 as the subscript of the dataset names. (b) Performance of for activation functions at neural nodes: Gaussian, sigmoid, and tanh respectively marked 1, 2, and 3 as the subscript of dataset names. The median of error-rates is marked in red; the average error-rate is marked in a green triangle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DESCRIPTIONS</head><label>I</label><figDesc>OF THE DATASETS USED IN THE EXPERIMENTS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II QUALITY</head><label>II</label><figDesc>OF TRADE-OFF OBTAINED BY HYPERVOLUME INDICATOR H i ON THREE VERSIONS OF MONT.</figDesc><table><row><cell>Data</cell><cell>MONT 1</cell><cell>MONT 2</cell><cell>MONT 3</cell></row><row><cell>aus</cell><cell>84.10</cell><cell>83.57</cell><cell>83.57</cell></row><row><cell>gls</cell><cell>44.14</cell><cell>56.16</cell><cell>52.35</cell></row><row><cell>hrt</cell><cell>78.74</cell><cell>77.46</cell><cell>77.46</cell></row><row><cell>ion</cell><cell>85.65</cell><cell>83.83</cell><cell>88.97</cell></row><row><cell>irs</cell><cell>85.60</cell><cell>90.00</cell><cell>89.97</cell></row><row><cell>pma</cell><cell>73.38</cell><cell>76.01</cell><cell>76.01</cell></row><row><cell>vhl</cell><cell>49.21</cell><cell>52.72</cell><cell>51.99</cell></row><row><cell>win</cell><cell>86.47</cell><cell>87.44</cell><cell>87.28</cell></row><row><cell>wis</cell><cell>90.68</cell><cell>90.55</cell><cell>91.34</cell></row><row><cell>Avg.</cell><cell>75.33</cell><cell>77.53</cell><cell>77.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III AVERAGE</head><label>III</label><figDesc>TEST ERROR-RATE F? AND VARIANCE F? OF 30 RUNS OF EXPERIMENTS ON MONT 3 AND OTHER ALGORITHMS Note: for all datasets three lowest average test error rates are marked in Bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>f 1</cell><cell>aus</cell><cell>hrt</cell><cell>ion</cell><cell>pma</cell><cell>wis</cell><cell>irs</cell><cell>win</cell><cell>vhl</cell><cell>gls</cell><cell>Avg.</cell></row><row><cell>MONT 3</cell><cell>f?</cell><cell>0.111</cell><cell>0.191</cell><cell>0.102</cell><cell>0.201</cell><cell>0.038</cell><cell>0.011</cell><cell>0.048</cell><cell>0.450</cell><cell>0.371</cell><cell>0.169</cell></row><row><cell></cell><cell>f?</cell><cell>0.002</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.003</cell><cell>0.001</cell><cell>0.021</cell></row><row><cell>HFNT</cell><cell>f?</cell><cell>0.174</cell><cell>0.230</cell><cell>0.178</cell><cell>0.284</cell><cell>0.065</cell><cell>0.189</cell><cell>0.176</cell><cell>0.591</cell><cell>0.601</cell><cell>0.276</cell></row><row><cell></cell><cell>f?</cell><cell>0.006</cell><cell>0.004</cell><cell>0.003</cell><cell>0.003</cell><cell>0.001</cell><cell>0.019</cell><cell>0.014</cell><cell>0.005</cell><cell>0.015</cell><cell>0.039</cell></row><row><cell>MLP</cell><cell>f?</cell><cell>0.175</cell><cell>0.213</cell><cell>0.094</cell><cell>0.249</cell><cell>0.024</cell><cell>0.040</cell><cell>0.037</cell><cell>0.183</cell><cell>0.367</cell><cell>0.154</cell></row><row><cell></cell><cell>f?</cell><cell>0.001</cell><cell>0.004</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.002</cell><cell>0.000</cell><cell>0.001</cell><cell>0.004</cell><cell>0.013</cell></row><row><cell>REP-T</cell><cell>f?</cell><cell>0.150</cell><cell>0.247</cell><cell>0.107</cell><cell>0.255</cell><cell>0.096</cell><cell>0.064</cell><cell>0.071</cell><cell>0.291</cell><cell>0.348</cell><cell>0.181</cell></row><row><cell></cell><cell>f?</cell><cell>0.001</cell><cell>0.004</cell><cell>0.002</cell><cell>0.001</cell><cell>0.003</cell><cell>0.001</cell><cell>0.000</cell><cell>0.001</cell><cell>0.005</cell><cell>0.012</cell></row><row><cell>NBC</cell><cell>f?</cell><cell>0.231</cell><cell>0.176</cell><cell>0.166</cell><cell>0.244</cell><cell>0.026</cell><cell>0.047</cell><cell>0.070</cell><cell>0.544</cell><cell>0.525</cell><cell>0.225</cell></row><row><cell></cell><cell>f?</cell><cell>0.001</cell><cell>0.003</cell><cell>0.002</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.008</cell><cell>0.035</cell></row><row><cell>DT</cell><cell>f?</cell><cell>0.146</cell><cell>0.312</cell><cell>0.126</cell><cell>0.337</cell><cell>0.514</cell><cell>0.070</cell><cell>0.370</cell><cell>0.463</cell><cell>0.337</cell><cell>0.297</cell></row><row><cell></cell><cell>f?</cell><cell>0.001</cell><cell>0.012</cell><cell>0.001</cell><cell>0.001</cell><cell>0.004</cell><cell>0.001</cell><cell>0.002</cell><cell>0.002</cell><cell>0.006</cell><cell>0.024</cell></row><row><cell>SVM</cell><cell>f?</cell><cell>0.455</cell><cell>0.461</cell><cell>0.073</cell><cell>0.353</cell><cell>0.532</cell><cell>0.029</cell><cell>0.369</cell><cell>0.754</cell><cell>0.353</cell><cell>0.376</cell></row><row><cell></cell><cell>f?</cell><cell>0.002</cell><cell>0.004</cell><cell>0.001</cell><cell>0.001</cell><cell>0.006</cell><cell>0.001</cell><cell>0.002</cell><cell>0.000</cell><cell>0.004</cell><cell>0.046</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="8">TWO-SIDED T-TEST: MONT 3 AGAINST OTHER ALGORITHMS</cell></row><row><cell cols="2">Data T-test</cell><cell>HFNT</cell><cell cols="2">MLP REP-T</cell><cell>NBC</cell><cell>DT</cell><cell>SVM</cell></row><row><cell>aus</cell><cell>t-stat</cell><cell>-7.02</cell><cell cols="5">-4.59 -13.52 -19.20 -6.42 -44.81</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>hrt</cell><cell>t-stat</cell><cell>-6.69</cell><cell>-4.31</cell><cell>-2.98</cell><cell cols="3">0.00 *  -5.94 -22.71</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00 *</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>ion</cell><cell>t-stat</cell><cell>-3.14</cell><cell>-7.68</cell><cell>0.91 *</cell><cell cols="2">-7.73 -0.78</cell><cell>3.67</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.36 *</cell><cell>0.00</cell><cell>0.44</cell><cell>0.00</cell></row><row><cell cols="2">pma t-stat</cell><cell>-22.68</cell><cell>-8.55</cell><cell>-9.04</cell><cell cols="3">-7.57 -8.28 -25.65</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>wis</cell><cell>t-stat</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>irs</cell><cell>t-stat</cell><cell>-8.24</cell><cell>-7.06</cell><cell>-3.79</cell><cell cols="2">-4.92 -7.25</cell><cell>-2.97</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>win</cell><cell>t-stat</cell><cell>-39.64</cell><cell>-5.98</cell><cell>3.91 ?</cell><cell cols="3">2.93 ? -4.85 -33.49</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00 ?</cell><cell>0.00 ?</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>vhl</cell><cell>t-stat</cell><cell cols="2">-3.01 -11.17</cell><cell cols="4">39.23 -16.33 21.61 -56.72</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.22</cell><cell>0.00</cell></row><row><cell>gls</cell><cell>t-stat</cell><cell cols="2">1.85 *  -10.45</cell><cell>-0.18</cell><cell cols="2">-9.51 1.18 *</cell><cell>0.84 *</cell></row><row><cell></cell><cell>p-Value</cell><cell>0.07 *</cell><cell>0.00</cell><cell>0.86</cell><cell cols="2">0.00 0.24 *</cell><cell>0.40 *</cell></row></table><note>Note: For all datasets NOT marked in Bold, the MONT's average test error-rate was statistically significant than listed algorithms. For results marked * , other algorithm's mean was better than MONT, but statistically NOT significant. Only for dataset "win" indicated with symbol ?, REP-Tree and NBC are statistically significant.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V BEST</head><label>V</label><figDesc>TEST ERROR-RATE OF 30 RUNS OF EXPERIMENTS ON MON 3 MONT 2 MONT 3 HFNT MONT 1 MONT 2 MONT</figDesc><table><row><cell></cell><cell></cell><cell cols="4">COMPARED WITH OTHER ALGORITHMS</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">algorithm</cell><cell></cell><cell></cell></row><row><cell cols="6">Data MONT 3 HFNT MLP REP-T NBC</cell><cell>DT</cell><cell>SVM</cell></row><row><cell>aus</cell><cell cols="2">0.101</cell><cell>0.072 0.123</cell><cell>0.087</cell><cell cols="3">0.181 0.094 0.326</cell></row><row><cell>hrt</cell><cell cols="2">0.093</cell><cell>0.111 0.074</cell><cell>0.148</cell><cell cols="3">0.074 0.167 0.315</cell></row><row><cell>ion</cell><cell cols="2">0.042</cell><cell>0.070 0.028</cell><cell>0.056</cell><cell cols="3">0.085 0.028 0.028</cell></row><row><cell>pma</cell><cell cols="2">0.182</cell><cell>0.195 0.201</cell><cell>0.195</cell><cell cols="3">0.182 0.293 0.292</cell></row><row><cell>irs</cell><cell cols="2">0.000</cell><cell>0.000 0.000</cell><cell>0.000</cell><cell cols="3">0.000 0.000 0.000</cell></row><row><cell>wis</cell><cell cols="2">0.018</cell><cell>0.009 0.000</cell><cell>0.028</cell><cell cols="3">0.000 0.345 0.361</cell></row><row><cell>win</cell><cell cols="2">0.000</cell><cell>0.000 0.009</cell><cell>0.026</cell><cell cols="3">0.027 0.254 0.281</cell></row><row><cell>vhl</cell><cell cols="2">0.388</cell><cell>0.447 0.147</cell><cell>0.218</cell><cell cols="3">0.482 0.400 0.712</cell></row><row><cell>gls</cell><cell cols="2">0.302</cell><cell>0.372 0.256</cell><cell>0.209</cell><cell cols="3">0.395 0.209 0.209</cell></row><row><cell cols="5">Note: the best three are marked in Bold</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="8">AVERAGE TRAINING ERROR AND TREE-SIZE OBTAINED BY THREE</cell></row><row><cell></cell><cell></cell><cell cols="5">VERSIONS OF MONT TRAINING AND HFNT</cell></row><row><cell></cell><cell cols="3">Avg. training error-rate</cell><cell></cell><cell cols="2">Avg. tree-size</cell></row><row><cell cols="8">data MONT 1 3 HFNT</cell></row><row><cell>aus</cell><cell>0.15</cell><cell>0.15</cell><cell cols="2">0.15 0.072 20.03</cell><cell>8.43</cell><cell>8.13</cell><cell>8.53</cell></row><row><cell>hrt</cell><cell>0.26</cell><cell>0.25</cell><cell cols="2">0.26 0.111 27.20</cell><cell>8.59</cell><cell>8.73</cell><cell>9.20</cell></row><row><cell>ion</cell><cell>0.16</cell><cell>0.18</cell><cell cols="2">0.18 0.070 32.16</cell><cell>9.6</cell><cell>9.83</cell><cell>7.63</cell></row><row><cell cols="2">pma 0.27</cell><cell>0.26</cell><cell cols="5">0.26 0.195 49.28 10.61 10.28 14.00</cell></row><row><cell>irs</cell><cell>0.18</cell><cell>0.07</cell><cell cols="5">0.08 0.000 218.51 17.07 16.13 156.73</cell></row><row><cell>wis</cell><cell>0.09</cell><cell>0.09</cell><cell cols="2">0.08 0.009 30.49</cell><cell>9.2</cell><cell>9.91</cell><cell>8.97</cell></row><row><cell>win</cell><cell>0.16</cell><cell>0.15</cell><cell cols="5">0.16 0.000 44.37 13.78 13.94 26.20</cell></row><row><cell>vhl</cell><cell>0.54</cell><cell>0.53</cell><cell cols="5">0.53 0.447 51.36 17.74 18.13 31.23</cell></row><row><cell>gls</cell><cell>0.52</cell><cell>0.5</cell><cell cols="5">0.54 0.372 85.53 25.87 25.83 60.20</cell></row><row><cell cols="2">Avg. 0.26</cell><cell>0.24</cell><cell cols="5">0.25 0.276 62.37 13.43 13.42 35.85</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<date type="published" when="1986-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metaheuristic design of feedforward neural networks: A review of two decades of research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sn??el</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="97" to="116" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data mining tasks and methods: Classification: decision-tree discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Data Mining and Knowledge Discovery</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating continuous distributions in bayesian classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 11th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="338" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving iterated functions using genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers</title>
		<meeting>the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2149" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A fast elitist nondominated sorting genetic algorithm for multi-objective optimization: NSGA-II,&quot; in Parallel Problem Solving from Nature PPSN VI, ser. Lecture Notes in Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1917</biblScope>
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="601" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Time-series forecasting using flexible neural tree model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="235" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ensemble of heterogeneous flexible neural trees using multiobjective genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sn??el</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="909" to="924" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiobjective programming for type-2 hierarchical fuzzy inference trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sn??el</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hybrid learning algorithm for evolving flexible beta basis function neural tree model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dhahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An improved dimension-sweep algorithm for the hypervolume indicator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paquete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?pez-Ib?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Evolutionary Computation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1157" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The number of evolutionary trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Felsenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systematic Zoology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="33" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Genetic Algorithms in Search. Optimization and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Normal-boundary intersection: A new method for generating the pareto surface in nonlinear multicriteria optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="631" to="657" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance assessment of multiobjective optimizers: An analysis and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laumanns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Da Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="132" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The lack of a priori distinctions between learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1341" to="1390" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The weka data mining software: an update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
