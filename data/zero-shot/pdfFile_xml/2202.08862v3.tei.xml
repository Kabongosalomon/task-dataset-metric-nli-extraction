<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RemixIT: Continual self-training of speech enhancement models via bootstrapped remixing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">RemixIT: Continual self-training of speech enhancement models via bootstrapped remixing</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised learning</term>
					<term>speech enhancement</term>
					<term>semi-supervised self-training</term>
					<term>zero-shot domain adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present RemixIT, a simple yet effective selfsupervised method for training speech enhancement without the need of a single isolated in-domain speech nor a noise waveform. Our approach overcomes limitations of previous methods which make them dependent on clean in-domain target signals and thus, sensitive to any domain mismatch between train and test samples. RemixIT is based on a continuous self-training scheme in which a pre-trained teacher model on out-of-domain data infers estimated pseudo-target signals for in-domain mixtures. Then, by permuting the estimated clean and noise signals and remixing them together, we generate a new set of bootstrapped mixtures and corresponding pseudo-targets which are used to train the student network. Vice-versa, the teacher periodically refines its estimates using the updated parameters of the latest student models. Experimental results on multiple speech enhancement datasets and tasks not only show the superiority of our method over prior approaches but also showcase that RemixIT can be combined with any separation model as well as be applied towards any semi-supervised and unsupervised domain adaptation task. Our analysis, paired with empirical evidence, sheds light on the inside functioning of our self-training scheme wherein the student model keeps obtaining better performance while observing severely degraded pseudo-targets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>One of the most fundamental problems in audio processing is speech enhancement, where the goal is to isolate and reconstruct the clean speech component from a noisy input recording <ref type="bibr" target="#b0">[1]</ref>. Several studies have shown that employing such denoising models as front-ends could be useful for building robust automatic speech recognition (ASR) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and speaker recognition <ref type="bibr" target="#b3">[4]</ref> systems. The universal applicability of neural networks has proven to be beneficial for a variety of signal processing problems, including speech enhancement. Sophisticated architectures such as convolutional networks <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, recurrent processing <ref type="bibr" target="#b8">[9]</ref> self-attention <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, generative adversarial networks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> as well as variational autoencoders <ref type="bibr" target="#b14">[15]</ref>, to name a few. Despite the effectiveness of the aforementioned approaches in cases where large amounts of in-domain training paired data are available, real-world applications necessitate the need for developing robust algorithms to train these models with in-the-wild mixtures.</p><p>In the context of speech enhancement, self-supervised learning (SSL) or unsupervised methods differ from semisupervised ones <ref type="bibr" target="#b15">[16]</ref> in the sense that the former do not have access to clean target signals. Orthogonal to these concepts, self-training refers to algorithms which are able to train a new model (student) based on pseudo-targets provided by a previously fitted model <ref type="bibr">(teacher)</ref>. Under this unified terminology, the proposed RemixIT framework can also be viewed as an unsupervised self-training algorithm when only unsupervised data are used to pre-train the teacher model.</p><p>Recent studies have shown that speech representations could be self-learned and be used later for other downstream audio processing tasks <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. However, in real-world settings, the speech recordings are degraded with additive noise, thus, self-learning robust embeddings becomes particularly challenging and demands the adaptation to the input noise distribution <ref type="bibr" target="#b19">[20]</ref>. Several unsupervised speech denoising algorithms have been proposed by identifying and training with relatively clean segments of the noisy speech mixture <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, using ASR losses <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> exploiting visual cues <ref type="bibr" target="#b24">[25]</ref>, and harnessing the spatial separability of the sources using mic-arrays <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Mixture invariant training (MixIT) <ref type="bibr" target="#b27">[28]</ref> enables unsupervised training of separation models only with real-world single-channel recordings by generating artificial mixtures of mixtures and estimating the independent sources. Although MixIT has been proven successful for various speech enhancement tasks <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>, MixIT assumes access to indomain noise samples which restricts its universal applicability. Overcoming the latter constraint by injecting additional out-of-domain (OOD) noise sources to the input mixture of mixtures <ref type="bibr" target="#b30">[31]</ref> further alters the input signal-to-noise ratio (SNR) distribution and its performance depends heavily on the distribution shift between the injected and real noise distributions. Thus, developing a SSL algorithm which does not depend on external modality information nor assumptions about in-domain data remains a challenging problem.</p><p>On the other hand, several self-training strategies have emerged and showed promising results in classification tasks using convex combinations of labeled and unlabeled data (e.g. Mixup <ref type="bibr" target="#b31">[32]</ref>) but have also been successfully applied to several audio tasks <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, a student model with a smaller number of estimated sources has been trained on <ref type="bibr">0000</ref> Teacher's speech estimates <ref type="figure">Fig. 1</ref>: RemixIT self-training procedure with a batch size of 4 noisy mixtures. A teacher speech enhancement model f T is pre-trained in a supervised or unsupervised way on out-of-domain (OOD) data and performs inference on a batch of noisy mixtures sampled from the in-domain noisy speech dataset m ? D m . The randomly permuted teacher's noise estimates P n are added together with the teacher's speech estimates s to form the bootstrapped mixtures m which are fed to the student speech enhancement network f S . The student is trained by regressing over the teacher's estimated sources which are now used as pseudo-targets under a specified signal-level loss function. After repeating the overall process for K optimization steps, the teacher model may be updated using the student's weights in a continuous self-training scheme.</p><p>a subset of outputs of a pre-trained MixIT model to solve the input SNR distribution mismatch. Furthermore, a student model could also perform test-time adaptation by using the teacher's estimated waveforms as targets <ref type="bibr" target="#b35">[36]</ref>. However, those approaches enforce only the consistency of the student's predictions over a frozen teacher's output pseudo-targets whereas other studies have shown that one can obtain significant gains using unsupervised data augmentation <ref type="bibr" target="#b36">[37]</ref>, averages of losses over multiple predictions <ref type="bibr" target="#b37">[38]</ref>, or their combination <ref type="bibr" target="#b38">[39]</ref>.</p><p>The student-teacher framework for singing-voice separation in <ref type="bibr" target="#b39">[40]</ref> bears the closest similarity to our work. The proposed setup assumes teacher pre-training on supervised OOD data, performing inference on the in-domain noisy dataset and storing the new pseudo-labeled dataset. At a second step, a student network is trained on randomly mixed estimated sources that score above a pre-defined confidence quality threshold. Unfortunately, if the teacher's estimates have low SNR and/or the threshold is not picked wisely then the student model would also perform poorly. In contrast, some of the most successful self-training approaches propose to iteratively update the teacher's weights using an exponential moving average scheme <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref> or sequentially update the teacher with the weights from a more expressive noisy student <ref type="bibr" target="#b43">[44]</ref>.</p><p>In this work, we propose RemixIT which is based on several aforementioned state-of-the-art SSL strategies for pseudolabeling and continual training while also providing a novel technique for training speech enhancement models with OOD data. Our method trains a student model using self-augmented mixtures generated by permuting and remixing the teacher's estimates and using them as pseudo-targets for regular regression. Moreover, RemixIT treats self-training as a lifelong process while continually updating the teacher model using the student's weights that consequently leads to faster and more robust convergence. RemixIT is the first method that:</p><p>? Performs self-supervised learning using only in-domain mixture datasets and OOD noise sources (e.g. MixIT pretrained teacher with an OOD dataset).</p><p>? Yields state-of-the-art results on several unsupervised and semi-supervised denoising tasks without the need of clean speech waveforms or ad-hoc filtering procedures.</p><p>? Has strong theoretical and empirical evidence of why it works under various noise levels.</p><p>? Is able to leverage huge amounts of unsupervised data and generalize in diverse training and adaptation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REMIXIT METHOD</head><p>RemixIT trains a speech enhancement model to isolate the clean speech signal from its noisy observation. In general, we train a separation model f which outputs M source waveforms for each input noisy speech recording with T time-domain samples. Thus, given as input a batch of B input waveforms x ? R B?T the network estimates all sound sources:</p><formula xml:id="formula_0">s, n = f (x; ?), x = s + M ?1 i=1 [n] i = s + M ?1 i=1 [ n] i , (1)</formula><p>where s, s ? R B?T , n, n ? R (M ?1)?B?T , ? are: the estimated speech signal, the clean speech target, the estimated noise signal, the noise target and the parameters of the model, respectively. We force the estimated sources s and n to add up to the initial input mixtures x by using a mixture consistency projection layer <ref type="bibr" target="#b44">[45]</ref>. We portray the inference and self-training aspects of RemixIT in <ref type="figure">Figure 1</ref>, summarize it in Algorithm 1 and analyze it in depth in Section II-C. For completion, we highlight how RemixIT differs from fully supervised training (assumes access to clean in-domain speech) and previous state-of-the-art semi-supervised training methods (MixIT assumes access to isolated in-domain noise recordings) in Sections II-A and II-B, respectively. Algorithm 1: REMIXIT for the noisy dataset D m .</p><formula xml:id="formula_1">? (0) T ? PRETRAIN TEACHER(f T , D ) ? S ? INITIALIZE STUDENT(f S ) for k = 0; k++; while k &lt;= K do for SAMPLE BATCH m ? D m , m ? R B?T do s, n ? f T (m; ? (k) T ) // Teacher's estimates m = s + P n // Bootstrapped remixing s, n ? f S ( m; ? (k) S ) // Student's estimates L RemixIT = B b=1 [L( s b , s b ) + L( n b , [P n] b )] ? S ? UPDATE STUDENT(? S , ? ? S L RemixIT ) end ? (k+1) T ? UPDATE TEACHER(? (k) T , ? S ) end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervised training</head><p>Supervised training is the straightforward way of training speech enhancement models. It assumes access to both indomain clean speech recordings, s ? D s , as well as noise sources drawn from n ? D n . Synthetic mixtures are generated at each training step m = s + n, by sampling a batch of clean speech recordings s ? D s and a batch of isolated noise samples n ? D n , which are then fed to the separation model f . For a sampled batch of B input mixtures, the model predicts M = 2 sources for each input mixture ( s, n = f (x; ?)) and the following targeted loss function is minimized:</p><formula xml:id="formula_2">L Supervised = B b=1 [L( s b , s b ) + L( n b , n b )] ,<label>(2)</label></formula><p>where L is any desired signal-level loss function used to penalize the reconstruction error between the estimates and their corresponding targets. However, this training process is completely dependent on the availability of clean speech and noise sources to capture the real-world mixture distribution, making the model vulnerable to a performance decline under unseen test conditions. This necessitates the development of SSL and adaptation techniques for speech enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mixture invariant training (MixIT)</head><p>MixIT <ref type="bibr" target="#b27">[28]</ref> is a simple yet effective idea for training a separation model using artificial mixtures of mixtures (MoMs). In essence, MixIT assumes availability of two sources of data during training, D m which consists of mixtures of speech and a noise source and D n which contains noise recordings from a single noise source. The training process boils down to sampling a batch of noisy speech recordings m ? D m (where m = s + n (1) ), and mixing them with another batch of isolated noise recordings n (2) ? D n . Note that the true noise distribution of the real-world D * n (n (1) ? D * n ) is unknown and not necessarily same as the one available D n . The separation model f M is trained using the synthetic batch of input-MoMs x = s + n 1 + n 2 and tries to reconstruct M = 3 sources s, n <ref type="bibr" target="#b0">(1)</ref> , n (2) = f M (x; ? M ), by minimizing the following permutation invariant <ref type="bibr" target="#b45">[46]</ref> loss function:</p><formula xml:id="formula_3">L (b) MixIT = min ??P L( s b + n (?1) b , m b ) + L( n (?2) b , n b ) , (3)</formula><p>where b is the batch's index and P := {(1, 2), (2, 1)} is the set of permutations between the model's noise output slots. One could also use a probabilistic assignment of the noise estimates n</p><formula xml:id="formula_4">(?1) b , n (?2) b</formula><p>to avoid emerging problems with the complex permutation invariant landscapes <ref type="bibr" target="#b46">[47]</ref>.</p><p>If the noise sources are independent from each other and the clean speech component, then the model can learn to minimize this loss by reconstructing the mixture using its first estimated slot and either one of the two noise slots available. Although MixIT has been proven effective for various simulated speech enhancement setups <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, the assumption about having access to a diverse set of in-domain noise recordings from D n which aptly captures the true distribution of the present background noises D * n make it impractical for many real-world settings. To this end, other works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref> have tried to deal with the distribution shift between the on-hand noise dataset D n and the actual noise distribution D * n in order to avoid the need of in-domain noise samples. Specifically, <ref type="bibr" target="#b30">[31]</ref> proposes to use extra noise injection from an OOD distribution and in <ref type="bibr" target="#b23">[24]</ref> ASR and disentanglement losses have been proposed. However, the performance of the former method still depends heavily the level of distribution shift between the actual noise distribution D * n and D n while the latter method is more restrictive since it requires large pre-trained ASR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RemixIT: Self-training with bootstrapped remixing</head><p>In contrast to the aforementioned two training procedures which require in-domain ground truth signals (e.g. supervised training requires clean speech samples from s ? D s as well as access to in-domain noise recordings sources drawn from n ? D n while MixIT requires only isolated in-domain noise waveforms), RemixIT does not depend on any other in-domain information besides the mixture dataset D m . Specifically, our method utilizes a student-teacher framework where the teacher's noise estimates are randomly permuted in a minibatch sense and remixed with the teacher's speech estimates to create bootstrapped mixtures. A student model is trained using as input the bootstrapped mixtures and regressing over the teacher's pseudo-target signals using a regular supervised loss at every optimization step (for a succinct description of the training procedure please see Algorithm 1). RemixIT also enjoys a continual refinement of the noisy pseudo-target signals, after a few optimization steps, where the student model weights are used to update the teacher network as it is illustrated in <ref type="figure">Figure 1</ref>.</p><p>1) RemixIT's teacher-student framework: For the initial teacher model, RemixIT can use any speech enhancement model pre-trained on an OOD dataset D which outputs the speech component and one or more noise estimated waveforms (see specification in Equation 1). To this end, RemixIT materializes into semi-supervised domain adaptation if the teacher was trained using a supervised loss and into a SSL training scheme if the teacher was trained using MixIT.</p><p>Formally, given an batch of in-domain noisy mixtures m = s + n ? R B?T , m ? D m , the teacher model estimates the speech and the noise components as follows:</p><formula xml:id="formula_5">s, n = f T (x; ? (k) T ), m = s + M ?1 i=1 [n ] i<label>(4)</label></formula><p>where ? (k) T denotes the parameters of the teacher model at the k-th optimization step. The second equation holds because we enforce mixture consistency. A MixIT pre-trained model would estimate M = 3 sources and we can easily get a consolidated noise estimate by summing the two latter noise estimated waveforms, namely, n =</p><formula xml:id="formula_6">M ?1 i=1 [n ] i .</formula><p>Notice that the teacher model f T does not need to be identical through the whole training process and could be updated using any user-specified protocol which results in a separation model that respects the constraints defined in Equation <ref type="bibr" target="#b3">4</ref>. The teacher's estimates within a batch of size B are used to generate the bootstrapped mixtures m by remixing the estimated speech and noise sources in a random order:</p><formula xml:id="formula_7">m = s + n (P) ? R B?T , n (P) = P n, P ? ? B?B ,<label>(5)</label></formula><p>where P is drawn uniformly from the set of all B ? B permutation matrices and is used to produce the permuted noise sources n (P) . The original teacher's speech estimates s and the permuted noise sources n (P) are now used as target pairs to train the student model f S on the newly generated batch of bootstrapped mixtures m as shown below:</p><formula xml:id="formula_8">s, n = f S ( m; ? (k) S ), s, n ? R B?T L (b) RemixIT = L( s b , s b ) + L( n b , P n b ), b ? {1, . . . , B}. L RemixIT = B b=1 [L( s b , s b ) + L( n b , P n b )]<label>(6)</label></formula><p>The loss function used is similar to a supervised setup (see <ref type="table" target="#tab_3">Equation 2</ref>) but instead of ground-truth clean source waveforms, we use the noisy estimates, s and n (P) , provided by the teacher network. If the signal-level loss function L also minimizes the Euclidean norm between the estimated signals and the target signals, the proposed cost function L RemixIT enjoys several convergence properties which enable our method to learn in a robust SSL fashion even in cases where the teacher's estimates are not close to the ground-truth source waveforms (see Section II-C2). Lastly, RemixIT refines the estimates of the teacher network f T using the weights from the latest available student models. The continual update protocols used in this study are the sequential and the running moving average update protocols which are explained in detail in Section III-C.</p><p>2) Error analysis under the Euclidean norm: In each optimization step, RemixIT tries to minimize a signal-level loss function between the student's estimates and the teacher's pseudo-targets. Since we are mostly interested in denoising, we focus on the speech estimates of the teacher and the student networks with initial mixtures M and the bootstrapped mixtures M as inputs, respectively. These estimates can also be expressed in the following way as random variables:</p><formula xml:id="formula_9">S = f ( s) T (M = S + N; ? (k) T ), M ? D m S = f ( s) S ( M = S + N (P) ; ? (k) S ).<label>(7)</label></formula><p>Now, the teacher's R T and student's R S errors w.r.t. the initial clean targets S are the following conditional probabilities:</p><formula xml:id="formula_10">R T = S ? S, R S = S ? S R T ? P ( R T |S, N), R S ? P ( R S | S, N, P).<label>(8)</label></formula><p>Using a signal-level loss L that minimizes the squared error between the estimated and the target signals in Equation <ref type="bibr" target="#b5">6</ref> and assuming unit-norm estimated and target signals ||s|| = || s|| = || s|| = 1, RemixIT loss function becomes equivalent to minimizing the following expression:</p><formula xml:id="formula_11">L RemixIT ? E[|| S ? S|| 2 2 ] = E[||( S ? S) ? ( S ? S)|| 2 2 ] = E || R S || 2 2 Supervised Loss + E || R T || 2 2 Constant w.r.t. ? S ?2 E R S , R T</formula><p>Errors' correlation <ref type="bibr" target="#b8">(9)</ref> Ideally, this loss could lead to the same optimization objective with a supervised setup if the last inner-product term was zero since the middle term becomes zero when computing the gradient w.r.t. the student's parameters ? S . R S , R T = 0 could be achieved if the teacher produced outputs indistinguishable from the clean target signals or the conditional error distributions in Equation 8 were independent. Intuitively, as we continually update the teacher model and refine its estimates, we minimize the norm of the teacher error which leads to higher fidelity reconstruction from the student (for further analysis of how the student learns to perform better than its teacher and for experimental validation of this claim we refer the reader to Section IV-D).</p><p>Additionally, the bootstrapped remixing process forces the errors to be more uncorrelated since the student tries to reconstruct the same clean speech signals s, similar to its teacher, but under a different mixture distribution. Formally, the student tries to reconstruct s when observing the bootstrapped mixtures m = s+ n (P) while the teacher tries to reconstruct s only from the initial input mixtures m = s + n. This phenomenon becomes apparent if we focus on the reconstruction of a single speech signal s * from the teacher and the student networks. In essence, we use the teacher network to provide an estimated s * from the corresponding mixture m * and some perturbed noise sources n b , ?b ? {1, . . . , B} to create bootstrapped mixtures:</p><formula xml:id="formula_12">s * , n * = f T (m * = s * + n * ; ? T ) s b , n b = f T (m b = s b + n b ; ? T ) m b = s * + n b , ?b ? {1, . . . , B}.<label>(10)</label></formula><p>In the student-training phase, we perform inference using the student network f S on the batch of the aforementioned bootstrapped mixtures m b . Because RemixIT's loss is computed under expectation (Equation 9), we can rearrange the order of batches that the student network sees. Thus, we focus on the learning aspect of the student network for the batch of bootstrapped mixtures above (Equation 10) and rewrite the last error correlation term as follows:</p><formula xml:id="formula_13">E R S , R T ? E 1 B B b=1 ( s b ? s * ) T ( s * ? s * ) = E[( s * ? s * ) T 1 B B b=1 f ( s) S ( s * + n b ; ? S ) ? s * Empirical mean student error ]<label>(11)</label></formula><p>The premise is that if the student sees a wide variety of bootstrapped mixtures which have been generated using the same teacher's speech estimate s * , then the mean interference error produced by injecting noisy teacher's estimates n b would go to zero under expectation. We prove this claim under ideal conditional independence of the student error vectors and infinite bootstrapped mixtures in Theorem II.1. In practice, the student could still minimize the errors' correlation term and still be able to learn from mixtures when the teacher performs poorly (please see Section IV-E which gives an empirical analysis of our claim).</p><p>Theorem II.1. Assuming a differentiability of the loss functions, access to infinite bootstrapped mixtures B ? ? generated by the teacher network f T , and conditional independence of the student errors given the same teacher speech pseudotarget (f ( s)</p><formula xml:id="formula_14">S ( s * + n i ; ? S ) ? s * ?f ( s) S ( s * + n j ; ? S ) ? s * with i = j)</formula><p>, then the gradients of RemixIT's loss function w.r.t. the student network weights ? S ) converge to the ones provided by an oracle supervised loss</p><formula xml:id="formula_15">? ? S L RemixIT ? ? ? S L Supervised</formula><p>Proof. Combining the definitions of the loss functions from Equations 2 and 6, their difference can be expressed as:</p><formula xml:id="formula_16">L RemixIT ? L Supervised = E || R T || 2 2 ? 2 R S , R T . (12)</formula><p>Following the same analysis with Section II-C2, for each target speaker waveform s * , we use the estimates of the teacher model for the target speech waveform s * in an input mixture m * and for randomly sampled noise sources n b in the corresponding mixtures m b as in Equations 5 to produce bootstrapped mixtures m b = s * + n b , ?b. Thus, the student estimates some speech waveform s b for each input bootstrapped mixture m b and the latter term of the error correlation can be written as follows:</p><formula xml:id="formula_17">E R S , R T = E ( s * ? s * ) T 1 B B b=1 ( s b ? s * ) = E ( s * ? s * ) T 1 B B b=1 [( s b ? s * ) + ( s * ? s * )] = E || R T || 2 2 + E ( s * ? s * ) T 1 B B b=1 ( s b ? s * ) .<label>(13)</label></formula><p>However, the error between each pseudo-target provided by the teacher student s * = f ( s)</p><p>T (m * = s * + n * ; ? T ) and the estimated speech signal by the student</p><formula xml:id="formula_18">s b = f ( s) S ( s * + n b ; ? S )</formula><p>is bounded for any masked-based network operating on some linear bases (we use a linear encoder/decoder as specified in Section III-B) <ref type="bibr" target="#b47">[48]</ref>. Formally, assuming that an the encoded representation of the input bootstrapped mixture m is v = P ? m , then the latent representation of a signal estimate i? v = M (P ? m ). Thus, the l 2 error is bounded by:</p><formula xml:id="formula_19">s b ? s * = ( M b ? M * ) (P ? m b ) ? max s * , s b , n b ? max {( M b ? M * ) P} ? s * + n b = C,<label>(14)</label></formula><p>where ? max {( M b ? M * ) P} &lt; ? denotes the maximum singular value of the masked unrolled synthesis-basis matrix P and s * + n b &lt; ? is the energy of the bounded-norm bootstrapped mixtures. Similarly, the teacher error is also bounded by some real value s * ? s * ? C &lt; ?, ?s * . Thus, by combining the above inequalities with Equations 12 and 13, we conclude that at the limit, the difference of the loss functions converges to a value which is constant with respect to the student network's parameters ? S as shown next:</p><formula xml:id="formula_20">lim B?? [L RemixIT ? L Supervised ] = ?E || R T || 2 2 + lim B?? E ( s * ? s * ) T 1 B B b=1 ( s b ? s * ) = ?E || R T || 2 2 + lim B?? E s * ( s * ? s * ) T ?( s * ) .<label>(15)</label></formula><p>where the last step comes from the application of the central limit theorem since by assumption the student estimates' errors are i.i.d. and bounded, thus, the sample mean converges in distribution to a normal distribution with mean equal to the mean student error ?( s * ) given the corresponding teacher's speech estimate s * . All parts of the right hand-side of the above equation are constant w.r.t. the student network parameters ? S which we try to optimize and thus by applying the gradient operator we can conclude that ? ? S L RemixIT ? ? ? S L Supervised . One could make this theorem even more applicable to real-world settings where the student errors given different bootstrapped mixtures from the initial teacher estimate s * are weakly dependent <ref type="bibr" target="#b48">[49]</ref> but we defer this derivation to future work. The clean speech samples are drawn from the LibriSpeech <ref type="bibr" target="#b50">[51]</ref> corpus and the noise recordings are taken from FSD50K <ref type="bibr" target="#b51">[52]</ref> representing a set of almost 200 classes of background noises after excluding all the human-made sounds from the AudioSet ontology <ref type="bibr" target="#b52">[53]</ref>. A detailed recipe of the dataset generation process is presented in <ref type="bibr" target="#b29">[30]</ref>. LFSD becomes an ideal candidate for semi-supervised/SSL teacher pre-training on OOD data given its mixture diversity. WHAM!: The generation process for this dataset produces 20,000 training noisy-speech pairs and 3,000 test mixtures from the initial WHAM! <ref type="bibr" target="#b53">[54]</ref> dataset and has been identical to the procedure followed in <ref type="bibr" target="#b29">[30]</ref> with active noise sources mixed at an average of ?1.3dB input SNR. The set of background noises in WHAM! is limited to 10 classes of urban sounds. VCTK: The VCTK dataset proposed in <ref type="bibr" target="#b54">[55]</ref> includes 586 synthetically generated noisy test mixtures, where a speech sample from the VCTK speech corpus <ref type="bibr" target="#b55">[56]</ref> is mixed with an isolated noise recording from the DEMAND <ref type="bibr" target="#b56">[57]</ref>. The VCTK and DNS test partitions are used to illustrate the effectiveness of RemixIT under a restrictive scenario zero-shot domain adaptation with limited data to perform self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speech enhancement model</head><p>In the supervised and RemixIT training recipes, the student has M = 2 output slots and always estimates the speech component and the noise source. For the models which are trained with MixIT, we increase the number of output slots to M = 3 to estimate the additional noise component. RemixIT is independent of the choice of the speech-enhancement model architecture as long as the latter estimates both speech and noise components of the input mixture.</p><p>Our model's choice was based on obtaining adequate quality of speech reconstruction with low computational and memory requirements (see <ref type="table" target="#tab_3">Table I</ref> for a head-to-head comparison in a supervised in-domain training setup with the previous stateof-the-art model). To this end, we used the Sudo rm -rf <ref type="bibr" target="#b57">[58]</ref> architecture with the more sparse computation blocks using shared sub-band processing via group communication <ref type="bibr" target="#b58">[59]</ref>. The selected network has shown to provide high-quality source estimates under speech enhancement <ref type="bibr" target="#b29">[30]</ref> as well as sound separation <ref type="bibr" target="#b59">[60]</ref> tasks while significantly reducing the model's size. We consider the selected architecture with a default encoder/decoder with 512 basis 41 filter taps and a hop-size of 20 time-samples, a depth of U = 8 U-ConvBlocks and the same parameter configurations as used in <ref type="bibr" target="#b29">[30]</ref>. In the sequential update protocol we increase the depth of the new student networks every 20 epochs from 8 to 16 and finally 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RemixIT's teacher update protocols configurations</head><p>RemixIT refines the estimates of the student network based on unsupervised and semi-supervised teachers pre-trained on an OOD dataset but also has the capability of repeatedly updating the teacher network to learn from higher-quality source estimates. In our experiments, we evaluate the proposed method under various online teacher updating protocols after k training epochs. Specifically, we consider the following: Static teacher: The teacher is frozen throughout the training process, for all optimization steps ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training and evaluation details</head><p>For the semi-supervised and unsupervised RemixIT's teachers we pre-train the corresponding models following the supervised training process (Section II-A) and MixIT (Section II-B), respectively. Although RemixIT can theoretically work with any valid signal-level loss functions (Equations 3, 6), we choose the negative scale-invariant signal to distortion ratio (SI-SDR) <ref type="bibr" target="#b60">[61]</ref> for training all models:   where we initialize a new student model and replace the teacher model with the latest available student. The sequential protocol shows significant gains over the static teacher protocols where the student network has a static architecture throughout training and the initial teacher is not updated (U = 8 with gray and U = 16 with black dashed lines).</p><p>SI-SDR becomes equivalent with SNR. We train all models using the Adam optimizer <ref type="bibr" target="#b61">[62]</ref> with a batch size of B = 2 and an initial learning rate of 10 ?3 which is divided by 2 every 6 epochs. We fix those hyper-parameters after some early experimentation with the validation set of LFSD. For all experiments, during training we assume that we do not have access to the input SNR distribution and thus, we mix a clean and a noise source without altering their corresponding power ratio. However, for the in-domain supervised training setup with DNS we randomly mix clean speech and noise recordings with SNR from a uniform distribution of [?2, 20]dB, which has been shown to be effective for multiple sound separation setups <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Finally, we normalize all input mixture waveforms by subtracting their mean and dividing by their standard deviation before feeding them to each model. We train and test models which operate at a 16kHz sampling rate. The robustness of all speech enhancement models is measured using the SI-SDR <ref type="bibr" target="#b60">[61]</ref>, the short-time objective intelligibility (STOI) <ref type="bibr" target="#b64">[65]</ref> and the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b65">[66]</ref>. We evaluate the model checkpoints after 100 epochs for the pre-trained teachers and the supervised models and after 60 epochs for all the other configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The need for continual refinement of teacher's estimates</head><p>In <ref type="figure">Figure 2</ref>, we show the speech enhancement performance of the student models produced by a static or a sequentially updated teacher every 20 epochs. Unsurprisingly, all protocols behave similarly until the 20th epoch since they use the same initial teacher. In contrast to the frozen teacher protocols, after the 20th epoch, the old teacher is replaced with the newly trained student with U =8 and a new student, with twice as much depth <ref type="bibr">(</ref>   <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref> and supervised in-domain training with the Sudo rm -rf model <ref type="bibr" target="#b59">[60]</ref> as well as the previous state-of-the-art FullSubNet <ref type="bibr" target="#b7">[8]</ref> supervised model ( * as it was presented in the paper). All teacher and student networks follow the same Sudo rm -rf model <ref type="bibr" target="#b59">[60]</ref> architecture with the specified number of U-ConvBlocks (U = 8 or U = 32). U : 8 ? 16 ? 32 denotes that we double the depth of the student network every 20 epochs and sequentially update the teacher with the latest available student, the reported number refers to the performance of the student with U = 32.</p><p>student separation model, even after the 40th epoch, compared to both models produced by the static teachers which saturate for the same number of training steps. Comparing between the students with U =8 and U =16 produced by static teacher models, it is evident that the more expressive student performs better but not on par with the same depth student produced by the sequentially updated teacher protocol. Specifically, both orange-solid and black-dashed lines at the 40-th epoch represent the performance of a student model with the same depth (U = 16) but the sequential update protocol clearly outperforms the frozen-teacher protocol. Thus, the combination of the bootstrapped remixing and the continual refinement of the teacher's estimates is key for the significant improvement that RemixIT yields. As a result, we have chosen the sequentially updated teacher protocol as the default strategy for RemixIT, except for the zero-shot adaptation where we use the exponential average teacher updating scheme because the number of available training mixtures could make the student prone to overfitting if trained from scratch. <ref type="table" target="#tab_3">Table I</ref> summarizes the mean speech enhancement performance of RemixIT against in-domain and cross-domain supervised and SSL baselines with the same architecture on the DNS test set. Notice that in both semi-supervised and unsupervised cases, the learned RemixIT's student does not assume access to in-domain clean speech nor to noise samples like the previous state-of-the-art SSL speech enhancement algorithms. For instance, SSL RemixIT's teacher pre-training is performed with OOD MixIT by using 80% of the LFSD noisy recordings D m and rest 20% to simulate the isolated noise recordings D n , whereas the student is trained solely on the vast amount of training mixtures in the DNS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervised and semi-supervised speech enhancement</head><p>Despite the fact that RemixIT makes no assumptions about the in-domain distribution of mixtures nor it assumes access to in-domain ground truth source waveforms, it significantly outperforms all the previous state-of-the-art MixIT-like approaches. The unsupervised student learned using the proposed method yields an improvement over the second-best unsupervised method of more than (14.5dB ? 16.0dB in terms of SI-SDR and 0.02 in terms of STOI) compared against in-domain MixIT and the recently proposed extra noise augmentation where an extra noise source is injected <ref type="bibr" target="#b30">[31]</ref>. In the semisupervised domain adaptation setup, we show that RemixIT's student still provides noticeable improvement over its initial teacher pre-trained in a supervised way assuming access to a smaller but diverse dataset like LFSD. Although we have used the same separation model architecture across our experiments, our method is independent of the model's choice and could be used with models that produce higher quality estimates. However, the bottom three rows in <ref type="table" target="#tab_3">Table I</ref> show that the model used in this study achieves state-of-the-art speech enhancement results when trained with in-domain ground-truth sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Zero-shot domain adaptation</head><p>In low-resource training scenarios, the training mixtures inhand might not be sufficient to train a model from scratch, thus, we show how RemixIT can be used as a zero-shot unsupervised domain adaptation algorithm. We perform teacher pre-training on larger OOD datasets and fine-tune a student model using limited in-domain mixtures. At the start of the adaptation process, the student is initialized using the pre-trained teacher's weights ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and we perform</head><p>RemixIT while periodically updating the teacher using the moving average protocol (see Section III-C). The cross-dataset adaptation results are illustrated in <ref type="figure">Figure 4</ref>. The proposed method delivers consistent improvements across datasets and pre-training techniques, up to 0.8dB in terms of SI-SDR over the non-calibrated models. Unsurprisingly, one can notice that the level of improvement is directly impacted by the amount of available noisy mixtures. We postulate that this is the main reason that our method obtains larger (smaller) gains for the adaptation on WHAM! (DNS) test partition which has 3,000 (only 150) mixtures, respectively. However, RemixIT performs adequately even in cases where there is a large distribution   <ref type="figure">Fig. 4</ref>: SI-SDR performance improvement that RemixIT's student yields over its initial OOD pre-trained teacher model for various low-resource adaptation datasets (e.g. DNS, LFSD and WHAM!, from left to right). Both teacher and student models have the exact same Sudo rm -rf architecture (U = 8 ConvBlocks) and we use the running mean teacher update protocol. RemixIT shows significant improvements against all teacher models used in this study, namely, MixIT pre-training on LFSD (blue/leftmost) and supervised training on LFSD (yellow/middle) and as well as on WHAM! (green/rightmost).</p><p>shift between the training and the adaptation-test sets (e.g. WHAM! contains only 10 classes of urban background noises while the DNS dataset is very diverse). Specifically, the significant improvement after adapting a supervised pre-trained model on WHAM! to the 150 mixtures of the very diverse DNS set, indicates the effectiveness of RemixIT under really challenging zero-shot learning conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Student learning progression</head><p>We analyze how a student speech enhancement model trained with RemixIT on the DNS train set refines its estimates as the training progresses and how it compares against its initial teacher. In <ref type="figure" target="#fig_6">Figure 3</ref>, we showcase the improvement obtained in terms of SI-SDR for various teachers and their performance brackets under a sequentially updated teacher every 20 epochs using the parameters from the student network. Note that the student is gradually learning to perform better than the initial teacher network in the regions where the latter performs better (rightmost plots row-wise) even if producing improvement over really good estimates (e.g. higher than 15dB) becomes harder. Thus, it becomes evident that the continual self-training scheme of RemixIT where the teacher network is updated using the latest student's weights is key to a larger performance boost. The result holds for both OOD supervised and MixIT teachers and is on par-with our theoretical analysis in Section II-C2 where we show how a better teacher helps the error correlation term of RemixIT's loss function to diminish and resemble supervised training. In contrast, for the low performing brackets ([?30, ?10]dB in terms of teacher SI-SDR (dB)), the student does not learn how to further increase its performance, even if it regresses over the estimated waveforms of updated teachers. The emergence of this learning pattern necessitates the discovery of more robust self-training algorithms which can recover from cases where the teacher network provides extremely noisy estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robust learning with very noisy teacher's estimates</head><p>We investigate the robustness of RemixIT in cases where the teacher model outputs a low quality speech estimate s. Building upon the analysis performed in Section II-C2, we reiterate on how important is for the student to be trained  </p><formula xml:id="formula_21">S ( s + n b ; ? S ).</formula><p>We show that as a fixed student network sees more input bootstrapped mixtures, the mean student performance becomes better on average than its teacher even early in training and in regions where the teacher performs poorly. on multiple bootstrapped mixtures m b = s + n b , ?b ? {1, . . . , B} produced using the same teacher's speech estimate s and independent teacher's noise estimates n b (see <ref type="bibr">Equations 10,</ref><ref type="bibr" target="#b10">11)</ref>. The distribution of the SNR performance improvement that the empirical mean student yields over the initial teacher after 10 training epochs is displayed in <ref type="figure" target="#fig_8">Figure 5</ref> while sweeping the number of input bootstrapped mixtures. For both cases of supervised and MixIT teachers we see that the mean SNR improvement is around 2 dB when increasing the number of bootstrapped mixtures B from 1 to 64. Notably, this result holds for really bad teacher estimates, namely, less than 5 dB and is obtained by simply performing inference over more augmentations of s without refining the student parameters. Assuming that all speech estimates and the ground-truth signals have unit-norm s = s = s b = 1, the maximization of SNR becomes equivalent to minimizing the l2 norm SNR( y, y) ? ? y?y . As a result, the mean SNR improvement of the empirical mean student leads the term E[ R S , R T ] closer to zero (Equation 11) and consequently, the student to learn in a more robust way, even in cases where the teacher's error term R T is far from zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Cross-domain generalization</head><p>A comparison for cross-domain generalization in selfsupervised and semi-supervised domain adaptation speech enhancement tasks is displayed in <ref type="table" target="#tab_3">Tables II and III, respectively.</ref> In <ref type="table" target="#tab_3">Table II</ref>, we notice that MixIT and its variants fail to generalize in cases where the noise distribution D n does not closely resemble the true in-domain distribution D * n . Notably,</p><p>RemixIT outperforms all MixIT methods without having access to in-domain datasets. For instance, in the case where one only has access to mixtures from the WHAM! (W!) dataset and noise sources from LFSD (L), the best noise augmented MixIT model obtains only 1.6 dB of SI-SDR improvement on the adaptation WHAM! dataset. In stark contrast, RemixIT with a pre-trained MixIT teacher on LFSD yields an improvement of 5.3dB (1.6 ? 6.9) over the best cross-dataset trained MixIT model and 0.7 dB (6.2 ? 6.9) over the teacher model.</p><p>In very harsh mismatched cases, such as when using noise samples from LFSD and mixture samples from LFSD and WHAM!, RemixIT shows strong results for all datasets (4.9 dB for DNS, 7.2 dB for LFSD and 6.9 dB for WHAM!) while even the best MixIT configuration fails to produce significant improvements over the input mixture (1.7 dB for DNS, ?1.7 dB for LFSD and 1.6 dB for WHAM!). Moreover, RemixIT can also improve the performance of a teacher model in the source dataset test-set by leveraging other target mixture datasets. Notice that RemixIT yields an improvement of 1.4dB (8.5 ? 9.9) on the LFSD test-set over the pre-trained teacher model on LFSD by using self-training over the diverse unsupervised DNS mixture dataset. Surprisingly, RemixIT also outperforms its teacher by a large margin (6.2 ? 8.2 dB) on the WHAM! test set even though it has not seen any data from this dataset which shows how RemixIT can provide a seamless solution to generalizing denoising models to unseen data.</p><p>Although RemixIT shows a small performance degradation in the adaptation L ? W! set compared to the adaptation with cleaner datasets, such as: L ? D (7.5 ? 6.8 for the  shallow student and 8.2 ? 6.9 for the U = 32 student on W!), notice that the same MixIT configuration suffers a major hit in denoising performance (5.3 ? 1.2 dB on W!) which makes it almost similar to a no-processing model. In essence, the input SNR of WHAM! (?1.3dB) prevents self-supervised algorithms from learning effectively and training on OOD but higher input-SNR datasets (e.g. DNS) leads to better results.</p><p>In <ref type="table" target="#tab_3">Table III</ref>, we show that RemixIT aptly performs semisupervised domain adaptation even for severely mismatched cases such as transferring knowledge from DNS to the much less diverse and lower input-SNR WHAM!. RemixIT yields the best performing model without clean in-domain source signals on the DNS test set (7.3 dB) when only starting from the OOD semi-supervised teacher on WHAM! with a much inferior performance of 6.1 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. RemixIT with in-domain noise recordings</head><p>Finally, we also propose an extension to our proposed selftraining method to adopt readily available isolated in-domain noise recordings n ? D n which can further enhance RemixIT's performance. To do so, we alter the bootstrapped remixing process presented in Equation 5 using a portion of the indomain noise recordings n ? D n instead of the teacher's noise estimates n ? f n T (m; ? (k) T ) as shown below:  where b indicates the batch-index and p n is the Bernoulli parameter of sampling an in-domain noise recording instead of a teacher's estimate for the corresponding batch-index. In <ref type="figure">Figure 6</ref> we show how our method performs against a stronger fine-tuned MixIT baseline using the same Sudo rm -rf architecture with U = 8 U-ConvBlocks on various splits of the DNS training data. The pre-trained model on LFSD data is used as an initialization checkpoint for MixIT finetuning and as the teacher network for performing RemixIT with bootstrapped mixtures from teacher's estimates and in-domain noise recordings. We set the probability of synthesizing a bootstrapped mixture with an isolated in-domain noise recording instead of a teacher's noise estimate equal to the ratio of the in-domain noise recordings compared to the mixture data p n = |Dn| /(|Dn|+|Dm|). We notice that RemixIT performs consistently better than the fine-tuned MixIT for the same ratio of in-domain noise recordings except of the rightmost point where the bootstrapped mixtures contain less diverse mixtures leading the student model to overfit to only a small amount of human utterances. Notably, RemixIT trains a full student model from scratch compared to the fine-tuned MixIT which has more trainable parameters (0.97 millions vs 0.56) and also enjoys the warm-start from a LFSD MixIT checkpoint. It is also evident that our proposed RemixIT extension becomes better with more supervised data for the generalization datasets (see <ref type="figure">Figure 6a</ref> for DNS and <ref type="figure">Figure 6c</ref> for WHAM!). This is also reflected on a small ablation study that we performed to set the in-domain noise sampling prior parameter p n in which we kept the amount of in-domain noise recordings and mixture data equal |D n | = |D m | and gradually increased the Bernoulli parameter p n : 0.01 ? 0.5. As a result, we noticed a performance increase in terms of SI-SDRi of 6.1 ? 6.4 (dB) for the DNS test-set and 8.6 ? 9.0 (dB) for the WHAM! dataset which enhances our claim that cleaner noise estimates 99.0% 75.0% 50.0% 25.0% 1.0% Available train DNS in-domain noise |D n |/(|D n | + |D m |)  <ref type="figure">Fig. 6</ref>: SI-SDR (dB) performance improvement of a sudo rm -rf (U = 8) model fine-tuned using MixIT (blue-dashed line) and trained using RemixIT with in-domain noise recordings recordings remixing (solid orange line) on different test sets with different DNS training set splits. For each plot the x-axis denote the split between the DNS-training partition between in-domain noise recordings D n and mixture available data D m . Both self-supervised speech enhancement methods start using the same pre-trained MixIT sudo rm -rf (U = 8) model with 20% in-domain isolated noise data and 80% mixture recordings from the LFSD dataset. For each method we evaluate the corresponding checkpoints that lead to the best performance on the LFSD test set after 20 full training epochs.</p><formula xml:id="formula_22">m b = s b + ?n b + (1 ? ?) n (P) b , ? ? Bernoulli(p n ),<label>(17)</label></formula><p>can lead to stronger gains through synthesizing bootstrapped mixtures with less interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a self-training scheme for speech enhancement models which is based on a lifelong bi-directional parameter update between a teacher and a student network. The proposed framework aptly transfers the knowledge of a pre-trained model on out-of-domain data using bootstrapped remixing and through the continual refinement of the teacher's outputs. We have experimentally shown that our method significantly outperforms all previous state-of-the-art selfsupervised methods while being more general and without the dependence on in-domain data. Moreover, our results illustrated that RemixIT can also perform semi-supervised and zero-shot domain adaptation setups with limited in-domain mixtures. Furthermore, our theoretical analysis is backed by empirical results and instrumental to the understanding of the teacher-student learning dynamics, especially in where our method can still learn with extremely noisy pseudo-target signals. In the future, we aim to strengthen the robustness of our algorithm by estimating a confidence-based proxy for the quality of the pseudo-targets <ref type="bibr" target="#b38">[39]</ref> as well as widen the applicability of our method by applying it to different domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A. Datasets DNS-Challenge (DNS): The DNSChallenge 2020 benchmark dataset [50] consists of a large collection of clean speech recordings which are mixed with a wide variety of noisy speech samples with 64,649 and 150 pairs of clean speech and noise recordings for training and testing, respectively. DNS is used for showing the effectiveness of the proposed selftraining scheme where large amounts of unsupervised training data is available and one needs to improve the performance of a model trained only on limited OOD supervised data. LibriFSD50K (LFSD): This data collection includes 45,602 and 3,081 mixtures for training and testing, correspondingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>TT</head><label></label><figDesc>, ?k. Sequentially updated teacher: Every 20 epochs or equivalently K = 20?|Dm| /B optimization steps, where |Dm| /B is the number of batches per-training epoch, we replace the teacher with the latest student, namely,?(k mod K) T := ? (k mod K) S .Exponentially moving average teacher: The teacher is gradually updated after every epoch using an exponential moving average scheme? , ?k with ? = 0.01, where j is a multiple of |Dm| /B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>( y, y) = ?SI-SDR( y, y) = ?20 log 10 ( ?y / ?y? y ). (16) ? = y y/ y 2 makes the loss invariant to the scale of the estimated source y and the target signal y. By setting ? = 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>[ 20 ,</head><label>20</label><figDesc>student performance improvement for various teacher performance brackets (a) Semi-supervised RemixIT with initial teacher pre-trained on WHAM! in a supervised way. 30] (dB) (b) Unsupervised RemixIT with initial teacher pre-trained on LFSD using MixIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 :</head><label>3</label><figDesc>SI-SDR (dB) performance improvement on the training portion of the DNS dataset that a RemixIT's student with a sequentially updated teacher every 20 epochs yields as the training progresses over the initial teacher's estimates. We show that similar learning patterns emerge for different initial teachers pre-trained in a semi-supervised way (top) and an unsupervised way (bottom). The median and the mean ?SI-SDR are denoted with a solid green line and an orange star, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>SNR performance improvement with B bootstrapped mixtures (a) Semi-supervised RemixIT with initial teacher pre-trained on WHAM! in a supervised way. Unsupervised RemixIT with initial teacher pre-trained on LFSD using MixIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>Distribution of SNR improvement (dB) on the DNS training set that the empirical mean RemixIT's student after 10 training epochs ( Equation 11) yields over its initial teacher in regions where the latter performs poorly. The solid orange line denotes the mean SNR improvement for each number of bootstrapped mixtures which are considered under expectation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-0000/00$00.00 ? 2022 IEEE arXiv:2202.08862v3 [cs.SD] 3 Aug 2022</figDesc><table><row><cell>Noisy mixtures</cell><cell></cell><cell></cell><cell>Teacher's noise estimates</cell><cell>Permuted noise estimates</cell></row><row><cell>Noisy</cell><cell>Pre-trained teacher</cell><cell></cell><cell>Bootstrapped mixtures</cell></row><row><cell>speech</cell><cell>on OOD</cell><cell></cell></row><row><cell>dataset</cell><cell>data</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Student's</cell><cell>Student's</cell></row><row><cell></cell><cell></cell><cell></cell><cell>noise</cell><cell>speech</cell></row><row><cell></cell><cell></cell><cell></cell><cell>estimates</cell><cell>estimates</cell></row><row><cell cols="3">Teacher update protocol after</cell><cell>optimization steps</cell></row><row><cell cols="2">Sequentially updated teacher</cell><cell cols="2">Running mean updated teacher</cell><cell>Student</cell></row><row><cell></cell><cell>OR</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>8 ? 16) is initialized. Surprisingly, the sequentially updated teacher protocol keeps teaching a better</figDesc><table><row><cell cols="2">Training method and model details</cell><cell>#Model Params (10 6 )</cell><cell cols="5">Available Training Data (%) Clean Speech Ds Noise Dn Mixture Dm DNS LFSD DNS LFSD DNS LFSD</cell><cell cols="2">Mean evaluation metrics SISDR PESQ STOI (dB)</cell></row><row><cell cols="2">Input Noisy Mixture</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.2</cell><cell>1.58</cell><cell>0.915</cell></row><row><cell>MixIT with Sudo rm-rf (U = 8)</cell><cell>In-domain noise OOD noise Extra OOD noise [31]</cell><cell>0.79 0.79 0.79</cell><cell></cell><cell>20%</cell><cell>20% 50%</cell><cell>80% 100% 100%</cell><cell></cell><cell>14.4 14.3 14.5</cell><cell>2.13 2.02 2.03</cell><cell>0.933 0.933 0.930</cell></row><row><cell>Unsupervised RemixIT (ours)</cell><cell>Teacher (U = 8) Student (U = 8) Student (U : 8 ? 16 ? 32)</cell><cell>0.79 0.56 0.73</cell><cell></cell><cell></cell><cell>20%</cell><cell>100% 100%</cell><cell>80%</cell><cell>14.8 15.5 16.0</cell><cell>2.15 2.27 2.34</cell><cell>0.940 0.947 0.952</cell></row><row><cell>Semi-supervised RemixIT (ours)</cell><cell>Teacher (U = 8) Student (U = 8) Student (U : 8 ? 16 ? 32)</cell><cell>0.56 0.56 0.73</cell><cell>100%</cell><cell></cell><cell>100%</cell><cell>100% 100%</cell><cell></cell><cell>17.6 17.6 18.0</cell><cell>2.61 2.52 2.60</cell><cell>0.958 0.956 0.959</cell></row><row><cell>Supervised in-domain training</cell><cell>FullSubNet  *  [8] Sudo rm -rf [60] (U = 8) Sudo rm -rf [60] (U = 32)</cell><cell>5.6 0.56 0.73</cell><cell>100% 100% 100%</cell><cell>100% 100% 100%</cell><cell></cell><cell></cell><cell></cell><cell>17.3 18.6 19.7</cell><cell>2.78 2.69 2.95</cell><cell>0.961 0.962 0.971</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Evaluation results for the speech enhancement task on the DNS test set using the proposed RemixIT method, MixIT approaches</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Self-supervised training mean SI-SDR improvement (dB) over the input mixture performance for MixIT baselines and RemixIT. The initial MixIT teacher uses a sudo rm -rf<ref type="bibr" target="#b59">[60]</ref> architecture with U = 8 blocks on the LFSD (L) dataset and is denoted with L . The evaluated RemixIT's student models follow a sequentially updated teacher protocol where they grow in depth as: U : 8 ? 16 ? 32 and are only trained using the corresponding bolded mixture dataset. Gray background colored cells denote the best performing model which did not have access to clean in-domain training data for the corresponding dataset (note that MixIT assumes access to clean in-domain noise recordings). The mean noisy input mixture SI-SDR performance is 9.2, 6.3 and ?1.3 dB for DNS, LFSD, and WHAM! datasets, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>RemixIT mean SI-SDR improvement (dB) over the input mixture for semi-supervised domain adaptation. The initial OOD supervised teachers with a sudo rm -rf<ref type="bibr" target="#b59">[60]</ref> with U = 8 blocks on the DNS (D) and the WHAM! (W!) datasets are denoted with D and W ! , respectively. The evaluated RemixIT's student models follow a sequentially updated teacher protocol where they grow in depth as: U : 8 ? 16 ? 32 and are only trained using the corresponding bolded mixture dataset. Gray background colored cells denote the best performing model which did not have access to clean in-domain training data for the corresponding dataset. The mean noisy input mixture SI-SDR performance is 9.2, 6.3 and ?1.3 dB for DNS (D), LFSD (L), and WHAM! (W!) datasets, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Performance on the DNS test set. Available train DNS in-domain noise |D n |/(|D n | + |D m |) Performance on the LFSD test set. Available train DNS in-domain noise |D n |/(|D n | + |D m |) RemixIT with in-domain noise remixing MixIT finetuning on DNS (c) Performance on the WHAM! test set.</figDesc><table><row><cell>SI-SDRi (dB)</cell><cell>4 6</cell><cell>(a) 99.0% 75.0% 50.0% 25.0% 1.0% (b) 99.0% 75.0% 50.0% 25.0% 1.0% 4 6 8 SI-SDRi (dB) 7 8 9 SI-SDRi (dB)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoji</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note>Speech enhancement</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multichannel signal processing with deep neural networks for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kean</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="965" to="979" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust speaker recognition based on single-channel and multi-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Taherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1293" to="1302" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeong-Seok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyogu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-stage deep learning for noisy-reverberant speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3291" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fullsubnet: A full-band and sub-band fusion model for real-time single-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6633" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional-recurrent neural networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuayb</forename><surname>Zarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2401" to="2405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech enhancement using self-adaptation and multihead self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuma</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Yatabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Masuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2020</title>
		<meeting>ICASSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Poconet: Better speech enhancement with frequency-positional embeddings, semi-supervised conversational data, and biased loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neerad</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2487" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense cnn with self-attention for time-domain speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1270" to="1279" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Feng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2031" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical speech enhancement based on probabilistic integration of variational autoencoder and non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsutoshi</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyoshi</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="716" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Incorporating real-world noisy speech in neural-network-based speech enhancement systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05172</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining of bidirectional speech encoders via masked reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2020</title>
		<meeting>ICASSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="6889" to="6893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tera: Self-supervised learning of transformer encoder representation for speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Andy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2351" to="2366" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise Adaptive Speech Enhancement Using Domain Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Feng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3148" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised denoising autoencoder with linear regression decoder for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassadaq</forename><surname>Ryandhimas E Zezario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugang</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2020</title>
		<meeting>ICASSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="6669" to="6673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Personalized speech enhancement through self-supervised data augmentation and purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aswin</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2676" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speech enhancement using end-to-end speech recognition objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Aswin Shanmugam Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murali</forename><forename type="middle">Karthick</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="234" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised speech enhancement with speech recognition embedding and disentanglement losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2022</title>
		<meeting>ICASSP, 2022</meeting>
		<imprint>
			<biblScope unit="page" from="391" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving multimodal speech enhancement by incorporating self-supervised and curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="4285" to="4289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multichannel speech enhancement based on time-frequency masking using subband long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="298" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised speech enhancement based on multichannel nmf-informed beamforming for noise-robust automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsutoshi</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyoshi</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="960" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised sound separation using mixture invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3846" to="3857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Noisy-target training: A training strategy for dnn-based speech enhancement without clean speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuma</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Yatabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoichi</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EUSIPCO, 2021</title>
		<meeting>EUSIPCO, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="436" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Separate but together: Unsupervised federated learning for speech enhancement from non-iid data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Casebeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA, 2021</title>
		<meeting>WASPAA, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training speech enhancement systems with noisy speech datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Fabbro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12315</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Teacher-student deep clustering for low-delay single channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Aihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Okato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="690" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hubert: How much can a bad teacher benefit asr pre-training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="6533" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Teacher-Student MixIT for Unsupervised and Semi-Supervised Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?t?lin</forename><surname>Zoril?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="3495" to="3499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Test-time adaptation toward personalized speech enhancement: Zero-shot learning with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA, 2021</title>
		<meeting>WASPAA, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="176" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6256" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="596" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised singing voice separation with noisy self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Momentum pseudo-labeling for semi-supervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2021</title>
		<meeting>Interspeech, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="726" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Iterative pseudo-labeling for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2020</title>
		<meeting>Interspeech, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1006" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Selftraining with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Differentiable consistency constraints for improved deep speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multitalker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Probabilistic Permutation Invariant Training for Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Midia</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Khorram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4604" to="4608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2020</title>
		<meeting>ICASSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The central limit theorem for weakly dependent random variables by the moment method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fleermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Kirsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04717</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech, 2020</title>
		<meeting>Interspeech, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2492" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fsd50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="829" to="852" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">WHAM!: Extending Speech Separation to Noisy Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><forename type="middle">Richard</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmett</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwight</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention waveu-net for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="249" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database: A database of multichannel environmental noise recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3591" to="3591" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sudo rm-rf: Efficient networks for universal audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MLSP, 2020</title>
		<meeting>MLSP, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ultra-lightweight speech separation via group communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Compute and memory efficient universal sound source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="259" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2840" to="2849" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2021</title>
		<meeting>ICASSP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andries P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
