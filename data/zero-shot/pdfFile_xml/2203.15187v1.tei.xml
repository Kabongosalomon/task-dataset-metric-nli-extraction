<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>He</surname></persName>
							<email>bohe@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Kang</surname></persName>
							<email>kangle01@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Cheng</surname></persName>
							<email>zhiyucheng@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhou</surname></persName>
							<email>zhouxin16@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<email>abhinav@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised temporal action localization aims to recognize and localize action segments in untrimmed videos given only video-level action labels for training. Without the boundary information of action segments, existing methods mostly rely on multiple instance learning (MIL), where the predictions of unlabeled instances (i.e., video snippets) are supervised by classifying labeled bags (i.e., untrimmed videos). However, this formulation typically treats snippets in a video as independent instances, ignoring the underlying temporal structures within and across action segments. To address this problem, we propose ASM-Loc, a novel WTAL framework that enables explicit, action-aware segment modeling beyond standard MIL-based methods. Our framework entails three segment-centric components: (i) dynamic segment sampling for compensating the contribution of short actions; (ii) intra-and inter-segment attention for modeling action dynamics and capturing temporal dependencies; (iii) pseudo instance-level supervision for improving action boundary prediction. Furthermore, a multistep refinement strategy is proposed to progressively improve action proposals along the model training process. Extensive experiments on THUMOS-14 and ActivityNet-v1.3 demonstrate the effectiveness of our approach, establishing new state of the art on both datasets. The code and models are publicly available at https://github. com/boheumd/ASM-Loc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIL-based</head><p>Up-Sample</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Weakly-supervised temporal action localization (WTAL) has attracted increasing attention in recent years. Unlike its fully-supervised counterpart, WTAL only requires action category annotation at the video level, which is much easier to collect and more scalable for building large-scale datasets. To tackle this problem, recent works <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> mostly rely on the multiple instance learning (MIL) framework <ref type="bibr" target="#b12">[13]</ref>, where the entire untrimmed video is treated as a labeled bag containing multiple unlabeled <ref type="bibr">Figure 1</ref>. Action-aware segment modeling for WTAL. Our ASM-Loc leverages the action proposals as well as the proposed segment-centric modules to address the common failures in existing MIL-based methods. instances (i.e., video frames or snippets). The action classification scores of individual snippets are first generated to form the temporal class activation sequences (CAS) and then aggregated by a top-k mean mechanism to obtain the final video-level prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>While significant improvement has been made in prior work, there is still a huge performance gap between the weakly-supervised and fully-supervised settings. One major challenge is localization completeness, where the models tend to generate incomplete or over-complete action segments due to the inaccurate predictions of action boundaries. Another challenge is the missed detection of short action segments, where the models are biased towards segments with longer duration and produce low-confidence predictions on short actions. <ref type="figure">Figure 1</ref> demonstrates an example of these two common errors. Although these challenges are inherently difficult due to the lack of segmentlevel annotation, we argue that the absence of segmentbased modeling in existing MIL-based methods is a key reason for the inferior results. In particular, these MIL-based methods treat snippets in a video as independent instances, where their underlying temporal structures are neglected in either the feature modeling or prediction stage.</p><p>In this paper, we propose a novel framework that enables explicit, action-aware segment modeling for weaklysupervised temporal action localization, which we term ASM-Loc. To bootstrap segment modeling, we first generate action proposals using the standard MIL-based methods. These proposals provide an initial estimation of the action locations in the untrimmed video as well as their duration. Based on the action proposals, we introduce three segment-centric modules that correspond to the three stages of a WTAL pipeline, i.e., the feature extraction stage, the feature modeling stage and the prediction stage.</p><p>First, a dynamic segment sampling module is proposed to balance the contribution of short-range and long-range action segments. As shown in <ref type="figure">Figure 1</ref>, action proposals with short duration are up-sampled along the temporal dimension, with the scale-up ratios dynamically computed according to the length of the proposals. Second, intra-and inter-segment attention modules are presented to capture the temporal structures within and across action segments at the feature modeling stage. Specifically, the intra-segment attention module utilizes self-attention within action proposals to model action dynamics and better discriminate foreground and background snippets. On the other hand, the inter-segment attention module utilizes self-attention across different actions proposals to capture the relationships, facilitating the localization of action segments that involve temporal dependencies (e.g., "CricketBowling" is followed by "CricketShotting" in <ref type="figure">Figure 1</ref>). Note that both attention modules are segment-centric, which is critical to suppress the negative impact of noisy background snippets in untrimmed videos. Third, a pseudo instance-level loss is introduced to refine the localization result by providing fine-grained supervision. The pseudo instance-level labels are derived from the action proposals, coupled with uncertainty estimation scores that mitigate the label noise effects. Finally, a multi-step proposal refinement is adopted to progressively improve the quality of action proposals, which in turn boosts the localization performance of our final model.</p><p>We summarize our main contributions as follows:</p><p>? We show that segment-based modeling can be utilized to narrow the performance gap between the weaklysupervised and supervised settings, which has been neglected in prior MIL-based WTAL methods. ? We introduce three novel segment-centric modules that enable action-aware segment modeling in different stages of a WTAL pipeline. ? We provide extensive experiments to demonstrate the effectiveness of each component of our design. Our ASM-Loc establishes new state of the art on both THUMOS-14 and ActivityNet-v1.3 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Temporal Action Localization (TAL). Compared with action recognition <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, TAL is an more challenging task for video understanding. Current fully-supervised TAL methods can be categorized into two groups: the anchorbased methods <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> perform boundary regression based on pre-defined action proposals, while the anchor-free methods <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> directly predict boundary probability or actionness scores for each snippet in the video, and then employ a bottom-up grouping strategy to match pairs of start and end for each action segment. All these methods require precise temporal annotation of each action instance, which is labor-intensive and time-consuming.</p><p>Weakly-supervised Temporal Action Localization. Recently, the weakly supervised setting, where only videolevel category labels are required during training, has drawn increasing attention from the community <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. Specifically, UntrimmedNet <ref type="bibr" target="#b0">[1]</ref> is the first to introduce the multiple instance learning (MIL) framework to tackle this problem, which selects foreground snippets and groups them as action segments. STPN <ref type="bibr" target="#b1">[2]</ref> improves Untrimmed-Net by adding a sparsity loss to enforce the sparsity of selected snippets. CoLA <ref type="bibr" target="#b8">[9]</ref> utilizes contrastive learning to distinguish the foreground and background snippets. UGCT <ref type="bibr" target="#b9">[10]</ref> proposes an online pseudo label generation with uncertainty-aware learning mechanism to impose the pseudo label supervision on the attention weight. All these MIL-based methods treat each snippet in the video individually, neglecting the rich temporal information at the segment-level. In contrast, our ASM-Loc focuses on modeling segment-level temporal structures for WTAL, which is rarely explored in prior work.</p><p>Pseudo Label Guided Training. Using pseudo labels to guide model training has been widely adopted in vision tasks with weak or limited supervision. In weakly supervised object detection, one of the seminal directions is selftraining <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>, which first trains a teacher model and then the predictions with high confidence are used as instancelevel pseudo labels to train a final detector. Similarly, in semi-supervised learning <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref> and domain adaptation <ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>, models are first trained on the labeled / source dataset and then used to generate pseudo labels for the unlabeled / target dataset to guide the training process. Similar to these works, our ASM-Loc utilizes pseudo segment-level labels (i.e., action proposals) to guide our training process in the WTAL task. However, we do not limit our approach to using pseudo labels for supervision only. Instead, we leverage the action proposals in multiple segment-centric modules, such as dynamic segment sampling, intra-and inter-segment attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WTAL Base Model</head><p>WTAL aims to recognize and localize action segments in untrimmed videos given only video-level action labels during training. Formally, let us denote an untrimmed training video as V and its ground-truth label as y ? R C , where C is the number of action categories. Note that y could be a multi-hot vector if more than one action is present in the video and is normalized with the l 1 normalization. The goal of temporal action localization is to generate a set of action</p><formula xml:id="formula_0">segments S = {(s i , e i , c i , q i )} I i=1</formula><p>for a testing video, where s i , e i are the start and end time of the i-th segment and c i , q i are the corresponding class prediction and confidence score.</p><p>Most existing WTAL methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> employ the multiple instance learning (MIL) formulation. A typical pipeline of MIL-based methods consists of three main stages (depicted in <ref type="figure" target="#fig_0">Figure 2</ref>): (i) The feature extraction stage takes the untrimmed RGB videos and optical flow as input to extract snippet-level features using pre-trained backbone networks.</p><p>(ii) The feature modeling stage transforms the extracted features to the task-oriented features by performing temporal modeling. (iii) The prediction stage generates class probabilities and attention weights for each time step and computes video-level loss following the MIL formulation during training. In the following subsections, we review the common practices of these three stages and present our base model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction and Modeling</head><p>Following the recent WTAL methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, we first divide each untrimmed video into non-overlapping 16frame snippets, and then apply a Kinetics-400 pre-trained I3D model <ref type="bibr" target="#b14">[15]</ref> to extract features for both RGB and optical flow input. After that, the RGB and optical flow features are concatenated along the channel dimension to form the snippet-level representations F ? R T ?D , where T is the number of snippets in the video and D = 2048 is the feature dimensionality. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">48]</ref>, the features are then fed into a temporal convolution layer and the ReLU activation for feature modeling: X = ReLU(conv(F )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Prediction and Training Losses</head><p>Given the embedded features X, a fully-connected (FC) layer is applied to predict the temporal class activation sequence (CAS) P ? R T ?(C+1) , where C + 1 denotes the number of action categories plus the background class. To better differentiate the foreground and background snippets, a common strategy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref> is to introduce an additional attention module that outputs the attention weights for each time step of the untrimmed video. Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">48]</ref>, we generate the attention weights A ? R T ?2 using an FC layer, where the two weight values at each time step are normalized by the softmax operation to obtain the foreground and background attention weights, respectively. Finally, the CAS and the attention weights are combined to get the attention weighted CAS:P m (c) = P (c)?A m , m ? {fg, bg}, where c indicates the class index and ? denotes elementwise multiplication.</p><p>Following the MIL formulation, the video-level classification score is generated by the top-k mean strategy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. For each class c, we take the k largest values of the attention weighted CAS and compute their averaged value:</p><formula xml:id="formula_1">p m (c) = 1 k</formula><p>Top-k(P m (c)). Softmax normalization is then performed across all classes to obtain the attention weighted video-level action probabilities. We adopt three video-level losses in such a weakly-supervised setting.</p><p>Foreground loss. To guide the training of video-level action classification, we apply the cross-entropy loss between the foreground-attention weighted action probabilitiesp fg and the video-level action label y fg = [y; 0], written as:</p><formula xml:id="formula_2">L fg = ? C+1 c=1 y fg (c) logp fg (c).<label>(1)</label></formula><p>Background loss. To ensure that the negative instances in the untrimmed video are predicted as the background class, we regularize the background-attention weighted action probabilitiesp bg with an additional background loss <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b48">48]</ref>. Specifically, we compute the cross-entropy between p bg and the background class label y bg :</p><formula xml:id="formula_3">L bg = ? C+1 c=1 y bg (c) logp bg (c),<label>(2)</label></formula><p>where y bg (C + 1) = 1 and y bg (c) = 0 for all other c.</p><p>Action-aware background loss. Although no action is taking place in background snippets, we argue that rich context information is still available to reflect the actual action category label. As an example in <ref type="figure" target="#fig_4">Figure 3</ref>(c), even though the background frames are stationary with only a billiard table, one can still expect the existence of the action category "Billiard" somewhere in the video. Therefore, the background instances are related to not only the background class label but also the action class label. Based on this observation, we formulate the actionaware background loss as the cross-entropy loss between the background-attention weighted action probabilitiesp bg and the video-level action label y fg :</p><formula xml:id="formula_4">L abg = ? C+1 c=1 y fg (c) logp bg (c).<label>(3)</label></formula><p>The total video-level loss for our base model is the weighted combination of all three losses:</p><formula xml:id="formula_5">L vid = ? fg L fg + ? bg L bg + ? abg L abg ,<label>(4)</label></formula><p>where ? fg , ? bg and ? abg are trade-off parameters for balancing the contribution of the three losses. , and denote element-wise multiplication, matrix multiplication, and element-wise addition. T , N are the number of snippets and action proposals, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>As discussed in Sec. 1, our base model follows the MIL formulation and neglects the temporal structures among video snippets. Nevertheless, the prediction results generated by the base model still provide a decent estimation of the action locations and durations in the untrimmed video, which can serve as a bootstrap for our segment modeling process. In particular, we generate the initial action proposals based on the prediction results of the base model: S ?S = {(s n , e n , c n )} N n=1 , where s n , e n and c n denote the start time, the end time, and the predicted category label of the n-th action proposal, respectively. More details on generating action proposals are available in the supplementary material. The main focus of our work is to leverage the action proposals for segment-level temporal modeling, as described in the following section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Action-aware Segment Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dynamic Segment Sampling</head><p>Action segments in an untrimmed video may have various duration, ranging from less than 2 seconds to more than 1 minute. Intuitively, short actions have small temporal scales, and therefore, their information is prone to loss or distortion throughout the feature modeling stage. As shown in <ref type="table">Table 5</ref>, we observe that models are indeed biased towards the segments with longer duration and produce lower confidence scores on short segments, resulting in missed detection or inferior localization results. Similar observations are in object detection, where smaller objects have worse detection performance than larger ones <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50]</ref>.</p><p>In order to address this problem in the WTAL setting, we propose a novel segment sampling module that dynamically up-samples action proposals according to their estimated duration. Formally, we first initialize a sampling weight vector W ? R T with values equal to 1 at all time steps. Then, we compute the updated sampling weight for short proposals with duration less than a pre-defined threshold ?:</p><formula xml:id="formula_6">W [s n : e n ] = ? e n ? s n , if (e n ? s n ) ? ?,<label>(5)</label></formula><p>where s n , e n denote the start and end time of the n-th action proposal. The sampling procedure is based on the Inverse Transform Sampling method as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b). The intuition is to sample snippets with frame rates proportional to their sampling weights W . We first compute the cumulative distribution function (CDF) of the sampling weights f W = cdf(W ), then uniformly sample T timesteps from the inverse of the CDF:</p><formula xml:id="formula_7">{x i = f ?1 W (i)} T i=1 .</formula><p>In this way, the scale-up ratio of each proposal is dynamically computed according to its estimated duration. We apply linear interpolation when up-sampling is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Intra-and Inter-Segment Attention</head><p>Intra-Segment Attention. Action modeling is of central importance for accurate action classification and temporal boundary prediction. Recent work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b51">51]</ref> applies temporal attention globally on trimmed videos for action recognition and achieves impressive performance. However, untrimmed videos are usually dominated by irrelevant background snippets which introduce extra noise to the action segment modeling process. Motivated by this observation, we propose the intra-segment attention module that performs self-attention within each action proposal.</p><p>We formulate this module using a masked attention mechanism, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c). Specifically, an attention mask M ? R T ?T is defined to indicate the foreground snippets corresponding to different action proposals. The attention mask is first initialized with 0 at all entries and assigned M [s n : e n , s n : e n ] = 1 for all proposals. The attention mask is then applied to the attention matrix computed by the standard self-attention approach <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b53">53]</ref>:</p><formula xml:id="formula_8">Q = XW Q , K = XW K , V = XW V ,<label>(6)</label></formula><formula xml:id="formula_9">A i,j = M i,j exp(Q i K T j / ? D) k M i,k exp(Q i K T k / ? D)<label>(7)</label></formula><formula xml:id="formula_10">Z = X + BN(AV W O ),<label>(8)</label></formula><p>where W Q , W K , W V , W O ? R D?D are the linear projection matrices for generating the query, key, value and the output. Multi-head attention <ref type="bibr" target="#b52">[52]</ref> is also adopted to improve the capacity of the attention module. In this way, we explicitly model the temporal structures within each action proposal, avoiding the negative impact of the irrelevant and noisy background snippets.</p><p>Inter-Segment Attention. Action segments in an untrimmed video usually involve temporal dependencies with each other. For example, "CricketBowling" tends to be followed by "CricketShotting", while "VolleyballSpiking" usually repeats multiple times in a video. Capturing these dependencies and interactions among action segments can therefore improve the recognition and localization performance.</p><p>Similar to the intra-segment attention module, we leverage a self-attention mechanism to model the relationships across multiple action proposals. As shown in <ref type="figure" target="#fig_0">Figure 2(d)</ref>, we first aggregate the snippet-level features within each action proposal by average pooling on the temporal dimension X n = 1 en?sn+1 en t=sn X(t). The multi-head self-attention is then applied on all segment-level features {X n } N n=1 to model the interactions between different action proposal pairs. The output features are replicated along the time axis and added to the original feature X in a residual manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pseudo Instance-level Loss</head><p>Due to the absence of segment-level annotation, standard MIL-based methods only rely on video-level supervision provided by the video-level action category label. To further refine the localization of action boundaries, we leverage the pseudo instance-level label provided by the action proposals and propose a pseudo instance-level loss that offers more fine-grained supervision than the video-level losses.</p><p>Given the action proposalsS = {s n , e n , c n } N n=1 , we construct the pseudo instance-level labelQ ? R T ?(C+1) by assigning action labels to the snippets that belong to the action proposals and assigning the background class label to all other snippets: </p><formula xml:id="formula_11">Q t (c) = ? ? ? 1, if ?n, t ? [s n ,</formula><p>Note thatQ is also normalized with the l 1 normalization.</p><p>As the action proposals are generated from the model prediction, it is inevitable to produce inaccurate pseudo instance-level labels. To handle the label noise effects, we follow the recent work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">[54]</ref><ref type="bibr" target="#b55">[55]</ref><ref type="bibr" target="#b56">[56]</ref> and introduce an uncertainty prediction module that guides the model to learn from noisy pseudo labels. Specifically, we employ an FC layer to output the uncertainty score U ? R T , which is then used to re-weight the pseudo instance-level loss at each time step. Intuitively, instances with high uncertainty scores are limited from contributing too much to the loss. Coupled with uncertainty scores, the pseudo instance-level loss can be written as the averaged cross-entropy between the temporal CAS P and the pseudo instance-level labelQ:</p><formula xml:id="formula_13">L ins = 1 T T t=1 exp(?U t ) ? C+1 c=1Q t (c) log(P t (c)) + ?U t<label>(10)</label></formula><p>where ? is a hyper-parameter for the weight decay term, which prevents the uncertainty prediction module from predicting infinite uncertainty for all time steps (and therefore zero loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multi-step Proposal Refinement</head><p>Action proposals play an important role in action-aware modeling. As discussed in Sec. 5.3, the quality of proposals is positively correlated with the performance of multiple components in our approach. While our initial action proposals are obtained from the base model, it is intuitive to leverage the superior prediction results generated by our ASM-Loc to generate more accurate action proposals. Based on this motivation, we propose a multi-step training process that progressively refines the action proposals via multiple steps.</p><p>As a bootstrap of segment modeling, we first train the base model (Sec. 3) for E epochs and obtain the initial action proposalsS 0 . After that, we train our ASM-Loc for another E epochs and obtain the refined action proposals S 1 with a more accurate estimation of the action location and duration. The same process can be applied for multiple steps until the quality of action proposals is converged. The complete multi-step proposal refinement process is summarized in Alg. 1. Finally, we train our ASM-Loc using the refined proposalsS until the model is converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Dataset. We evaluate our method on two popular action localization datasets: THUMOS-14 <ref type="bibr" target="#b60">[60]</ref> and ActivityNet-v1.3 <ref type="bibr" target="#b61">[61]</ref>. THUMOS-14 contains untrimmed videos from 20 categories. The video length varies from a few seconds to several minutes and multiple action instances may exist in a single video. Following previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>, we use the 200 videos in the validation set for training and the 213 videos in the testing set for evaluation. ActivityNet-v1.3 is a large-scale dataset with 200 complex daily activities. It has 10,024 training videos and 4,926 validation videos. Following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>, we use the training set to train our model and the validation set for evaluation. Implementation Details. We employ the I3D <ref type="bibr" target="#b14">[15]</ref> network pretrained on Kinetics-400 <ref type="bibr" target="#b14">[15]</ref> for feature extraction. We apply TVL1 <ref type="bibr" target="#b62">[62]</ref> algorithm to extract optical flow from RGB frames. The Adam optimizer is used with the learning rate of 0.0001 and with the mini-batch sizes of 16, 64 for THUMOS-14 and ActivityNet-v1.3, respectively. The number of sampled snippets T is 750 for THUMOS-14 and 150 for ActivityNet-v1.3. For the multi-step proposal refinement, E is set to 100 and 50 epochs for THUMOS-14 and ActivityNet-v1.3, respectively. Action proposals are generated at the last epoch of each refinement step. More dataset-specific training and testing details are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the State of the Art</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare our ASM-Loc with state-ofthe-art WTAL methods on THUMOS-14. Selected fully- Train ASM-Loc for E epochs withS l?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Update action proposals withS l . 6 end supervised methods are presented for reference. We observe that ASM-Loc outperforms all the previous WTAL methods and establishes new state of the art on THUMOS-14 with 45.1% average mAP for IoU thresholds 0.1:0.7. In particular, our approach outperforms UGCT <ref type="bibr" target="#b9">[10]</ref>, which also utilizes pseudo labels to guide the model training but without explicit segment modeling. Even compared with the fully supervised methods, ASM-Loc outperforms SSN <ref type="bibr" target="#b24">[25]</ref> and TAL-Net <ref type="bibr" target="#b21">[22]</ref> and achieves comparable results with GTAN <ref type="bibr" target="#b57">[57]</ref> and P-GCN <ref type="bibr" target="#b58">[58]</ref> when the IoU threshold is low. The results demonstrate the superior performance of our approach with action-aware segment modeling.</p><p>We also conduct experiments on ActivityNet-v1.3 and the comparison results are summarized in <ref type="table">Table 2</ref>. Again, our ASM-Loc obtains a new state-of-the-art performance of 25.1% average mAP, surpassing the latest works (e.g. UGCT <ref type="bibr" target="#b9">[10]</ref>, FAC-Net <ref type="bibr" target="#b11">[12]</ref>). The consistent superior results on both datasets justify the effectiveness of our ASM-Loc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies on THUMOS-14</head><p>Contribution of each component. In <ref type="table" target="#tab_2">Table 3</ref>, we conduct an ablation study to investigate the contribution of each component in ASM-Loc. We first observe that adding the background loss L bg and the action-aware background loss L abg largely enhance the performance of the base model. The two losses encourage the sparsity in the foreground attention weights by pushing the background attention weights to be 1 at background snippets, and therefore improve the foreground-background separation.</p><p>For action-aware segment modeling, it is obvious that a consistent gain (?1%) can be achieved by adding any of our proposed modules. In particular, introducing segment modeling in the feature modeling stage (i.e., intra-and intersegment attention) significantly increases the performance by 2.4%. The two attention modules are complementary to each other, focusing on modeling temporal structure within and across action segments. When incorporating all the action-aware segment modeling modules together, our approach boosts the final performance from 40.3% to 45.1%. Are action proposals necessary for self-attention?. We propose an intra-segment attention module that performs self-attention within action proposals to suppress the noise from background snippets. To verify the effectiveness of our design, we compare different settings for self-attention in <ref type="table" target="#tab_3">Table 4</ref>. Specifically, the "Global" setting indicates that the self-attention operation is applied directly to all snippets in the untrimmed video. It can be observed that this setting does not provide any gain to the baseline, as the model fails to capture meaningful temporal structure due to the existence of irrelevant and noisy background snippets. Moreover, the "BG" setting, which stands for self-attention on background snippets only, has negative impact and achieves even worse localization results. Finally, our intra-segment attention outperforms these two settings by a large margin, indicating the importance of applying self-attention within action proposals. We also present the settings of using the ground-truth action segments as proposals for intra-segment attention. This setting can be viewed as an upper bound of our approach and it provides even more significant gains over the baseline. This observation inspires us to further improve the action proposals by multi-step refinement.</p><p>Impact of dynamic segment sampling. In <ref type="table">Table 5</ref>, we evaluate the impact of dynamic segment sampling for action segments with different durations. We divide all action segments into five groups according to their duration in seconds and evaluate the averaged mAP <ref type="bibr" target="#b65">[65]</ref> separately for each group. As mentioned in the introduction, localization performance on short actions (XS, S) is much worse than longer actions (M, L, XL). By up-sampling the short actions with our dynamic segment sampling module, the model achieves significant gains on short actions (+4.9% for XS and +1.2% for S) and improves the overall performance by 1.1%. Similarly, we present the results using groundtruth segment annotation for dynamics segment sampling, which achieves even larger improvement over the baseline.</p><p>Impact of uncertainty estimation. We propose an uncertainty estimation module to mitigate the noisy label problem in pseudo instance-level supervision. <ref type="table" target="#tab_4">Table 6</ref> shows that using uncertainty estimation consistently improves the localization performance at different IoU thresholds, and increases the average mAP by 1%.</p><p>Impact of multi-step refinement. <ref type="table">Table 7</ref> shows the results   of increasing the number of refinement steps for multi-step proposal refinement. We can see that the performance improves as the number of steps increases, indicating that better localization results can be achieved by refined proposals. We adopt 3 refinement steps as our default setting since the performance saturates after that. <ref type="figure" target="#fig_4">Figure 3</ref> shows the visualization comparisons between the base model and our ASM-Loc. We observe that the common errors in existing MIL-based methods can be partly addressed by our action-aware segment modeling method, such as the missed detection of short actions and incomplete localization of the action "VolleySpiking" <ref type="figure" target="#fig_4">(Figure 3(a)</ref>) and the over-complete localization of the action "BaseballPitch" <ref type="figure" target="#fig_4">(Figure 3(b)</ref>). We also provide a failure case in <ref type="figure" target="#fig_4">Figure 3</ref>(c), where our method fails to localize the first action segment due to the largely misaligned action proposal generated by the base model. This also verifies the importance of improving the quality of action proposals and should be further studied in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel WTAL framework named ASM-Loc which enables explicit action-aware segment modeling beyond previous MIL-based methods. We introduce three novel segment-centric modules corresponding to the three stages of a WTAL pipeline, which narrows the performance gap between the weakly-supervised and fully-supervised settings. We further introduce a multi-step training strategy to progressively refine the action proposals  till the localization performance saturates. Our ASM-Loc achieves state-of-the-art results on two WTAL benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Sec. A reports additional experiments and analysis. Sec. B elaborates on the procedure of action proposal generation. Sec. C provides more dataset-specific implementation details and hyper-parameters for training and testing. We also provide more qualitative results in Sec. D. We discuss the limitation and broader impact of our work in Sec. E and Sec. F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Experiments and Analysis</head><p>Error analysis. To analyze the effectiveness of our ASM-Loc, we conduct a DETAD <ref type="bibr" target="#b66">[66]</ref> false positive analysis of the base model without any action-aware segment modeling modules and our ASM-Loc. We present the results in <ref type="figure" target="#fig_6">Figure 4</ref>. It shows a detailed categorization of false positive errors and summarizes the distribution of these errors. G represents the number of ground truth segments in the THUMOS-14 dataset. We can observe that ASM-Loc generates more true positive predictions with high confidence scores and produces less localization error and confusion error (at the top 1G scoring predictions). It verifies that ASM-Loc improves the detection results by predicting more accurate action boundaries with our action-aware segment modeling modules.</p><p>Ablation on the increased receptive field. To further demonstrate that the effectiveness of our intra-and intersegment attention modules is due to the segment-centric design instead of the increased receptive field, we replace our intra-and inter-segment attention modules with convolutional layers and compare the experimental results. From <ref type="table" target="#tab_5">Table 8</ref> we can see that by replacing the attention modules with convolutional layers, the performances drop by at least 3.3%, and even fall below the base model. We hypothesize that increasing the kernel size of the convolutional layers may lead to confusion between foreground and background snippets especially near the action boundaries. In contrast, our segment-centric attention design can model temporal structures within and across action segments and localize actions more precisely. The results verify that the segmentcentric design is the key to our intra-and inter-segment attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Proposal Generation</head><p>In Alg. 2, we present the details of how to generate action proposalsS from the action localization results (i.e., action segments) S. Specifically, we first sort all the segment scores across the set S(c) for each ground-truth class c. Then we sum the confidence scores of all the action segments and output q sum , and pick the top-K action segments with their confidence scores summation equal to ? * q sum to  form the action proposals. Note that the number of the action proposals is video-adaptive and content dependent, despite ? is shared for all videos. Finally, following the common practice in temporal action localization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b67">67]</ref>, we extend each proposal on both ends by ? of the proposal length to get an extended proposal with a longer temporal duration which can take more context-related snippets into consideration.</p><p>To verify the effectiveness of our proposal generation design, we compare three different settings of the segment selection procedure: (a) Fixed number of selected action segments where K is a fixed value for each class, which is not video-adaptive and content dependent; (b) K proportional to the number of predicted action segments in S(c), where K = ? * |S(c)|; (c) our design. In <ref type="table" target="#tab_6">Table 9</ref>, we can see that our design achieves the best results among the three designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Details</head><p>For the hyper-parameters, we set ? fg = 1, ? bg = 0.5, ? abg = 0.5, ? = 0.2, ? = 6, H = 8, ? = 0.5, ? = 0.7 for THUMOS-14 and ? fg = 5, ? bg = 0.5, ? abg = 0.5, ? = 0.2, ? = 10, H = 8, ? = 0, ? = 0.3 for ActivityNet-v1.3.</p><p>Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b48">48]</ref>, during inference, we use a set of thresholds to obtain the predicted action instances, then perform non-maximum suppression to remove overlapping segments. Specifically, for THUMOS-14, we set the foreground attention threshold from 0.1 to 0.9 with step 0.025, and perform NMS with a t-IOU threshold of 0.45. For ActivityNet-v1.3, we set the foreground-attention threshold from 0.005 to 0.02 with step 0.005, and apply NMS with a t-IoU threshold of 0.9.</p><p>We implement our method in PyTorch <ref type="bibr" target="#b68">[68]</ref> and train it on a single NVIDIA RTX1080Ti gpu.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Qualitative Results</head><p>We provide more qualitative results in <ref type="figure" target="#fig_8">Figure 5</ref>. The first example of action "HammerThrow" shows the missed detection of short actions and over-completeness error. The second and third example of action "Shotput" and action "CleanAndJerk" shows the incompleteness error. It clearly shows that our ASM-Loc can help address these errors with more accurate action boundary predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Limitation</head><p>The main limitation of our ASM-Loc is that the performance of our action-aware segment modeling modules depends on the generated action proposals. When the action proposals are largely misaligned with the ground-truth action segments, our ASM-Loc is not able to fix the error and generate correct predictions, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Broader Impacts</head><p>As the most popular media format nowadays, most information is spread in the format of videos. The temporal action localization task aims at finding the temporal boundaries and classifying category labels of actions of interest in untrimmed videos. Unlike supervised learning based approach that requires dense segment-level annotations, our proposed weakly-supervised temporal action localization model ASM-Loc only requires video-level labels. Therefore, WTAL is much more valuable in the real-world applications such as popular video-sharing social-network services, where billions of videos have only video-level usergenerated tags. Besides, WTAL has broad applications in various fields, e.g. event detection, video summarization, highlight generation and video surveillance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Framework Overview. The gray modules indicate the components of the base model (e.g. conv and FC), while the others are our action-aware segment modeling modules. (b) Dynamic segment sampling is based on the cumulative distribution of the sampling weight vector W . The red dots on the T -axis represent the final sampled timesteps. Shorter action segments have higher scale-up ratios. (c) Intra-segment attention applies self-attention within each action proposal. (d) Inter-segment attention applies self-attention among all proposals in a video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 (</head><label>2</label><figDesc>a) illustrates an overview of our ASM-Loc framework. Given the action proposals generated by the base model, we introduce action-aware segment modeling into all three stages of the WTAL pipeline: dynamic segment sampling in the feature extraction stage (Sec. 4.1), intra-and inter-segment attention in the feature modeling stage (Sec. 4.2) and pseudo instance-level supervision in the prediction stage (Sec. 4.3). A multi-step proposal refinement is adopted to progressively improve the action proposals and the localization results, as discussed in Sec. 4.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e n ] and c = c n 1, if ?n, t / ? [s n , e n ] and c = C + 1 0, otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) An example of "VolleyballSpiking" action (b) An example of "BaseballPitch" action GT Base (c) An example of "Billards" action (failure case)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of ground-truth, predictions and action proposals. Top-2 predictions with the highest confidence scores are selected for the base model and our ASM-Loc. Transparent frames represent background frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Diagnosing detection results. We present DETAD<ref type="bibr" target="#b66">[66]</ref> false positive profiles of the base model and our ASM-Loc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 2 : 2 S 3 q</head><label>223</label><figDesc>Action Proposal GenerationInput: Predicted Action Segments S = {(s i , e i , c i , q i )} I i=1 , selection ratio ?, segment extension parameter ? Output: Action ProposalsS = {(s n ,? n ,c n )} N n=1 1 for ground-truth class c do (c) sorted ? SORT(S(c)) // sort segments by scores of class c sum = q i // sum confidence scores for all segments4 Select K, s.t. max K K i=1 q i ? ? * q sum // select top-K segments from S(c) sorted 5S (c) : {s i ,? i ,c i } K i=1 = {s i ? ?(e i ? s i ), e i + ?(e i ? s i ), c i } K i=1// extend selected segments on both sides 6 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>(a) An example of "HammerThrow" action (b) An example of "Shotput" Visualization of ground-truth, predictions and action proposals. Top-2 predictions with the highest confidence scores are selected for the base model and our ASM-Loc. Transparent frames represent background frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Multi-step Proposal Refinement Input: Training epochs E, refinement steps L Output: Action proposalsS 1 Train the base model for E epochs. Get initial action proposals:S 0 . 3 for l in {1, ..., L} do 4</figDesc><table /><note>2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison with state-of-the-art methods on THUMOS-14 dataset. The average mAPs are computed under the IoU thresholds [0.1,0.1,0.7]. UNT and I3D are abbreviations for UntrimmedNet features and I3D features, respectively. Comparison with state-of-the-art methods on ActivityNet-v1.3 dataset. The AVG column shows the averaged mAP under the IoU thresholds [0.5:0.05:0.95].</figDesc><table><row><cell cols="2">Supervision</cell><cell>Method</cell><cell></cell><cell cols="2">Publication</cell><cell></cell><cell></cell><cell></cell><cell cols="2">mAP@IoU (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>AVG</cell></row><row><cell></cell><cell></cell><cell cols="2">SSN [25]</cell><cell cols="2">ICCV 2017</cell><cell>66.0</cell><cell>59.4</cell><cell>51.9</cell><cell>41.0</cell><cell>29.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Full (-)</cell><cell></cell><cell cols="2">TAL-Net [22] GTAN [57] P-GCN [58]</cell><cell cols="2">CVPR 2018 CVPR 2019 ICCV 2019</cell><cell>59.8 69.1 69.5</cell><cell>57.1 63.7 67.8</cell><cell>53.2 57.8 63.6</cell><cell>48.5 47.2 57.8</cell><cell>42.8 38.8 49.1</cell><cell>33.8 --</cell><cell>20.8 --</cell><cell>45.1 --</cell></row><row><cell></cell><cell></cell><cell cols="2">VSGN [59]</cell><cell cols="2">ICCV 2021</cell><cell>-</cell><cell>-</cell><cell>66.7</cell><cell>60.4</cell><cell>52.4</cell><cell>41.0</cell><cell>30.4</cell><cell>-</cell></row><row><cell cols="2">Weak (UNT)</cell><cell cols="2">AutoLoc [30] CleanNet [31] Bas-Net [6]</cell><cell cols="2">ECCV 2018 ICCV 2019 AAAI 2020</cell><cell>---</cell><cell>---</cell><cell>35.8 37.0 42.8</cell><cell>29.0 30.9 34.7</cell><cell>21.2 23.9 25.1</cell><cell>13.4 13.9 17.1</cell><cell>5.8 7.1 9.3</cell><cell>---</cell></row><row><cell></cell><cell></cell><cell cols="2">STPN [2]</cell><cell cols="2">CVPR 2018</cell><cell>52.0</cell><cell>44.7</cell><cell>35.5</cell><cell>25.8</cell><cell>16.9</cell><cell>9.9</cell><cell>4.3</cell><cell>27.0</cell></row><row><cell></cell><cell></cell><cell cols="2">CMCS [4]</cell><cell cols="2">CVPR 2019</cell><cell>57.4</cell><cell>50.8</cell><cell>41.2</cell><cell>32.1</cell><cell>23.1</cell><cell>15.0</cell><cell>7.0</cell><cell>32.4</cell></row><row><cell></cell><cell></cell><cell cols="2">WSAL-BM [32]</cell><cell cols="2">ICCV 2019</cell><cell>60.4</cell><cell>56.0</cell><cell>46.6</cell><cell>37.5</cell><cell>26.8</cell><cell>17.6</cell><cell>9.0</cell><cell>36.3</cell></row><row><cell>Weak (I3D)</cell><cell></cell><cell cols="2">DGAM [33] TSCN [7] ACM-Net [48]</cell><cell cols="2">CVPR 2020 ECCV 2020 TIP 2021</cell><cell>60.0 63.4 68.9</cell><cell>54.2 57.6 62.7</cell><cell>46.8 47.8 55.0</cell><cell>38.2 37.7 44.6</cell><cell>28.8 28.7 34.6</cell><cell>19.8 19.4 21.8</cell><cell>11.4 10.2 10.8</cell><cell>37.0 37.8 42.6</cell></row><row><cell></cell><cell></cell><cell cols="2">CoLA [9]</cell><cell cols="2">CVPR 2021</cell><cell>66.2</cell><cell>59.5</cell><cell>51.5</cell><cell>41.9</cell><cell>32.2</cell><cell>22.0</cell><cell>13.1</cell><cell>40.9</cell></row><row><cell></cell><cell></cell><cell cols="2">UGCT [10]</cell><cell cols="2">CVPR 2021</cell><cell>69.2</cell><cell>62.9</cell><cell>55.5</cell><cell>46.5</cell><cell>35.9</cell><cell>23.8</cell><cell>11.4</cell><cell>43.6</cell></row><row><cell></cell><cell></cell><cell cols="2">AUMN [35]</cell><cell cols="2">CVPR 2021</cell><cell>66.2</cell><cell>61.9</cell><cell>54.9</cell><cell>44.4</cell><cell>33.3</cell><cell>20.5</cell><cell>9.0</cell><cell>41.5</cell></row><row><cell></cell><cell></cell><cell cols="2">FAC-Net [12]</cell><cell cols="2">ICCV 2021</cell><cell>67.6</cell><cell>62.1</cell><cell>52.6</cell><cell>44.3</cell><cell>33.4</cell><cell>22.5</cell><cell>12.7</cell><cell>42.2</cell></row><row><cell></cell><cell></cell><cell cols="2">ASM-Loc (Ours)</cell><cell>-</cell><cell></cell><cell>71.2</cell><cell>65.5</cell><cell>57.1</cell><cell>46.8</cell><cell>36.6</cell><cell>25.2</cell><cell>13.4</cell><cell>45.1</cell></row><row><cell>Method</cell><cell cols="2">Publication</cell><cell cols="2">mAP@IoU (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.5 0.75 0.95 AVG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STPN [2]</cell><cell cols="4">CVPR 2018 29.3 16.9 2.6</cell><cell>16.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASSG [63]</cell><cell cols="2">MM 2019</cell><cell cols="2">32.3 20.1 4.0</cell><cell>18.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CMCS [4]</cell><cell cols="4">CVPR 2019 34.0 20.9 5.7</cell><cell>21.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bas-Net [6]</cell><cell cols="2">AAAI 2020</cell><cell cols="2">34.5 22.5 4.9</cell><cell>22.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSCN [7]</cell><cell cols="4">ECCV 2020 35.3 21.4 5.3</cell><cell>21.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A2CL-PT [64]</cell><cell cols="4">ECCV 2020 36.8 22.0 5.2</cell><cell>22.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACM-Net [48]</cell><cell cols="2">TIP 2021</cell><cell cols="2">37.6 24.7 6.5</cell><cell>24.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TS-PCA [10]</cell><cell cols="4">CVPR 2021 37.4 23.5 5.9</cell><cell>23.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UGCT [10]</cell><cell cols="4">CVPR 2021 39.1 22.4 5.8</cell><cell>23.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AUMN [35]</cell><cell cols="4">CVPR 2021 38.3 23.5 5.2</cell><cell>23.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAC-Net [12]</cell><cell cols="2">ICCV 2021</cell><cell cols="2">37.6 24.2 6.0</cell><cell>24.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASM-Loc (ours)</cell><cell></cell><cell></cell><cell cols="2">41.0 24.9 6.2</cell><cell>25.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Contribution of each component. Lfg, Lbg and Labg represents the foreground, background and action-aware background loss, which are based on MIL with video-level labels. While DSS, Intra, Inter, and Lins denote the dynamic segment sampling, intra-segment attention, inter-segment attention, and pseudo instance-level loss, respectively, which exploit segment-level information.</figDesc><table><row><cell></cell><cell cols="2">Base model</cell><cell></cell><cell cols="2">ASM-Loc</cell><cell></cell><cell>AVG</cell></row><row><cell cols="8">L fg L bg L abg DSS Intra Inter L ins 0.1:0.7</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.3</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.6</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>41.4</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>41.8</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>42</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>41.3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>42.7</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>43.7</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>44.3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>45.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation on self-attention under different settings. "Global", "BG" indicate self-attention on all and background snippets, respectively.</figDesc><table><row><cell>Label</cell><cell>Setting</cell><cell></cell><cell cols="4">mAP@IoU (%)</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell></cell><cell>0.7 AVG</cell></row><row><cell></cell><cell>Base</cell><cell cols="5">67.8 51.8 30.7 10.1 40.3</cell></row><row><cell></cell><cell cols="6">Global 67.3 50.8 30.2 10.5 40.1</cell></row><row><cell>Action</cell><cell>BG</cell><cell>66</cell><cell cols="4">50.1 30.6 10.4 39.6</cell></row><row><cell>Proposal</cell><cell>Ours</cell><cell cols="5">68.6 53.4 32.5 11.8 41.8</cell></row><row><cell>Ground</cell><cell>BG</cell><cell cols="5">64.7 49.6 30.3 9.7</cell><cell>38.8</cell></row><row><cell>Truth</cell><cell>Ours</cell><cell cols="5">73.3 56.2 33.6 13.2 44.3</cell></row><row><cell cols="7">Table 5. Impact of dynamic segment sampling</cell></row><row><cell cols="7">(DSS). Actions are divided into five duration</cell></row><row><cell cols="7">groups (seconds): XS (0, 1], S (1, 2], M (2, 4],</cell></row><row><cell cols="4">L (4, 6], and XL (6, inf).</cell><cell></cell><cell></cell></row><row><cell>Label</cell><cell>Setting</cell><cell></cell><cell cols="4">Averaged mAP (%)</cell></row><row><cell></cell><cell></cell><cell>XS</cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL AVG</cell></row><row><cell></cell><cell>Base</cell><cell cols="5">10.6 33.7 45.9 48.3 38.3 40.3</cell></row><row><cell>Action</cell><cell>+DSS</cell><cell cols="5">15.5 34.9 47.1 48.6 38.5 41.4</cell></row><row><cell>Proposal</cell><cell>?</cell><cell cols="5">+4.9 +1.2 +1.2 +0.3 +0.2 +1.1</cell></row><row><cell>Ground</cell><cell>+DSS</cell><cell>20</cell><cell>38</cell><cell cols="3">47.6 49.7 38.8</cell><cell>43</cell></row><row><cell>Truth</cell><cell>?</cell><cell cols="5">+9.4 +4.3 +1.7 +1.4 +0.5 +2.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Effectiveness of the uncertainty estimation module.</figDesc><table><row><cell>Uncer.</cell><cell></cell><cell cols="2">mAP@IoU (%)</cell></row><row><cell></cell><cell>0.3</cell><cell>0.5</cell><cell>0.7 AVG</cell></row><row><cell></cell><cell cols="3">55.5 35.5 13.8 44.1</cell></row><row><cell>?</cell><cell cols="3">57.1 36.6 13.4 45.1</cell></row><row><cell cols="2">Table 7.</cell><cell cols="2">Ablation on the</cell></row><row><cell cols="4">number of refinement steps.</cell></row><row><cell cols="4">"0" indicates the base model</cell></row><row><cell cols="4">without action-aware segment</cell></row><row><cell cols="2">modeling.</cell><cell></cell></row><row><cell>Num.</cell><cell></cell><cell cols="2">mAP@IoU (%)</cell></row><row><cell></cell><cell>0.3</cell><cell>0.5</cell><cell>0.7 AVG</cell></row><row><cell>0</cell><cell cols="3">51.8 30.7 10.1 40.3</cell></row><row><cell>1</cell><cell cols="3">54.4 34.1 12.5 43.1</cell></row><row><cell>2</cell><cell cols="3">56.2 35.4 13.8 44.7</cell></row><row><cell>3</cell><cell cols="3">57.1 36.6 13.4 45.1</cell></row><row><cell>4</cell><cell cols="3">57.3 36.7 14.1 45.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Ablation on the increased receptive field.</figDesc><table><row><cell>Modeling</cell><cell>Kernel Size</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="2">mAP@IoU (%) 0.4 0.5</cell><cell>0.6</cell><cell>0.7 AVG</cell></row><row><cell>Base</cell><cell>-</cell><cell cols="7">67.8 60.7 51.8 41.3 30.7 19.9 10.1 40.3</cell></row><row><cell></cell><cell>3</cell><cell cols="7">66.2 59.3 50.5 39.9 29.9 19.2 9.1</cell><cell>39.2</cell></row><row><cell>Conv</cell><cell>5 9</cell><cell cols="7">66.5 58.9 51.0 40.0 29.7 19.3 9.8 67.1 59.8 50.4 40.1 29.1 19.2 10.2 39.4 39.3</cell></row><row><cell>Attention</cell><cell>-</cell><cell cols="4">68.9 63.1 54.9 44.5</cell><cell>34</cell><cell cols="2">22.0 11.9 42.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Ablation on different action proposal selection methods. 64.6 57.3 46.8 35.7 24.3 14.2 44.8 (c) 71.2 65.5 57.1 46.8 36.6 25.2 13.4 45.1</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell cols="2">mAP@IoU (%)</cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7 AVG</cell></row><row><cell>(a)</cell><cell cols="2">69.9 63.8</cell><cell>56</cell><cell cols="4">45.8 36.6 25.0 13.5 44.4</cell></row><row><cell>(b)</cell><cell>70.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the Air Force (STTR awards FA865019P6014, FA864920C0010) and Amazon Research Award to AS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wtalc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with expectation-maximization multiinstance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11320" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-stream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A hybrid attention mechanism for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00545</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cola: Weakly-supervised temporal action localization with snippet contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uncertainty guided collaborative training for weakly supervised temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The blessings of unlabeled background in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Foreground-action consistency network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8002" to="8011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization using deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond short clips: End-to-end videolevel learning with collaborative memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="7567" to="7576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gta: Global temporal attention for video action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical contrastive motion learning for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A dynamic frame selection framework for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">Steven</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01529</idno>
		<title level="m">Bevt: Bert pretraining of video transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Nanning Zheng, and Gang Hua</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Phuc Xuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Refineloc: Iterative refinement for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action unit memory network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">W2f: A weakly-supervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Instance-aware, context-focused, and memoryefficient weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10598" to="10607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11067</idno>
		<title level="m">Semi-supervised vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring uncertainty in pseudo-label guided unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">106996</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph matching and pseudo-label guided deep unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debasmit</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cs George Lee</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="342" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Acm-net: Action context modeling network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqing</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02967</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09300</idno>
		<title level="m">Sniper: Efficient multi-scale training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yooju</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08199</idno>
		<title level="m">Learning from noisy labels with deep neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video selfstitching graph network for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13658" to="13667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">The tvl1 model: a geometric point of view. Multiscale Modeling &amp; Simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="154" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adversarial seeded sequence growing for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Futai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on multimedia</title>
		<meeting>the 27th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="738" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adversarial background-aware loss for weakly-supervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="283" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
