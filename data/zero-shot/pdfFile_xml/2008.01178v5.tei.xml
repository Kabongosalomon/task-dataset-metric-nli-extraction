<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Gonthier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">T?l?com Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>19 Place Marguerite Perey</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>Saint-Aubin</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa?d</forename><surname>Ladjal</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">T?l?com Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>19 Place Marguerite Perey</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">T?l?com Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>19 Place Marguerite Perey</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>weakly supervised object detection</term>
					<term>domain adaptation</term>
					<term>non-photographic images</term>
					<term>multiple instance learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised object detection (WSOD) using only image-level annotations has attracted a growing attention over the past few years. Whereas such task is typically addressed with a domain-specific solution focused on natural images, we show that a simple multiple instance approach applied on pre-trained deep features yields excellent performances on non-photographic datasets, possibly including new classes. The approach does not include any finetuning or cross-domain learning and is therefore efficient and possibly applicable to arbitrary datasets and classes. We investigate several flavors of the proposed approach, some including multi-layers perceptron and polyhedral classifiers. Despite its simplicity, our method shows competitive results on a range of publicly available datasets, including paintings (People-Art, IconArt), watercolors, cliparts and comics and allows to quickly learn unseen visual categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of object detection has witnessed great progresses over the last few years, most notably through the development of clever and pragmatic combinations of region proposal methods and deep neural network architectures <ref type="bibr" target="#b0">[1]</ref>. Nevertheless, the training of such architectures is well known to necessitate huge databases of manually annotated images. In the case of object detection, these annotations are extremely costly. It requires around one minute for a non expert to draw a bounding box around an object <ref type="bibr" target="#b1">[2]</ref>. For more specialized datasets, such as artworks databases for instance, experts are likely to be reluctant to such annotations. The usual way to annotate such databases is to rely on specialized micro-tasks platforms such as Amazon Mechanical Turk. This, by creating social exploitation and excessive precariousness, poses serious ethical concerns <ref type="bibr" target="#b2">[3]</ref>. For these reasons, reducing the annotation stage is of great importance. In particular, many Weakly Supervised Object Detection (WSOD) methods have been developed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> in order to train detection architectures using annotations only at image level, thus avoiding the precise localization of objects.</p><p>On the other hand, many different image modality exist for which object detection is desirable. Such modality include photographs taken in difficult conditions, as it is common in the case of autonomous driving <ref type="bibr" target="#b6">[7]</ref>, different imaging modality as in medical <ref type="bibr" target="#b7">[8]</ref> or satellite imaging <ref type="bibr" target="#b8">[9]</ref> or even hand created images such as artworks, clipart, etc. In such cases, available databases may be small and it is essential to be able to reuse information gathered on existing large photographic databases, a strategy known as domain adaptation <ref type="bibr" target="#b9">[10]</ref>.</p><p>In particular, methods for the weakly supervised detection of objects have been developed to deal with domain adaptation. But while this problem has been extensively studied for photographic images, much less attention has been paid to WSOD in the case of strong domain shifts, as in the case of non-photographic images, possibly including domain-specific visual category. Some works focus on cross-domain weakly supervised object detection (i.e. where bounding boxes are available for the same visual category but in an other domain than the target one), as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Methods that detect objects in photographs have been developed thanks to massive image databases on which several classes (such as cats, people, cars) have been manually localised with bounding boxes. The PASCAL VOC <ref type="bibr" target="#b12">[13]</ref> and MS COCO <ref type="bibr" target="#b13">[14]</ref> datasets have been crucial in the development of detection methods and the more recent Google Open Image Dataset (2M images, 15M boxes for 600 classes) is expected to push further the limits of detection. Even though large databases of artistic images have been build by many cultural institutions or academic research teams, e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, these databases include image-level annotations and, to the best of our knowledge, none includes location annotations. Besides, manually annotating such large databases is tedious and must be performed each time a new category is searched for. There is therefore a strong need for methods permitting the weakly supervised detection of objects for non-photographic images. In particular, only a few studies have been dedicated to the case of painting or drawings.</p><p>Moreover, these studies are mostly dedicated to the cross depiction problem: they learn to detect the same objects in photographs and in paintings, in particular man-made objects (cars, bottles ...) or animals. While these may be useful in some contexts, it is obviously needed, e.g. for art historian, to detect more specific objects or attributes such as ruins or nudity, and characters of iconographic interest such as Mary, Jesus as a child or the crucifixion of Jesus, for instance. These last categories can hardly be directly inherited from photographic databases.</p><p>In this work, we take interest in weakly supervised object detection in the case of extreme domain shifts, namely non-photographic images, possibly addressing the detection of new, never seen classes. We claim that an efficient way to perform this task is to rely on a simple Multiple Instance Learning (MIL) paradigm that is applied directly to the deep features of a pre-trained network. This approach does not involve any cross-domain learning step and can therefore be applied to arbitrary datasets and classes. Beside being efficient, as we will see in the experimental section, such a strategy also enables one to have relatively small training times. First, no fine-tuning is involved and second, we introduce a MIL strategy that is much lighter than the classical SVM approaches <ref type="bibr" target="#b17">[18]</ref>.</p><p>In order to illustrate the usefulness and efficiency of the approach, we focus on databases of man-made images, namely paintings, drawings, cliparts or comics. This poses a serious challenge because of both the lack or scarcity 1 of annotated databases and the great variety of depicting styles. Being able to detect objects in such image modality has become an important issue, mostly because of the large digitization campaigns of fine arts. These include digital scans and photographs of artworks (mainly done by the museums and other public institutions) and scans of archive photographs (such as the Cini Foundation archive <ref type="bibr" target="#b22">[23]</ref>).</p><p>In a previous conference paper <ref type="bibr" target="#b23">[24]</ref> we have shown that the proposed method is a valid strategy when dealing with extreme domain shifts. In this paper, we fully develop the approach, exploring several extensions of the model such as a multi-layers version of the Multiple Instance perceptron and a polyhedral version obtained by aggregating several linear classifiers. We also thoroughly evaluate the performances of the approach by comparing it to several state-of-the-art approaches on databases with challenging domain shifts, including paintings, drawings and cliparts. The experimental section shows that in such cases, the approach outperforms methods specially developed for the considered databases, as well as classical MIL approaches and some state-of-the-art WSOD approaches.</p><p>The paper is organized as follows. In the next section we review WSOD algorithms and MIL methods as well as some deep learning applications to recognition tasks in non-photorealistic images. In section 3, we then present our algorithm as well as some of its variants. In section 4, extensive experiments are presented, including comparisons to alternative algorithms and study of sensitivity of our method to its parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section we first review some state-of-the-art WSOD algorithms (an exhaustive review of this field is beyond the scope of the paper) and then explore MIL methods. Eventually, we make a brief survey of applications of deep learning for visual recognition in non-photographic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly Supervised Object Detection</head><p>Computer vision methods often treat WSOD as a Multiple Instance Learning (MIL) problem <ref type="bibr" target="#b24">[25]</ref>, especially in realistic cases where objects are not necessarily centered and with cluttered background <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref>. In such cases, the image is viewed as a collection of potential instances of the object to be found (for example crops of various sizes and positions).</p><p>A sketch of a typical weakly supervised detector is as follows:</p><p>3. Classification: this is often done with a MIL algorithm to obtain an instance classifier.</p><p>These general steps can be alternated or entangled (for example to enhance the region proposition or feature extraction parts based on the performance of the final classifier). In <ref type="bibr" target="#b27">[28]</ref> steps 1 and 2 are handled by extracting the features (and regions) proposed by RCNN <ref type="bibr" target="#b28">[29]</ref> . These features are passed to a smoothed version of SVM that serves as a MIL algorithm. Particular attention is paid to the initialization phase, which is crucial due to the fact that the MIL problem is essentially non-convex even if the SVM algorithm is.</p><p>More recent methods tend to entangle all the mentioned steps in an end-to-end manner. For instance, some CNN based methods group feature extraction and classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> whereas others group the three steps together <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b3">[4]</ref> propose a Weakly Supervised Deep Detection Network (WSDDN) based on Fast RCNN <ref type="bibr" target="#b32">[33]</ref>. It consists in transforming a pre-trained network by replacing its classification part by a two streams network (a region ranking stream and a classification one) combined with a weighted MIL pooling strategy. This work has been improved in many ways <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. For instance, <ref type="bibr" target="#b38">[39]</ref> refine the prediction iteratively through multistage instance classifier. Later, this model was improved by adding a clustering of the region proposals <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b33">[34]</ref>, the WSDDN model has been improved by adding two entropy term at the loss function to minimize the randomness of object localization during learning, whereas in <ref type="bibr" target="#b37">[38]</ref>, the authors propose to tackle the non-convexity of the MIL pooling by using a series of smoothed loss functions.</p><p>In <ref type="bibr" target="#b39">[40]</ref>, a two steps strategy is proposed, first collecting good regions by a mask-out classification, then selecting the best positive region in each image by a MIL formulation and then fine-tuning a detector with those propositions acting as ground truth bounding boxes. This pseudo-labeling step is often used in the weakly supervised pipeline. In <ref type="bibr" target="#b4">[5]</ref> a region proposal generator is built using weak supervision. The feature maps are transformed into a graph then into an objectness score map. This objectness score ponderates the feature maps that are subsequently fed to a classification layer. In <ref type="bibr" target="#b40">[41]</ref> the authors proposed to train two collaborative networks one of it being a Conditional Network with noisy extra-channel. The goal is to jointly minimize the dissimilarity between the prediction distribution and the conditional distribution.</p><p>It is worth noting that although CNN feature maps contain some localization information <ref type="bibr" target="#b41">[42]</ref>, the main difficulty for weakly supervised detection is the construction of an efficient box proposal model. Most works in the field use effective unsupervised methods for region proposals such as Selective Search <ref type="bibr" target="#b42">[43]</ref> or EdgeBoxes <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generic Multiple-Instance Learning</head><p>As stated above, the problem of weakly supervised object detection can be recast into a multiple instance learning (MIL) problem <ref type="bibr" target="#b24">[25]</ref>. More precisely, we are interested in instance classification as opposed to bag classification. We want to find an object among several candidate boxes in order to detect the object of interest. In <ref type="bibr" target="#b17">[18]</ref> a solution based on iterative applications of a Support Vector Machine (SVM) has been proposed to solve the MIL problem. Actually two flavors are considered, mi-SVM and MI-SVM. In the case of mi-SVM, each element of positive bags is assigned a label and the SVM margin is imposed at the instance level. In the case of MI-SVM, the SVM margin is imposed the most positive element of each positive bag and to the least negative element of each negative bag. In both cases, at test time, the learned classifier can be applied at the instance level. In <ref type="bibr" target="#b44">[45]</ref> a reformulation of MI-SVM is proposed and called latent SVM (LSVM). But in this work, a bag of instance represents the set of parts of an object and the MIL formulation is used to train an object detector with a fully-supervised training.</p><p>Several heuristics to solve the non convex-problem posed by the MIL have been proposed. For example, in <ref type="bibr" target="#b45">[46]</ref> is introduced a new objective function that try to estimate the quantity of positive examples in a positive bag, before using deterministic annealing to optimize it. In contrast to the MI-SVM method, the algorithm can consider several elements as positive in the positive bag. In <ref type="bibr" target="#b46">[47]</ref>, the authors propose a convex relaxation of the softmax loss. A comprehensive review of SVM based MIL methods can be found in <ref type="bibr" target="#b47">[48]</ref>. From this review it appears that mi-SVM and MI-SVM are still competitive on the tasks studied there. <ref type="figure" target="#fig_0">Figure 1</ref> summarizes the instances on which the SVM margins are imposed in the most popular SVM based MIL methods.</p><p>Another approach to the MIL problem is to use neural networks whose architecture treats each instance symmetrically, before an explicit aggregation (max, average) is performed. From this point a classical neural network performs a classification task <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. An improvement using more recent deep learning building blocks is proposed in <ref type="bibr" target="#b50">[51]</ref>. The aforementioned works did not focus on the instance classification performance. They all, by design, provide an instance classification network (present the network with a bag consisting of one item).</p><p>From a recent survey <ref type="bibr" target="#b51">[52]</ref> on multiple Instance Learning it appears that the most efficient algorithm for an instance level classification seems to be a clever variation of bagging and multiple classifiers to deal with multimodal distributions <ref type="bibr" target="#b52">[53]</ref>. Based on these surveys, we are driven to propose a method that mimics an SVM within a neural network. The main difference between our approach and the SVM based MIL methods is that iterations are performed during the training of the neural network and the multi-modal nature of the objects to be found drives us to consider multiple linear classifiers of each considered class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep Learning for visual recognition in non-photographic images</head><p>As almost all applications of computer vision, tasks dealing with hand-drawn or computer generated nonphotographic images benefited from the resurgence of neural networks. One point in common between all works in the field is the reuse of architectures that where originally designed for photographs classification. Some works use the pre-final features of a network as the only features retained to represent an image and do not fine-tune the network for the task at hand. Other methods allow for a certain amount of fine-tuning and add a specific network after the original architecture. Another significant difference between the papers we are going to cite is whether or not the considered classes where present in the training dataset of the original network. In the simplest setting, features from a pre-trained network are retained and used to train a linear SVM <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>, the task being the recognition of classes already present in the original training set the network was pre-trained on.</p><p>Several works have also shown that pre-trained CNN architecture can be efficiently transferred for learning new semantic visual categories, those networks either being used as features extractors <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> or being fine-tuned <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>A large body of works investigate the fine-tuning of CNN for style recognition <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, material <ref type="bibr" target="#b60">[61]</ref>, scene <ref type="bibr" target="#b61">[62]</ref> or author classification <ref type="bibr" target="#b62">[63]</ref>. The use of CNN also opens the way to efficient artwork analysis tasks, such as visual links retrieval <ref type="bibr" target="#b63">[64]</ref>, posture estimation <ref type="bibr" target="#b64">[65]</ref>, visual question answering <ref type="bibr" target="#b65">[66]</ref> and instance recognition <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>. Some works try to tackle several of those tasks at the same time <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70]</ref>. A survey about machine learning for cultural heritage have been recently published <ref type="bibr" target="#b70">[71]</ref>.</p><p>The object detection problem (recognize and locate an object) in artworks has been less studied. In <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b56">[57]</ref> it is proposed to fine-tuned a detection network in a fully supervised manner to detect people and classical Pascal VOC classes, respectively. In <ref type="bibr" target="#b10">[11]</ref>, an efficient pipeline is proposed to train a detector on new artistic modalities in a semi-supervised manner. This approach requires natural images with bounding boxes annotation of those classes and involves a relatively costly style transfer procedure. In particular, this method only allows the detection of object classes that are present and have been annotated in natural images. This specific problem have been recently studied by different research teams <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b11">12]</ref>. The same is true for many works focusing on recognizing the same object categories in different modalities <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b73">74]</ref>. Only very few work have focused on visual categories that are new and specific to artworks <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b23">24]</ref>. In <ref type="bibr" target="#b74">[75]</ref>, the authors proposed an interactive search engine to detect objects in artistic images for object categories such as praying hands, cross or grape. In <ref type="bibr" target="#b23">[24]</ref>, the authors proposed a simple MIL classifier coupled with Faster RCNN <ref type="bibr" target="#b0">[1]</ref> to weakly learn to detect new visual categories such as Mary or Saint Sebastian. The present work extends the MIL model proposed in this paper by allowing polyhedral classification and evaluate its performances on various modality such as paintings, drawings or cliparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multiple instance perceptron for the weakly supervised detection of objects</head><p>In this section, we first give the general motivation behind this work, before recalling the classical MIL framework and then introducing our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>As explained earlier, we tackle in this paper the problem of weakly supervised object detection (WSOD) in the following sense : we assume that for each image to be analyzed, bounding boxes are available, together with a global classification information. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the situation we face at training time. For each image and for a given category, we are given a set of bounding boxes and a global label, equal to +1 (the visual category of interest is present at least once in the image) or ?1 (the category is not present in this image).</p><p>Since we are especially interested by non-photographic images, for which databases may be limited, we wish to keep the learning step as light as possible. We therefore choose to combine a pre-trained detector with a classical MIL strategy. For the task of instance level classification, this approach can be used to weakly transfer an object detector to a new domain or to new visual category. Now, the MIL framework involves the minimisation of a non-convex energy, which results in heavy computational costs. For this reason, efficient relaxation schemes have been proposed <ref type="bibr" target="#b46">[47]</ref>. In this paper we propose a simple and fast heuristic to this problem, together with several variants. This, combined with the fact that we avoid fine-tuning by using features extracted from pre-trained CNNs, permits a flexible on-the-fly learning of new category in a few minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The MIL framework</head><p>We give here some basic notations related to Multiple Instance Learning. Let B = {B 1 , B 2 , . . . B N } denotes a set of N bags, each bag B i being a collection of feature vectors (instances) :</p><formula xml:id="formula_0">{X i,1 , X i,2 , . . . X i,Ki } where X i,k ? R M .</formula><p>To each feature X i,k is associated a label y i,k . In the MIL framework, each bag is associated a label which is positive if at least one instance is positive, and negative if all instances are negative. That is, the bags labels Y i are defined as :</p><formula xml:id="formula_1">Y i = +1 if ? k ? {1, . . . , K i } : y i,k = +1 ?1 if ?k ? {1, . . . , K i } : y i,k = ?1</formula><p>In this paper we consider the task of instance level classification, that is the task of infering the unknown instance labels y i,k from the known bag labels. Another classical MIL problem is the one of bag-level classification.</p><p>In an object detection setting each feature vector will represent a region. As in a typical classification problem, the goal is to learn a prediction function f w , parametrized by w, so that the predicted output f w (X) =? minimizes the empirical risk. The typical way to do so is to minimize a loss function that measures the correctness of the prediction over the training examples.</p><p>There are two main ways to tackle the fact that we only have bag level ground truth information. First, one can aggregate all the predictions of one bag to a single prediction (at bag level) during training. Hence we can write? i = g({? i,k } k?{1...Ki} ) with g an aggregation function over the elements of a bag i. In this case, the loss function can be written as</p><formula xml:id="formula_2">L(Y i ,? i ) = l(Y i , g({? i,k } k?{1...Ki} )).</formula><p>Second, one can consider each instance of a bag individually (as in the mi-SVM case, see <ref type="figure" target="#fig_0">Figure 1</ref>) and the loss function can be written as</p><formula xml:id="formula_3">L(Y i , {? i,k } k?{1...Ki} ) = g(l(h i,k (Y i ), {? i,k } k?{1...Ki} ))</formula><p>where g is an aggregation function (usually an average), l a penalty function and h i,k a modification function of the label associated to the instance k and depending on the bag label Y i , usually named a latent label (see <ref type="bibr" target="#b44">[45]</ref>). If we consider that the label of a bag is equal to the label of its instances, h i,k is the identity, otherwise it is a function from {?1, 1} to {?1, 1} depending on the bag and the instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A multiple instance perceptron</head><p>In contrast with classical approaches to the MIL problem, such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b52">53]</ref>, based on costly iterations of SVM or complex bagging methods, we propose a simple heuristic to solve the multiple instance problem. It is a multiple instance extension of the perceptron <ref type="bibr" target="#b75">[76]</ref> with a maximum taken over the instances of a bag. Our model can be seen as a latent perceptron if we use the same designation as <ref type="bibr" target="#b44">[45]</ref>.</p><p>We denote our model MI-max as introduced in <ref type="bibr" target="#b23">[24]</ref>. As we consider each class individually, we focus on the case of binary classification.</p><p>We build on a linear model</p><formula xml:id="formula_4">f w (X i,k ) = W T X i,k + b with W ? R M , b ? R,</formula><p>which we combine with a maximum aggregation function g = max k?{1...Ki} and a per example loss function equal to l(y,?) = 1 ? y T anh(?) = 1 ? T anh(y?).</p><p>(1)</p><p>We also use a regularization term on the norm of W and a weighting of the two classes, so that the complete loss function is:</p><formula xml:id="formula_5">L(W, b) = 2 ? N i=1 Y i n Yi T anh max k?{1...Ki} W T X i,k + b + C||W || 2 ,<label>(2)</label></formula><p>with n 1 the number of positive examples in the training set and n ?1 the number of negative examples. As mentioned before, the intuition behind this formulation is that minimizing L(W, b) amounts to seek a hyperplane separating the most positive element of each positive image from the least negative element of the negative image (i.e. from all examples in the negative bags). Also this loss seeks to maximize the margin.</p><p>If the hyperplane W T X + b = 0 exactly separates the most positive examples of each positive bag from the set of all examples of all negative bags, then replacing C, W and b by ?C, 1 ? W and 1 ? b respectively and taking ? to 0 will lead to a loss as close to 0 as desired. This implies that if the MIL problem admits an exact linear solution, then our loss accepts it provided C is small enough. In the worst case scenario, its value is 4 (plus the regularization term).</p><p>One advantage of this formulation is that it can be tackled by a simple gradient descent, therefore avoiding the very costly iterative procedures of other MIL solutions such as <ref type="bibr" target="#b17">[18]</ref>. Taking the max over all instance of a bag is akin to what is done in MI-SVM (mentioned in section 2.2) when after each full training of an SVM, a new representative element of each bag is selected for the next SVM training. We can switch to a stochastic gradient descent by iterating on random batches when the dataset is too big. Of course, since our loss is not convex, we are not guaranteed to find the global minimizer of the function. To tackle this problem, we run r times the model with a random initialization and pick the best one on the training set evaluation of the loss function.</p><p>If we refer to the simple description of the WSOD standard pipeline, we only focus on the multiple instance classification task and not on the boxes proposals algorithms, features extraction or refinement methods mentioned section 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">From multiple instance learning to weakly supervised object detection in images</head><p>In the context of Weakly Supervised Object Detection (WSOD), each bag i corresponds to an image and each instance k corresponds to a candidate region to be labeled. We here assume that candidate regions are returned by a classical detection network, together with a high level semantic feature vector of size M X i,k and a class-agnostic objectness score s i,k . We ignore the classification ability of the detection network: no classification label is used.</p><p>For simplicity, we consider only one class. Assume we have N images, with K bounding boxes. When an image is a positive example (the visual category is present), it is given an image-level label Y i = +1 when it is ); otherwise it is given the label Y i = ?1. The number of positive examples in the training set is denoted by n 1 , and the number of negative ones by n ?1 . Training a WSOD model from scratch, especially when the database is rather small and from another domain, is a very hard problem. Thus, reusing as much as possible models that have been trained on large datasets is advisable. In this paper, we will rely on the faster RCNN detection network but other networks could be used. We assume that features are associated to each box. We do not rely on any classification information, but we assume that an objectness score is associated to each box. The idea is to give more importance to the classification of boxes with the highest score. We observed that using the class-agnostic objectness score attached to each proposed box consistently gave better results (see section 4.3.1). We chose to multiply each W T X i,k + b by the objectness score of the region k before taking the maximum:</p><formula xml:id="formula_6">f w (X i,k ) = (s i,k + ) W T X i,k + b ,<label>(3)</label></formula><p>with ? 0 and where s i,k is the class-agnostic objectness score of the region k, as returned by the detection network. The motivation behind this formulation is that the score s i,k , roughly a clue that there is an object in box k, provides a prioritization between boxes. The same idea is used in the WSDDN model <ref type="bibr" target="#b3">[4]</ref> or in MELM <ref type="bibr" target="#b33">[34]</ref>. At test time, the instance level decision is made as before according to the sign of W T x + b , since multiplication by a positive score does not change the sign. Indeed, the hyperplane W , b is chosen to separate two classes and the loss L aims at maximizing the margin with respect to this hyperplane. It stands to reason that the instance level classification must be related to the relative position of the instance and the hyperplane. Nevertheless, we will propose in section 4 a non maximal suppression strategy that will once again use the objectness score to filter the boxes proposed for each class. More precisely the non maximal suppression algorithm will use the following score:</p><formula xml:id="formula_7">S(x) = T anh{(s(x) + ) W T x + b }<label>(4)</label></formula><p>which mixes the objectness score s(x) and the signed distance from the hyperplane W T x + b . We now present two natural extensions of our core model. We first make use a neural network to transform the bare features X i,k , so that the transformed features can be more relevant to the task at hand. Then, we investigate the interest of a polyhedral separation instead of a hyperplane for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Extensions of our model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">One hidden layer network</head><p>In this extension, called MI-max-HL, the bare features X i,k are transformed by a hidden layer before the MI-max approach is applied. This can be summarized by modifying the function f w as follows: </p><formula xml:id="formula_8">f w (X i,k ) = ? T T anh W T X i,k + b + ?, with W ? R L?M , b ? R L , ? ? R L , ? ? R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Multiple linear classifier model</head><p>As mentioned in the introduction, an improvement of the linear model consists in learning several hyperplanes in parallel, so that the binary classification is performed in a collaborative manner instead of selecting the best hyperplane. The contributions of several hyperplanes are gathered with a maximum function, so that the model can be defined as:</p><formula xml:id="formula_9">f w (X i,k ) = max j?{1...r} W T j X i,k + b j<label>At</label></formula><p>each iteration of the gradient descent only one of the couple (W j , b j ) is updated. For the inference the r hyperplanes are used.</p><p>This model, named Polyhedral MI-max yields a concave polyhedral boundary between the two classes. The concept of convex polyhedral separability has been introduced by <ref type="bibr" target="#b76">[77]</ref> and well studied in the framework of polyhedral and piece-wise linear classifier. In our case, this allows one to get more complex boundary at a modest extra-cost compared to a kernel SVM.</p><p>These models will be experimentally compared in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Discussions</head><p>The MIL part of our model MI-max-HL is close in spirit to the multiple instance neural networks proposed by <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b49">[50]</ref> 2 and further extended in <ref type="bibr" target="#b50">[51]</ref>. The best way to aggregate instance level predictions in order to find a classifier separating each of the individual vectors X i,k of each bag at test time is still an open-problem. Some works use the max operator <ref type="bibr" target="#b49">[50]</ref>, the average operator or the Log-Sum-Exponential <ref type="bibr" target="#b48">[49]</ref> for the pooling. Indeed, since the training is done with only bag level information, at test time the learned classifier must be able to handle each instance almost independently from the others because of the variety of objects that may appear in the test image.</p><p>None of these works use such approach for instance level classification and even less for weakly supervised object detection. We include in the experimental comparisons some applications (that we will call MI net or mi net <ref type="bibr" target="#b50">[51]</ref>) of this MIL methodology to the same deep features used in our method. These can be seen as variations on the general approach proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Features extraction: We use the Faster RCNN detection network <ref type="bibr" target="#b0">[1]</ref> as a feature extractor and region proposal algorithm. We extract 300 regions per image along with their high-level features 3 and the class-agnostic objectness score attached to each proposed box by the Region Proposal Network (RPN). Let us stress that, by using Faster R-CNN, our system uses a subpart that has been trained on databases with bounding boxes ground truth. In WSOD setups such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b77">78]</ref>, the models have not seen any bounding boxes, even on different modality. Observe nevertheless that, in contrast with domain adaptation methods such as <ref type="bibr" target="#b10">[11]</ref>, our method allows the detection of new classes.</p><p>According to <ref type="bibr" target="#b78">[79]</ref>, the ResNet family of networks appears to be the best architecture for transfer learning by feature extraction. Among this family we chose ResNet 152 layers trained on MS COCO <ref type="bibr" target="#b13">[14]</ref>. Therefore, the backbone we used has been trained on ImageNet, then fine-tuned on MS COCO. Remember that we chose not to fine-tune the backbone in order to provide a fast and flexible tool that can be used on small data sets. As a consequence, the backbone of our model only saw photographs for its two-phase training (ImageNet, MS COCO).</p><p>Parameters of the models: For training our MIL models, we use a batch size of 1000 examples (for smaller sets, all features are loaded into the GPU), 300 iterations of gradient descent for the linear model, performed with a constant learning rate of 0.01 and = 0.01 and C = 1 (equations <ref type="formula" target="#formula_6">(3)</ref> and <ref type="formula" target="#formula_5">(2)</ref>). The complete training takes about 6 minutes for 7 classes on the IconArt dataset <ref type="bibr" target="#b23">[24]</ref> with 12 random starting points per class using a consumer GPU (GTX 1080Ti). In the case of Polyhedral MI-max and MI-max-HL we used 3000 iterations which increase the training time to 1 hour. For MI-max-HL, we use a maximum batch size of 500 elements. Actually, the random restarts and classes are performed in parallel to take advantage of the presence of the features in the GPU memory, thus reducing the GPU-CPU transfer times. Typically, 20 classes can be learned in parallel on a standard GPU, due to the light weight of the model. One of other the advantage of not fine-tuning the network is that there is no need to store the heavy weights of the new trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and comparison to other methods</head><p>In this section, we perform weakly supervised object detection experiments on different databases.We compare our different models MI-max, Polyhedral MI-max and MI-max-HL, to the three types of methods.</p><p>The first group of methods are those specifically targeted at WSOD using fine-tuned networks. We have included state-of-the-art methods for which a source code is available: Soft Proposal Network 4 (SPN <ref type="bibr" target="#b4">[5]</ref>) and Proposal Cluster Learning 5 (PCL <ref type="bibr" target="#b77">[78]</ref>). For some of the datasets, we also include results from the Weakly supervised detection network (WSDDN <ref type="bibr" target="#b3">[4]</ref>) from <ref type="bibr" target="#b10">[11]</ref>. For those datasets we also show the performance obtained by the mixed supervised method with domain adaptation proposed by <ref type="bibr" target="#b10">[11]</ref>, a method that assume that datasets with bounding boxes for the same classes on different modality are available.</p><p>The second family of methods are generic MIL-methods directly applied to the set of deep features vectors generated by Faster RCNN. Observe that these methods ignore the objectness scores returned by the detection network. The first ones are MI-SVM and mi-SVM 6 from <ref type="bibr" target="#b17">[18]</ref>. These two methods require to train several SVMs and are therefore costly. In some cases (for the datasets PeopleArt and IconArt) we performed a PCA on the training set to reduce the number of components from 2048 to around 650 dimensions by keeping 90% of the variance (to fit the SVM in the CPU memory). We experimentally observed on the other datasets that this dimensionality reduction doesn't reduce the performances. Eventually, the computationally lighter MI Net, MI Net with Deep 3 The output of layer fc7 often called 2048-D. <ref type="bibr" target="#b3">4</ref> Trained with the following hyperparameters: batch size = 16, learning rate = 0.01, multi-scale strategy with image of sizes 112, 224 and 560, with 20 epochs. There is no regularization term in this method. <ref type="bibr" target="#b4">5</ref> Trained with the following hyperparameters: batch size = 2, learning rate = 0.001, decay=0.0005, step decay = 7, momemtum of 0.9 and defaut number of clusters (3), with 13 epochs. Those parameters correspond to the ones used by the authors for the Pascal VOC07 dataset. There is no regularization term in this method either. <ref type="bibr" target="#b5">6</ref> We allow up to 50 iterations of the algorithm (i.e. the complete training of a SVM for each class). We experimentally observe that the re-initialization of the model does not improve the performance in our case. Supervision (DS) or Residual Connection (RC) and mi Net from <ref type="bibr" target="#b50">[51]</ref> are also considered 7 . Although those models are designed for bag level classification, we used them for instance level prediction. Again, these can be seen as variants on the method we develop in this paper (the weakly detection of objects is not addressed in <ref type="bibr" target="#b50">[51]</ref>).</p><p>The last type of methods are those who (before any training) use the objectness score of the proposed regions to keep only one feature vector for each positive image. The method MAX keeps one feature vector per image and learns a linear SVM classifier that separates the positive vectors from the negative one <ref type="bibr" target="#b79">[80]</ref>. The variant MAXA also keeps one vector per positive image but uses all vectors from the negative ones. Again, a linear SVM is learned. In both cases a 3-fold cross validation is performed for determining the main hyperparameter of the SVM.</p><p>At test time, the labels and the bounding boxes are used to evaluate the performance of the methods in term of Average Precision par class. The generated boxes are filtered by a NMS with an IoU threshold of 0.3 <ref type="bibr" target="#b12">[13]</ref> and a confidence threshold of 0.05 for all methods.  As explained above, we concentrate on non-photographic databases for which a ground truth is available for object detection on the test set. We report in Tables tables 2 to 7 the performances for the weakly supervised object detection task for 6 different non-photographic datasets: PeopleArt <ref type="bibr" target="#b21">[22]</ref>, Watercolor2k, Clipart1k, Comic2k <ref type="bibr" target="#b10">[11]</ref>, IconArt <ref type="bibr" target="#b23">[24]</ref> and CASPApaintings <ref type="bibr" target="#b73">[74]</ref>. CASPApaintings is the paintings subset of the CASPA dataset 12 proposed in <ref type="bibr" target="#b73">[74]</ref> with bounding boxes associated to 8 visual categories (only animals) for most of the images.</p><p>When the method is not too costly we provide standard deviation and mean score computed on 10 runs of it. First, we can see that for all databases, the end-to-end weakly supervised methods (WSDDN, SPN and PCL) yield relatively poor results. Possible explanations are that the model overfits on the training set or that the model <ref type="bibr" target="#b6">7</ref> For this method, we consider the following hyperparameters: three fully-connected layers with 256, 128 and 64 hidden units, a kernel l2 regularization with a weight equal to 0.005, an initial learning rate equal to 0.001 with a momentum of 0.9 and a decay of 10 ?4 for 20 epochs <ref type="bibr" target="#b7">8</ref> The performance comes from the original paper <ref type="bibr" target="#b10">[11]</ref>. <ref type="bibr" target="#b8">9</ref> The performance comes from the original paper <ref type="bibr" target="#b10">[11]</ref>. <ref type="bibr" target="#b9">10</ref> The performance comes from the original paper <ref type="bibr" target="#b10">[11]</ref>. <ref type="bibr" target="#b10">11</ref> Trained with the following hyperparameters: batch size = 2, learning rate = 0.001, epochs = 13 and number of clusters by default. 12 http://people.cs.pitt.edu/ ? chris/artistic objects/ is stuck in bad local minima, so that the weakly supervised setting is not adequate with a relatively small training dataset. Moreover in the case of PCL, the boxes are proposed by the Selective Search algorithm <ref type="bibr" target="#b42">[43]</ref> which, as shown in <ref type="table" target="#tab_0">Table 11</ref>, completely fails on the considered non-photographic datasets. That alone can explain the poor results of PCL on those datasets. Recall also that these methods do use features inherited from systems such as FasterCNN that are pretrained with bounding box annotations. When comparing the performances of the different multiple instance neural networks, we can see that MI Net (Maximum Bag Margin Formulation) outperforms the other MIL networks on three datasets. Moreover the multiple instance neural network outperforms the multiple instance SVM (mi-SVM and MI-SVM), which can be due to the fact that a linear SVM that are not complex enough.</p><p>We can notice that the Maximum Pattern margin methods (mi-SVM and mi Net) never perform better that the Bag margin ones. This is rather unexpected since those models are designed to better take into account the whole positive bag by assigning an individual label per instance. These models appear to be badly suited for the task of weakly supervised detection in non-photographic databases.</p><p>When comparing our MI-max and Polyhedral MI-max models to the baseline MAX and MAXA, we observe that our models consistently perform better. Nevertheless the MAXA model performs well especially on the IconArt or CASPApaintings databases, probably because this model uses all the regions of the negatives images, yielding good discrimination of background regions during inference. The MAX baseline sometimes provides equivalent performances to more complex methods (such as MI-SVM or MI Net), illustrating the fact that the objectness score (used for selecting candidates in MAX) contains useful information. Also observe that it is faster to train a multiple instance perceptron than several linear SVMs, as is needed for MI-SVM or mi-SVM. This is quantified in Section 4.2.1.</p><p>Finally, we observe that both our models MI-max and Polyhedral MI-max provides better results than the others methods on PeopleArt, CASPApaintings, Comic2k, Clipart1k and Watercolor2k datasets.</p><p>The dataset IconArt appear to be much more challenging. In this case, our multiple instance methods provide equivalent performances compared to the multiple instance networks. The best performance is obtained by the MI Net, the MI-max-HL performance being very similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Execution Time</head><p>One advantage of our method is the relativel short time needed for training, as can be seen in <ref type="table" target="#tab_8">Table 8</ref>. As can be expected, the SPN and PCL methods are the longest to train due to the fine-tuning of the whole network. Observe also that the traingin time for our method MI-max is almost independent of the number of classes and restarts, which is a strong advantage compared to the MI-SVM, mi-SVM, MI Net and mi Net models which all need one full training per class and per re-initialization. The SVM based methods are more costly because they don't take advantage of GPU computational power.</p><p>Nevertheless, due to the aggregation of several hyperplan with a maximum operator in the Polyhedral MI-max model, we need to do 10 time more epochs that when using MI-max, which explain the strong overload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine MI-max models Analysis</head><p>In this section we discuss the details of our models and some variations. In particular, we provide an ablation study where we analyze how the choices of a different loss, different set of features and use of the objectness score impact the performances of our models. In Section 4.3.2 a thorough investigation of the main parameters' influence is conducted. From this study we are able to recommend a set of parameters that are suited for our models, thus providing the user with a safe baseline for re-using them. Then, we experimentally show that our method also permits to transfer easily the knowledge between datasets and artistic modalities. In section 4.3.3, we also evaluate the generalization ability of our models across different modalities of images (using classes shared by the different datasets). Finally, in section 4.3.4 some visual results are commented to give an insight on the strengths and shortcomings of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Ablation study</head><p>Choice of the loss function: In <ref type="table" target="#tab_10">Table 9</ref>, we gather different versions of the two models MI-max and Polyhedral MI-max with two possible modifications. First we replace the T anh based loss in equation <ref type="formula">(1)</ref> by the Hinge loss. Second we suppress the objectness score in the loss function (see section 3.4).</p><p>The first conclusion that can be drawn is that the use of objectness score significantly increase the performances of our models. This is especially true for the PeopleArt dataset where the performances very srongly decrease without using the objectness score. For the other datasets the performances are always significantly lower without the objectness score. Note that for some classes this drop in detection score is due to the fact that the model detects parts of the object instead of the whole object when the objectness score is ignored. Such an example can be seen in figure 9 section 4.3.4, where the class for Saint Sebastian is confused with arrows, which is understandable in this case but not desirable. The use of the objectness score often helps avoiding such partial detection cases.</p><p>The second conclusion is that replacing the T anh based loss function in equation <ref type="formula">(1)</ref> by a Hinge loss l(y,?) = 1?max(0, 1?y?) generally hinders the performances, except for two cases among the 12 cases of the (dataset,model) possible combinations. In particular the Polyhedral MI-max methods never benefits from a different loss function. This may be due to the fact that, given the difficulty of the task, errors are likely to happen and the T anh function may be more robust and forgiving than the Hing loss which will try hard to correct any errors, especially those with a high negative margin.</p><p>Features extraction and region proposals choices: We have investigated alternative choices for the Faster RCNN's features and box proposals: for the boxes we used the unsupervised box proposal algorithm EdgeBoxes <ref type="bibr" target="#b43">[44]</ref> and for the features we used a ResNet-152 trained on ImageNet applied to each proposed box. By doing so we must drop the objectness score that is not included in the output of EdgeBoxes.</p><p>We can see in <ref type="table" target="#tab_0">Table 10</ref> the performances of the model MI-max (without the objectness score) using those features/boxes compared to the Faster RCNN features/boxes (without objectness score for fair comparison). Regarding the detection task the performances clearly drop when using EdgeBoxes. To further investigate this drop of performance we present in <ref type="table" target="#tab_0">Table 11</ref> the recall score of three box proposals methods (the percentage of ground-truth boxes that are present in the set of all proposed boxes). We can see that EdgeBoxes performs very poorly on a data-set like PeopleArt and never matchs the boxes proposed by Faster RCNN.</p><p>For the classification task we can see that the MI-max method without objectness score performs honorably in this setting when compared to the use of Faster RCNN's boxes/features (even slightly better on the IconArt database). This is another proof that bag-level classification (the aim of the training of a MIL algorithm) is not a good proxy for instance-level classification (which is the aim of a detection algorithm). The objectness score can be seen as a very helpful cue to guide the training of a WSOD method. As shown by <ref type="bibr" target="#b80">[81]</ref> for classification task, transfer learning of deep models trained for detection tasks is the best way to obtain a detector on new domains even when no bounding boxes are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Influence of the parameters of the model</head><p>In this section, we analyse the influence of the different hyperparameters of our MI-max model. We show in <ref type="figure" target="#fig_3">Figure 3</ref> the performances with respect to each of the three following parameters: the number of restarts, the batch size and the regularization term C. We vary one parameter at a time while keeping the others fixed to the already mentioned values (i.e. 11 for the number of restarts, 1000 for the batch size and 1.0 for C).</p><p>Although the study in <ref type="bibr" target="#b47">[48]</ref> shows that restarts from random points is not always useful for nonconvex models, we find that having about 10 restarts slightly improves the performances and can be taken as a rule of thumb for our models. Notice that the variance of the outcomes is also reduced for such a parameter choice. We also found experimentally that restarts for mi-SVM or MI-SVM reduce the performance in accordance with the experiments in <ref type="bibr" target="#b47">[48]</ref>. Then, we observe that increasing the batch size provides better results and often yields a reduction of the variance. For the regularization term, we observe relatively constant performances between 1.0 and 2.0. The value 0.5 seems to be the best for 2 of the datasets (PeopleArt and IconArt, but with a great variance). These experiments also show the necessity of using a regularization term in the loss function.  <ref type="table" target="#tab_0">Tables tables 12 and 13</ref> present across-domain performance for two our models Polyhedral MI-max and MI-max. We compute the performances of detection for the classes that are shared between the different datasets. Those performances (one run) are compared to the mean performance on the same modality (several runs as before). This experiment illustrates the fact that our method can be transferred to other modality of images. This is sometimes called the "Cross-Depiction Problem" <ref type="bibr" target="#b81">[82]</ref>: recognizing visual objects regardless of whether they are painted or depicted in different artistic style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Cross modalities Knowledge Transfer</head><p>First, we can see that the Polyhedral MI-max model trained on PeopleArt outperforms the one learned on the target modality for 2 of the 3 datasets (first line). This can be due to the fact the PeopleArt dataset contains many different artistic style. We also observe that the MI-max model badly fails on those three datasets and that the Polyhedral MI-max model generalizes better. Observe also that the fact that the class person is well detected can also be due to the Faster RCNN features that have been trained on a dataset (MS COCO) containing this class.</p><p>Finally, we can notice that some datasets such as CASPApaintings and Clipart1k are more challenging that the other maybe due to the difference in the modality for the second one.</p><p>This experiment illustrates the fact that our model Polyhedral MI-max generalize well but also that providing a diverse and numerous training set can help to get a better detector trained in a weakly supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Visual results from the Polyhedral MI-max model.</head><p>In order to give some intuitive insight on the ability of the proposed method, we show some visual illustrations of the performance of the proposed model Polyhedral MI-max, both in successful and failure cases.</p><p>Successful detections: We show successful results on various datasets. In figs. 4 and 5 we show various examples of the visual categories we are able to detect, respectively on Watercolor2k and CASPApainting datasets. On <ref type="figure">Figure 6</ref>, we can see the large stylistic diversity that the model is able to detect for a same class, namely person, on the PeopleArt dataset. On <ref type="figure">Figure 7</ref>, one can see some detections on the challenging IconArt dataset.</p><p>Failures examples: We can categorize the failures cases into five main categories:</p><p>1. Discriminative elements are detected instead of the whole object: the hand for instance in <ref type="figure">Figure 8</ref> for the Polyhedral MI-max without score model or the arrows instead of Saint Sebastian in <ref type="figure">Figure 9</ref>) for the MI-max model without score. 2. Detection of a whole group instead of individual instances ( <ref type="figure" target="#fig_0">Figure 10</ref>). 3. Misclassification of correct bounding box, as in <ref type="figure" target="#fig_0">Figure 11</ref>         <ref type="bibr" target="#b17">[18]</ref> 3000s Yes Yes mi-SVM <ref type="bibr" target="#b17">[18]</ref> 30000s Yes Yes MI Net <ref type="bibr" target="#b50">[51]</ref> 1200s (20 epochs) Yes Yes MI Net with DS <ref type="bibr" target="#b50">[51]</ref> 1800s (20 epochs) Yes Yes MI Net with RC <ref type="bibr" target="#b50">[51]</ref> 1600s (20 epochs) Yes Yes mi Net <ref type="bibr" target="#b50">[51]</ref> 1800s <ref type="formula" target="#formula_5">(20 epochs</ref>       Horse 0.981 0.76 0.98 <ref type="figure" target="#fig_0">Figure 12</ref>: Failure examples using our our Polyhedral MI-max detection scheme on different datasets. We only show boxes whose scores are over 0.75. Those are confusing images. In the first one a bear in an human posture is detected as a person. In the middle, the horse, the man and other animals are deformed. The last one is a confusing case between Saint John the Baptist and Jesus children who are visually similar. Figure must be seen in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we confirm that transfer learning of pretrained CNN can provide good model to automatically analyze non photo-realistic images databases. This was previously shown for classification and fully supervised detection tasks, and was here investigated in the case of weakly supervised object detection. We proposed a simple and quick model to solve the multiple instance problem we are facing. In future works, we plan to add some constraint in the polyhedral case to force the hyperplanes to be as distinct as possible to get better boundaries, to develop on piece-wise linear model. It might be beneficial to take in more than one instance per bag to learn better detector and catch multi-modal visual category. A more extensive investigation of the different possible features extractor and boxes proposals algorithms could show the flexibility of our model. Another exciting direction is to investigate the potential of weakly supervised learning on large databases with only image-level annotations. For instance, this framework could be used to develop versatile search engine for diverse modalities of images, avoiding the time consuming annotation task. Moreover, we plan to supervise the training of weak detector with a fullytrained classifier in order to remove some obvious mis-classified box candidate as it can be done in classical WSOD method <ref type="bibr" target="#b33">[34]</ref>. This could help to provide better detection performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of standard SVM based MIL models. The blue dotted lines show the hyperplanes learned by the models, and the blue circles show the instances used during the SVM training. Figure must be seen in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of positive and negative sets of detections (bounding boxes) for the angel category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and L the dimension of the hidden layer. When compared with MI-max the parameters to be learned are ?, ?, W, b for a total dimension of L + 1 + L ? M + L = L ? (M + 2) + 1 compared to the original M + 1 scalars. We keep the function T anh as activation function to be coherent with the previous model; using a ReLU instead has little effect on the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Impact of the different hyperparameters on the MI-max model. Figure must be seen in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>. 4 .Figure 4 :Figure 5 :Figure 6 :Figure 7 :Figure 8 :Figure 9 :</head><label>4456789</label><figDesc>Confusing images (Figure 12, relatively advanced knowledge in art history is needed to know that the child on the left is Saint John the Baptist). One successful example per class using our Polyhedral MI-max detection scheme on Watercolor2k test set. We only show boxes whose scores are over 0.75. Figure must be seen in color. Successful examples of animal detection using Polyhedral MI-max on CASPA paintings test set (there is no "person" class in the training set). We only show boxes whose scores are over 0.75, except for the elephant image. Figure must be seen in color. Successful examples using our Polyhedral MI-max detection scheme on PeopleArt test set. One can observe the strong stylistic differences between the images. We only show boxes whose scores are over 0.75. Figure must be seen in color. Successful examples of detection of iconographic characters using our Polyhedral MI-max detection scheme on IconArt test set. We only show boxes whose scores are over 0.75. Figure must be seen in color. Failure examples using our our Polyhedral MI-max detection scheme on different datasets. We only show boxes whose scores are over 0.75. The most discriminative boxes correspond to parts of the whole objects. On the first image, the gloves are detected instead of a person. On the second one, the back legs and tail are detected as a dog. On the last one, the legs are detected as nudity.Figure mustbe seen in color. An example of wrongly detected object at test time, when using MI-max without or with the objectness score. In the first case, arrows or spike are detected instead of Saint Sebastian. Figure must be seen in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Failure examples using our our Polyhedral MI-max detection scheme on different datasets. We only show boxes whose scores are over 0.75. Whole groups are detected instead of the instances.Figure must be seen in color. Dog 0.883 Cat 0.809 Failure examples using our our Polyhedral MI-max detection scheme on different datasets. We only show boxes whose scores are over 0.75. Mis-classified boxes: on the first image the bird is classified as a dog and on the second one the dog is detected as a cat. Figure must be seen in color. Person 0.891 Jesus Child 0.797 Nudity 0.912 0.888</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall information of the evaluated datasets.</figDesc><table><row><cell>Reference</cell><cell>Dataset</cell><cell cols="4"># Images # Images # Instances # Classes in train in test in test</cell><cell>Min # Images per class</cell><cell cols="2">Classes from natural images Pascal VOC Classes from</cell></row><row><cell>[22]</cell><cell>PeopleArt</cell><cell>3007</cell><cell>1616</cell><cell>1137</cell><cell>1</cell><cell>968</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>[11]</cell><cell>Watercolor2k</cell><cell>1000</cell><cell>1000</cell><cell>3315</cell><cell>6</cell><cell>27</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>[11]</cell><cell>Clipart1k</cell><cell>500</cell><cell>500</cell><cell>3615</cell><cell>20</cell><cell>21</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>[11]</cell><cell>Comic2k</cell><cell>1000</cell><cell>1000</cell><cell>6389</cell><cell>6</cell><cell>87</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>[74]</cell><cell>CASPA paintings</cell><cell>1045</cell><cell>1033</cell><cell>1486</cell><cell>36</cell><cell>8</cell><cell>Yes</cell><cell>6 out of 8</cell></row><row><cell>[24]</cell><cell>IconArt</cell><cell>2978</cell><cell>1480</cell><cell>3009</cell><cell>7</cell><cell>75</cell><cell>No</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Network</cell><cell>Method</cell><cell>Model</cell><cell>person</cell></row><row><cell>VGG16-IM</cell><cell>Weakly supervised fine tuning</cell><cell>SPN [5] PCL [78]</cell><cell>10.0 3.4</cell></row><row><cell></cell><cell></cell><cell>MAX [80]</cell><cell>25.9</cell></row><row><cell></cell><cell></cell><cell>MAXA</cell><cell>48.9</cell></row><row><cell></cell><cell></cell><cell>MI-SVM [18]</cell><cell>13.3</cell></row><row><cell></cell><cell></cell><cell>mi-SVM [18]</cell><cell>5.6</cell></row><row><cell>RES-152-COCO</cell><cell>Features extraction</cell><cell cols="2">MI Net [51] MI Net with DS [51] 19.5 ? 11.4 33.0 ? 6.0 MI Net with RC [51] 12.5 ? 8.3</cell></row><row><cell></cell><cell></cell><cell>mi Net [51]</cell><cell>26.5 ? 8.5</cell></row><row><cell></cell><cell></cell><cell>MI-max</cell><cell>55.5 ? 1.0</cell></row><row><cell></cell><cell></cell><cell>Polyhedral MI-max</cell><cell>58.3 ? 1.2</cell></row><row><cell></cell><cell></cell><cell>MI-max-HL</cell><cell>57.3 ? 2.0</cell></row></table><note>People-Art (test set) Average precision (%). Comparison of the proposed MI-max, Polyhedral MI-max and mi-perceptron methods to alternative approaches. In red the best weakly supervised method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Watercolor2k (test set) Average precision (%). Comparison of the proposed MI-max, Polyhedral MI-max and mi-perceptron methods to alternative approaches. In green the best mixed supervised method and in red the best weakly supervised one.</figDesc><table><row><cell>Net</cell><cell>Method</cell><cell>Model</cell><cell>bike bird car</cell><cell>cat</cell><cell cols="2">dog person</cell><cell>mean</cell></row><row><cell>SSD</cell><cell>Mixed + DA</cell><cell>DT+PL [11] 8</cell><cell cols="3">76.5 54.9 46.0 37.4 38.5</cell><cell>72.3</cell><cell>54.3</cell></row><row><cell>VGG16 IM</cell><cell>Weakly supervised fine-tuning</cell><cell>WSDDN [4] 8 SPN [5] PCL [78]</cell><cell cols="2">1.5 26.0 14.6 0.4 0.0 18.9 0.0 0.0 0.0 0.0 0.0 0.0</cell><cell>0.5 0.0 0.0</cell><cell>33.3 23.6 0.0</cell><cell>12.7 7.1 0.0</cell></row><row><cell></cell><cell></cell><cell>MAX [80]</cell><cell cols="3">76.0 33.8 33.0 20.8 22.7</cell><cell>19.8</cell><cell>34.3</cell></row><row><cell></cell><cell></cell><cell>MAXA</cell><cell cols="3">60.6 39.2 39.6 30.9 32.0</cell><cell>61.2</cell><cell>43.9</cell></row><row><cell></cell><cell></cell><cell>MI-SVM [18]</cell><cell cols="3">66.8 20.9 7.6 14.1 8.5</cell><cell>13.2</cell><cell>21.8</cell></row><row><cell></cell><cell></cell><cell>mi-SVM [18]</cell><cell>10.6 10.9 1.4</cell><cell>2.0</cell><cell>0.8</cell><cell>5.9</cell><cell>5.3</cell></row><row><cell>RES-152-COCO</cell><cell>Features extraction</cell><cell cols="4">MI Net [51] MI Net with DS [51] 73.4 22.4 25.8 17.6 11.2 77.6 32.4 35.5 24.7 16.2 MI Net with RC [51] 32.3 19.2 20.1 6.7 6.8</cell><cell>18.0 10.3 15.4</cell><cell>34.1 ? 1.0 26.8 ? 2.4 16.7 ? 6.3</cell></row><row><cell></cell><cell></cell><cell>mi Net [51]</cell><cell cols="3">66.4 30.3 14.9 14.4 8.6</cell><cell>20.5</cell><cell>25.8 ? 3.5</cell></row><row><cell></cell><cell></cell><cell>MI-max</cell><cell cols="3">84.1 47.4 48.2 30.9 27.9</cell><cell>58.2</cell><cell>49.5 ? 0.9</cell></row><row><cell></cell><cell></cell><cell>Polyhedral MI-max</cell><cell cols="3">77.8 44.7 45.5 25.6 26.7</cell><cell>59.2</cell><cell>46.6 ? 1.3</cell></row><row><cell></cell><cell></cell><cell>MI-max-HL</cell><cell cols="3">79.3 46.1 43.6 26.9 28.8</cell><cell>57.0</cell><cell>47.0 ? 1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Clipart1k (test set) Average precision (%). Comparison of the proposed MI-max, Polyhedral MI-max and mi-perceptron methods to alternative approaches. In those case, we use a line search for MAX and MAXA. In green the best mixed supervised method and in red the best weakly supervised one.</figDesc><table><row><cell>mean</cell><cell>46.0</cell><cell>39.9</cell><cell>34.9</cell><cell>4.4</cell><cell></cell></row><row><cell>aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor</cell><cell>35.7 61.9 26.2 45.9 29.9 74.0 48.7 2.8 53.0 72.7 50.2 19.3 40.9 83.3 62.4 42.4 22.8 38.5 49.3 59.5</cell><cell></cell><cell></cell><cell>1.6 3.6 0.6 2.3 0.1 11.7 4.5 0.0 3.2 0.1 2.8 2.3 0.9 0.1 14.4 16.0 4.5 0.7 1.2 18.3</cell><cell></cell></row><row><cell>Model</cell><cell>DT+PL [11] 9</cell><cell>DT+PL [11] 9</cell><cell>DT+PL [11] 9</cell><cell>WSDDN [4] 9</cell><cell>SPN [5]</cell></row><row><cell>Method</cell><cell>Mixed supervised</cell><cell>with domain</cell><cell>adaptation</cell><cell>Weakly</cell><cell>supervised</cell></row><row><cell>Net</cell><cell>SSD</cell><cell>Yolov2</cell><cell>Faster RCNN</cell><cell></cell><cell>VGG16-IM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comic2k (test set) Average precision (%). Comparison of the proposed MI-max method to alternative approaches. no GS means no Grid Search on the hyperparameters of the SVM otherwise it is the case.</figDesc><table><row><cell>Net</cell><cell>Method</cell><cell cols="2">Model</cell><cell cols="3">bike bird car</cell><cell>cat</cell><cell cols="2">dog person</cell><cell>mean</cell></row><row><cell>SSD</cell><cell>Mixed supervised with domain adaptation</cell><cell>DT+PL</cell><cell>[11] 10</cell><cell cols="5">76.5 54.9 46.0 37.4 38.5</cell><cell>72.3</cell><cell>54.3</cell></row><row><cell></cell><cell>Weakly</cell><cell cols="2">WSDDN [4] 10</cell><cell cols="4">1.5 26.0 14.6 0.4</cell><cell>0.5</cell><cell>33.3</cell><cell>12.7</cell></row><row><cell>VGG16-IM</cell><cell>supervised</cell><cell cols="2">SPN [5]</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>3.1</cell><cell>0.0</cell><cell>4.1</cell><cell>1.2</cell></row><row><cell></cell><cell>fien tuning</cell><cell cols="2">PCL [78]</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">MAX[80]</cell><cell cols="5">15.2 2.7 29.4 2.3 16.8</cell><cell>4.9</cell><cell>11.9</cell></row><row><cell></cell><cell></cell><cell cols="2">MAXA</cell><cell cols="4">36.8 5.6 27.1 8.2</cell><cell>6.1</cell><cell>34.8</cell><cell>19.8</cell></row><row><cell></cell><cell></cell><cell cols="2">MI-SVM [18]</cell><cell cols="4">34.2 3.0 20.0 5.2</cell><cell>2.5</cell><cell>12.9</cell><cell>13.0</cell></row><row><cell></cell><cell></cell><cell cols="2">mi-SVM no GS [18]</cell><cell cols="2">10.8 2.3</cell><cell>5.5</cell><cell>3.2</cell><cell>2.1</cell><cell>3.6</cell><cell>4.6</cell></row><row><cell>RES-152-COCO</cell><cell>Features extraction</cell><cell cols="7">MI Net [51] MI Net with DS [51] 40.8 13.3 32.5 5.7 42.9 15.5 33.1 11.8 13.4 9.1 MI Net with RC [51] 19.8 5.4 16.4 2.8 9.8</cell><cell>20.4 16.1 13.9</cell><cell>22.8 ? 1.1 19.6 ? 1.6 11.4 ? 4.4</cell></row><row><cell></cell><cell></cell><cell cols="2">mi Net [51]</cell><cell cols="4">42.1 10.9 24.5 8.8</cell><cell>8.8</cell><cell>22.1</cell><cell>19.5 ? 2.1</cell></row><row><cell></cell><cell></cell><cell cols="2">MI-max</cell><cell cols="5">45.3 9.7 33.7 14.4 21.6</cell><cell>37.0</cell><cell>27.0 ? 0.8</cell></row><row><cell></cell><cell></cell><cell cols="2">Polyhedral MI-max</cell><cell cols="5">44.9 5.2 26.2 14.1 11.0</cell><cell>38.4</cell><cell>23.3 ? 1.6</cell></row><row><cell></cell><cell></cell><cell cols="2">MI-max-HL</cell><cell cols="5">43.0 5.1 31.5 11.8 13.8</cell><cell>36.4</cell><cell>23.6 ? 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>CASPA paintings (test set) Average precision (%). Comparison of the proposed MI-max method to alternative approaches. no GS means no Grid Search on the hyperparameters of the SVM otherwise it is the case.</figDesc><table><row><cell>Net</cell><cell>Method</cell><cell>Model</cell><cell cols="8">bear bird cat cow dog elephant horse sheep</cell><cell>mean</cell></row><row><cell>VGG16-IM</cell><cell>Weakly supervised fine tuning</cell><cell>SPN [5] PCL [78]</cell><cell>0.5 0.0</cell><cell>0.1 0.0</cell><cell>1.6 0.0</cell><cell>0.9 0.0</cell><cell>0.5 0.0</cell><cell>1.4 0.0</cell><cell>0.6 0.0</cell><cell>0.0 0.0</cell><cell>0.7 0.0</cell></row><row><cell></cell><cell></cell><cell>MAX[80]</cell><cell>22.0</cell><cell cols="4">2.1 14.5 3.5 14.2</cell><cell>8.8</cell><cell>12.8</cell><cell>0.5</cell><cell>9.8</cell></row><row><cell></cell><cell></cell><cell>MAXA</cell><cell cols="4">26.3 13.1 26.9 5.4</cell><cell>8.3</cell><cell>18.1</cell><cell>14.9</cell><cell>3.9</cell><cell>14.6</cell></row><row><cell></cell><cell></cell><cell>MI-SVM [18]</cell><cell>9.3</cell><cell>0.2</cell><cell>6.7</cell><cell>1.5</cell><cell>0.1</cell><cell>0.6</cell><cell>0.9</cell><cell>0.4</cell><cell>2.5</cell></row><row><cell></cell><cell></cell><cell>mi-SVM no GS [18]</cell><cell>1.3</cell><cell>1.6</cell><cell>3.0</cell><cell>0.8</cell><cell>1.0</cell><cell>0.3</cell><cell>1.5</cell><cell>0.3</cell><cell>1.2</cell></row><row><cell>RES-152-COCO</cell><cell>Features extraction</cell><cell cols="2">MI Net [51] MI Net with DS [51] 29.0 32.8 MI Net with RC [51] 16.9</cell><cell cols="3">5.4 14.1 5.2 1.6 8.3 3.0 0.9 6.6 2.6</cell><cell>6.2 3.2 2.9</cell><cell>15.0 5.9 8.2</cell><cell>11.1 7.1 4.7</cell><cell>4.2 2.6 2.1</cell><cell>11.7 ? 1.6 7.6 ? 1.2 5.6 ? 2.1</cell></row><row><cell></cell><cell></cell><cell>mi Net [51]</cell><cell>26.7</cell><cell cols="3">8.9 12.5 1.5</cell><cell>3.4</cell><cell>7.1</cell><cell>5.1</cell><cell>2.4</cell><cell>8.4 ? 1.7</cell></row><row><cell></cell><cell></cell><cell>MI-max</cell><cell cols="5">28.3 15.7 25.6 5.3 13.7</cell><cell>17.2</cell><cell>18.8</cell><cell>5.1</cell><cell>16.2 ? 0.4</cell></row><row><cell></cell><cell></cell><cell>Polyhedral MI-max</cell><cell cols="5">26.2 16.9 23.9 5.4 10.1</cell><cell>9.7</cell><cell>18.8</cell><cell>4.5</cell><cell>14.4 ? 0.7</cell></row><row><cell></cell><cell></cell><cell>MI-max-HL</cell><cell cols="5">26.5 15.7 26.3 4.8 14.2</cell><cell>10.1</cell><cell>11.5</cell><cell>6.2</cell><cell>14.4 ? 0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>IconArt detection test set detection average precision (%) at IoU 0.5. Comparison of the proposed MI-max, Polyhedral MI-max and mi-perceptron methods to alternative approaches. In those case, we use a grid search for MAX and MAXA. In red, the best weakly supervised method.</figDesc><table><row><cell>Net</cell><cell>Method</cell><cell>Model</cell><cell cols="7">angel JCchild crucifixion Mary nudity ruins StSeb</cell><cell>mean</cell></row><row><cell>VGG16-IM</cell><cell>Weakly supervised fien tuning</cell><cell>SPN [5] PCL 11 [78]</cell><cell>0.0 2.9</cell><cell>0.8 0.3</cell><cell>22.3 1.0</cell><cell>12.0 26.3</cell><cell>6.8 2.3</cell><cell>10.4 7.2</cell><cell>1.2 1.4</cell><cell>7.7 5.9</cell></row><row><cell></cell><cell></cell><cell>MAX[80]</cell><cell>1.4</cell><cell>1.3</cell><cell>11.5</cell><cell>2.8</cell><cell>3.8</cell><cell>0.3</cell><cell>4.5</cell><cell>3.7</cell></row><row><cell></cell><cell></cell><cell>MAXA</cell><cell>1.3</cell><cell>4.4</cell><cell>18.2</cell><cell>28.0</cell><cell>15.3</cell><cell>0.2</cell><cell>16.4</cell><cell>12.0</cell></row><row><cell></cell><cell></cell><cell>MI-SVM [18]</cell><cell>0.7</cell><cell>4.4</cell><cell>21.6</cell><cell>0.6</cell><cell>1.0</cell><cell>0.0</cell><cell>0.0</cell><cell>4.0</cell></row><row><cell></cell><cell></cell><cell>mi-SVM [18]</cell><cell>1.3</cell><cell>5.1</cell><cell>3.9</cell><cell>3.6</cell><cell>2.9</cell><cell>0.3</cell><cell>2.2</cell><cell>2.8</cell></row><row><cell>RES-152-COCO</cell><cell>Features extraction</cell><cell>MI Net [51] MI Net with DS [51] MI Net with RC [51]</cell><cell>9.7 8.6 8.2</cell><cell>42.6 35.6 36.9</cell><cell>21.1 19.6 20.5</cell><cell>6.9 5.3 4.8</cell><cell>17.6 15.9 16.2</cell><cell>5.1 3.2 1.6</cell><cell>2.5 3.1 0.9</cell><cell>15.1 ? 1.5 13.0 ? 1.7 12.7 ? 1.6</cell></row><row><cell></cell><cell></cell><cell>mi Net [51]</cell><cell>8.2</cell><cell>28.4</cell><cell>15.1</cell><cell>11.2</cell><cell>15.8</cell><cell>6.8</cell><cell>4.5</cell><cell>12.9 ? 1.2</cell></row><row><cell></cell><cell></cell><cell>MI-max</cell><cell>0.3</cell><cell>0.1</cell><cell>42.7</cell><cell>4.4</cell><cell>21.9</cell><cell>0.6</cell><cell>13.7</cell><cell>12.0 ? 0.9</cell></row><row><cell></cell><cell></cell><cell>Polyhedral MI-max</cell><cell>3.1</cell><cell>9.8</cell><cell>33.0</cell><cell>7.4</cell><cell>29.2</cell><cell>0.1</cell><cell>8.5</cell><cell>13.0 ? 2.2</cell></row><row><cell></cell><cell></cell><cell>MI-max-HL</cell><cell>4.3</cell><cell>6.7</cell><cell>35.7</cell><cell>15.6</cell><cell>24.0</cell><cell>0.1</cell><cell>15.2</cell><cell>14.5 ? 1.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Execution time of the different models for datasets Watercolor2k and Comic2k, with 1000 images in the training set and 6 visual categories.</figDesc><table><row><cell>Method</cell><cell>Training Duration</cell><cell cols="2">Linear to number of class Linear to number of restarts</cell></row><row><cell>No Boxes proposals</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPN [5]</cell><cell>3000s (20 epochs)</cell><cell>No</cell><cell>?</cell></row><row><cell>Selective Search Bounding Boxes proposal</cell><cell>6600s</cell><cell></cell><cell></cell></row><row><cell>PCL [78]</cell><cell>12000s (13 epochs)</cell><cell>No</cell><cell>?</cell></row><row><cell>Faster RCNN Features and boxes proposals</cell><cell>200s</cell><cell></cell><cell></cell></row><row><cell>MAX</cell><cell>52s</cell><cell>Yes</cell><cell>?</cell></row><row><cell>MAXA</cell><cell>2000s</cell><cell>Yes</cell><cell>?</cell></row><row><cell>MI-SVM</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Mean average precision over the classes of the different datasets (%). Comparison of the proposed MI-max and Polyhedral MI-max methods with different settings. Standard deviation is computed on 10 runs of the method.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MI-max</cell><cell></cell><cell></cell><cell cols="2">Polyhedral MI-max</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">Main Model Without score Hinge loss</cell><cell cols="4">Without score Main Model Without score Hinge loss and hinge loss</cell><cell>Without score and hinge loss</cell></row><row><cell>PeopleArt</cell><cell>55.5 ? 1.0</cell><cell>0.9 ? 0.4</cell><cell>57.6 ? 1.0</cell><cell>1.7 ? 0.9</cell><cell>58.3 ? 1.2</cell><cell>10.1 ? 3.3</cell><cell>56.6 ? 4.4</cell><cell>18.1 ? 8.6</cell></row><row><cell>Watercolor2k</cell><cell>49.5 ? 0.9</cell><cell>32.8 ? 2.2</cell><cell>46.7 ? 1.5</cell><cell>33.8 ? 1.6</cell><cell>46.6 ? 1.3</cell><cell>18.3 ? 4.7</cell><cell>37.5 ? 2.1</cell><cell>24.8 ? 3.3</cell></row><row><cell>Clipart1k</cell><cell>38.4 ? 0.8</cell><cell>24.2 ? 1.6</cell><cell>34.8 ? 1.2</cell><cell>22.2 ? 1.8</cell><cell>30.5 ? 2.3</cell><cell>11.9 ? 2.6</cell><cell>16.5 ? 1.2</cell><cell>5.1 ? 1.1</cell></row><row><cell>Comic2k</cell><cell>27.0 ? 0.8</cell><cell>17.4 ? 1.5</cell><cell>25.5 ? 1.1</cell><cell>17.3 ? 1.1</cell><cell>23.3 ? 1.6</cell><cell>11.6 ? 2.8</cell><cell>15.0 ? 1.8</cell><cell>9.5 ? 1.8</cell></row><row><cell>CASPA paintings</cell><cell>16.2 ? 0.4</cell><cell>18.7 ? 0.8</cell><cell>16.1 ? 0.5</cell><cell>12.6 ? 0.9</cell><cell>14.4 ? 0.7</cell><cell>8.6 ? 1.4</cell><cell>9.0 ? 0.9</cell><cell>3.2 ? 0.6</cell></row><row><cell>IconArt</cell><cell>12.0 ? 0.9</cell><cell>6.7 ? 2.5</cell><cell>14.3 ? 2.1</cell><cell>8.2 ? 2.3</cell><cell>13.0 ? 2.2</cell><cell>6.4 ? 2.3</cell><cell>13.3 ? 2.8</cell><cell>8.3 ? 2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Average precision for detection and classification (%). Two different feature extraction methods are considered in this table (both without objectness score).</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell cols="2">Faster RCNN EdgeBoxes</cell></row><row><cell>PeopleArt</cell><cell>AP IuO 0.5 Classif AP</cell><cell>0.9 ? 0.4 92.5 ? 0.3</cell><cell>0.0 ? 0.0 92.1 ? 0.2</cell></row><row><cell>Clipart1k</cell><cell>AP IuO 0.5 Classif AP</cell><cell>24.2 ? 1.6 59.4 ? 1.7</cell><cell>3.1 ? 0.3 42.8 ? 1.3</cell></row><row><cell>Comic2k</cell><cell>AP IuO 0.5 Classif AP</cell><cell>17.4 ? 1.5 54.9 ? 2.0</cell><cell>1.8 ? 0.3 47.9 ? 1.5</cell></row><row><cell>Watercolor2k</cell><cell>AP IuO 0.5 Classif AP</cell><cell>32.8 ? 2.2 78.0 ? 1.2</cell><cell>2.7 ? 0.5 71.8 ? 1.3</cell></row><row><cell>CASPA</cell><cell>AP IuO 0.5 Classif AP</cell><cell>12.6 ? 0.5 48.6 ? 0.6</cell><cell>0.3 ? 0.1 45.0 ? 1.2</cell></row><row><cell>IconArt</cell><cell>AP IuO 0.5 Classif AP</cell><cell>6.7 ? 2.5 60.4 ? 1.1</cell><cell>5.3 ? 0.3 69.2 ? 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Recall (%) at IuO 0.5 of the boxes proposals for the different methods and databases. Mean over the classes.</figDesc><table><row><cell>Dataset</cell><cell cols="3">RPN of Pre-trained EdgeBoxes Selective Search Faster RCNN [1] [44] [43]</cell></row><row><cell>Number of boxes</cell><cell>300</cell><cell>300</cell><cell>3000-5000</cell></row><row><cell>PeopleArt</cell><cell>94.0</cell><cell>15.4</cell><cell>55.7</cell></row><row><cell>Clipart1k</cell><cell>91.4</cell><cell>14.4</cell><cell>49.4</cell></row><row><cell>Comic2k</cell><cell>82.7</cell><cell>54.1</cell><cell>46.2</cell></row><row><cell>Watercolor2k</cell><cell>93.6</cell><cell>61.4</cell><cell>56.8</cell></row><row><cell>CASPA</cell><cell>76.6</cell><cell>34.3</cell><cell>51.6</cell></row><row><cell>IconArt</cell><cell>75.9</cell><cell>60.0</cell><cell>56.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Mean AP (%) at IuO 0.5 for the common classes between the source and target sets with the MI-max model. In parenthesis the mean performance obtained by learning the detection on the same set (modality).</figDesc><table><row><cell>source set</cell><cell>target set</cell><cell cols="2">PeopleArt Watercolor2k</cell><cell>Comic2k</cell><cell>Clipart1k</cell><cell>CASPApaintings</cell></row><row><cell>PeopleArt</cell><cell></cell><cell>-</cell><cell>0.0 (58.2)</cell><cell>0.0 (37.0)</cell><cell>0.0 (55.5)</cell><cell>/</cell></row><row><cell cols="2">Watercolor2k</cell><cell>47.4 (55.5)</cell><cell>-</cell><cell cols="2">25.8 (27.0) 12.2 (33.4)</cell><cell>15.6 (18.3)</cell></row><row><cell>Comic2k</cell><cell></cell><cell>50.4 (55.5)</cell><cell>47.3 (49.5)</cell><cell>-</cell><cell>10.0 (33.4)</cell><cell>15.0 (18.3)</cell></row><row><cell>Clipart1k</cell><cell></cell><cell>36.2 (55.5)</cell><cell>44.3 (49.5)</cell><cell>25.2 (27.0)</cell><cell>-</cell><cell>10.8 (14.0)</cell></row><row><cell cols="2">CASPApaintings</cell><cell>/</cell><cell>33.4 (35.4)</cell><cell cols="2">12.2 (15.2) 4.7 (22.5)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Mean AP (%) at IuO 0.5 for the common classes between the source and target sets with the Polyhedral MI-max model. The mean performance obtained by learning the detection on the same set (modality) is displayed between brackets.</figDesc><table><row><cell>source set</cell><cell>target set</cell><cell cols="3">PeopleArt Watercolor2k</cell><cell>Comic2k</cell><cell>Clipart1k</cell><cell>CASPApaintings</cell></row><row><cell>PeopleArt</cell><cell></cell><cell>-</cell><cell>60.0 (59.2)</cell><cell></cell><cell cols="2">42.1 (39.5) 54.3 (55.4)</cell><cell>/</cell></row><row><cell cols="2">Watercolor2k</cell><cell>56.0 (57.3)</cell><cell>-</cell><cell></cell><cell cols="2">23.1 (24.1) 11.2 (24.6)</cell><cell>13.8 (18.3)</cell></row><row><cell>Comic2k</cell><cell></cell><cell>48.9 (57.3)</cell><cell>42.4 (46.6)</cell><cell></cell><cell>-</cell><cell>7.2 (24.6)</cell><cell>12.5 (18.3)</cell></row><row><cell>Clipart1k</cell><cell></cell><cell>52.0 (57.3)</cell><cell>36.7 (46.6)</cell><cell></cell><cell>19.6 (24.1)</cell><cell>-</cell><cell>7.7 (13.6)</cell></row><row><cell cols="2">CASPApaintings</cell><cell>/</cell><cell>27.5 (39.0)</cell><cell></cell><cell>9.9 (18.1)</cell><cell>4.2 (12.5)</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Angel 0.959</cell><cell cols="2">Nudity 0.958 0.947</cell></row><row><cell></cell><cell></cell><cell cols="4">Horse 0.994 0.903 0.926 0.873 0.916 0.908</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Classical databases used for training networks are made of millions of natural images (Imagenet<ref type="bibr" target="#b18">[19]</ref>(millions of images), PASCAL VOC<ref type="bibr" target="#b12">[13]</ref>, MS COCO<ref type="bibr" target="#b13">[14]</ref> Google Open Image Dataset (9M images)<ref type="bibr" target="#b19">[20]</ref>). In contrast, datasets for recognition in non-photographic images are rare and usually only containing image-level annotations, as in the iMet dataset (375k)<ref type="bibr" target="#b20">[21]</ref> or BAM! (2.5M)<ref type="bibr" target="#b16">[17]</ref>. The very few datasets with bounding boxes such as PeopleArt<ref type="bibr" target="#b21">[22]</ref>, used later in this paper, are very small.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">These models involve a sigmoid activation and they are trained with a quadratic loss l(y,?) = (y ??) 2 and no re-initialization (r = 0).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by the "IDI 2017" project funded by the IDEX Paris-Saclay, ANR-11-IDEX-0003-02 and by T?l?com Paris.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
		<idno type="arXiv">99arXiv:1506.01497</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">91</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowdsourcing Annotations for Visual Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Micro-work, artificial intelligence and the automotive industry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tubaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Casilli</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40812-019-00121-1</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Industrial and Business Economics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="345" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly Supervised Deep Detection Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.311</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soft Proposal Networks for Weakly Supervised Object Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.204</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1859" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly Supervised Region Proposal Network and Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation via Disentangled Representations: Application to Cross-Modality Liver Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_29</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2019</title>
		<editor>D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap, A. Khan</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="255" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive Batch Normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.03.005</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adapting Visual Category Models to New Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15561-1_16</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010</title>
		<editor>K. Daniilidis, P. Maragos, N. Paragios</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11365</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeply Aligned Adaptation for Cross-domain Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02093</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-009-0275-4</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coco</forename><surname>Microsoft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Online Collection Catalogue -Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rijksmuseum</surname></persName>
		</author>
		<ptr target="https://www.rijksmuseum.nl/en/research/online-collection-catalogue" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image and Data Resources -The Metropolitan Museum of Art</title>
		<ptr target="https://www.metmuseum.org/about-the-met/policies-and-documents/image-resources" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>MET</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08614</idno>
		<title level="m">BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1211" to="1220" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-020-01316-z</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaeser-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vesom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00901</idno>
		<idno>arXiv:1906.00901</idno>
		<title level="m">The iMet Collection 2019 Challenge Dataset</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting people in artwork with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Westlake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46604-0_57</idno>
		<idno type="arXiv">arXiv:1610.08871</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<editor>G. Hua, H. J?gou</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New Techniques for the Digitization of Art Historical Photographic Archives -the Case of the Cini Foundation in Venice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Costiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Di Lenardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="DOI">10.2352/issn.2168-3204.2018.1.0.2</idno>
	</analytic>
	<monogr>
		<title level="j">Archiving Conference</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly Supervised Object Detection in Artworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gonthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ladjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bonfait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="692" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0004-3702(96)00034-3</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised discriminative localization and classification: A joint learning process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2009.5459426</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1925" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126261</idno>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.81</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly Supervised Cascaded Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.545</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5131" to="5139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04331</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple Instance Detection Network with Online Instance Classifier Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.326</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Min-Entropy Latent Model for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1297" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Zigzag Learning for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09466</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4262" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Dual-Network Progressive Approach to Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123455</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Multimedia, MM &apos;17</title>
		<meeting>the 25th ACM International Conference on Multimedia, MM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Mil</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05647</idno>
		<title level="m">Continuation Multiple Instance Learning for Weakly Supervised Object Detection, CVPR</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly Supervised Learning of Deformable Part-Based Models for Object Detection via Region Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2016.2614862</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="407" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly Supervised Object Localization with Progressive Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dissimilarity Coefficient based Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10016</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Is object localization for free? -Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-013-0620-5</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Locating Object Proposals from Edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edge</forename><surname>Boxes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_26</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part-Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.167</idno>
		<idno>doi:10.1109/ TPAMI.2009.167</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deterministic Annealing for Multiple-Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A convex relaxation for weakly supervised classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ICML</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A theoretical and empirical analysis of support vector machine methods for multiple-instance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-013-5429-5</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="102" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi Instance Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML-2000 Workshop on Attribute-Value and Relational Learning</title>
		<meeting>the ICML-2000 Workshop on Attribute-Value and Relational Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Intelligent Information Technology</title>
		<meeting>the International Conference on Intelligent Information Technology<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="455" to="459" />
		</imprint>
	</monogr>
	<note>Neural Networks for Multi-Instance Learning</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting Multiple Instance Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02501</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multiple Instance Learning: A Survey of Problem Characteristics and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gagnon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.10.009</idno>
		<idno type="arXiv">arXiv:1612.03365</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="353" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust multiple-instance learning ensembles using random subspace instance selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gagnon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.03.035</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="83" to="99" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">In search of art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at the European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<title level="m">Visual Recognition in Art using Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object recognition in art drawings: Transfer of a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Monson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Honig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2016.7472087</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Shanghai</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2299" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Strezoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worning</surname></persName>
		</author>
		<idno type="DOI">10.1145/3273022</idno>
		<title level="m">OmniArt: A Large-scale Artistic Benchmark, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) -Special Section on Deep Learning for Intelligent Multimedia Analytics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recognizing Art Style Automatically in painting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lecoutre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Negrevergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">DeepArt : Learning Joint Representations of Visual Arts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Shape of Art History in the Eyes of the Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07729</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep Transfer Learning for Art Classification Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabatelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computer Vision for Art Analysis ECCV</title>
		<meeting><address><addrLine>Munich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Domain Transfer for Delving into Deep Networks Capacity to De-Abstract Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Badea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Vertan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59126-1</idno>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10269</biblScope>
			<biblScope unit="page" from="337" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning scale-variant and scale-invariant features for deep image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Postma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.06.005</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="583" to="592" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visual Link Retrieval in a Database of Paintings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Striolo</forename><surname>Carlotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabella</forename><surname>Dilenardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaplan</forename><surname>Frederic</surname></persName>
		</author>
		<idno>doi:978-3-319-46604-0_52</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Linking Art through Human Poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jenicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03537</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1338" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bongini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Becattini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<idno type="DOI">10.1088/1757-899X/949/1/012074</idno>
		<idno type="arXiv">arXiv:2003.09853</idno>
		<title level="m">IOP Conf. Series: Materials Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">949</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Visual Question Answering for Cultural Heritage</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Discovering Visual Patterns in Art Collections with Spatially-consistent Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02678</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Webly-supervised Zero-shot Learning for Artwork Instance Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chiaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2019.09.027</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="420" to="426" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Context-Aware Embeddings for Automatic Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Renoust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<idno type="DOI">10.1145/3323873.3325028</idno>
		<idno type="arXiv">33arXiv:1904.04985</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 on International Conference on Multimedia Retrieval</title>
		<meeting>the 2019 on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multitask Painting Categorization by Deep Multibranch Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.05.036</idno>
		<idno type="arXiv">arXiv:1812.08052</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="90" to="101" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiorucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoroshiltseva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Traviglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2020.02.017</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Cultural Heritage: A Survey</title>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Strong-Weak Distribution Alignment for Adaptive Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04798</idno>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Deeper, Broader and Artier Domain Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03077</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Artistic Object Recognition by Unsupervised Style Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11139</idno>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="460" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Ommer, Finding Visual Patterns in Artworks: An Interactive Search Engine to Detect Objects in Artistic Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>DH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The Perceptron: A Probabilistic Model for Information Storage and Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="386" to="408" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On the complexity of polyhedral separability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02187916</idno>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="337" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">PCL: Proposal Cluster Learning for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03342</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Do Better ImageNet Models Transfer Better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">2671arXiv:1805.08974</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">2661</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The Art of Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="721" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Cross-depiction problem: Recognition and synthesis of photographs and artwork</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Corradi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41095-015-0017-1</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="103" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
