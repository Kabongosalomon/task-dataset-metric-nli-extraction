<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Sample Selection for Robust Learning under Label Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deep</forename><surname>Patel</surname></persName>
							<email>deeppatel@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<region>Karnataka</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
							<email>sastry@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<region>Karnataka</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Sample Selection for Robust Learning under Label Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Neural Networks (DNNs) have been shown to be susceptible to memorization or overfitting in the presence of noisily-labelled data. For the problem of robust learning under such noisy data, several algorithms have been proposed. A prominent class of algorithms rely on sample selection strategies, motivated by curriculum learning. For example, many algorithms use the 'small loss trick' wherein a fraction of samples with loss values below a certain threshold are selected for training. These algorithms are sensitive to such thresholds, and it is difficult to fix or learn these thresholds. Often, these algorithms also require information such as label noise rates which are typically unavailable in practice. In this paper, we propose a data-dependent, adaptive sample selection strategy that relies only on batch statistics of a given mini-batch to provide robustness against label noise. The algorithm does not have any additional hyperparameters for sample selection, does not need any information on noise rates and does not need access to separate data with clean labels. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Label noise is inevitable when employing supervised learning based algorithms in practice. The deep learning models, which are highly effective in a variety of applications, need vast amounts of training data. Such large-scale labelled data is often generated through crowd-sourcing or automated labeling, which naturally cause random labelling errors. In addition, subjective biases in human annotators too can cause such errors. The training of deep networks is adversely affected by label noise and hence robust learning under label noise is an important problem of current interest.</p><p>Indeed, recent years have seen a lot of interest in developing algorithms for robust learning of classifiers under label noise. Many different approaches have been proposed, such as, robust loss functions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref>, loss correction <ref type="bibr" target="#b27">[28]</ref>, meta-learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>, sample reweighting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, etc. In this paper we present a novel algorithm that adaptively selects samples based on the statistics of observed loss values in a minibatch and achieves good robustness to label noise. Our algorithm does not use any additional system for learning weights for examples, does not need extra data with clean labels and does not assume any knowledge of noise rates. The algorithm is motivated by curriculum learning and can be thought of as a way to design an adaptive curriculum.</p><p>The curriculum learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> is a general strategy of sequencing of examples so that the networks learn the 'easy' examples well before learning the 'hard' ones. This is often brought about by giving different weights to different examples in the training set. In the context of label noise, one can think of clean examples as the easy ones and the examples with wrong labels as the hard ones. Many of the recent algorithms for robust learning based on sample reweighting can be seen as motivated by a similar idea. In all such approaches, the weight assigned to an example is essentially determined by the loss function value on that example with a heuristic that, low loss values indicate reliable labels. Many different ways of fixing/learning such weights have been proposed (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>). A good justification for this approach of assigning weights to examples for achieving robustness comes from some recent studies on the effects of noisily-labelled data on learning deep neural networks. It is empirically shown in <ref type="bibr" target="#b42">[43]</ref> that deep neural networks can learn to achieve zero training error on completely randomly labelled data, a phenomenon termed as 'memorization'. However, further studies such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> have shown that the networks, when trained on randomly-labelled data, learn simpler patterns (corresponding to cleanly-labelled data) first before overfitting to the noisily-labelled data.</p><p>Motivated by this, several strategies of 'curriculum learning' have been devised that aim to select (or give more weightage to) 'clean' samples for obtaining some degree of robustness against label noise <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. All such methods essentially employ the heuristic of 'small loss' for sample selection or weighting wherein (a fraction of) small-loss valued samples are preferentially used for learning the network. Algorithms such as Co-Teaching <ref type="bibr" target="#b7">[8]</ref> and Co-Teaching+ <ref type="bibr" target="#b41">[42]</ref> use two networks and select samples with loss value below a threshold in one network to train the other. In Co-Teaching, the threshold is chosen based on the knowledge of noise rates and is fixed throughout the training. The same threshold is used in Co-Teaching+ but the sample selection is based on disagreement between the two networks. The MentorNet <ref type="bibr" target="#b12">[13]</ref>, another recent algorithm based on curriculum learning, uses an auxiliary neural network trained to serve as a sample selection function. There are other adaptive sample reweighting schemes based on meta-learning such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> which learn sample weights for the examples.</p><p>Algorithms in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> use a separate network to learn a mapping from loss values to weights of examples. Such methods need additional computing resources as well as access to extra data without label noise and may need careful choice of hyperparameters. In addition, all such methods, in effect, assume that one can assess whether or not an example has clean label based on some function of the loss value of that example. However, loss value of any specific example is itself a function of the current state of learning and it evolves with epochs. Loss values of even clean samples may change over a significant range during the course of learning. Further, the loss values achievable by a network even on clean samples may be different for examples of different classes. Other methods based on curriculum learning such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref> use a threshold to pick up the examples with small loss values in each mini-batch. This threshold is fixed using the noise rate which is not known and is often difficult to estimate reliably. (As we show in this paper, the method is somewhat sensitive to errors in estimated noise rate). While they adapt this threshold with epochs in a fixed manner, it is not really dependent on the current state of learning.</p><p>Motivated by these considerations, we propose a simple, adaptive curriculum based selection strategy called BAtch REweighting (BARE). The idea is to focus on the current state of learning, in a given mini-batch, for identifying the noisily labelled data in it. The statistics of loss values of all examples in a mini-batch would give useful information on current state of learning. Our algorithm utilizes these batch statistics to compute the threshold for sample selection in a given mini-batch. This will give us what is essentially a dynamic or adaptive curriculum where the selection of examples is naturally tied to state of learning. For example, it is possible that the automatically calculated threshold is different for different mini-batches even within the same epoch. Thus, our method uses a dynamic threshold which naturally evolves as learning proceeds. In addition, while calculating the batch statistics we take into consideration the class labels also and hence the dynamic thresholds are also dependent on the given labels of the examples.</p><p>The main contribution of this paper is an adaptive sample selection strategy for robust learning that is simple to implement, does not need any clean validation data, needs no knowledge at all of the noise rates and also does not have any additional hyperparameters. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets: MNIST <ref type="bibr" target="#b17">[18]</ref>, CIFAR-10 <ref type="bibr" target="#b14">[15]</ref>, and Clothing-1M <ref type="bibr" target="#b38">[39]</ref> and show that our algorithm is much more efficient in terms of time and has as good or better robustness compared to other algorithms for different types of label noise and noise rates.</p><p>The rest of the paper is organized as follows: Section 2 discusses related work, Section 3 discusses our proposed algorithm. Section 4 discusses our empirical results and concluding remarks are provided in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Curriculum learning (CL) as proposed in <ref type="bibr" target="#b3">[4]</ref> is the designing of an optimal sequence of training samples to improve the model's performance. The order of samples in this sequence is to be decided based on a notion of easiness which can be fixed based on some prior knowledge. A curriculum called Self-Paced Learning (SPL) is proposed in <ref type="bibr" target="#b16">[17]</ref> wherein easiness is decided upon based on how small the loss values are. A framework to unify CL and SPL is proposed in <ref type="bibr" target="#b11">[12]</ref> by incorporating the prior knowledge about curriculum and feedback from the model during training with the help of self-paced functions that are to be used as regularizers. SPL with diversity <ref type="bibr" target="#b10">[11]</ref> improved upon SPL by proposing a sample selection scheme by encouraging selection of a diverse set of easy examples for learning with the help of a group sparsity regularizer. This is further improved in <ref type="bibr" target="#b46">[47]</ref> by encouraging more exploration during early phases of learning.</p><p>Motivated by similar ideas, many sample reweighting algorithms are proposed for tackling label noise in neural networks. Building on the empirical observations that the neural networks learn from clean data first before overfitting to the noisy data, these methods attempt to reduce this gradual overfitting by identifying the clean (easy) samples from the noisy (hard) samples with the help of various heuristics. So, sample selection / reweighting algorithms for robust deep learning can be viewed as designing a fixed or adaptive curriculum. A sample selection algorithm based on the 'small loss' heuristic wherein the algorithm will select a fraction of small loss valued samples for training is proposed in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>. Two networks are cross-trained with samples selected by each other based on this criterion. <ref type="bibr" target="#b20">[21]</ref> also relies on 'small loss' heuristic but the threshold for sample selection is adapted based on the knowledge of label noise rates. When a (small amount of) separate data with clean labels is available, <ref type="bibr" target="#b12">[13]</ref> proposes a data-dependent, adaptive curriculum learning method wherein an auxiliary network trained on this clean data set is used to select reliable examples from the noisy training data for training the classifier. When such clean data is not available, it reduces to a non-adaptive, self-paced learning scheme. Another sample selection algorithm is proposed in <ref type="bibr" target="#b24">[25]</ref> where the idea is to train two networks and update the network parameters only in case of a disagreement between the two networks. These sample selection functions are mostly hand-crafted and, hence, they can be sub-optimal. A general strategy is to solve a bilevel optimization problem to find the optimal sample weights. For instance, the sample selection function used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref> is sub-optimally chosen for which <ref type="bibr" target="#b39">[40]</ref> proposes an AutoML-based approach to find a better function, by fine-tuning on separate data with clean labels. Sample reweighting algorithms proposed in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref> use online meta-learning and need some extra data with clean labels. The method in <ref type="bibr" target="#b30">[31]</ref> uses the gradients of loss on noisy and clean data to learn the weights for samples while the method in <ref type="bibr" target="#b31">[32]</ref> tries to learn these weights as a function of their loss values.</p><p>Apart from the sample selection/reweighting approaches described above, there are other approaches to tackling label noise. Label cleaning algorithms <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> attempt at identifying and correcting the potentially incorrect labels through joint optimization of sample weights and network weights. Loss correction methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref> suitably modify loss function (or posterior probabilities) to correct for the effects of label noise on risk minimization; however, they need to know (or estimate) the noise rates. There are also theoretical results that investigate robustness (to label noise) of risk minimization under some special loss functions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref>. Regularization methods, of which sample reweighting approaches are a part, employ explicit or implicit regularization to reduce overfitting to noisy data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>. In this paper our interest is in the approach of sample selection for achieving robustness to label noise.</p><p>The BARE algorithm proposed here is a simple, adaptive curriculum to select samples which relies only on statistics of loss values (or, equivalently, statistics of class posterior probabilities because we use CCE loss) in a given mini-batch. We do not need any extra data with clean labels or any knowledge about label noise rates. Since it uses batch statistics, the selection thresholds are naturally tied to the evolving state of learning of the network without needing any tunable hyperparameters. Unlike in many of the aforementioned algorithms, we do not need any auxiliary networks for learning sample selection function, online reweighting or cross-training, or noise rate estimation and, thus, our algorithm is computationally more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Batch Reweighting Algorithm</head><p>In this section we describe the proposed sample reweighting algorithm that relies on mini-batch statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formualtion and Notation</head><p>Under label noise, the labels provided in the training set may be 'wrong' and we want a classifier whose test error with respect to 'correct' labels is good. We begin by making this more precise and introducing our notation.</p><p>Consider a K-class problem with X as the feature/pattern space and Y = {0, 1} K as the label space. We assume all labels are one-hot vectors and denote by e k the one-hot vector corresponding to class k. Let S c = {(x i , y c i ), i = 1, 2, ? ? ? , m} be iid samples drawn according to a distribution D on X ? Y. Let us assume we are interested in learning a classifier that does well on a test set drawn according to D. We can do so if we are given S c as training set. However, we do not have access to this training set and what we have is a training set S = {(x i , y i ), i = 1, 2, ? ? ? , m} drawn according to a distribution D ? . The y i here are the 'corrupted' labels and they are related to y c i , the 'correct' labels through P [y i = e k | y c i = e k ] = ? kk The ? kk are called noise rates. (In general the above probability can depend on the feature vector, x i , too though we do not consider that possibility in this paper). ? kk gives the probability of label k getting changed into label k . We call this general model as class conditional noise because here the probability of label corruption depends on the original label. A special case of this is the so called symmetric noise where we assume ? kk = (1 ? ?) and ? kk = ? K?1 , ?k = k. Here, ? represents the probability of a 'wrong' label. Symmetric noise corresponds to the case where the corrupted label is equally likely to be any other label.</p><p>We can represent ? kk as a matrix and we assume it is diagonal dominant (that is, ? kk &gt; ? kk , ?k = k). Note that this is true for symmetric noise if ? &lt; K?1 K . Under this condition, if we take all patterns labelled by a specific class in the label-corrupted training set, then patterns that truly belong to that specific class are still in majority in that set. (Note that, for a 10-class problem with symmetric noise, this condition is satisfied if ? &lt; 0.9). Now the problem of robust learning under label noise can be stated as follows: We want to learn a classifier for the distribution D but given training data drawn from D ? .</p><p>We denote by f (?; ?) a classifier function parameterized by ?. We assume that the neural network classifiers that we use have softmax output layer. Hence, while the training set labels, y i , are one-hot vectors, we will have f (x; ?) ? ? K?1 , where ? K?1 ? [0, 1] K is the probability simplex. We denote by L(f (x; ?), y) the loss function used for the classifier training which in our case is the CCE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Curriculum through Batch Statistics</head><p>General curriculum learning can be viewed as minimization of a weighted loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>:</p><formula xml:id="formula_0">min ?,w?[0,1] m L wtd (?, w) = m i=1 w i L(f (x i ; ?), y i ) +G(w) + ?||?|| 2</formula><p>where G(w) represents the curriculum. Since one normally employs SGD for learning, we will take m here to be the size of a mini-batch. One simple choice for the curriculum is <ref type="bibr" target="#b16">[17]</ref> G(w) = ??||w|| 1 , ? &gt; 0. Putting this in the above, omitting the regularization term and taking l i = L(f (x i ; ?), y i ), the optimization problem becomes</p><formula xml:id="formula_1">min ?,w?[0,1] m L wtd (?, w) = m i=1 (w i l i ? ?w i ) = m i=1 (w i l i + (1 ? w i )?) ? m?</formula><p>Under the usual assumption that loss function is non-negative, for the above problem, the optimal w for any fixed ? is: w i = 1 if l i &lt; ? and w i = 0 otherwise. If we want an adaptive curriculum, we want ? to be dynamically adjusted based on the current state of learning. First, let us consider the case where we make ? depend on the class label. The optimization problem becomes</p><formula xml:id="formula_2">min ?,w?[0,1] m L wtd (?, w) = m i=1 (w i l i ? ?(y i )w i ) = K j=1 i:yi=ej (w i l i ? ? j w i ) = K j=1 i:yi=ej (w i l i + (1 ? w i )? j ) ? K j=1 i:yi=ej ? j where ? j = ?(e j ).</formula><p>As is easy to see, the optimal w i (for any fixed ?) are still given by the same relation: for an i with y i = e j , w i = 1 when l i &lt; ? j . Note that this relation for optimal w i is true even if we make ? j a function of ? and of all x i with y i = e j . Thus we can have a truly dynamically adaptive curriculum by making these ? j depend on all x i of that class in the mini-batch and the current ?.</p><p>The above is an interesting theoretical insight: in the Self-Paced Learning formulation <ref type="bibr" target="#b16">[17]</ref>, the nature of the final solution is same even if we make the ? parameter a function of the class-labels and also other feature vectors corresponding to that class. This gives rise to class-label-dependent thresholds on loss values. To the best of our knowledge, this direction of curriculum learning has not been explored. It would be interesting to see how we can have curriculum designs that can exploit this. Here, we are going to heuristically choose a function of class labels and feature vectors for ?.</p><p>So now the question is how we should choose these ? j . As we mentioned earlier, we want these to be determined by the statistics of loss values in the mini-batch.</p><p>Consider those i for which y i = e j . We would be setting w i = 1 and hence use this example to update ? in this minibatch if this l i &lt; ? j . We want ? i to be fixed based on the observed loss values of this mini-batch. Since there is sufficient empirical evidence that we tend to learn the clean samples before overfitting to the noisy ones, some quantile or similar statistic of the set of observed loss values in the mini-batch (among patterns labelled with a specific class) would be a good choice for ? j .</p><p>Since we are using CCE loss, we have l i = ? ln (f j (x i ; ?)) and as the network has softmax output layer, f j (x i ; ?) is the posterior probability of class-j under current ? for x i . Since the loss and this posterior probability are inversely related, our criterion for selection of an example could be that the assigned posterior probability is above a threshold which is some statistic of the observed posterior probabilities in the mini-batch. In this paper we take the statistic to be mean plus one standard deviation.</p><p>We can sum up the above discussion of our method of adaptive curriculum based on mini-batch statistics as follows. In any mini-batch we set the weights for samples as</p><formula xml:id="formula_3">w i = 1 if f yi (x i ; ?) ? ? yi = ? yi + ? yi 0 else (1) where ? yi = 1 |Sy i | s?Sy i f yi (x s ; ?) and ? 2 yi = 1 |Sy i | s?Sy i (f yi (x s ; ?)?? yi ) 2</formula><p>indicate the sample mean and sample variance of the class posterior probabilities for samples having class label y i . [Note:</p><formula xml:id="formula_4">S yi = {k ? [m] | y k = y i } where m is the size of mini-batch]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Implementation</head><p>Keeping in mind that neural networks are trained in a mini-batch manner, Algorithm 1 consists of three parts: i.) computing sample selection thresholds, ? yx , for a given mini-batch of data (Step 9-14), ii.) sample selection based on these thresholds (Steps 16-20) as per Equation 1, and iii.) network parameter updation using these selected samples (Step 21).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Noisy Dataset</head><p>Dataset: We demonstrate the effectiveness of the proposed algorithm on two benchmark image datasets: MNIST and CIFAR10. These data sets are used to benchmark almost all algorithms for for p = 1 to K do </p><formula xml:id="formula_5">? t+1 = ? t ? ?? 1 |R| (x,yx)?R L(x, y x ; ? t )</formula><p>// parameter updates <ref type="bibr">21:</ref> end for 22: end for 23: Output: ? t robust learning under label noise and we briefly describe the data sets. (The details of data sets are given in <ref type="table" target="#tab_0">Table 1</ref>). MNIST contains 60,000 training images and 10,000 test images (of size 28 ? 28) with 10 classes. CIFAR-10 contains 50,000 training images and 10,000 test images (of size 32 ? 32) with 10 classes.</p><p>We test the algorithms on two types of label noise: symmetric and class-conditional label noise. In symmetric label noise, each label is randomly flipped to any of the remaining classes with equal probability, whereas for class-conditional noise, label flipping is done in a set of similar classes. For the simulations here, for MNIST, the following flipping is done: 1 ? 7, 2 ? 7, 3 ? 8, and 5 ? 6. Similarly, for CIFAR10, the following flipping is done: TRUCK ? AUTOMOBILE, BIRD ? AIRPLANE, DEER ? HORSE, CAT ? DOG. We use this type of noise because that is arguably a more realistic scenario and also because it is the type of noise, in addition to symmetric noise, that other algorithms for learning under label noise have used. Apart from this, we also provide results with an arbitrary nose rate matrix. For all the datasets, 80% of the training set is used for training and, from the remaining 20% data, we sample 1000 images that constitute the validation set.</p><p>We also experiment with the Clothing-1M dataset <ref type="bibr" target="#b38">[39]</ref> which is a large-scale dataset obtained by scrapping off the web for different images related to clothing. It contains noise that can be characterized as somewhat close to feature-dependent noise, the most generic kind of label noise. An estimated 40% images have noisy labels. The training dataset contains 1 million images and the number of classes are 14. There are additional training, validation, and test sets of 50k, 14k, and 10k images respectively with clean labels. Since there's a class imbalance, following similar procedure as in existing baselines, we use 260k images from the original noisy training set for training while ensuring equal number of images per class in the set and test set of 10k images for performance evaluation. Data Augmentations: For MNIST we use no data augmentation. For CIFAR-10 we do a random cropping with padding of 4, and random horizontal flips. For Clothing-1M we do random cropping while ensuring image size is fixed.</p><p>Baselines:We compare the proposed algorithm with the following algorithms from literature:</p><p>(1.) Co-Teaching (CoT) <ref type="bibr" target="#b7">[8]</ref> which involves cross-training of two similar networks by selecting a fraction (dependent on noise rates) of low loss valued samples; (2.) Co-Teaching+ (CoT+) <ref type="bibr" target="#b41">[42]</ref> which improves upon CoT with the difference being sample selection only from the subset upon which the two networks' predictions disagree; (3.) Meta-Ren (MR) <ref type="bibr" target="#b30">[31]</ref>, which involves meta-learning of sample weights on-the-fly by comparing gradients for clean and noisy data; (4.) Meta-Net (MN) <ref type="bibr" target="#b31">[32]</ref>, which improves upon MR by explicitly learning sample weights via a separate neural network; (5.) Curriculum Loss (CL) <ref type="bibr" target="#b20">[21]</ref>, which involves a curriculum for sample selection based on (estimated) noise rates; (6.) Standard (CCE), which is the usual training through empirical risk minimization with cross-entropy loss (using the data with noisy labels).</p><p>Among these baselines, CoT, CoT+, and CL are sample selection algorithms that require knowledge of noise rates. The algorithms CoT+ and CL need a few initial iterations without any sample selection as a warm-up period; we used 5 epochs and 10 epochs as warm up period during training for MNIST and CIFAR-10 respectively. MR and MN assume access to a small set of clean validation data. Because of this, and for a fair comparison among all the baselines, a clean validation set of 1000 samples is used in case of MR and MN, and the same set of samples but with the noisy labels is used for the rest of the algorithms including the proposed one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network architectures &amp; Optimizers:</head><p>While most algorithms for learning under label noise use MNIST and CIFAR10 data, different algorithms use different network architectures. Here we have decided to use small networks that give state of art performance on clean data and investigate the robustness we get by using our algorithm. We use one MLP and one CNN architecture. For MNIST we train a 1-hidden layer fully-connected network with Adam (learning rate = 2 ? 10 ?4 and a learning rate scheduler: ReduceLROnPlateau). (This is same as the network used for Co-Teaching and Co-Teaching+ <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>). For CIFAR-10 we train a 4-layer CNN with Adam <ref type="bibr" target="#b13">[14]</ref> (learning rate = 2 ? 10 ?3 and a learning rate scheduler: ReduceLROnPlateau). All networks are trained for 200 epochs. For MR, SGD optimizer with momentum 0.9 and learning rate of 1 ? 10 ?3 is used as the meta-optimizer. For MN, SGD optimizer with learning rate of 2 ? 10 ?3 is used as meta-optimizer. For CL, soft hinge loss is used as suggested in <ref type="bibr" target="#b20">[21]</ref> instead of cross-entropy loss. Rest of the algorithms implemented in this chapter use cross-entropy loss. All the simulations are run for 5 trials. A pre-trained ResNet-50 is used for training on Clothing-1M with SGD (learning rate of 1 ? 10 ?3 that is halved at epochs 6 and 11) with a weight decay of 1 ? 10 ?3 and momentum 0.9 for 14 epochs. All experiments use PyTorch <ref type="bibr" target="#b26">[27]</ref>, NumPy <ref type="bibr" target="#b8">[9]</ref>, scikit-learn <ref type="bibr" target="#b28">[29]</ref>, and NVIDIA Titan X Pascal GPU with CUDA 10.0. <ref type="table" target="#tab_1">Table 2</ref> contains details about the network architecture used for training on MNIST and CIFAR-10 datasets. These settings of optimizer, learning rate, and learning rate scheduler were found to work the best for our experimental and hardware setup. All the codes for these experiments are available here: https://github.com/dbp1994/masters_thesis_codes/tree/main/BARE Performance Metrics: For all algorithms we compare test accuracies on a separate test set with clean labels. The main idea in all sample selection schemes is to identify noisy labels. Hence, in addition to test accuracies, we also compare precision (# clean labels selected / # of selected labels) and recall (# clean labels selected / # of clean labels in the data) in identifying noisy labels. <ref type="figure" target="#fig_2">Figure 1</ref> shows the evolution of test accuracy (with training epochs) under symmetric (? ? {0.5, 0.7}) and class conditional (? = 0.45) label noise for different algorithms. We can see from the figure that the proposed algorithm outperforms the baselines for symmetric noise. For the case of   <ref type="figure" target="#fig_2">Figure 1</ref> showed the evolution of test accuracies with epochs. We tabulate the final test accuracies of all algorithms in <ref type="table" target="#tab_2">Tables 3 -5</ref>. The best two results are in bold. These are accuracies achieved at the end of training. For CoT <ref type="bibr" target="#b7">[8]</ref> and CoT+ <ref type="bibr" target="#b41">[42]</ref>, we show accuracies only of that network which performs the best out of the two that are trained.   COT <ref type="bibr" target="#b7">[8]</ref> 87.17 ? 0.45 COT+ <ref type="bibr" target="#b41">[42]</ref> 87.26 ? 0.67 MR <ref type="bibr" target="#b30">[31]</ref> 85.10 ? 0.28 MN <ref type="bibr" target="#b31">[32]</ref> 65.52 ? 21.35 CL <ref type="bibr" target="#b20">[21]</ref> 88. <ref type="bibr" target="#b27">28</ref>    It is to be noted that while test accuracies for our algorithm stay saturated after attaining maximum performance, the other algorithms' performance seems to deteriorate as can be seen in the form of accuracy dips towards the end of training. This suggests that our proposed algorithm doesn't let the network overfit even after long durations of training unlike the case with other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discussion of Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Performance on MNIST</head><p>All the algorithms, except the proposed one, have hyperparameters (in the sample selection/weighting method) and the accuracies reported here are for the best possible hyperparameter values obtained through tuning. The MR and MN algorithms are particularly sensitive to hyperparameter values in the meta learning algorithm. In contrast, BARE has no hyperparameters for the sample selection and hence no such tuning is involved.</p><p>As in the case of MNIST, we tabulate the final test accuracies of all algorithms on CIFAR in <ref type="table" target="#tab_6">Tables 6  -8</ref>. The best two results are in bold.</p><p>It may be noted from the tables giving test accuracies on MNIST and CIFAR-10, that sometimes the standard deviation in the accuracy for MN is high. As we mentioned earlier, we noticed that MN is very sensitive to the tuning of hyper parameters. While we tried our best to tune all the hyper parameters, may be the final ones we found for these cases are still not the best and that is why the standard deviation is high. 62.96 ? 0.70 MN <ref type="bibr" target="#b31">[32]</ref> 51.65 ? 1.49 CL <ref type="bibr" target="#b20">[21]</ref> 66.124 ? 0.45 CCE 54.83 ? 0.28</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARE (OURS)</head><p>75.85 ? 0.41  <ref type="table">Table 8</ref>: Test Accuracy (%) for CIFAR-10 -? = 0.4 (class-conditional) ALGORITHM TEST ACCURACY COT <ref type="bibr" target="#b7">[8]</ref> 65.26 ? 0.78 COT+ <ref type="bibr" target="#b41">[42]</ref> 63.05 ? 0.39 MR <ref type="bibr" target="#b30">[31]</ref> 70.27 ? 0.77 MN <ref type="bibr" target="#b31">[32]</ref> 63.84 ? 0.41 CL <ref type="bibr" target="#b20">[21]</ref> 64.48 ? 2.02 CCE 64.06 ? 0.32</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARE (OURS)</head><p>70.63 ? 0.46  <ref type="table" target="#tab_8">Table 9</ref> shows the typical run times for 200 epochs of training with all the algorithms. It can be seen from the table that the proposed algorithm takes roughly the same time as the usual training with CCE loss whereas all other baselines are significantly more expensive computationally. In case of MR and MN, the run times are around 8 times that of the proposed algorithm for CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Performance on Clothing1M</head><p>The results are summarized in <ref type="table" target="#tab_0">Table 10</ref>. We report the baseline accuracies as observed in the corresponding papers. And, for this reason, the baselines with which we compare our results are different. These are the baselines that have reported results on this dataset. It shows that that even for datasets used in practice which have label noise that isn't synthetic unlike the symmetric and class-conditional label noise used for aforementioned simulations, the proposed algorithm performs better than all but one baselines. However, it is to be noted that DivideMix requires about 2.4 times the computation time required for BARE. In addition to this, DivideMix requires tuning of 5 hyperparameters whereas no such tuning is required for BARE.  <ref type="bibr" target="#b22">[23]</ref> 69.47 GCE <ref type="bibr" target="#b44">[45]</ref> 69.75 FORWARD <ref type="bibr" target="#b27">[28]</ref> 69.84 COT [8] <ref type="bibr" target="#b0">1</ref> 70.15 JOCOR <ref type="bibr" target="#b36">[37]</ref> 70.30 SEAL <ref type="bibr" target="#b5">[6]</ref> 70.63 DY <ref type="bibr" target="#b1">[2]</ref> 71.00 SCE <ref type="bibr" target="#b34">[35]</ref> 71.02 LRT <ref type="bibr" target="#b45">[46]</ref> 71.74 PTD-R-V <ref type="bibr" target="#b37">[38]</ref> 71.67 JOINT OPT. <ref type="bibr" target="#b33">[34]</ref> 72.23 BARE (OURS) 72.28 DIVIDEMIX <ref type="bibr" target="#b18">[19]</ref> 74.76  <ref type="figure" target="#fig_5">Figure 3</ref> and <ref type="figure" target="#fig_6">Figure 4</ref> show the label precision (across epochs) of the various algorithms on MNIST and CIFAR-10 respectively. One can see from these figures that BARE has comparable or better precision. Thus, compared to other sample selection algorithms, a somewhat higher fraction of samples selected for training by BARE have clean labels. <ref type="figure">Figure 5</ref> show the label recall values for CoT, CoT+, CL, and BARE for MNIST (5(a)) and CIFAR-10 (5(b) &amp; 5(c) ). It can be noted that BARE consistently achieves better recall values compared to the baselines. Higher recall values indicate that the algorithm is able to identify clean samples more reliably. This is useful, for example, to employ a label cleaning algorithm on the samples flagged as noisy (i.e., not selected) by BARE. CoT+ selects a fraction of samples where two networks disagree and, hence, after the first few epochs, it selects very few samples (? 3000) in each epoch. Since these are samples in which the networks disagree, a good fraction of them may have noisy labels. This may be the reason for the poor precision It can be noted that, as noise rate is to be supplied to CoT and CL, they select 1 ? ? = 0.6 fraction of data with every epoch. Whereas, in case of CoT+, the samples where the networks disagree is small because of the training dynamics and as a result, after a few epochs, it consistently selects very few samples. Since the noise is class-conditional, even though ? = 0.4, the actual amount of label flipping is ? 20%. And this is why it's interesting to note that BARE leads to an approximate sample selection ratio of 80%. As is evident from these figures, BARE is able to identify higher fraction of clean samples effectively even without the knowledge of noise rates. While test accuracies and label precision values do demonstrate the effectiveness of algorithms for this study, it's also instructive and essential to look at the label recall values as one may want to know if and when one needs to perform the tasks of cleaning/rectifying the noisy labels apart from sample selection. Label recall also tells us how poorly a sample selection algorithm performs when it comes to selecting reliable, clean samples. <ref type="figure">Figure 5</ref> show the label recall values for CoT, CoT+, CL, and BARE for MNIST and CIFAR10. It can be noted that the proposed algorithm consistently performs better by exhibiting high recall values compared to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Sensitivity to noise rates</head><p>Some of the baselines schemes such as CoT, CoT+, and CL require knowledge of true noise rates beforehand. This information is typically unavailable in practice. One can estimate the noise rates but there would be inevitable errors in estimation. <ref type="figure">Figure 7</ref> shows the effect of mis-specification of noise rates for these 3 baselines schemes. As can be seen from these figures, while the algorithms can exhibit robust learning when the true noise rate is known, the performance deteriorates if the estimated noise rate is erroneous. Obviously, BARE does not have this issue because it does not need any information on noise rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.7">Results on Arbitrary Noise Matrix</head><p>Earlier, we showed results for special cases of class-conditional noise. There the noise rate is still specifiable by a single number and there is a pre-fixed pairs of classes that can be confused with each other. As explained earlier, we have used this type of noise because that was what was used in literature.</p><p>We now provide results for class-conditional noise with an arbitrary, diagonally-dominant noise matrix. We show the noise matrices above. These noise matrices are chosen arbitrarily. However, as can be seen, now in each row more than two entries can be non-zero.</p><p>The results obtained with different algorithms are shown in <ref type="table" target="#tab_0">Tables 11-14</ref>. As mentioned earlier, algorithms CoT, CoT+ and CL need knowledge of noise rate. With an arbitrary noise matrix, it is not clear what number can be supplied as the noise rate for these algorithms, even if we know all the noise rates. For these simulations, ? = 0.45 and ? = 0.4 are supplied as the estimated noise rates to CoT, CoT+, and CL baselines for MNIST and CIFAR-10 respectively. In the tables, the best two results are in bold. It can be seen that the proposed algorithm continues to perform well.     <ref type="bibr" target="#b41">[42]</ref> 68.56 CL <ref type="bibr" target="#b20">[21]</ref> 72.12 BARE (OURS) 76.22</p><formula xml:id="formula_6">? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose an adaptive, data-dependent sample selection scheme, BARE, for robust learning in the presence of label noise. The algorithm relies on statistics of assigned posterior probabilities of all COT <ref type="bibr" target="#b7">[8]</ref> 71.86 COT+ <ref type="bibr" target="#b41">[42]</ref> 68.99 CL <ref type="bibr" target="#b20">[21]</ref> 72.27</p><formula xml:id="formula_7">BARE (OURS) 75.96</formula><p>samples in a mini-batch to select samples from that minibatch. The mini-batch statistics are used as proxies for determining current state of learning here. Unlike other algorithms in literature, BARE neither needs an extra data set with clean labels nor does it need any knowledge of the noise rates. Further it has no hyperparameters in the selection algorithm. Comparisons with baseline schemes on benchmark datasets show the effectiveness of the proposed algorithm both in terms of performance metrics and computational complexity.</p><p>The current algorithms for sample selection in literature rely on heuristics such as cross-training multiple networks or meta-learning of sample weights which is often computationally expensive. They also need knowledge of noise rates or some data with clean labels which may not be easily available. From the results shown here, it seems that relying on the current state of learning via batch statistics alone is helping the proposed algorithm confidently pick out the clean data and ignore the noisy data (without even the need for cross training of two networks). This, combined with the fact that there are no hyperparameters to tune, shows the advantage that BARE can offer for robust learning under label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of BARE on low noise rates</head><p>As we can see in <ref type="table" target="#tab_0">Table 15</ref>, the performance of BARE is good even when the noise rates are very low.</p><p>That's why we don't show these numbers in the paper when noise rate is low. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architectures and Performance</head><p>We have either used the same or very similar architectures as the ones used in Co-Teaching and Co-Teaching+ papers for a fairer comparison.</p><p>Since the proposed method is a sample selection algorithm, we provided the precision and recall curves which very clearly show the advantages and effectiveness of BARE in picking training examples with clean labels. Since BARE is a sample selection algorithm, we compare against state-of-art sample reweighting algorithms. Our algorithm offers advantages such as efficiency in computation and higher label precision and recall indicating that the method is indeed more attractive than the baselines.</p><p>Since different papers in literature use different architectures and different tuning of learning, we cannot directly compare raw accuracies. <ref type="table" target="#tab_0">Table 10</ref> shows that even for bigger architectures the advantages of the algorithm hold.</p><p>Relationship between loss values, j , and sample selection threshold ? j</p><p>We are motivated by an interesting theoretical insight: in the Self-Paced-Learning (optimization) formulation, the nature of the final solution is same even if we make the ? parameter a function of class-label and also other feature vectors of that class; this gives rise to class-label-dependent thresholds on loss values. To the best of our knowledge, this direction of curriculum learning that we pointed out has not been explored. And, in this work, we suggest a heuristic method to use this flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does the proposed algorithm have no hyperparameters?</head><p>In the proposed algorithm, there are no additional hyperparameters to tune. Even while learning with no label noise, mini-batch size, learning-step-size, regularization, etc. are all hyperprameters that need to be tuned. Our sample selection method does not have any new hyperparameters to be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity to Batch Size</head><p>To show the insensitivity to batch size, we show in <ref type="table" target="#tab_0">Table 16</ref> results on MNIST &amp; CIFAR-10 for both types of label noise and three batch sizes: 64, 128 (used in paper), &amp; 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The case of large number of classes</head><p>With large number of classes, the number of samples of a given class in a mini-batch may be small. One way of tackling it is to make mini-batches so that each mini-batch contains samples only from, say, 10 classes and different mini-batches contain different subsets of classes. Class imbalance in mini-batch does not affect learning as badly as does class imbalance in the whole training set. We have some preliminary results on this. (On CIFAR-100 the computational time is still only about 15% more than that of CCE, as is the case with the times reported in the paper). We can add a comment in the final version.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 4 : 5 : 6 :</head><label>1456</label><figDesc>BAtch REweighting (BARE) Algorithm 1: Input: noisy dataset D ? , # of classes K, # of epochs T max , learning rate ? 2: Initialize: Network parameters, ? 0 , for classifier f (?; ?) 3: for t = 0 to T max ? 1 do Shuffle the training dataset D ? for i = 1 to |D ? |/|M| do Draw a mini-batch M from D ? 7: m = |M| // mini-batch size 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>9 :? 2 p = 1 |Sp|</head><label>91</label><figDesc>S p = {k ? [m] | y k = e p } // collect indices of samples with class-p 10:? p = 1 |Sp| s?Sp f p (x s ; ? t ) //mean posterior prob. for samples with class-p 11: s?Sp (fp(xs; ?t) ? ?p) 2 // variance in posterior prob. for samples with class-p 12: ? p ? ? p + ? p // sample selection threshold for class-p as per Equation ? // selected samples in M 15:for each x ? M do16:    if f yx (x; ? t ) ? ? yx then17: R ? R ? (x, y x ) // Select sample as per Equation 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Test Accuracies -MNIST -Symmetric (a &amp; b) &amp; Class-conditional (c) Label Noise class-conditional noise, the test accuracy of the proposed algorithm is marginally less than the best of the baselines, namely CoT and MR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>shows the test accuracies of the various algorithms as the training progresses for both symmetric (? ? {0.3, 0.7}) and class-conditional (? = 0.4) label noise. We can see from the figure that the proposed algorithm outperforms the baseline schemes and its test accuracies are uniformly good for all types of label noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Test Accuracies -CIFAR10 -Symmetric (a &amp; b) &amp; Class-conditional (c) Label Noise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Label Precision -MNIST -Symmetric (a &amp; b) &amp; Class-conditional (c) Label Noise 4.1.5 Efficacy of detecting clean samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Label Precision -CIFAR10 -Symmetric (a &amp; b) &amp; Class-conditional (c) Label Noise and recall values of CoT+ as seen in these figures. This can be seen from Figure 6 as well which shows the fraction of samples chosen by the sample selection algorithms as epochs go by for MNIST &amp; CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Label Recall -Symmetric (a &amp; b) &amp; Class-conditional (c) Label Noise (a) (b) (c) (a): Sample fraction values for ? = 0.5 (symmetric noise) on MNIST, (b): Sample fraction values for ? = 0.7 (symmetric noise) on CIFAR-10, (c): sample fraction values for ? = 0.4 (class-conditional noise) on CIFAR-10 Test accuracies when estimated (symmetric) noise rate, ? = 0.5, and true noise rate, ? = 0.7, for (a): MNIST &amp; (b): CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Arbitrary Noise Matrix for CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset details</figDesc><table><row><cell></cell><cell>TRAIN SIZE</cell><cell>TEST SIZE</cell><cell># CLASS</cell><cell>SIZE</cell></row><row><cell>MNIST</cell><cell>60,000</cell><cell>10,000</cell><cell>10</cell><cell>28?28</cell></row><row><cell>CIFAR-10</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell><cell>32?32</cell></row><row><cell>CLOTHING-1M</cell><cell>10,00,000</cell><cell>10,000</cell><cell>14</cell><cell>224?224</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Network Architectures used for training on MNIST and CIFAR-10 datasets</figDesc><table><row><cell>MNIST</cell><cell>CIFAR-10</cell></row><row><cell></cell><cell>3?3 CONV., 64 RELU, STRIDE 1, PADDING 1</cell></row><row><cell></cell><cell>BATCH NORMALIZATION</cell></row><row><cell></cell><cell>2?2 MAX POOLING, STRIDE 2</cell></row><row><cell></cell><cell>3?3 CONV., 128 RELU, STRIDE 1, PADDING 1</cell></row><row><cell></cell><cell>BATCH NORMALIZATION</cell></row><row><cell>DENSE 28?28 ? 256</cell><cell>2?2 MAX POOLING, STRIDE 2</cell></row><row><cell></cell><cell>3?3 CONV., 196 RELU, STRIDE 1, PADDING 1</cell></row><row><cell></cell><cell>BATCH NORMALIZATION</cell></row><row><cell></cell><cell>3?3 CONV., 16 RELU, STRIDE 1, PADDING 1</cell></row><row><cell></cell><cell>BATCH NORMALIZATION</cell></row><row><cell></cell><cell>2?2 MAX POOLING, STRIDE 2</cell></row><row><cell>DENSE 256 ? 10</cell><cell>DENSE 256 ?10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test Accuracy (%) for MNIST -? = 0.5 (symmetric)</figDesc><table><row><cell>ALGORITHM</cell><cell>TEST ACCURACY</cell></row><row><cell>COT [8]</cell><cell>90.80 ? 0.18</cell></row><row><cell>COT+ [42]</cell><cell>93.17 ? 0.3</cell></row><row><cell>MR [31]</cell><cell>90.39 ? 0.07</cell></row><row><cell>MN [32]</cell><cell>74.94 ? 9.56</cell></row><row><cell>CL [21]</cell><cell>92.00 ? 0.26</cell></row><row><cell>CCE</cell><cell>74.30 ? 0.55</cell></row><row><cell>BARE (OURS)</cell><cell>94.38 ? 0.13</cell></row></table><note>4.1.2 Performance on CIFAR-10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test Accuracy (%) for MNIST -? = 0.7 (symmetric)</figDesc><table><row><cell>ALGORITHM</cell><cell>TEST ACCURACY</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test Accuracy (%) for MNIST -? = 0.45 (class-conditional)</figDesc><table><row><cell>ALGORITHM</cell><cell>TEST ACCURACY</cell></row><row><cell>COT [8]</cell><cell>95.20 ? 0.22</cell></row><row><cell>COT+ [42]</cell><cell>91.10 ? 1.51</cell></row><row><cell>MR [31]</cell><cell>95.40 ? 0.31</cell></row><row><cell>MN [32]</cell><cell>75.03 ? 0.59</cell></row><row><cell>CL [21]</cell><cell>81.52 ? 3.27</cell></row><row><cell>CCE</cell><cell>74.96 ? 0.21</cell></row><row><cell>BARE (OURS)</cell><cell>94.11 ? 0.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Test Accuracy (%) for CIFAR-10 -? = 0.3 (symmetric)</figDesc><table><row><cell>ALGORITHM</cell><cell>TEST ACCURACY</cell></row><row><cell>COT [8]</cell><cell>71.72 ? 0.30</cell></row><row><cell>COT+ [42]</cell><cell>60.14 ? 0.35</cell></row><row><cell>MR [31]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">Test Accuracy (%) for CIFAR-10 -? = 0.7 (symmetric)</cell></row><row><cell>ALGORITHM</cell><cell>TEST ACCURACY</cell></row><row><cell>COT [8]</cell><cell>58.95 ? 1.31</cell></row><row><cell>COT+ [42]</cell><cell>37.69 ? 0.70</cell></row><row><cell>MR [31]</cell><cell>45.14 ? 1.04</cell></row><row><cell>MN [32]</cell><cell>23.23 ? 0.65</cell></row><row><cell>CL [21]</cell><cell>44.82 ? 2.42</cell></row><row><cell>CCE</cell><cell>23.46 ? 0.37</cell></row><row><cell>BARE (OURS)</cell><cell>59.53 ? 1.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Algorithm run times for training (in seconds)</figDesc><table><row><cell cols="3">ALGORITHM MNIST CIFAR10</cell></row><row><cell>BARE</cell><cell>310.64</cell><cell>930.78</cell></row><row><cell>COT</cell><cell>504.5</cell><cell>1687.9</cell></row><row><cell>COT+</cell><cell>537.7</cell><cell>1790.57</cell></row><row><cell>MR</cell><cell>807.4</cell><cell>8130.87</cell></row><row><cell>MN</cell><cell>1138.4</cell><cell>8891.6</cell></row><row><cell>CL</cell><cell>730.15</cell><cell>1254.3</cell></row><row><cell>CCE</cell><cell>229.27</cell><cell>825.68</cell></row><row><cell>4.1.3 Efficiency of BARE</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Test accuracies on Clothing-1M dataset</figDesc><table><row><cell>ALGORITHM</cell><cell>TEST ACCURACY (%)</cell></row><row><cell>CCE</cell><cell>68.94</cell></row><row><cell>D2L</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Test Accuracy (%) for MNIST -? est = 0.45 (arbitrary noise matrix)</figDesc><table><row><cell>ALGORITHM</cell><cell>TEST ACCURACY</cell></row><row><cell>COT [8]</cell><cell>95.3</cell></row><row><cell>COT+ [42]</cell><cell>93.07</cell></row><row><cell>CL [21]</cell><cell>88.41</cell></row><row><cell>BARE (OURS)</cell><cell>95.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Avg. Test Accuracy (last 10 epochs) (%) for MNIST -? est = 0.45 (arbitrary noise matrix)</figDesc><table><row><cell>ALGORITHM</cell><cell>AVG. TEST ACCURACY (LAST 10 EPOCHS)</cell></row><row><cell>COT [8]</cell><cell>95.22</cell></row><row><cell>COT+ [42]</cell><cell>93.08</cell></row><row><cell>CL [21]</cell><cell>88.56</cell></row><row><cell>BARE (OURS)</cell><cell>95.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Test Accuracy (%) for CIFAR10 -? est = 0.4 (arbitrary noise matrix)</figDesc><table><row><cell>ALGORITHM</cell><cell>TEST ACCURACY</cell></row><row><cell>COT [8]</cell><cell>71.92</cell></row><row><cell>COT+</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Avg. Test Accuracy (last 10 epochs) (%) for CIFAR10 -? est = 0.4 (arbitrary noise matrix)ALGORITHM   AVG. TEST ACCURACY (LAST 10 EPOCHS)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>Test accuracies for BARE on MNIST &amp; CIFAR-10</figDesc><table><row><cell>DATASET</cell><cell cols="2">NOISE RATE TEST ACCURACY (%)</cell></row><row><cell>MNIST</cell><cell>0%</cell><cell>96.93 (?0.19)</cell></row><row><cell>MNIST</cell><cell>10% (SYM.)</cell><cell>96.68 (?0.15)</cell></row><row><cell>MNIST</cell><cell>20% (SYM.)</cell><cell>96.1 (?0.25)</cell></row><row><cell>CIFAR-10</cell><cell>0%</cell><cell>79.59 (?0.38)</cell></row><row><cell cols="2">CIFAR-10 10% (SYM.)</cell><cell>78.76 (?0.43%)</cell></row><row><cell cols="2">CIFAR-10 20% (SYM.)</cell><cell>77.04 (?0.61%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Test Accuracy (%) of BARE on MNIST &amp; CIFAR-10 with batch sizes ? {64, 128, 256}</figDesc><table><row><cell>DATASET</cell><cell>NOISE (?)</cell><cell cols="2">BATCH SIZE TEST ACCURACY</cell></row><row><cell>MNIST</cell><cell>50% (SYM.)</cell><cell>64</cell><cell>95.31 ? 0.16</cell></row><row><cell>MNIST</cell><cell>50% (SYM.)</cell><cell>128</cell><cell>94.38 ? 0.13</cell></row><row><cell>MNIST</cell><cell>50% (SYM.)</cell><cell>256</cell><cell>94.44 ? 0.48</cell></row><row><cell>MNIST</cell><cell>45% (CC)</cell><cell>64</cell><cell>93.31 ? 0.63</cell></row><row><cell>MNIST</cell><cell>45% (CC)</cell><cell>128</cell><cell>94.11 ? 0.77</cell></row><row><cell>MNIST</cell><cell>45% (CC)</cell><cell>256</cell><cell>94.68 ? 0.63</cell></row><row><cell cols="2">CIFAR-10 30% (SYM.)</cell><cell>64</cell><cell>76.77 ? 0.38</cell></row><row><cell cols="2">CIFAR-10 30% (SYM.)</cell><cell>128</cell><cell>75.85 ? 0.41</cell></row><row><cell cols="2">CIFAR-10 30% (SYM.)</cell><cell>256</cell><cell>74.56 ? 0.53</cell></row><row><cell>CIFAR-10</cell><cell>40% (CC)</cell><cell>64</cell><cell>71.87 ? 0.28</cell></row><row><cell>CIFAR-10</cell><cell>40% (CC)</cell><cell>128</cell><cell>70.63 ? 0.46</cell></row><row><cell>CIFAR-10</cell><cell>40% (CC)</cell><cell>256</cell><cell>69.03 ? 0.35</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">as reported in<ref type="bibr" target="#b5">[6]</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank NVIDIA for providing us with NVIDIA Titan X Pascal &amp; Maxwell GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno>PMLR, 2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno>PMLR, 2019. 11</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On symmetric losses for learning from corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nontawat</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR, 2019. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Beyond class-conditional assumption: A primary attempt to combat instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05458,2020.11</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarrod</forename><surname>Charles R Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><forename type="middle">H</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Van Kerkwijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fern?ndez Del R?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearu</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>G?rard-Marchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hameer</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliphant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep bilevel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust loss functions for learning multi-class classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 23rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Curriculum loss: Robust learning and generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="6543" to="6553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alche-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1919" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training noise-robust deep neural networks via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Part-dependent label noise: Towards instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Searching to exploit memorization effect in learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Tin-Yau</forename><surname>Kwok</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07836</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Error-bounded correction of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<idno>PMLR, 2020. 11</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="11447" to="11457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
