<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Image Inpainting by End-to-End Cascaded Refinement with Mask Awareness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Image Inpainting by End-to-End Cascaded Refinement with Mask Awareness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image Inpainting</term>
					<term>Mask Awareness</term>
					<term>Dynamic Filtering</term>
					<term>Cascaded Refinement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inpainting arbitrary missing regions is challenging because learning valid features for various masked regions is nontrivial. Though U-shaped encoder-decoder frameworks have been witnessed to be successful, most of them share a common drawback of mask unawareness in feature extraction because all convolution windows (or regions), including those with various shapes of missing pixels, are treated equally and filtered with fixed learned kernels. To this end, we propose our novel mask-aware inpainting solution. Firstly, a Mask-Aware Dynamic Filtering (MADF) module is designed to effectively learn multi-scale features for missing regions in the encoding phase. Specifically, filters for each convolution window are generated from features of the corresponding region of the mask. The second fold of mask awareness is achieved by adopting Pointwise Normalization (PN) in our decoding phase, considering that statistical natures of features at masked points differentiate from those of unmasked points. The proposed PN can tackle this issue by dynamically assigning point-wise scaling factor and bias. Lastly, our model is designed to be an end-to-end cascaded refinement one. Supervision information such as reconstruction loss, perceptual loss and total variation loss is incrementally leveraged to boost the inpainting results from coarse to fine. Effectiveness of the proposed framework is validated both quantitatively and qualitatively via extensive experiments on three public datasets including Places2, CelebA and Paris StreetView.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Inpainting arbitrary missing regions is challenging because learning valid features for various masked regions is nontrivial. Though U-shaped encoder-decoder frameworks have been witnessed to be successful, most of them share a common drawback of mask unawareness in feature extraction because all convolution windows (or regions), including those with various shapes of missing pixels, are treated equally and filtered with fixed learned kernels. To this end, we propose our novel mask-aware inpainting solution. Firstly, a Mask-Aware Dynamic Filtering (MADF) module is designed to effectively learn multi-scale features for missing regions in the encoding phase. Specifically, filters for each convolution window are generated from features of the corresponding region of the mask. The second fold of mask awareness is achieved by adopting Pointwise Normalization (PN) in our decoding phase, considering that statistical natures of features at masked points differentiate from those of unmasked points. The proposed PN can tackle this issue by dynamically assigning point-wise scaling factor and bias. Lastly, our model is designed to be an end-to-end cascaded refinement one. Supervision information such as reconstruction loss, perceptual loss and total variation loss is incrementally leveraged to boost the inpainting results from coarse to fine. Effectiveness of the proposed framework is validated both quantitatively and qualitatively via extensive experiments on three public datasets including Places2, CelebA and Paris StreetView. I MAGE inpainting has been extensively adopted in many real-world applications, such as retouching photos, restoring images and concealing errors. Specifically, the task focuses on recovering a damaged image with a given mask that indicates the missing regions. It faces great challenge because extracting valid features for the missing regions is nontrivial. Early methods such as <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref> typically search for the best matched patches based on hand-crafted features to fill in the holes. They usually can synthesize visually realistic stationary textures, but fail to generate semantically correct results.</p><p>Recently, deep encoder-decoder based methods <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> become popular for image inpainting. The input image is firstly encoded into a latent high-level feature space, and then decoded back to low-level pixels. Especially the U-shaped encoder-decoder frameworks show great ability in generating highly structured images. For instance, PEN-Net <ref type="bibr" target="#b8">[9]</ref> proposes <ref type="bibr">When</ref>   to progressively learn region affinity by attention and fill holes from low-resolution to high-resolution in a U-shaped pyramid structure. There are also two-stage based solutions proposed to synthesize the missing contents in a coarse-to-fine way and to utilize pre-defined intermediate clues to reduce the difficulty of directly generating missing contents. For example, <ref type="bibr" target="#b9">[10]</ref> synthesizes a blurry image as guidance at first for inpainting the missing regions. EdgeConnect <ref type="bibr" target="#b10">[11]</ref> comprises of an edge generator followed by an image completion network. Hallucinated edges of the missing regions are leveraged by image completion network for recovery. Analogically, SPG-Net <ref type="bibr" target="#b11">[12]</ref> factorizes the image inpainting into segmentation repair stage and segmentation guidance stage. Thus image segmentation map is used to guide the pixel generation. StructureFlow <ref type="bibr" target="#b12">[13]</ref> can be regarded as a two-stage solution which firstly reconstructs structure and then generates textures using appearance flow.</p><p>However, we find existing deep learning based solutions can be further improved from the following aspects. Firstly, conventional convolution operation in the encoding phase is mask unaware. As the red boxes in <ref type="figure" target="#fig_1">Figure 1</ref> shown, valid pixels in a convolution window are of various shapes due to arbitrary masked regions. It is necessary to adaptively extract features from these regions, however, conventional convolution treats all windows equally, i.e., each of the convolution windows is filtered by fixed kernels. This strategy can hardly handle the variety of shapes of valid pixels. Secondly, twostage models usually suffer from artifacts such as blurry boundary and distorted structures when the missing contents are large and complex, because of the lack of necessary cues to infer the missing contents. As indicated by <ref type="bibr" target="#b13">[14]</ref>, if the first The architecture of our framework. M ADF l and R l are the MADF module and recovery decoder block at l-th level respectively. F l,k represents the k-th refinement decoder block at l-th level. m l is the l-th mask feature map of the encoder. The convolution operation marked in green in the encoder takes e l?1 as input and its kernel for each convolution window is generated from corresponding region of m l?1 . "DConv" denotes the transpose convolution, "LReLU" denotes the leaky ReLU, and "up" denotes increasing channels by 1 ? 1 convolution layers. u l?1 and r l are inputs to R l to generate the feature map r l?1 at the recovery decoder and F l,k takes f l?1,k?1 and f l,k as input and generates feature map f l?1,k . Note that r L and f L,k are all equivalent to u L and f l,1 equals r l . stage fails to generate correct intermediate clues, the wrong information would be inherited by the second stage. Moreover, the manually defined characteristics make the entire inpainting procedure sensitive to the choice of the intermediate clues.</p><p>To address the aforementioned issues, we propose a novel cascaded refinement framework with mask awareness in this paper. The mask awareness is two-fold. Firstly, we propose a Mask-Aware Dynamic Filtering (MADF) module for effectively feature learning in the encoder. In more detail, filters for each convolution window are generated dynamically from features of the corresponding region of the mask. It is noteworthy that, the proposed MADF module is different from gated convolution (GConv) which is proposed in <ref type="bibr" target="#b14">[15]</ref>. GConv generalizes partial convolution by providing an attention mechanism to select features from output activations. However, the typical convolution operation is unchanged. Once the kernels are learned and fixed, all convolution windows are filtered by the shared kernels. Although GConv is equipped with attention mechanism, secondhand mask information is applied on the output activation through the attention scores, where the vanilla convolutions stand in between. Therefore, the mask information can not be fully exploited in GConv. In contrast, our MADF module dynamically produces customized kernels for each of convolution windows depending on the corresponding mask information. The mask information is utilized directly and adaptively. Hence, our framework is able to handle various shapes of missing regions.</p><p>Secondly, statistical characteristics (i.e., mean and variance) of features in hole regions could be different from those of non-hole regions at different decoding phases, applying batch normalization using mean and variance calculated from the whole feature map would still suffer from the well-known "covariant shift", due to mismatching mean and variance value for both hole and non-hole points. We consider mask awareness from normalization perspective for the first time in image inpainting task. We propose to improve batch normalization by adopting Point-wise Normalization (PN) which is achieved by adaptively assigning scaling factor and bias on per feature point basis. Our PN is largely inspired by and closely related to <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref> which use semantic clues for spatial feature modulation in image synthesis or super-resolution, aiming at avoiding semantic-layout washing away. We empirically verify similar technique can be utilized for "covariant shift removal" in inpainting task.</p><p>The other contribution of our framework is that, the proposed cascaded refinement architecture is capable of stable image inpainting by exploiting the multi-scale feature space in an end-to-end fashion. Particularly, based on the traditional U-shaped network, we extend the decoder part to recovery decoder and refinement decoders. The refinement decoders are parallel to the recovery decoder. On one hand, the recovery decoder maps the encoded features back to an coarse image. On the other hand, the refinement decoders work parallelly with the recovery decoder. A refinement decoder refines the features from its preceding refinement decoder or the recovery decoder. Furthermore, the inpainting results are progressively refined by incrementally adding supervision information of reconstruction loss, perceptual loss, total variance loss and so on to the recovery decoder and refinement decoders. Thus, our framework relies on none of explicit intermediate clues, what intermediate information is most robust and helpful is learned by end-to-end training. Besides, our model leverages the learned intermediate signals from multi-scale feature spaces as well as the RGB space.</p><p>Experiments on several publicly available datasets including Places2 <ref type="bibr" target="#b17">[18]</ref>, CelebA <ref type="bibr" target="#b18">[19]</ref> and Paris StreetView <ref type="bibr" target="#b19">[20]</ref> demonstrate that the proposed framework can generate semantically plausible and richly detailed contents in various scenes even when the missing regions are large and complex. Exemplar results are shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Comparing with the existing state-of-the-art methods, our approach significantly outperforms them both quantitatively and qualitatively.</p><p>In a nutshell, we summarize our technical contributions in the following:</p><p>? We firstly propose the "different kernels for different convolution windows" mechanism for image inpainting via a novel Mask Aware Dynamic Filtering (MADF) module in the CNN encoder; ? Point-wise Normalization (PN) is designed in the CNN decoding phase to avoid "covariant shift" issue at hole and non-hole regions introduced by batch normalization; ? Our proposed coarse-to-fine cascaded refinement architecture is proven to effectively boost the inpainting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Traditional patch-based image inpainting approaches such as <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref> typically propagated appearance information from the remained regions or other source images into the missing regions through various manually defined similarity metrics between patches. Due to the expansive computational costs of patch matching as well as the limitation of their capability, these approaches were not practical in reality. PatchMatch <ref type="bibr" target="#b1">[2]</ref>, a fast similar patch searching method had been proposed to accelerate the inpainting process. However, with only low level information, PatchMatch still can not generate semantically correct results especially for complex textures. Partial differential equation (PDE) based methods <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> are designed to connect edges or to extend lines also work well in reconstructing lines, curves and small holes, but they suffer from blurring artifacts when completing relatively large regions. There are also geometry based inpainting solutions <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref> proposed to leverage geometry guidance for image completion based on exemplar. In the literature, statistics based solutions are also extensively studied <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>. More detailed review of conventional algorithms can be find from <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Deep learning based method was firstly introduced to image inpainting by Context Encoders <ref type="bibr" target="#b5">[6]</ref>, which employed an encoder-decoder based structure and trained with adversarial loss <ref type="bibr" target="#b32">[33]</ref>. Built upon context encoders, Global&amp;Local <ref type="bibr" target="#b6">[7]</ref> employed global and local discriminators to get better consistency around the boundary of missing regions. <ref type="bibr" target="#b33">[34]</ref> introduced an U-Net structure <ref type="bibr" target="#b34">[35]</ref> combined with special shift-connection layers for filling in holes of arbitrary shape. PEN-Net <ref type="bibr" target="#b8">[9]</ref> also leveraged U-Net with attention to progressively reconstruct missing regions. SSDCGN <ref type="bibr" target="#b35">[36]</ref> proposed single-shot dense connected generative network to address the blurry boundary and distorted structure issue of inpainting output to improving performance. Some other methods divided image inpainting into two stages, and utilized explicitly defined intermediate clues such as edges/textures <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, segmentation maps <ref type="bibr" target="#b11">[12]</ref>, image structure <ref type="bibr" target="#b12">[13]</ref> and blurry images <ref type="bibr" target="#b9">[10]</ref> as the guidance to synthesize the missing contents. <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref> disentangled the image inpainting problem into the cucontour completion module and image completion module. There are also frameworks that focus on avoiding the complexity of inpainting model by parallel decoding network <ref type="bibr" target="#b39">[40]</ref> or concentrate on generating multiple and diverse plausible solutions for image completion <ref type="bibr" target="#b40">[41]</ref>. Our model abandons two-stage design to avoid error propagation from the first stage to the final stage and an end-to-end cascaded refinement structure is used to boost the inpainting result. There are also other frameworks proposed for many other low-level tasks, such as image rain removal <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref> and super-resolution <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>. Most of them use vanilla convolution and are not easy to be effectively applied to inpainting. Spatial feature transformation <ref type="bibr" target="#b16">[17]</ref> was proposed for image super-resolution and it worked quite similar to the gated convolution proposed for image inpainting <ref type="bibr" target="#b14">[15]</ref>.</p><p>Given convolutional responses are based on both valid pixels and invalid values in the missing holes, these methods would suffer from artifacts such as distortion and blurriness. PConv <ref type="bibr" target="#b47">[48]</ref> proposed partial convolution where the convolution is masked and re-normalized to be based on only valid pixels. <ref type="bibr" target="#b48">[49]</ref> improved the partial convolutional network by a learnable attention map for feature re-normalization and mask updating. <ref type="bibr" target="#b14">[15]</ref> proposed a gated convolution (GConv) which generalized partial convolution by providing a learnable dynamic feature selection mechanism for feature maps at each level. However, in these works the vanilla convolution operation is still unchanged. Once the kernels are learned and fixed, all local convolution windows are treated equally. Different from GConv, where the conventional convolution is improved by re-weighting the output activation, our model applies dynamic convolution filters according to the shape of valid pixels in each local convolution window. Specifically, the proposed MADF module adaptively generates one particular set of kernels for each of the convolution windows and the mask information is exploited during the kernel generating process. Hence, each convolution location has its own one set of kernels which is customized by the MADF module. This approach is more flexible than barely scaling the output activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>A. Model Architecture <ref type="figure">Figure 2</ref> shows the overall architecture of our proposed framework, whose inputs are the damaged image I in and the corresponding binary mask M. Our model consists of three components: a) an encoder E that encodes the damaged image with mask into latent high-level feature maps, b) a recovery decoder R to fill the holes in the feature maps, and c) a sequence of refinement decoders {F 1 , F 2 , . . . , F k } to refine the feature maps K times and decode the features back to low-level pixels. We empirically use two refinement decoders to strike a good tradeoff between efficiency and performance. Fisrt of all, we describe the notations used in the following text. m l and e l are the mask feature map and the image feature map generated by our Mask-Aware Dynamic Filtering (MADF) module at the l th level of encoder E. u l is the final encoded feature of the l th level of E. We use r l and f l,k to denote feature map at the l th level of the recovery decoder R l and the k th refinement decoder F l,k . There are in total L levels in our model. Note that r L and f L,k are all equivalent to u L . f l,1 equals r l . We use C l * to denote number of feature channels of tensor * at level l and * ? [u, m, r].</p><formula xml:id="formula_0">k3--s2--conv--16,RELU k3--s2--conv--16,RELU k3--s2--conv--16,RELU k3--s2--conv--16,RELU k5--s2--conv--16,RELU k7--s2--conv--16,RELU k3--s2--conv--16,RELU k1--s1--conv--(3*7*7*16) k1--s1--conv--(16*5*5*32) k1--s1--conv--(32*3*3*64) k1--s1--conv--(64*3*3*128) k1--s1--conv--(128*3*3*128) k1--s1--conv--(128*3*3*128) k1--s1--conv--(128*3*3*128) k7--s2--conv--16,BN,RELU k1--s1--conv--64,RELU k5--s2--conv--32,BN,RELU k1--s1--conv--128,RELU k3--s2--conv--64,BN,RELU k1--s1--conv--256,RELU k3--s2--conv--128,BN,RELU k1--s1--conv--512,RELU k3--s2--conv--128,BN,RELU k1--s1--conv--512,RELU k3--s2--conv--128,BN,RELU k1--s1--conv--512,RELU k3--s2--conv--128,BN,RELU k1--s1--conv--</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder</head><p>During the encoding phase, the encoder E generates multiple level feature maps. In order to make a distinction between missing pixels and valid pixels, we introduce Mask-Aware Dynamic Filtering (MADF). It dynamically generates convolution kernels for each convolution window from the features of the corresponding position on the mask, as shown in <ref type="figure">Figure 2 (d)</ref>. Assume a k ? k convolution with stride s will be applied to e l?1 ? R H?W ?C l?1 e , and there are N H ? N W convolution windows in total. The mask-aware dynamic convolution layer marked in green operates as follows: on m l?1 ? R H?W ?C l?1 m , k ? k convolution with stride s and ReLU activation are firstly applied to generate m l ? R N H ?N W ?C l m , then we utilize 1 ? 1 convolution to generate the kernel tensor</p><formula xml:id="formula_1">? l ? R N H ?N W ?D , where D equals C l?1 e ? k ? k ? C l e .</formula><p>Finally, each of all the N H ? N W windows in e l?1 is convolved using the kernel reshaped from the corresponding point of ? l , i.e., the convolution kernel of the [n H , n W ]-th window in e l?1 is reshaped from ? l [n H , n W , :]. To keep computational cost in mind, we choose to apply MADF to extract relatively low dimensional latent feature e l and then map it to higher dimension u l .</p><p>It is obvious MADF is different from conventional convolution layers which employ "one set of kernels for all convolution windows". Our module works as "one set of kernels per convolution window". Remarkably, recent state-of-the-art PConv <ref type="bibr" target="#b47">[48]</ref>, which applies one set of kernels for all windows but re-normalizes outputs on per window basis with rule-based scaling factors oriented from the mask and hand-crafted mask updating mechanism, can be viewed as a special case of our MADF. MADF learns to dynamically derive filters from mask (or feature maps of mask) for each of its convolution windows rather than relying on any predefined scaling factors and learns to update mask features automatically, so it is more flexible in handling the variously shaped valid pixels on each window. We will show the superiority of MADF over PConv in our experiment.</p><p>Difference from Gated Convolution. GConv is designed to generalize the hand-crafted renormalization strategy of PConv to a learning-based one. GConv performs "one set of filters for all windows" and relies on learned attention map for feature modulation. GConv uses shared convolutional kernels at different conv locations, then it assigns a scaling factor to each location. The kernels corresponding to different locations differ from each other with barely a scaling factor. However, the kernels of MADF for different convolution locations are  <ref type="figure">Fig. 4</ref>: The details of recovery decoder and refinement decoders. "dconv-*" denotes transposed convolutional layer with * output channels. "PN" is the point-wise normalization module and all convolution layers therein are with kernel size of 3 and stride of 1. Note that "? 2" means 2? upsample and is applied to f l,k in refinement decoders and "cat" means channel concatenation.</p><p>generated dynamically according to mask, i.e., how many locations (windows) there are, how many sets of kernels there will be. The design of MADF is more flexible than re-scaling vanilla convolution output of each convolution window and it is orthogonal to the gating mechanism proposed in GConv. It is noteworthy that, MADF is computationally more efficient than GConv. Learning attention maps for modulating features requires not only modules to generate attention maps but also element-wise multiplication per channel on per-point basis.</p><p>We will provide quantitative analysis in Section IV-E. Besides, MADF is compatible with gating mechanism. Combining both of them may further improve the performance if extra computational cost of gating mechanism is not a major concern.</p><p>C. Decoders</p><p>Recovery Decoder. The recovery decoder aims to roughing out multi-scale feature maps r 1 , . . . , r L?1 . The l th recovery decoder block R l takes r l and u l?1 as the inputs and generates r l?1 as outputs. In detail, a transposed convolutional layer with stride 2 is used to up-sample r l , which is then concatenated with u l?1 . Then a convolution is applied after the concatenation to further blend the u l?1 and r l and leaky ReLU is used as the nonlinear activation, as shown in <ref type="figure">Figure 2</ref>(e). The U-shaped architecture allows us to fuse the information of r l and u l?1 level by level.</p><p>Refinement Decoders. After the recovery decoder, we employ a sequence of refinement decoders {F 1 , F 2 , . . . , F K } for further feature refinement. Each refinement decoder has L ? 1 refinement decoding blocks and generates L ? 1 feature maps of different scales. We use F l,k to indicate the l th block of F k and use f l,k to indicate the corresponding l th feature map.</p><p>Because cascaded refinement framework is used and supervision information, such as reconstruction loss, perceptual loss, total variance loss etc., is incrementally leveraged to boost the inpainting results, we infer that statistical characteristics (mean value of ? and standard variance ?) of features of the hole regions and those of the non-hole regions are different, and so is the case for different refinement levels. Applying traditional batch normalization would neglect such difference and attenuate the effect of "covariant shift removal". To address this problem, we utilize Point-wise Normalization (PN) in the refinement phase to convolve the high-level feature f l?1,k?1 from the prior decoder F k?1 to dynamically produce the mask-aware scale and bias of batch normalization for the current decoder F k . Mask awareness can be transmitted to the next refinement levels through PN. Taking F l,k as an example, f l,k is firstly passed through transposed convolutional layer, and then the results x are processed by a batch normalization layer whose affine transformation parameters ? and ? are conditioned on f l?1,k?1 . Formally, PN behaves as the following equation shows:</p><formula xml:id="formula_2">y = ?(f l?1,k?1 ) ? (x ? ?)/? + ?(f l?1,k?1 ),<label>(1)</label></formula><p>where ? means element-wise multiplication, x in the input to PN and y is its output. Specifically, f l?1,k?1 is firstly projected onto an latent space, and then convolved to produce scale and bias tensors, which are of the same spatial dimension as x. The scale tensor is element-wise multiplied to the normalized activation of f l,k and the bias tensor is elementwise added to the result. Though mean and variance are calculated with both hole and non-hole regions, it can be adjusted via the adaptive point-wise scaling factor and bias term, achieving better normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed Network Configurations</head><p>We demonstrate detailed configurations of the encoder, recovery decoder and refinement decoders using block-diagrams in <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="figure">Figure 4</ref>, respectively. In these two figures, convolution kernel size, stride, number of output channels as well as activation function of each layer at every level is described in detail. The kernel sizes and strides are set empirically meanwhile channel numbers are carefully designed with model complexity kept in mind. Specifically, at the encoder, C l m is set to a relatively small value of 16 and output channel number C l e is limited by min(128, 16 * 2 l ) for any l. Then we increase the channels of each e l by 1 ? 1 convolution with ReLU activation to produce u l . At each decoder, we set base channel number to be 64 and increase channel numbers by 2? at every deeper level and the maximum channel number is also limited to be 512 to save computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Functions</head><p>We train the network with the same four types of loss functions as partial convolution <ref type="bibr" target="#b47">[48]</ref>. However, the losses are added incrementally to the decoders.</p><p>Per-pixel Reconstruction Loss Given the mask M , the result I out , and the ground-truth image I gt , we define the L 1 loss for the hole and outside-hole pixels as L hole and L valid respectively:</p><formula xml:id="formula_3">L hole = 1 N Igt (1 ? M ) (I out ? I gt ) 1 (2) L valid = 1 N Igt M (I out ? I gt ) 1<label>(3)</label></formula><p>where N Igt denotes the number of elements in the ground truth image I gt and M is the binary mask with zeros indicating missing pixels. Perceptual Loss A VGG-based perceptual loss forces the decoder to generate image semantically closer to the groundtruth <ref type="bibr" target="#b49">[50]</ref>. We use layer pool1, pool2, pool3 of the ImageNet pretrained VGG-16 <ref type="bibr" target="#b50">[51]</ref> for our loss calculation,</p><formula xml:id="formula_4">L perc = P ?1 p=0 ? Iout p ? ? Igt p 1 N ? I gt p + P ?1 p=0 ? Icom p ? ? Igt p 1 N ? I gt p<label>(4)</label></formula><p>where P denotes the number of layers selected from VGG, here it is 3. I com is the composition of the hole pixels from the raw output image I out and the non-hole pixels from the ground truth. ? I * p denotes the activation of the p-th selected VGG layer given the input I * . N ? I gt p is the dimension of the feature vector ? Igt p and is used as a normalization factor.</p><p>Style Loss A VGG-based style loss is sort of similar to the perceptual loss, but we perform an auto-correlation (Gram matrix) on each selected VGG feature map before applying L 1 . Style loss makes the texture of generated image similar to that of the ground-truth.</p><formula xml:id="formula_5">L style = P ?1 p=0 K p ((? Iout p ) T (? Iout p ) ? (? Igt p ) T (? Igt p )) 1 C p C p + P ?1 p=0 K p ((? Icom p ) T (? Icom p ) ? (? Igt p ) T (? Igt p )) 1 C p C p<label>(5)</label></formula><p>here, (C p , H p , W p ) denotes the shape of the ? * p . K p equals to 1/C p H p W p for normalization.</p><p>Total Variation Loss It acts as the smoothing penalty <ref type="bibr" target="#b51">[52]</ref>.</p><formula xml:id="formula_6">L tv = (i,j)?R,(i,j+1)?R I i,j+1 com ? I i,j com 1 N Icom + (i,j)?R,(i+1,j)?R I i+1,j com ? I i,j com 1 N Icom (6)</formula><p>where R represents the 1-pixel dilated hole regions. The above losses are incrementally leveraged to train our cascaded refinement framework. Specifically, for the output of the recovery decoder, we use L 1 loss:</p><formula xml:id="formula_7">L 1 = L valid + 6L hole .<label>(7)</label></formula><p>For the output of the first refinement decoder, we use L1 loss and perceptual loss: L 1 + 0.05L perc , and the output of last refinement decoder is supervised by L total :</p><formula xml:id="formula_8">L total = L 1 + 0.05L perc + 120L style + 0.1L tv<label>(8)</label></formula><p>The loss weights are empirically set by following the practice in <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce the experimental settings, then we conduct ablation study to verify the effectiveness of design choices in our architecture. Finally, we compare with other methods from objective quantitative and subjective perception aspects to show strength of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate our model on three well-known public datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reference State-of-the-Arts</head><p>We compare our framework with 8 recent state-of-the-art ones. For the purpose of fairness, we try to directly use the officially released pre-trained model with the same experimental settings as much as possible. For PatchMatch <ref type="bibr" target="#b1">[2]</ref> (PM), we use a third-party implementation. For Contextual Attention <ref type="bibr" target="#b9">[10]</ref> (CA), EdgeConnect <ref type="bibr" target="#b10">[11]</ref> (EC) and Gated convolution <ref type="bibr" target="#b14">[15]</ref> (GConv), we directly use the officially release pre-trained model. Since the source code of Partial convolution <ref type="bibr" target="#b47">[48]</ref> (Pconv) is not available, we implement it with the experimental settings in the paper. Bidirectional Attentional Maps <ref type="bibr" target="#b48">[49]</ref> (LBAM) is tested with the source code and the pertrained models provided by its authors. Performances of PEN-Net <ref type="bibr" target="#b8">[9]</ref> and StructureFlow <ref type="bibr" target="#b12">[13]</ref> are also evaluated using their official released codes and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details and training process</head><p>We use the same training and testing irregular masks as <ref type="bibr" target="#b47">[48]</ref>, including 55,116 training masks and 12,000 testing masks. The testing masks can be categorized into 6 categories through hole-to-image area ratios: (0.01,0.1], (0.1,0.2], (0.2,0.3], (0.3,0.4], (0.4,0.5], (0.5,0.6]. During training we augment the mask set by randomly flipping, cropping, rotating and dilating. We train the model by the Adam <ref type="bibr" target="#b52">[53]</ref> optimizer with ? 1 = 0.9 and ? 2 = 0.999 and learning rate is set to 0.0002. All the models are trained on 8 NVIDIA V100 GPUs (32G) with batch size of 24. We train our model for about 300K iterations. The training process takes about 5 days. The weight decay factor is set to be 0 by default and the learning rate policy is a fixed one, i.e., learning rate is a constant for each of the 300K iterations. There is no appropriate pretrained model for initialization. We train our model from scratch and its model parameters are randomly initialized from a normal distribution, the mean and variance of the normal distribution is 0 and 0.01, respectively. Our model does not require any post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study</head><p>We implement several variants of our framework and conduct ablation experiments on Places2 testset. We use 12K testing images with random masks. As previous image inpainting works do, we measure our models in terms of PSNR, SSIM <ref type="bibr" target="#b53">[54]</ref> and Frechet Inception Distance (FID) <ref type="bibr" target="#b54">[55]</ref>. PSNR and SSIM are pixel level evaluation metrics while FID measures the Wasserstein-2 distance between real and fake images.</p><p>1) Design Choice of Network Architecture: We carry out experiments to analyze effectiveness of each component of our network, including MADF, PN and cascaded refinement decoder. Both qualitative and quantitative results (see <ref type="table" target="#tab_2">Table  II</ref>) are provided for comparison. In Tabel II, "PConv" denotes partial convolution.</p><p>MADF v.s. PConv From <ref type="table" target="#tab_2">Table II</ref>, it can be seen that using the basic U-shaped encoder-decoder framework (i.e., the number of refinement decoder is 0), MADF outperforms partial convolution. The PSNR is improved from 25.49dB to 25.70dB by replacing partial convolution with our proposed mask-aware dynamic filtering.</p><p>Furthermore, we visualize 16 7?7 MADF kernels generated from their corresponding mask regions at the first convolution layer of the encoder in <ref type="figure" target="#fig_5">Figure 5</ref>. It does show that our model has learned to dynamically generate appropriate kernels according to the mask. Specifically, each row in <ref type="figure" target="#fig_5">Figure 5</ref> represents a region of the input and each column corresponds to one of the 16 kernels. For example, the first row corresponds   to a region that all pixels are valid and the energy of most kernels is high. While, the last row corresponds to a region where all pixels are masked, the energy of all the kernels is very small. In the second and third rows, where partial pixels are missing, if we check column (b) (e) (f) and (m), we can see that the energy at the valid part is higher than the energy at the masked part.</p><p>MADF v.s. GConv In <ref type="table">Table 1</ref>, MADF+K0 is an UNet architecture without refinement decoders, which achieves average PSNR of 25.7dB. We get a variant by replacing the MADF module in MADF+K0 with GConv, which is denoted as GConv+K0. Following the same training strategies and experimental settings, GConv+K0 achieves average PSNR of 25.61dB, SSIM of 0.863 and FID of 5.53, which is inferior to MADF+K0. This validates the effectiveless of MADF.</p><p>PN v.s. BN The results in the third row and fifth row demonstrate the performance with our proposed PN module. Compared to the BN counterpart, the PSNR and SSIM index are significantly improved, showing that point-wise normalization is able to capture the discrepancy among different refinement levels, and learn the adaptive scale and bias parameters for assisting batch normalization. Besides, PN constantly outperforms BN whatever the number of refinement decoders is.</p><p>Cascaded Refinement Here we also show that empirically use two refinement decoders is a good design choice. From <ref type="table" target="#tab_2">Table II</ref>, we can see no matter whether PN is enabled or not, performances of our model will be consistently improved when adding refinement decoder until 3 of them are used. Meanwhile the 3rd refinement decoder gains few. So we finally use 2 refinement decoders.  <ref type="figure">Fig. 6</ref>: The "coarse-to-fine" supervision schema helps our framework to recover image. Best viewed with zoom-in.</p><p>2) Incremental Supervision: Each intermediate decoder produces an intermediate recovered result. Now, we explore whether the addition of intermediate supervisions will improve the performance. We take intermediate results without supervisions as baseline, and compare with the "coarse to fine" supervision schema and the "same" supervision schema. The "coarse to fine" schema denotes our strategy. For the "same" schema, all the decoders take the same loss function of Eq.8. It can be observed from <ref type="table" target="#tab_2">Table III</ref>, "same" schema contributes little to the performance, while "coarse-to-fine" schema can bring obvious improvements. By dividing the challenging task into many easier sub-tasks, "coarse-to-fine" schema helps the model to generate features and intermediate results in a coarseto-fine way, as shown in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative Comparisons with SOTAs</head><p>Places2 dataset contains various scenes, which is similar to the distribution of natural scene. So we compare with existing methods on this dataset for quantitative results. We use 12,000 testing images, which are randomly masked with the testing masks. Comparison experiments are conducted with irregular masks and regular masks respectively. Due to the measurements are sensitive to the size of image, we test all the methods with the same image size 256?256 following the common practice <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b48">[49]</ref>. <ref type="table" target="#tab_2">Table I</ref> shows the evaluation results. It can be seen that across different mask shapes and different missing areas, our model outperforms all the other state-of-the-art methods on these measurements. PSNR measures the L2 distance, SSIM measures structure similarity meanwhile FID is recently proposed to mimic human perception of similarity in images. It is worthy of noting that the PSNR, SSIM and FID achieved by our model are consistently superior. Besides, when small missing regions appear in the input image, the performance of our model is extremely good. For example, when missing ratio is lower than 0.1, the inpainting results demonstrate average PSNR of 35.74dB, SSIM of 0.988 and FID of 2.69. It shows our model can well capture the context information to fill in the small missing regions. When moderate regions are to be inpainted, the results are also plausible, for instance, when missing ratio is (0.2,0.3], the SSIM is 0.922. When a large portion of image is missing, say missing ratio is (0.5,0.6], the visual quality will be largely degraded and the FID metric is 34.13. This is also reasonable because the valid pixels are very limited in this case and context information is not sufficient for the model to fill in holes. Last but not least, missing ratio of regular hole    <ref type="figure">Figure 9</ref> show the qualitative comparisons of our method with existing methods on Places2, CelebA and Paris StreetView respectively. As shown in these figures, PatchMatch can synthesize smooth textures, but it can not synthesize semantically correct content when the holes are crossing foreground objects. PConv and LBAM can generate plausible structures, but artifacts still exist when the holes are relatively large. CA does not work well with the irregular masks since it is not specially designed for irregular masks. EdgeConnect shows the potential of generating highly structural images with the hallucinated edges from its first stage, but it fails to generate correct structure when the hallucinated edges contain errors. GConv is able to generate both smooth and plausible textures, but the results still show unpleasant flaws. Compared with the results of these baselines, details of our results are enhanced and the generated edges are kept distinct and smooth, even though the damaged contents are relatively large and complex. We contribute this to the characteristic of our method, which utilizes the mask awareness and refines the result progressively in a cascade way. With mask awareness, our model can learn to generate better features representations to denote the image content for decoding. Besides, at the decoding phase, PN is leverage to eliminate "covariant shift" when normalizing the features. Therefore, the features of hole regions at decoder is as effective and homogeneous as the non-hole regions. Further combining refinement decoders, the results become more plausible. <ref type="bibr" target="#b21">22</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. User study</head><p>There is a gap between the evaluation metrics and human perception, therefore we conduct user study on these datasets for a more credible conclusion. We invite 20 volunteers with image processing background to participate in the experiments. For each dataset, 100 images with random masks are randomly sampled from the test set. We perform pairwise A/B tests. Each time, a pair of generated images by different methods are shown to the volunteers. Without telling the mask information or the original ground truth image, the volunteers are asked to choose the more visually plausible and natural one from the two images. The results are shown in <ref type="figure" target="#fig_1">Figure 10</ref>. As can be seen, most of time volunteers prefer our results at all these datasets. <ref type="figure" target="#fig_1">Fig. 11</ref>: Examples of object removal using our framework. From left to right: original images, unwanted objects removed with masks, generated images. Best viewed with zoom-in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Application Showcase</head><p>To show the generalization ability, we apply our model trained on Places2 to natural images downloaded from web. <ref type="figure" target="#fig_1">Figure 11</ref> shows some examples of object removal results using our framework which is one of the most important real use cases of image inpainting. Our model shows great generalization capability. <ref type="figure" target="#fig_1">Fig. 12</ref>: Demonstration of two failure cases on Places2 dataset. Best viewed with zoom-in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Discussion on Failure Cases</head><p>To make clear the limitations of our models, we conduct case studies on Places2. Several failure cases are presented in <ref type="figure" target="#fig_1">Figure 12</ref>. From these results, we find that our model works not so well when large missing region is encountered. In such case, the model trends toward filling in the holes with smooth content information according to its surroundings. It is because context information is very limited and a general inpainting model cannot well hallucinate the lost region. Besides, when very special information is lost, such as texts and logos, the model also fails due to its lack of knowledge. Interestingly, our model trained on CelebA can produce plausible results when a large potion of face region is missing. This is because the model is specially trained for face scenario. Conditioned on the surroundings, it is not so difficult for a specifically well trained model to generate faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel framework which can fill arbitrary missing regions for images in a cascaded refinement with mask awareness fashion. The novel MADF module generates convolution kernels adaptively to corresponding regions in the mask for all convolution windows, thus the extracted feature can get awareness of validity of each point in a convolution window and be more representative for decoding. Intuition of mask awareness also motivates us to adopt pointwise normalization, which is proven to be powerful. Besides, in contrast to two-stage methods that use hand-crafted clues, our model is of cascaded refinement design and uses high-level multi-scale features as the guidance for pixel generation from coarse to fine. Extensive experiments in various scenes verify that our method outperforms the current state-of-the-art methods in both subjective visual quality and objective quantitative measurements. Codes and pretrained models are released: https://github.com/MADF-inpainting/Pytorch-MADF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Index Terms-Image Inpainting, Mask Awareness, Dynamic Filtering, Cascaded Refinement I. INTRODUCTION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Inpainting results generated by our framework. The missing regions are shown in white. The red boxes show local regions with various shaped valid pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2: The architecture of our framework. M ADF l and R l are the MADF module and recovery decoder block at l-th level respectively. F l,k represents the k-th refinement decoder block at l-th level. m l is the l-th mask feature map of the encoder. The convolution operation marked in green in the encoder takes e l?1 as input and its kernel for each convolution window is generated from corresponding region of m l?1 . "DConv" denotes the transpose convolution, "LReLU" denotes the leaky ReLU, and "up" denotes increasing channels by 1 ? 1 convolution layers. u l?1 and r l are inputs to R l to generate the feature map r l?1 at the recovery decoder and F l,k takes f l?1,k?1 and f l,k as input and generates feature map f l?1,k . Note that r L and f L,k are all equivalent to u L and f l,1 equals r l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The details of encoder. "k*" denotes kernel size, "s*" denotes stride, "conv-*" denotes * output channels. The green blocks contain dynamic convolution operations of MADF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>k3--s1--conv--3 k3--s1--conv--64,LRELU k3--s1--conv--128,LRELUk3--s1--conv--256,LRELU k3--s1--conv--512,LRELU k3--s1--conv--512,LRELU k3--s1--conv--512,LRELU % ? 2,cat,k3--s1--conv--3 ? 2,cat,k3--s1--conv--3 k4--s2--dconv--64,PN,LRELU k4--s2--dconv--64,PN,LRELU k4--s2--dconv--128,PN,LRELU k4--s2--dconv--128,PN,LRELU k4--s2--dconv--256,PN,LRELU k4--s2--dconv--512,PN,LRELU k4--s2--dconv--512,PN,LRELU k4--s2--dconv--512,PN,LRELU k4--s2--dconv--512,PN,LRELU k4--s2--dconv--512,PN,LRELU k4--s2--dconv--512,PN,LRELU k4--s2--dconv--512,PN,LRELU s2--dconv--512,BN,LRELU k4--s2--dconv--512,BN,LRELU ? ? k4--s2--dconv--512,BN,LRELU ? k4--s2--dconv--256,BN,LRELU ? k4--s2--dconv--128,BN,LRELU ? k4--s2--dconv--64,BN,LRELU ? k4--s2--dconv--3 ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of the generated 16 kernels of the 1st conv layer in the encoder. The corresponding mask patch is shown in the first column, and the missing regions are shown in black. The original kernels are 7 ? 7 and are resized to 70 ? 70.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Masked (b) output of R (c) output of F 1 (d) Output of F 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :Fig. 8 :Fig. 9 :</head><label>789</label><figDesc>Qualitative comparison on Places2. Best viewed with zoom-in. Qualitative comparison on CelebA. Best viewed with zoom-in. Qualitative comparison on Paris StreetView. Best viewed with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 ,</head><label>7</label><figDesc>Figure 8 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>User study results. The value indicates the percentage of being selected as the better one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>this work was done, Manyu Zhu, Dongliang He, Xin Li, Chao Li, Fu Li, Xiao Liu and Errui Ding were all with Department of Computer Vision (VIS) Technology, Baidu Inc. Zhaoxiang Zhang is currently with Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China. Corresponding author: Dongliang He (hedlcc@126.com).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Results over Places2 dataset, "Regular" means a 128 ? 128 rectangle mask in the center of image. "ALL" indicates measuring in the whole testset. ? means higher is better, means lower is better.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="6">(0.01,0.1] (0.1,0.2] (0.2,0.3] (0.3,0.4] (0.4,0.5] (0.5,0.6]</cell><cell>ALL</cell><cell>Regular</cell></row><row><cell></cell><cell>PM [2]</cell><cell>32.28</cell><cell>26.37</cell><cell>23.55</cell><cell>21.40</cell><cell>19.70</cell><cell>17.58</cell><cell>23.48</cell><cell>19.70</cell></row><row><cell></cell><cell>CA [10]</cell><cell>29.67</cell><cell>23.81</cell><cell>20.74</cell><cell>18.68</cell><cell>17.32</cell><cell>15.94</cell><cell>21.03</cell><cell>19.52</cell></row><row><cell></cell><cell>EC [11]</cell><cell>33.58</cell><cell>27.87</cell><cell>24.89</cell><cell>22.79</cell><cell>21.10</cell><cell>18.95</cell><cell>24.86</cell><cell>20.44</cell></row><row><cell>P SN R  ?</cell><cell>PConv [48]</cell><cell>34.05</cell><cell>28.42</cell><cell>25.54</cell><cell>23.24</cell><cell>21.42</cell><cell>19.10</cell><cell>25.30</cell><cell>21.06</cell></row><row><cell></cell><cell>GConv [15]</cell><cell>32.45</cell><cell>26.68</cell><cell>23.73</cell><cell>21.45</cell><cell>19.70</cell><cell>17.40</cell><cell>23.57</cell><cell>19.11</cell></row><row><cell></cell><cell>LBAM [49]</cell><cell>34.09</cell><cell>28.13</cell><cell>25.13</cell><cell>22.83</cell><cell>21.02</cell><cell>18.77</cell><cell>25.00</cell><cell>20.68</cell></row><row><cell></cell><cell>PEN-Net [9]</cell><cell>31.61</cell><cell>25.76</cell><cell>23.04</cell><cell>21.07</cell><cell>19.39</cell><cell>18.29</cell><cell>23.19</cell><cell>20.20</cell></row><row><cell></cell><cell>StructureFlow [13]</cell><cell>34.92</cell><cell>29.13</cell><cell>25.89</cell><cell>23.58</cell><cell>21.63</cell><cell>19.35</cell><cell>25.77</cell><cell>21.23</cell></row><row><cell></cell><cell>Ours</cell><cell>35.74</cell><cell>29.42</cell><cell>26.29</cell><cell>23.84</cell><cell>21.92</cell><cell>19.44</cell><cell>26.11</cell><cell>21.32</cell></row><row><cell></cell><cell>PM [2]</cell><cell>0.979</cell><cell>0.935</cell><cell>0.877</cell><cell>0.806</cell><cell>0.724</cell><cell>0.573</cell><cell>0.816</cell><cell>0.717</cell></row><row><cell></cell><cell>CA [2]</cell><cell>0.961</cell><cell>0.893</cell><cell>0.804</cell><cell>0.715</cell><cell>0.628</cell><cell>0.511</cell><cell>0.752</cell><cell>0.735</cell></row><row><cell></cell><cell>EC [11]</cell><cell>0.980</cell><cell>0.945</cell><cell>0.895</cell><cell>0.837</cell><cell>0.768</cell><cell>0.634</cell><cell>0.843</cell><cell>0.747</cell></row><row><cell>SSIM  ?</cell><cell>PConv [48]</cell><cell>0.983</cell><cell>0.951</cell><cell>0.908</cell><cell>0.853</cell><cell>0.788</cell><cell>0.652</cell><cell>0.856</cell><cell>0.756</cell></row><row><cell></cell><cell>GConv [15]</cell><cell>0.979</cell><cell>0.939</cell><cell>0.886</cell><cell>0.822</cell><cell>0.751</cell><cell>0.609</cell><cell>0.831</cell><cell>0.726</cell></row><row><cell></cell><cell>LBAM [49]</cell><cell>0.983</cell><cell>0.949</cell><cell>0.903</cell><cell>0.845</cell><cell>0.777</cell><cell>0.638</cell><cell>0.849</cell><cell>0.748</cell></row><row><cell></cell><cell>PEN-Net [9]</cell><cell>0.976</cell><cell>0.910</cell><cell>0.835</cell><cell>0.753</cell><cell>0.6456</cell><cell>0.547</cell><cell>0.778</cell><cell>0.726</cell></row><row><cell></cell><cell>StructureFlow [13]</cell><cell>0.985</cell><cell>0.957</cell><cell>0.913</cell><cell>0.861</cell><cell>0.796</cell><cell>0.654</cell><cell>0.863</cell><cell>0.767</cell></row><row><cell></cell><cell>Ours</cell><cell>0.988</cell><cell>0.961</cell><cell>0.922</cell><cell>0.873</cell><cell>0.812</cell><cell>0.679</cell><cell>0.872</cell><cell>0.768</cell></row><row><cell></cell><cell>PM [2]</cell><cell>4.99</cell><cell>12.69</cell><cell>22.33</cell><cell>31.94</cell><cell>43.42</cell><cell>52.29</cell><cell>11.38</cell><cell>12.68</cell></row><row><cell></cell><cell>CA [2]</cell><cell>7.13</cell><cell>16.25</cell><cell>28.48</cell><cell>40.31</cell><cell>52.89</cell><cell>60.46</cell><cell>15.84</cell><cell>11.42</cell></row><row><cell></cell><cell>EC [11]</cell><cell>4.06</cell><cell>8.78</cell><cell>14.51</cell><cell>20.25</cell><cell>27.30</cell><cell>37.52</cell><cell>5.76</cell><cell>10.87</cell></row><row><cell>F ID</cell><cell>PConv [48]</cell><cell>4.64</cell><cell>10.01</cell><cell>16.47</cell><cell>22.93</cell><cell>31.69</cell><cell>43.31</cell><cell>7.53</cell><cell>15.91</cell></row><row><cell></cell><cell>GConv [15]</cell><cell>4.18</cell><cell>9.93</cell><cell>16.93</cell><cell>24.25</cell><cell>32.44</cell><cell>44.42</cell><cell>7.88</cell><cell>13.22</cell></row><row><cell></cell><cell>LBAM [49]</cell><cell>4.21</cell><cell>9.34</cell><cell>15.69</cell><cell>22.25</cell><cell>30.31</cell><cell>42.38</cell><cell>7.06</cell><cell>12.07</cell></row><row><cell></cell><cell>PEN-Net [9]</cell><cell>4.32</cell><cell>11.81</cell><cell>19.16</cell><cell>27.27</cell><cell>35.44</cell><cell>48.73</cell><cell>8.29</cell><cell>10.91</cell></row><row><cell></cell><cell>StructureFlow [13]</cell><cell>3.11</cell><cell>6.82</cell><cell>13.78</cell><cell>19.82</cell><cell>24.97</cell><cell>37.33</cell><cell>5.54</cell><cell>8.24</cell></row><row><cell></cell><cell>Ours</cell><cell>2.69</cell><cell>6.71</cell><cell>11.64</cell><cell>16.95</cell><cell>23.27</cell><cell>34.13</cell><cell>4.45</cell><cell>8.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Ablation study on model components. ? means higher is better, means lower is better. K * denotes the number of the refinement decoders. "PN" denotes using pointwise normalization.</figDesc><table><row><cell></cell><cell>P SN R  ?</cell><cell>SSIM  ?</cell><cell>F ID</cell></row><row><cell>PConv+K0</cell><cell>25.49</cell><cell>0.860</cell><cell>5.82</cell></row><row><cell>GConv+K0</cell><cell>25.61</cell><cell>0.863</cell><cell>5.53</cell></row><row><cell>MADF+K0</cell><cell>25.70</cell><cell>0.864</cell><cell>5.21</cell></row><row><cell>MADF+K1 PN</cell><cell>25.87</cell><cell>0.867</cell><cell>5.03</cell></row><row><cell>MADF+K1</cell><cell>25.75</cell><cell>0.865</cell><cell>5.23</cell></row><row><cell>MADF+K2 PN</cell><cell>26.11</cell><cell>0.872</cell><cell>4.45</cell></row><row><cell>MADF+K2</cell><cell>25.96</cell><cell>0.869</cell><cell>4.96</cell></row><row><cell>MADF+K3 PN</cell><cell>26.16</cell><cell>0.874</cell><cell>4.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Impact of supervision schemes for intermediate decoder. ? means higher is better, means lower is better.</figDesc><table><row><cell>Loss</cell><cell>P SN R  ?</cell><cell>SSIM  ?</cell><cell>F ID</cell></row><row><cell>None</cell><cell>25.97</cell><cell>0.868</cell><cell>4.99</cell></row><row><cell>Same</cell><cell>26.02</cell><cell>0.869</cell><cell>4.84</cell></row><row><cell>Coarse-to-fine</cell><cell>26.11</cell><cell>0.872</cell><cell>4.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison on complexity-effectiveness trade-off among different methods. equals 0.25, however, the performance on regular hole is much worse than free-form missing region with ratio of (0.2, 0.3]. It suggests that random missing region is easier to be filled than regular missing region.For comprehensive judging our work, we compare FLOPs of our models with several existing SOTAs and the results are listed inTable IV. In this paper, we measure the model complexity by the well-known metric of "FLOPs". It measures the float point operations of a model. We report the number of float point multiplications instead of number of model parameters in this paper, because float point multiplication contributes much more to the model complexity compared to addition operation and it is a better indicator of model inference speed compared to number of model parameters. Without refinements (MADF+K0 inTable II), our model requires 22.13G FLOPs, which is slightly larger than previous SOTA PConv (18.95G), however, the performance is much better (25.70dB v.s. 25.49dB). Our final model (MADF+K2+PN in Table II) requires 51.77G FLOPs and performance is boosted to 26.11dB, GConv, LBAM, EdgeConnect and Contextual Attention requires 55.51G, 22.11G, 122.67G and 51.62G, respectively. Here FLOPs are calculated under test image resolution of 256 ? 256.</figDesc><table /><note>F. Qualitative Comparisons with SOTAs</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="861" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="page">107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5485" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning pyramid-context encoder network for high-quality image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5505" to="5514" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Edgeconnect: Generative image inpainting with adversarial edge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00212</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spg-net: Segmentation prediction and guidance network for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03356</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structureflow: Image inpainting via structure-aware appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image inpainting via generative multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="331" to="340" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="606" to="615" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="103" to="110" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Examplar-based inpainting based on local geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image inpainting using nonlocal texture matching and nonlinear filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodr?guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1705" to="1719" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Laplacian patch-based image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2727" to="2735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A combined pde and texture synthesis approach to inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grossauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Strong-continuation, contrast-invariant inpainting with a third-order optimal pde</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1934" to="1938" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometrically guided exemplar-based inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Masnou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1143" to="1179" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning how to inpaint from global image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple pyramids based image inpainting using local patch statistics and steering kernel feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghorai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5495" to="5509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inpainting and zooming using sparse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Fadili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="79" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image inpainting: Overview and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="144" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exemplarbased inpainting: Technical review and new heuristics for better geometric reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buyssens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daisy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tschumperl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>L?zoray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1809" to="1824" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shift-net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Single-shot semantic image inpainting with densely connected generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Highresolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image inpainting using multi-scale feature image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08590</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Foreground-aware image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5840" to="5848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pepsi: Fast image inpainting with parallel decoding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sagong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="360" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pluralistic image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1438" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-scale progressive fusion network for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8346" to="8355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detail-recovery image deraining via context aggregation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="560" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A model-driven deep neural network for single image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3103" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2472" to="2481" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="286" to="301" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1664" to="1673" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Image inpainting with learnable bidirectional attention maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00968</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6626" to="6637" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Zhu is a senior research and development engineer at Department of Computer Vision (VIS) Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manyu</forename><surname>Manyu Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Baidu Inc. She received the M.S. degree in pattern recognition and intelligent system from Huazhong University of Science and Technology. Her research interests focus on computer vision based on deep learning, image processing and multimodal deep learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dongliang He Dongliang He is a senior research and development engineer at Department of Computer Vision (VIS) Technology, Baidu Inc. He received Bachelor and Ph.D. degree in computer science from University of Science and Technology of China in</title>
	</analytic>
	<monogr>
		<title level="m">respectively. His research interests focus on computer vision, deep learning and multimedia processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Xin Li Lixin is a senior research and development engineer at Department of Computer Vision (VIS) Technology</title>
	</analytic>
	<monogr>
		<title level="m">Baidu Inc. He received the M.S. degree in EE from TsingHua University. His research interests focus on computer vision based on deep learning</title>
		<imprint/>
	</monogr>
	<note>image processing</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Chao Li is a senior researcher and engineer at Department of Computer Vision (VIS) Technology, Baidu Inc. He received his Ph.D. degree in The University of Queensland</title>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Australia</pubPlace>
		</imprint>
	</monogr>
	<note>His research interests include Computer Vision, Multimedia Search and Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Baidu Inc. He received M.E. degree from Dalian University of Science and Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China in 2015. His research interests focus on video understanding and generative adversarial networks</title>
		<imprint/>
	</monogr>
	<note>Li is a senior R&amp;D engineer at Department of Computer Vision Technology (VIS)</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Liu is a researcher in the Tomorrow Advancing Life Education Group (TAL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<imprint>
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science from Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>He received the PhD degree in. in 2015. He previously worked for Baidu from 2015 to 2019. His research interests include computer vision and learning system</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">and currently is the director of Computer Vision Technology Department (VIS) of Baidu Inc. In recent years, he has published tens of papers on top-tier conferences and was awarded Best Paper Runner-up by ICDAR 2019. He also co-organized two competition tracks, i.e., ArT and CSVT</title>
	</analytic>
	<monogr>
		<title level="m">ICDAR 2019 and the 2nd Workshop on Learning from Imperfect Data in CVPR 2020. As a member</title>
		<imprint/>
	</monogr>
	<note>Errui Ding Errui Ding received Ph.D degree in 2008 from Xidian University. he serves in special committee of China Society of Image and Graphics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
