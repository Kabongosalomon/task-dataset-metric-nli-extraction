<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Background Augmentations for Self-Supervised Learning Characterizing and Improving the Robustness of Self-Supervised Learning through Background Augmentations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Ryali</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
							<email>dschwab@gc.cuny.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
							<email>arimorcos@fb.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">ITS, CUNY Graduate Center</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<address>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Background Augmentations for Self-Supervised Learning Characterizing and Improving the Robustness of Self-Supervised Learning through Background Augmentations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>self-supervised learning</term>
					<term>contrastive learning</term>
					<term>representation learning</term>
					<term>back- ground augmentation</term>
					<term>out-of-distribution generalization</term>
					<term>robustness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in self-supervised learning has demonstrated promising results in multiple visual tasks. An important ingredient in high-performing self-supervised methods is the use of data augmentation by training models to place different augmented views of the same image nearby in embedding space. However, commonly used augmentation pipelines treat images holistically, ignoring the semantic relevance of parts of an image-e.g. a subject vs. a background-which can lead to the learning of spurious correlations. Our work addresses this problem by investigating a class of simple, yet highly effective "background augmentations", which encourage models to focus on semantically-relevant content by discouraging them from focusing on image backgrounds. Through a systematic investigation, we show that background augmentations lead to substantial improvements in performance across a spectrum of state-of-the-art self-supervised methods (MoCo-v2, BYOL, SwAV) on a variety of tasks, e.g. ? +1-2% gains on ImageNet, enabling performance on par with the supervised baseline. Further, we find the improvement in limited-labels settings is even larger (up to 4.2%). Background augmentations also improve robustness to a number of distribution shifts, including natural adversarial examples, ImageNet-9, adversarial attacks, ImageNet-Renditions. We also make progress in completely unsupervised saliency detection, in the process of generating saliency masks used for background augmentations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning useful representations in the absence of labels is a critical challenge in machine learning. Recently, self-supervised (SSL) methods such as SimCLR <ref type="bibr" target="#b25">(Chen et al., 2020a)</ref>, MoCo-v2 <ref type="bibr" target="#b28">Chen et al., 2020c)</ref>, BYOL <ref type="bibr">(Grill et al., 2020)</ref>, and SwAV <ref type="bibr">(Caron et al., 2020)</ref> have risen to prominence because they are able to produce high-quality representations that rival supervised representations on vision tasks. These methods differ in the details of their approach-e.g. some are instance based (MoCo-v2, SimCLR) while others are cluster based (SwAV), some explicitly utilize negatives while others do not (BYOL), and some use a memory bank . In fact, competitive performance has recently been achieved by SimSiam <ref type="bibr" target="#b27">(Chen and He, 2020)</ref> without any of these additions. However, a central ingredient common to all high performing SSL methods is their reliance on data augmentation as a means of encoding desired invariances. Two views of an image are created via independent samples from the data augmentation pipeline, and the objective is viewinvariance, i.e. the encoder is trained to place them near each other in representational space. Thus, the choice of data augmentation is critical, as augmentations and the invariances they encourage are the primary teaching signal these methods utilize to create semantically meaningful representations.</p><p>In fact, <ref type="bibr" target="#b25">Chen et al. (2020a)</ref> explored a large space of standard augmentations and demonstrated that the choice of these augmentations can have dramatic effects on performance. However, this standard suite of augmentations used in most SSL methods was modified from augmentations designed for supervised approaches. It may therefore be useful to design new augmentation schemes for SSL that specifically target semantic focus for this setting.</p><p>A parallel line of inquiry has found that supervised models often rely on non-semantic features that may nonetheless be predictive at test time. Models often overly focus on backgrounds <ref type="bibr" target="#b96">Sehwag et al., 2020;</ref><ref type="bibr" target="#b17">Beery et al., 2018)</ref>, are brittle to distribution shift in foreground-background statistics, and rely on high-frequency information <ref type="bibr" target="#b67">(Jo and Bengio, 2017;</ref><ref type="bibr" target="#b63">Ilyas et al., 2019)</ref>. Models are also susceptible to adversarial attacks <ref type="bibr" target="#b43">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b67">Jo and Bengio, 2017)</ref>, often rely on texture over shape <ref type="bibr" target="#b38">(Geirhos et al., 2019</ref><ref type="bibr" target="#b39">(Geirhos et al., , 2020</ref><ref type="bibr">Hermann et al., 2020)</ref> and are brittle to distribution shift in local texture (e.g. paintings, sculpture, <ref type="bibr" target="#b58">Hendrycks et al. (2021)</ref>) as well as to corruptions (e.g. blur, contrast, <ref type="bibr" target="#b55">Hendrycks and Dietterich (2019)</ref>). Importantly, the benefits or limitations of a modeling choice on robustness are not apparent from metrics on standard tasks <ref type="bibr" target="#b56">(Hendrycks et al., 2019a)</ref>. All of these results showcase the need for comprehensive model evaluation across diverse data sets and settings. We broadly encompass such comprehensive evaluation under robustness, e.g. robustness to distribution shifts (e.g. paintings, blurring, different background statistics), robustness to adversarial attacks, robustness to label scarcity.</p><p>While there has been much work investigating robustness properties in the supervised setting, the self-supervised setting has received relatively less attention. As SSL methods shrink the gap to their supervised counterparts, it has become increasingly important to characterize their robustness properties and gain a more holistic understanding. The aim of this work is twofold: characterizing the robustness of high performing SSL methods and investigating approaches for improved semantic focus via a class of augmentations called background augmentations.</p><p>We conduct a systematic, comprehensive investigation of the robustness properties of SSL methods as well as the impact of background augmentations in improving semantic focus across a) a spectrum of high performing SSL methods, b) training durations, c) three variants of background augmentations, d ) different foreground extraction methods used in background augmentations, and e) a wide range of downstream data sets and tasks, including 17 distribution shift settings.</p><p>Specifically, we study three classes of approaches: BG RM, in which a subset of backgrounds are removed during the augmentation process, BG Random, in which backgrounds are replaced with random backgrounds from other images in the mini-batch, and BG Swaps, in which a selection of backgrounds are swapped between positive and negative images to match backgrounds across the query and the negative, thereby explicitly penalizing background focus.</p><p>We highlight the following contributions:</p><p>? Novel background augmentation method. We develop and analyze a novel, highly effective background augmentation method BG Swaps, which manipulates the backgrounds of positives and negatives in a structured manner, yielding large performance and robustness benefits.</p><p>? Sizeable performance benefits. We show sizeable performance improvements for all view-invariant SSL methods, yielding consistent improvements of ?1-2% in linear evaluation on ImageNet; these improvements allow us to reach an accuracy of 76.1% (63.8%) on ImageNet (ImageNet-v2), on par with the standard supervised baseline 76.4% (63.8%) for ResNet-50. Further, background augmentations enable us to reach a benchmark accuracy of 74.4%, outperforming Barlow Twins <ref type="bibr">(Zbontar et al., 2021)</ref>, MoCo-v3 <ref type="bibr" target="#b29">(Chen et al., 2021)</ref> and BYOL trained for 800-1000 epochs in only 100 epochs; this result takes a large step forward in reducing the amount of training required for competitive performance in SSL.</p><p>In the limited-label setting, we show the performance benefits are even larger, e.g. in the 1% (ImageNet) label setting, BG Swaps confers a 4.0% accuracy gain for MoCo-v2 and in the 10% label setting BG Random enables BYOL to reach 72% accuracy using only 10% of ImageNet labels.</p><p>? Improved robustness. We find that background augmentations (especially BG Swaps) lead to significantly improved robustness in many settings including ImageNet-9 (shift in foreground-background statistics), ImageNet-A (natural adversarial examples), ImageNet-R (ImageNet-Renditions), against adversarial attack, and ImageNet ReaL.</p><p>? Scientific Insight. We investigate the impact of background augmentations in a) the supervised setting and b) RotNet, and find that they do not confer a performance gain, giving us insight into when and how background augmentations work. We also gain further insight by shape-bias probing as well as by systematically perturbing the quality of the augmentations.</p><p>? Improvement in saliency detection. In order to separate foregrounds and backgrounds without any supervision, we also make progress in completely unsupervised saliency detection, matching or outperforming weakly supervised as well as many supervised methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-Supervised Learning Methods</head><p>We consider a diverse test bed of high performing self-supervised learning methods: MoCo-v2 <ref type="bibr" target="#b28">(Chen et al., 2020c)</ref>, BYOL <ref type="bibr">(Grill et al., 2020)</ref>, and SwAV <ref type="bibr">(Caron et al., 2020)</ref> to ensure generality of our results. As in the respective original works, we use a standard ResNet-50 <ref type="bibr" target="#b51">(He et al., 2016)</ref> as the default architecture in all experiments (SSL and supervised) unless otherwise noted. A small subset of our experiments are based on RotNet <ref type="bibr" target="#b42">(Gidaris et al., 2018)</ref>, using an AlexNet <ref type="bibr" target="#b70">(Krizhevsky et al., 2012)</ref> architecture following the respective original work. All reported numbers are based on our reproduction unless otherwise stated. Where possible, we follow the protocol from the original works.</p><p>Here, we provide a brief overview of MoCo-v2, BYOL and SwAV and some implementation details, with further details in Appendix A. We defer an overview of RotNet to Section 4.10 and relegate implementation details to Appendix A.</p><p>Overview. Broadly, each method uses a pair of Siamese networks <ref type="bibr" target="#b19">(Bromley et al., 1994</ref>)i.e. weight-sharing neural networks, to encode differently augmented "views" of the same image and maximize similarity between them, thereby encouraging the learning of "desirable" invariances. Concretely, two views v s , v t of an image x are generated by sampling from a random augmentation pipeline. The student network f ? is used to encode v s as z s = f ? (v s ) and similarly the teacher network 1 f ? , is used to encode v t as z t = f ? (v t ). Then, z s is used to predict a target generated from z t ; the specific form of this pretext prediction task varies with the SSL method. Learning/"pre-training" is by optimization of the prediction loss over ?.</p><p>MoCo-v2 is an instance of contrastive learning <ref type="bibr" target="#b48">(Hadsell et al., 2006)</ref>, a framework for learning representations from data that are organized into similar/dissimilar pairs. The prediction task in MoCo-v2 is one of instance discrimination: a differently augmented view of the same image x needs to be discriminated from a set Q of "distractors"-views of images different from x, in a (|Q| + 1)-way classification. Two images form a similar/positive pair if they are views of the same image and otherwise form a negative pair. MoCo-v2 uses the InfoNCE <ref type="bibr" target="#b84">(Oord et al., 2018)</ref> loss for this task and instantiates Q as a queue comprised of previous mini-batches of 2 normalized outputs from the teacher. The prediction isz s and the target isz t , wherez = z /||z|| 2 .</p><p>In the terminology of the original work, the predictionz s is called the query (denoted q), the targetz t is called the positive key (denoted k + ) and the distractors (here elements of Q) are known as negatives keys (denoted Q = {k ? }). Thus, the loss encourages similarity between q and k + and dissimilarity between q and k ? .</p><p>In BYOL, a prediction Multi-Layer Perceptron (MLP) q ? is used to generate the prediction q ? (z s ), the target isz t and the loss used is Mean Squared Error (MSE). In SwAV, the target is generated by an online-clustering process andz s is used to predict the cluster assignment ofz t ; the loss used is Cross-Entropy (CE). Thus, SwAV is a clustering-based approach, while MoCo-v2 and BYOL are instance-based approaches. SwAV and BYOL are not explicitly contrastive, since they do use negative instances.</p><p>All methods use 2 "global" views, while SwAV additionally uses L "local" views-low resolution crops that cover only small parts of the image; by default L = 6. Using global and local views is known as multi-crop augmentation. Local views are typically only used for prediction and not used in generating the targets. Intuitively, since local views are expected to be predictive of global views, models are discouraged from representing only the most discriminative features for solving the pretext prediction task.</p><p>It is typical to use a projection MLP <ref type="bibr" target="#b25">(Chen et al., 2020a</ref>) on top of a backbone network and discard the projection MLP after pre-training (but see <ref type="bibr" target="#b26">Chen et al. (2020b)</ref>). In our notation, f subsumes the backbone g and the projection MLP h, i.e. f = h ? g. At the end of pre-training, only the backbone g ? is kept. The outputs of g ? are called representations and the corresponding outputs of h are called the embeddings/projections.</p><p>(Abuse of ) Notation: For simplicity, we refer to the embedding from the student network as the query q and the embedding from the teacher corresponding to the same image x as the positive key k + , across all methods. We also use the terms student (teacher) and query (key) network interchangeably.</p><p>Implementation. MoCo-v2 is trained using SGD and a larger (than the standard 256) batch size of 1024 (distributed across 32 GPUs) with a 20 epoch linear warmup for 220 (800) epochs in the medium (full) setting. These settings were chosen to increase training speed while matching the reported performance at a similar number of epochs in <ref type="bibr" target="#b28">Chen et al. (2020c)</ref>.</p><p>BYOL and SwAV were trained using LARS <ref type="bibr" target="#b118">(You et al., 2017)</ref> using a batch size of 4096, distributed across 64 GPUs with synchronized batch normalization <ref type="bibr" target="#b64">(Ioffe and Szegedy, 2015)</ref> for groups of 8 GPUs. BYOL (SwAV) is trained for 300 (100) epochs in the medium setting and 1000 (800) epochs in the full setting. See Appendix A for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Background Augmentations</head><p>We apply all background augmentations (BG RM, BG Random, BG Swaps) after all other augmentations in the respective augmentation pipeline. However, we note that we observed similar results applying background augmentations before all other augmentations as well (Appendix C.5). While we apply background augmentations to (views of) images, when it is clear from context, we will refer instead to the corresponding embeddings q, k + , k ? . To discourage focus on backgrounds...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same background for query and negative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BG_RM BG_Random BG_Swaps</head><p>Figure 2: Schematic of different types of background augmentations. BG RM (left) replaces backgrounds with grayscale, effectively removing any background information. BG Random (middle) replaces backgrounds with random backgrounds, creating a random signal which is uncorrelated with the foreground. BG Swaps (right) exploits the structure of contrastive learning to ensure that the query and the positive have the same foreground but different backgrounds, while the query and one negative have matched backgrounds. As a result, BG Swaps makes it so that models are penalized for focusing on the background.</p><p>Unless otherwise mentioned, background augmentations are applied independently with a probability p pos to both q and k + (the positive teaching pair). When a method has explicit negative instances (MoCo-v2), we denote by p neg the probability of including a negative whose background matches q; by default, this is independent of background augmentation in q and k + . Values for p pos and p neg were optimized independently for each background augmentation. When it is clear from context, we will sometimes drop the subscript. Note that in MoCo-v2, k + is placed in the queue Q for use in subsequent batches as a negative, so that augmentations applied to k + , also indirectly apply to k ? via Q. When multi-crop augmentation is used (as in SwAV), we apply background augmentations only to the global views. Background augmentations are only applied during self-supervised pre-training and are not applied when training linear classification layers for evaluation. Below, we describe the details of each of the background augmentations we study.</p><p>In BG RM, the background of an image is removed by using a foreground mask (obtained using a saliency detector, see Section 3), and replaced with a solid grayscale background whose intensity is drawn uniformly from [0, 1], though we note that a solid black background produced similar results. See illustrative examples in <ref type="figure">Figure 2</ref>, left column.</p><p>In BG Random, we replace the background with a background from a different image in the same batch. As in , tiled backgrounds corresponding to an image are generated by filling in the foreground information using the surrounding background.</p><p>In BG Swaps, we generate a negative image with a background matched to that of the query q. In practice, we create a background matched negative as m q r + (1 ? m q )q, where m q is the binary foreground mask of the query q and r is a random image. We generate all foreground masks and tiled backgrounds offline and cache them to increase throughput at train time. Note that foreground masks may include multiple foreground objects when they are present (e.g. last row of <ref type="figure">Figure 3</ref> or <ref type="figure">Figure A7</ref>). Substantial noise is tolerable in the quality of the foreground masks (see Appendix B). More generally, there is substantial flexibility and tolerance in instantiating the main ingredients of background augmentations, which we expand on in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Supervised Training</head><p>We largely follow the protocol from <ref type="bibr" target="#b45">Goyal et al. (2018)</ref>, unless otherwise indicated. We train all supervised models (with or without background augmentation) with a batch size of 4096 with a 5 epoch linear warmup due to the large batchsize. Models are trained for 90 epochs, with a step schedule <ref type="bibr">(30,</ref><ref type="bibr">60,</ref><ref type="bibr">80)</ref> and a decay factor of 0.1, using SGD with a base learning rate of 0.1 scaled linearly (lr=BatchSize/256?0.1) and momentum of 0.9, and the standard augmentations RandomResizedCrop and RandomHorizontalFlip. We also exclude bias and batch normalization parameters from weight decay, which was set to 1 ? 10 ?4 . The ? in each residual block's last BatchNorm layer is zero initialized. Our supervised baseline for ResNet-50 reaches the standard baseline <ref type="bibr" target="#b45">(Goyal et al., 2018)</ref> performance of ?76.4% Top-1 accuracy on ImageNet <ref type="bibr" target="#b94">(Russakovsky et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Saliency Detection</head><p>We use saliency detection to generate the foreground masks used in background augmentations (see methods, Section 2.2). However, state-of-the-art saliency detection methods (e.g. U 2 Net, <ref type="bibr" target="#b88">Qin et al. (2020)</ref>) are generally reliant on manually annotated, accurate pixel-level Ground Truth (GT) saliency labels for training, making their usage inappropriate in a truly selfsupervised benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Weakly Supervised Saliency Detection</head><p>Recent "unsupervised" saliency detection methods <ref type="bibr" target="#b125">Zhang et al., 2018b</ref><ref type="bibr" target="#b123">Zhang et al., , 2017a</ref> demonstrate promising results by leveraging psuedo-labels generated by hand-crafted saliency methods in lieu of manually annotated GT saliency labels. Briefly, noisy psuedolabels generated by hand-crafted saliency methods are iteratively refined by using them as targets to train a Fully Convolutional Network (FCN) for saliency detection, and obtaining refined pseudo-labels from the denoised predictions. Refined pseudo-labels from multiple hand-crafted methods are then jointly used to train a re-initialized FCN to obtain the final saliency detector. While these methods are "unsupervised" in that they do not use manually annotated saliency labels, their success implicitly relies on human annotation-the FCN used is pre-trained in a supervised manner using ImageNet class and CityScapes <ref type="bibr" target="#b30">(Cordts et al., 2016)</ref> segmentation labels. Indeed, we find that if we use a randomly initialized FCN instead, the resulting saliency predictions are worse than the noisy psuedo-labels used as targets. As such, these methods are also not appropriate to generate foreground masks for our purpose; we thus refer to these methods as weakly supervised methods in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth DeepUSPS DeepUSPS 2 <ref type="figure">Figure 3</ref>: Examples of saliency masks generated by DeepUSPS 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Saliency Detection: DeepUSPS 2</head><p>In order to train a completely unsupervised saliency detector, we build upon DeepUSPS , a recent state-of-the-art weakly supervised saliency detection method. We first pre-train a DRN-D-105 <ref type="bibr" target="#b119">(Yu et al., 2017)</ref> network in a self-supervised manner for 500 epochs on ImageNet, using BYOL. We then use this pre-trained network to refine pseudo-labels and train a saliency detector, which we call DeepUSPS 2 , employing a training protocol modified from DeepUSPS (see Appendix A.2); some example saliency predictions are shown in <ref type="figure">Figure 3</ref>. Training images were 2500 images from the MSRA-B data set <ref type="bibr" target="#b75">(Liu et al., 2011)</ref>. We find that DeepUSPS 2 performs better than or on par with DeepUSPS and other recent state-of-the-art weakly supervised and even some supervised saliency detectors on common saliency benchmark data sets MSRA-B, ECSSD <ref type="bibr" target="#b116">(Yan et al., 2013)</ref>, and DUT <ref type="bibr" target="#b116">(Yan et al., 2013)</ref>, yet DeepUSPS 2 does not rely on any human annotation at any stage in the pipeline, see <ref type="table">Table 1</ref>. For each data set, following common protocol <ref type="bibr" target="#b11">Achanta et al., 2009)</ref>, we report the F-score,</p><formula xml:id="formula_0">F ? = (1 + ? 2 ) ? precision ? recall ? 2 ? precision + recall ,</formula><p>where ? 2 = 0.3 to weigh precision more than recall and the MAE (Mean Absolute Error) on the test split. We use DeepUSPS 2 as the default saliency detector to generate foreground masks in our experiments unless otherwise indicated. To ablate the method of mask generation and for control experiments, we also use U 2 Net <ref type="bibr" target="#b88">(Qin et al., 2020)</ref>, a state-of-the-art saliency detector that is trained in a supervised manner on DUTS-TR <ref type="bibr" target="#b108">(Wang et al., 2017a)</ref>, which contains 10553 pixel-level manual saliency annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Representation Learning with Background Augmentations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Do Background Augmentations that Encourage Semantic Focus Increase Performance?</head><p>Deep neural networks often rely on non-semantic, superficial features and thus may be easily misled by backgrounds. Nonetheless, these non-semantic features are often predictive at test time <ref type="bibr" target="#b96">Sehwag et al., 2020;</ref><ref type="bibr" target="#b63">Ilyas et al., 2019)</ref>, so it is not a priori obvious whether background augmentations that encourage semantic focus on the foreground will benefit performance. We investigate this question by exploring the space of possible background augmentations. First, we study removing backgrounds probabilistically, where the strength of the augmentation is controlled by a parameter p, which sets the probability that the background is removed from the query or positive key. See <ref type="figure">Figure 2</ref>, left for an example of the BG RM setting. Across SSL methods, we find that BG RM substantially improves linear classification on ImageNet, improving performance by ?0.6-1.4% <ref type="table" target="#tab_4">(Table 2)</ref>. For all methods, we found that a moderate value of p between 0.1 and 0.3 is generally a good setting. However, despite its PCL-v2 <ref type="bibr" target="#b73">(Li et al., 2021b)</ref> 200 67.6 -CMC <ref type="bibr" target="#b104">(Tian et al., 2020a)</ref> 200 66.   improved performance, because BG RM introduces images with solid gray backgrounds, it induces a distribution shift between the unsupervised pre-training phase and the supervised downstream tasks which may limit performance improvements. Note that DiLo (MoCo-v2) is similar to BG RM applied MoCo-v2, but results only in a small gain of +0.2, while we obtain a 7? larger gain (and as we will show later, a 9? gain by developing an improved background augmentation method BG Swaps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Can we make Background Augmentations more In-Distribution?</head><p>In the previous section, we explored removing backgrounds and replacing them with uniform grayscale, which results in the data being out-of-distribution (OOD) relative to the downstream tasks. To mitigate this OOD issue, we instead replace backgrounds with a randomly chosen background from another instance in the same batch. We term this method BG Random <ref type="figure">(Figure 2</ref>, middle). Interestingly, despite the fact that BG Random is more in-distribution than BG RM, we found that performance was similar (e.g. 69.1% for BG RM <ref type="table" target="#tab_4">(Table 2)</ref> vs. 69.2% for BG Random <ref type="table">(Table A6)</ref> with MoCo-v2). However, we note that these two settings are not necessarily directly comparable. For example, in the case of MoCo-v2, augmented positive keys are added to the queue to be used as negatives for subsequent mini-batches. As a result, BG RM might actually penalize background focus whereas BG Random may simply result in an uninformative background. This is because BG RM features a constant gray background which can be matched between the query and negatives that were used as positive keys in a previous mini-batch, whereas BG Random features distinct backgrounds for each augmented image.</p><p>It is therefore unclear whether the similar performance of BG RM and BG Random stems from distributional shift or the matched gray backgrounds which can serve to make negatives more challenging. To disentangle these two factors, we performed a control experiment in which background augmented images were only included for the query (with p = 0.1)-and thus not used in subsequent mini-batches as the positive or the negative. This setting maintains the distribution shift of BG RM, but removes the possibility of a teaching signal originating from matched gray backgrounds across the query and negative.  confound stemming from mask quality, we use higher quality foreground masks generated by U 2 Net, a state-of-the-art saliency detector trained with supervision instead of DeepUSPS 2 . This control reveals that, when only applied to the query but not the positive or negative, BG RM has similar performance <ref type="table" target="#tab_5">(Table 3b</ref>) as no BG RM <ref type="table" target="#tab_5">(Table 3a</ref>), suggesting that BG RM benefits substantially from the teaching signal of negatives with matching (constant) backgrounds. In contrast, we found that, when only present in the query, BG Random still improves performance <ref type="table" target="#tab_5">(Table 3c</ref>), but the improvement is decreased suggesting that having augmented images with randomized backgrounds in the positive keys provides additional benefit <ref type="table" target="#tab_5">(Table 3e)</ref>; here, for an apples-to-apples comparison with the control experiments (or "partial" augmentations), we also report performance for the "full" augmentations <ref type="table" target="#tab_5">(Table 3d</ref>, e) using U 2 Net masks and the same augmentation strength.</p><p>These analyses demonstrate both the importance of using background augmentations which remain close to the unaugmented input distribution and highlight the potential for methods which provide an additional teaching signal via negatives with query-matched backgrounds. Inspired by these results, we next investigate how to combine these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Exploiting the Structure of Contrastive Instance Discrimination via Background Matched Negatives</head><p>Thus far, we have explored two background augmentations-BG RM and BG Random-both of which operate independently on the query and the positive and encourage semantic focus on foregrounds by simply removing backgrounds altogether or replacing them with randomized backgrounds so there's no effective signal in the background. This removes the incentive for models to focus on background information, but does nothing to directly penalize focus on backgrounds. However, contrastive instance discrimination (CID) methods (e.g., MoCo, SimCLR), use the query to discriminate between the positive and negative instances and thus feature structure that we can exploit to not only remove signal from backgrounds, but go further and provide explicitly misleading signal in the backgrounds. Note that BYOL and SwAV are not CID methods, since they do not use negative instances. We accomplish this through two modifications with a method we call BG Swaps ( <ref type="figure">Figure  2</ref>, right). First, as in BG Random, we ensure that the query and the positive feature distinct random backgrounds. Models which focus on backgrounds would therefore place the positive and query further apart than they should since the semantic content is identical, but the background features differ. Second, we modify the negative set to include one additional negative 2 whose background matches the query: a network which focuses on backgrounds would view the background-matched negative as highly similar to the query and receive strong negative supervision. As with BG RM and BG Random, we introduce the backgroundmatched negative with probability p neg (and include a randomly selected negative with probability 1 ? p neg , so that the total number of negatives is always |Q| + 1).</p><p>These background matched negatives can be considered an example of "hard negatives", which have been explored recently in the context of SSL to improve learning <ref type="bibr">(Kalantidis et al., 2020;</ref><ref type="bibr" target="#b93">Robinson et al., 2021;</ref><ref type="bibr" target="#b20">Cai et al., 2020)</ref>. In this vein, one could consider the positive pair (q and k + ) with different backgrounds as "hard positives". For MoCo-v2, including a background matched negative further increases performance over BG RM by an additional 0.4% <ref type="table" target="#tab_4">(Table 2)</ref>. Consistent with our previous findings, we also found that it is important for the statistics of the augmentations to be similar for the positive and the negatives in order to achieve the best performance. In general, we found that the probability of an augmentation in the query and positive, p pos , matching the probability of an augmentation in the negative, p neg , so that p pos p neg , gives good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablating BG Swaps</head><p>To characterize which components of the BG Swaps augmentation matter and how much, we perform systematic ablations. As shown in <ref type="table" target="#tab_7">Table 4</ref>, we found that each independent component of BG Swaps leads to a performance improvement (in contrast with BG RM). In particular, we find benefits when employing each of the following: BG Random in the query (p = 0.1); BG Random in the positive key (p = 0.1); background matched negative (p = 0.2). Notably, the improvements from randomized backgrounds in q and k + stack superlinearly <ref type="table" target="#tab_7">(Table 4d</ref>), suggesting that incorporating both of these augmentations provides a greater advantage due to their interaction than either does independently; using background matched negatives further improves performance substantially <ref type="table" target="#tab_7">(Table 4e</ref>). As in the control experiments in Section 4.2, to minimize confound stemming from mask quality, we use higher quality foreground masks generated by U 2 Net for these ablations.</p><p>There is significant design flexibility in how one could implement BG Swaps. For example, is it a better teaching signal to have independent or correlated background augmentations in the query/positive and the negatives? Is it better to have a negative whose background matches the query or the positive? We find that BG Swaps is robust to these specific choices (Appendix C), making it a promising candidate for more general deployment in augmentation pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Longer Training</head><p>We also evaluate the impact of background augmentations on longer training ranging from 800 to 1000 epochs ( <ref type="table" target="#tab_4">Table 2)</ref>. As with the shorter training, we found that background augmentations consistently increased performance across models, e.g. enabling SwAV to reach 76.1% with a ResNet-50 on ImageNet, only 0.3% less than the standard supervised baseline. Interestingly, however, we found that the magnitude of the improvement decreased 2. We explored using multiple background matched negatives, but found no improvement over a single matched negative. See Appendix C for details.  slightly in the longer training runs, which may be a saturation effect but also raises the more interesting possibility that SSL models initially learn representations that depend on backgrounds, but eventually learn some background invariance when trained for long enough. However, we later discuss (Section 5.1) evidence that does not find support for the latter possibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Diagnosing and Improving SwAV + Background Augmentations</head><p>As previously discussed, due to BG RM being OOD, we might generally expect BG Random, BG Swaps to be on par or better than BG RM. Our results in <ref type="table" target="#tab_4">Table 2</ref> show that while this is generally true across SSL methods and training durations, BG RM &gt; BG Random for SwAV trained for a short duration. Since BG RM and BG Random result in the same final accuracy upon longer training <ref type="table" target="#tab_4">(Table 2)</ref>, we hypothesized that there maybe early optimization difficulty arising from an interaction between SwAV's objective function and attempting to learning invariance to random natural backgrounds (in contrast with solid grayscale backgrounds in BG RM), at a stage in the pre-training when the representations are still quite poor. Consistent with this hypothesis, when BG Random is used, the loss lingers at chance early in pre-training, while the corresponding loss for BG RM falls rapidly. We reasoned that further increasing the augmentation strength of BG Random should result in higher optimization difficulty and consequently, worse performance. Consistent with this expectation, the performance of BG Random rapidly declines past a point, while the performance of BG RM remains stable, see <ref type="figure" target="#fig_1">Figure 4</ref> (right, black dashed lines).</p><p>To alleviate this issue, we propose two solutions: a) increasing the projection MLP capacity and b) warming up background augmentations. We show results from (a) in <ref type="figure" target="#fig_1">Figure  4</ref> (left, <ref type="table">Table)</ref>, finding both improved performance (a baseline effect) and removing the gap between BG RM and BG Random. Note that the default projection MLP capacity for SwAV is 2048/128. We report the results of (a) and (b) across a range of augmentation strengths in <ref type="figure" target="#fig_1">Figure 4</ref> (right) 3 . In addition to increasing MLP capacity, warming up BG Random further stabilizes performance when stronger augmentation is used. More broadly, these analyses show that additional factors such as ease of optimization play an important role in determining performance apart from whether an augmentation induces a distribution shift.</p><p>Our analyses here have broader implications. For instance, they shed new light on the role of the projection MLP and may help explain recent puzzling findings in literature; specifically, <ref type="bibr">Zbontar et al. (2021)</ref> observed that their method, Barlow Twins, works best for large dimensionality of the projection MLP and noted that "This result is quite surprising because the output...acts as a dimensionality bottleneck in our model and sets the limit of the intrinsic dimensionality of the representation". Our analyses suggest that it is important for the projection MLP to be of appropriate capacity for the pretext prediction task-more "difficult" (e.g. due to stronger augmentation) prediction losses may benefit from a higher capacity MLP.</p><p>One important limitation of current SSL methods is the long training required for competitive performance, typically 800-1000 epochs, in contrast with supervised learning. Our results in <ref type="figure" target="#fig_1">Figure 4</ref> (left) show that background augmentations enable a step forward in reducing the amount of training required for competitive performance in SSL. In these results, aside from diagnosing and fixing early optimization issues, we simply used the default settings for SwAV. However, there remains much room for improvement in conjunction with background augmentations. We briefly explore one such improvement here.</p><p>Recall that SwAV uses multi-crop augmentation, where local crops covering small parts of the image are expected to be predictive of global crops. Here, we increase the area that the small crops may cover of the full image 4 . While the small crops may feature more of the background with this change, background augmentations already prevent excessive focus on the background. This simple change improves the performance of BG RM (BG Random) from 74.1% to 74.4% (74.2%). In only 100 epochs, performance exceeds many recent high performing SSL methods trained for 800-1000 epochs, e.g. Barlow Twins (73.2%, 1000 epochs), MoCo-v3 (Chen et al., 2021) (73.8%, 800 epochs) and BYOL (74.3%, 1000 epochs). In contrast, with the same change, the SwAV baseline fails to train and the loss at the end of pre-training is at chance. Note that our default setting for SwAV does not include the modifications discussed in this section unless otherwise indicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">What is the Impact of Mask Quality?</head><p>While DeepUSPS 2 is better than or on par with weakly supervised saliency methods and even some recent supervised saliency methods, state-of-the-art supervised saliency methods like U 2 Net achieve better performance on saliency benchmarks. We perform an ablation using foreground masks generated by U 2 Net for background augmentations. While the resulting models are not truly self supervised, they can nevertheless help us understand if using better foreground masks can lead to larger performance improvements. We report</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Saliency ImageNet acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Original ReaL  <ref type="table">Table 5</ref>: Ablating the impact of the saliency method used for foreground extraction. We find nearly identical performance when we use U 2 Net, a state-of-the art saliency detector that is trained with supervision. Foreground extraction using higher quality masks results in slightly better performance when trained for fewer epochs, but this benefit disappears with longer training. All numbers are based on our implementation. Notation: MoCo-v2 (800) indicates that MoCo-v2 was trained for 800 epochs.</p><p>the results of these experiments in <ref type="table">Table 5</ref>, finding that performance is nearly identical whether DeepUSPS 2 or U 2 Net are used to extract foreground masks. Using higher quality masks leads to slightly better performance when trained for fewer epochs but this gap disappears with longer training. In later sections, we evaluate both sets of models on a range of downstream tasks to gain further insight.</p><p>While these results suggest that there may be diminishing gains to using higher quality masks, some natural questions arise, e.g. which SSL methods and background augmentations are more robust to mask quality? How does performance vary as a function of mask quality? We systematically perturb mask quality in numerous ways (via mask rotation, shearing, translation, flips and replacing masks with bounding-box masks) to answer these questions in Appendix B. Overall, we find that there is substantial robustness to mask quality. Of the SSL methods and background augmentations considered, we find that SwAV and BG Swaps are particularly robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Limited-Label Setting</head><p>While linear evaluation using 100% of ImageNet labels is a standard evaluation metric, it is also somewhat impractical due to the large amount of labels involved -after all, one of the more important goals of SSL is good performance when labeled data is highly limited. Linear evaluation in limited label settings reveals a large improvement in performance from background augmentations. For 1% and 10% labels, we use the same fixed splits of ImageNet labeled training data as in <ref type="bibr" target="#b26">Chen et al. (2020b)</ref>. We similarly find large performance benefits in semi-supervised evaluation (fine-tuning the pre-trained backbone in addition to learning a linear classifier). We report Top-1 and Top-5 accuracies in <ref type="table">Table 6</ref>.</p><p>Our first key finding is that the improvement in performance in limited label settings, for both linear and semi-supervised evaluation, is substantially larger than in 100% linear evaluation, with improvements up to 4.2%. Large gains in linear evaluation especially reflect a much better learned representation, since the backbone is frozen. Our second key finding is that BG Swaps is especially effective in limited label settings. Indeed, in the 1% setting, the gain from BG Swaps is nearly 3? the gain from BG RM in semi-supervised evaluation and ? 2? that of BG RM in linear evaluation, demonstrating the effectiveness of using negatives matched to the query's background.</p><p>Our third finding is that it is generally better to use BG Random or BG Swaps over BG RM, consistent with our previous results. Our findings here set new, stronger baselines: 60.9% Top-1 in the 1% labels setting and 72% Top-1 in the 10% labels setting. It is worth noting that ?71% is the linear evaluation baseline for MoCo-v2 using 100% of the labels. Note that our reproduction of BYOL's performance in limited label settings already improves upon the published baseline (by +4.1%, +1.8% in the 1% and 10% labels settings respectively) by adopting a much smaller learning rate for the pre-trained backbone than the classifier head-background augmentations further improve on these stronger baselines.</p><p>Finally, we note that nearly identical findings hold when we instead use U 2 Net for foreground extraction, see <ref type="table" target="#tab_5">Table A13</ref>. All models receive full pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>1% Labels 10% Labels</p><p>Top-1 Top-5 Top-1 Top-5</p><p>Supervised    <ref type="table">Table 6</ref>: Limited-Labels Setting. Background augmentations improve performance in the limited-labels setting. Linear evaluation using 100% of ImageNet labels though a standard benchmark, is a somewhat unrealistic setting. Evaluation in the more practical setting of limited-labels reveals even larger improvement in performance. We highlight performance gains due to background augmentations. Best (second best) results are in bold (underlined).</p><p>baseline (p = 0.0): 76.4 p = 0.1 p = 0.2 p = 0.3 p = 0.5  We have found that background augmentations provide a significant performance boost to a suite of high-performing SSL methods, and shrink the gap to the supervised baseline down to 0.3%. We note that most SSL methods utilize an augmentation suite that is inherited from supervised training. By designing augmentations specifically for SSL, we were able to induce a substantial increase in performance; this raises the question of whether a similar performance boost would be observed when applying background augmentations to supervised training. Interestingly, we find that background augmentations do not confer a performance benefit in the supervised setting. In <ref type="table">Table 7</ref>, we report the performance of BG RM and BG Random, sweeping over p, finding no setting that outperforms the supervised benchmark 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BG</head><p>One may wonder if this lack of improvement is an artifact of the evaluation protocol, which is different from the SSL setting, where evaluation is either by training a linear classifier on top of the frozen trunk or by fine-tuning the whole network (trunk + linear classifier) without background augmentations. We therefore, a) re-train a linear classifier without background augmentations on top of the frozen trunk (of the supervised network trained with background augmentations) and (separately) b) fine-tune the whole network without background augmentations, once again finding no performance benefit. 5. Note that BG Swaps is not applicable here since there is no concept of a negative to match. baseline (p = 0.0): 36.1 p = 0.1 p = 0.2 p = 0.3 p = 0.5  <ref type="table">Table 9</ref>: RotNet: Background augmentations do not improve over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BG</head><p>In the supervised setting, strong augmentations may require much longer training to be effective (e.g. as in the case of CutMix <ref type="bibr" target="#b120">(Yun et al., 2019)</ref>). To account for a similar possibility in the case of background augmentations, we include background augmentations in supervised training and follow a much longer training schedule (see Appendix A for details) for 300 epochs (following CutMix) and find no significant performance benefits, see <ref type="table" target="#tab_12">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">So, When do Background Augmentations Help?</head><p>Our results in the previous section suggest that the utility of background augmentations in SSL does not generalize to the supervised setting. Given the importance of augmentations to SSL (e.g., <ref type="bibr" target="#b25">Chen et al. (2020a)</ref>), these results highlight the need to evaluate and explore augmentations tailor-made for the SSL setting and are consistent with similar findings <ref type="bibr" target="#b25">(Chen et al., 2020a)</ref> for color distortion and blur augmentations. While the test bed of high performing SSL methods we have considered thus far is diverse, they share a commonality: they all use Siamese networks to compare or contrast views of images, raising the natural question of whether this is the only SSL setting where background augmentations confer an advantage.</p><p>To investigate this question, we turn to RotNet <ref type="bibr" target="#b42">(Gidaris et al., 2018</ref>)-a simple, yet surprisingly effective SSL method that is not based on a Siamese architecture nor on comparisons between images. Training a RotNet involves augmenting the data with rotated images and training a network to categorize the orientation of an image, thereby forcing the network to learn a meaningful representation to accomplish this task. We implemented background augmentations in RotNet, i.e. we either perform BG RM or BG Random followed by rotating the image (and training the network to classify the orientation). Interestingly, we found that background augmentations confer no performance benefits, see <ref type="table">Table 9</ref>. Here, BG Random and BG RM decorrelate the foreground and the background, while BG RM additionally reduces the incentive to encode background information, since a grayscale background is not informative for the pretext task of categorizing image orientation. Thus, merely decorrelating the foreground and background or disincentivizing focus on the background are not sufficient to improve semantic focus.</p><p>Based on our findings, we speculate that background augmentations are most helpful when there is a similarity comparison between images, and can help prevent the model from using the background as a shortcut to place images nearby (or far away) in embedding space which can hinder learning about the semantic content present in an image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Generality of Representations Induced by Background Augmentations</head><p>If background augmentations lead to increased focus on semantic content and decreased focus on non-robust predictors for classification (e.g., <ref type="bibr" target="#b63">Ilyas et al. (2019)</ref>), we expect that these augmentations would also lead to improved performance on out-of-distribution downstream tasks. In particular, we expect gains on those tasks which have proven especially challenging for supervised networks. Here, we discuss several such tasks, including ImageNet-9 , adversarial attacks <ref type="bibr" target="#b43">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b71">Kurakin et al., 2016;</ref><ref type="bibr" target="#b78">Madry et al., 2018)</ref>, natural adversarial examples , ImageNet-Renditions <ref type="bibr" target="#b58">(Hendrycks et al., 2021)</ref> and ReaL ImageNet <ref type="bibr" target="#b18">(Beyer et al., 2020)</ref>, finding improved performance across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Improved Robustness to Shift in Foreground-Background Statistics</head><p>ImageNet-9 (IN-9), introduced in Xiao et al. <ref type="formula">(2021a)</ref>, consists of out-of-distribution data sets that are different variations of a 9-class subset of ImageNet. The variants are designed to have different amounts of foreground and background signal, see <ref type="figure" target="#fig_2">Figure 5</ref> for examples. In the Only-BG-B and Only-BG-T variants, the foreground is removed and replaced either with a black box (Only-BG-B) or a tiled version of the background (Only-BG-T); No-FG features images with the foreground shape cut out (and discernible), while Only-FG features the foreground alone on a black background (similar to our BG RM); Mixed-Same, Mixed-Rand, and Mixed-Next, feature foregrounds pasted onto backgrounds from different images of the same class (Mixed-Same), random images (Mixed-Rand), and deterministically from the next class such that backgrounds provide systematically misleading information (Mixed-Next). If models learn to focus on the semantically meaningful foreground and ignore the background,  <ref type="table">Table 10</ref>: Robustness: Foreground-Background Shifts. Background augmentations result in large performance gains on ImageNet-9 across all SSL methods, with BG Swaps generally exhibiting similar or better performance than BG RM. We highlight the performance benefit on the variants of ImageNet-9 especially relevant to our work. All accuracies reported for background augmented SSL methods are averages of 3 independent runs (we exclude SEM to avoid clutter, see <ref type="table" target="#tab_7">Table A14</ref> for an expanded table that includes SEM). All pre-training durations correspond to respective medium settings. Note that ImageNet-9 uses only 9 classes, so chance is ?11.1%.</p><formula xml:id="formula_1">Pre-Train Duration MoCo-v2 BYOL SwAV baseline BG RM BG Swaps baseline BG RM BG Random baseline BG RM BG Random</formula><p>Med. 11.1 7.7 (-3.4) 3.8 (-7.3) 6.6 5.4 (-1.2) 4.6 (-2.0) 10.9 9.0 (-1.9) 9.3 (-1.6) Full 10.0 6.8 (-3.2) 4.4 (-5.6) 9.1 5.3 (-3.8) 4.4 (-4.7) 11.4 9.3 (-2.1) 9.0 (-2.4) we should expect classification performance to decrease for Only-BG-B and Only-BG-T, and to increase for Only-FG, Mixed-Same, Mixed-Rand, and Mixed-Next 6 . We evaluate the baseline SSL methods as well as models with background augmentations on all variants of IN-9 in <ref type="table">Table 10</ref>. As in the supervised setting (see , we found that models which perform better on the Original IN-9 also perform better across other IN-9 variants. Critically, we also found that background augmentations consistently improved performance on IN-9, especially on the images with misleading backgrounds (Mixed-X), and in some cases, enable outperforming the supervised baseline. We also found that BG Swaps consistently improved performance over BG RM. For example, on Mixed-Next, the MoCo-v2 baseline has an accuracy of 67.0%, worse than the supervised baseline's performance of 77.2%, but incorporating BG RM and BG Swaps increases this to 73.0% and 82.2%, respectively. These results demonstrate that background augmentations do indeed encourage semantic focus on the foreground, and that explicitly discouraging background focus (as in BG Swaps) is beneficial over simply removing positive signal in the background. We also note that BG Random generally confers larger improvements over BG RM.</p><p>To quantify the impact of foreground-background correlations in the learned representations, we compute the BG-Gap  as the difference between accuracies in the Mixed-Same and Mixed-Rand settings and find that background augmentations decrease  the BG-Gap in the SSL methods considered, relative to the baselines. For the baselines, we also find that the BG-Gap slightly increases when trained for longer <ref type="table" target="#tab_15">(Table 11)</ref> for BYOL and SwAV, while it slightly decreases for MoCo-v2. We speculate that this is due to the use of a large number (|Q| = 65536) of negative instances in MoCo-v2-it is possible some of the negative instances have backgrounds similar to the query q, thereby implicitly discouraging background focus. As such, SSL models do not seem to learn much background invariance when trained for longer duration. When background augmentations are used, the BG-Gap is roughly the same for shorter or longer training duration-in other words, background augmentations do not require long training to be effective. Additional results: Appendix D <ref type="table" target="#tab_7">(Tables A14, A15</ref>, A16, A17, A18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ReaL Imagenet Confirms Improvement of Semantic Focus</head><p>Next, we evaluate performance using Reassessed Labels (ReaL, <ref type="bibr" target="#b18">Beyer et al. (2020)</ref>) for ImageNet, which relabel ImageNet to better represent the semantic content of the images. Using ReaL, <ref type="bibr" target="#b18">Beyer et al. (2020)</ref> found that the gains due to many recent methods were smaller than when the original labels are used. As with the original ImageNet labels, we found that background augmentations substantially improve performance on ImageNet ReaL <ref type="table" target="#tab_4">(Table 2)</ref>, confirming that background augmentations do induce increased semantic focus rather than simply facilitating overfitting to the original ImageNet labels. In fact, the improvement on ReaL is slightly larger when trained for fewer epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Improvement on ImageNet-v2 and ObjectNet</head><p>We next evaluate performance on ImageNet-v2 <ref type="bibr" target="#b90">(Recht et al., 2019)</ref> and ObjectNet <ref type="bibr" target="#b15">(Barbu et al., 2019)</ref>. ImageNet-v2 is a test set for ImageNet and can be considered a "natural" distribution shift setting. ObjectNet is a challenging test set where the object orientation, viewpoint and background are varied in a controlled manner. We find that background augmentations confer sizeable performance benefits in both of these settings, see <ref type="table" target="#tab_4">Tables 12  and 13</ref>.</p><p>Notably, on ImageNet-v2, background augmentations enable SwAV to perform on par with the supervised baselines. Specifically, the torchvision ResNet50 baseline has an accuracy of 63.3% on ImageNet-v2, while our re-implementation of the standard, stronger baseline <ref type="bibr" target="#b45">(Goyal et al., 2018)</ref> has an accuracy of 63.8%. Additional results: Appendix D (Tables A19, A20, A21).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Natural Adversarial Examples</head><p>We next evaluate classification performance on a particularly difficult distribution shift data set: ImageNet-A, a data set of natural adversarial examples that were found to be consistently mis-classified across models. These are extremely challenging for even supervised methods with ResNet-50 accuracy at only ?2.2% . As a first experiment, we investigate whether the difficulty of natural adversarial examples partially stems from misleading signal in the background. To test this, we modify the ImageNet-A data set by removing backgrounds such that only the foreground is present (Only-FG ImageNet-A). Indeed, we find that performance of supervised ResNet-50 improves by +2.8% 7 , suggesting that some amount of the difficulty of natural adversarial examples stems from misleading information in the background. We note that the magnitude of this number must be interpreted with some caution, since this data set is also challenging for saliency detection. We next investigate the performance of standard SSL methods on this task, finding substantively improved performance relative to the supervised methods <ref type="table" target="#tab_7">(Table 14)</ref>. Despite this improvement, comparing the performance of SSL methods for the unmodified ImageNet-A vs. Only-FG ImageNet-A (see Appendix D.3) demonstrates that SSL models perform worse on the version of ImageNet-A with only foregrounds, suggesting that SSL methods still may be overly focused on backgrounds. Together with the supervised results, this suggests that background augmentations in SSL should prove helpful. Indeed, we find that they are, with all versions of background augmentations resulting in substantially improved performance on ImageNet-A. In particular, we found BG Swaps to be more effective than BG RM, suggesting the importance of using background matched negatives. These results demonstrate that part of the challenge of ImageNet-A stems from images with misleading 7. We use the same pre-trained torchvision ResNet-50 model which was used in the construction of the data set. Since images mis-classified by this particular pre-trained model comprise the data set, the ImageNet-A (Only-FG ImageNet-A) accuracy for this specific model is 0% (2.8%), though a model trained from scratch has an ImageNet-A accuracy of ?2.2%.</p><p>Background Augmentations for Self-Supervised Learning    <ref type="table" target="#tab_4">(Tables A22, A23)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Improvement on ImageNet-Renditions</head><p>We next investigate the performance on ImageNet-R <ref type="bibr" target="#b58">(Hendrycks et al., 2021)</ref>, a data set curated to measure generalization to various abstract visual renditions (e.g. paintings, embroidery, cartoons etc., see <ref type="figure">Figure A2</ref> for examples) of ImageNet classes. This is a challenging OOD data set for classifiers trained on ImageNet, since they often rely heavily on natural texture cues. Indeed, the supervised baseline accuracy for ResNet-50 is only 36.1%. We find that background augmentations confer significant performance benefits of ?2-6%, suggesting that they help with generalizing to abstract visual renditions. Additional results: Appendix D <ref type="table" target="#tab_4">(Table A24)</ref>. <ref type="bibr">et al. (2019)</ref> demonstrated that adversarial examples are partially driven by the learning of non-robust, high frequency features which can be predictive of ground-truth classification labels, but which are also highly susceptible to adversarial attacks. Since background augmentations encourage focus on semantically meaningful content in images, a natural question is whether these augmentations also confer increased robustness to adversarial perturbations. To test this, we use a popular adversarial attack: FGSM <ref type="bibr" target="#b43">(Goodfellow et al., 2015)</ref>. We found that background augmentations did indeed result in increased robustness, with BG Swaps consistently conferring a greater benefit than BG RM <ref type="table" target="#tab_23">(Table 16)</ref>, once again emphasizing the importance of penalizing focus on backgrounds. Additional results: Appendix D <ref type="table" target="#tab_4">(Table A25)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Background Augmentations Improve Robustness to Adversarial Perturbations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ilyas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Evaluation on CIFAR-10 and 100</head><p>We find that the performance benefits of including background augmentations extends to CIFAR-10 and 100, see <ref type="table">Table 17</ref>. All methods used the same protocol to be directly comparable. All models receive full pre-training. Additional results: Appendix D <ref type="table" target="#tab_4">(Table  A26</ref>).  <ref type="table">Table 17</ref>: CIFAR-10, 100. Background augmentations improve performance on linear evaluation on CIFAR-10 and 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">A Limitation of Learning Background Invariance</head><p>We have characterized the impact of background augmentations in view-invariant SSL, finding improved generalization, robustness, label and training efficiency. Here, we discuss an important limitation of our work. As previously discussed, by design SSL augmentations are meant to induce "desirable" invariances-what is desirable depends on the downstream tasks (e.g. <ref type="bibr">Purushwalkam and Gupta (2020)</ref>; <ref type="bibr" target="#b114">Xiao et al. (2021b)</ref>; <ref type="bibr" target="#b105">Tian et al. (2020b)</ref>).</p><p>Consequently, when background is informative to the task at hand, we expect poorer performance. We demonstrate this by linear evaluation on Places-205, finding that this is indeed the case, see <ref type="table" target="#tab_12">Table 18</ref>. Note that this limitation is not specific to background augmentations. Indeed, "aggressive" cropping is an integral part of the augmentation pipeline in nearly all high performing SSL methods but can be detrimental <ref type="bibr">(Purushwalkam and Gupta, 2020</ref>) like background augmentations, in similar situations. This limitation of background augmentations on domains different from intended application may be overcome by training a multi-head network with a shared backbone (as in <ref type="bibr" target="#b114">Xiao et al. (2021b)</ref>), so that one head is trained to be background invariant, while one head is not. All models receive full pre-training; foreground masks used for background augmentations were based on U 2 Net to control for mask quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Object Detection and Instance Segmentation</head><p>We report evaluation on the downstream tasks of object detection and instance segmentation, since these are common evaluations for SSL methods. However, a priori we expect background augmentations to yield only small gains in these tasks, since the models receive extensive supervised information about object identities and locations during finetuning. Indeed, identity information alone can induce strong localization ability <ref type="bibr" target="#b100">(Simonyan et al., 2013)</ref>. Consistent with our expectations, we see only small gains in these tasks in   note that it is possible that background augmentations may yield larger gains in these tasks with less training or by incorporating the augmentations into the finetuning pipeline <ref type="bibr" target="#b41">(Ghiasi et al., 2020)</ref>. Additional results: Appendix D <ref type="table" target="#tab_4">(Table A27)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Background Augmentations Increase the Shape Bias of SSL Methods</head><p>Supervised Convolutional Neural Networks (CNNs) have been found to be biased toward texture, i.e. they tend to classify based on the texture information in an image over shape, whereas humans are more shape biased; increasing the shape bias of supervised CNNs has been found to increase accuracy and robustness <ref type="bibr" target="#b38">(Geirhos et al., 2019)</ref>. Recent work <ref type="bibr" target="#b39">(Geirhos et al., 2020)</ref> has also found that many SSL methods are heavily texture biased like their supervised counterparts. We use the shape bias measure <ref type="bibr" target="#b38">(Geirhos et al., 2019)</ref> to probe the pre-trained SSL models to gain some insight. The shape bias of a model is computed using texture-shape cue conflict stimuli (the shape and texture cues in the image correspond to different ImageNet classes, e.g. see <ref type="figure">Figure A3</ref>) as the fraction of classification decisions that correspond to shape information. We find that (see <ref type="table" target="#tab_4">Table 20</ref>) while the SSL methods considered are heavily texture biased, they are less so than their supervised counterpart, with the exception of SwAV. However, the default setting of SwAV uses multi-crop with 2 global views and 6 local views; the local views may be expected to push the model to be biased toward local texture features. Consistent with this hypothesis, SwAV trained without multi-crop 8 has a shape bias of 27.4. Our second finding is that across all SSL methods, background augmentations increase shape bias (Tables 20, A28). We note that our improvements on the ImageNet-R data set, whose texture cues are OOD relative to ImageNet, may have been driven in-part by the increased shape bias of the models trained using background augmentations. Our findings  <ref type="table" target="#tab_4">Table 20</ref>: Background augmentations increase shape bias. SSL methods considered generally have a higher shape bias than the supervised baseline. SwAV deviates from this pattern due to multi-crop (SwAV w/o multi-crop shape bias: 27.4).</p><p>raise the intriguing possibility that background augmentations induce representations that are (slightly) more brain-like. All models receive full pre-training. Other works have demonstrated the importance of high-frequency information for classification, both in traditional image classification <ref type="bibr" target="#b67">(Jo and Bengio, 2017)</ref> and in the context of adversarial robustness <ref type="bibr" target="#b63">(Ilyas et al., 2019)</ref>. There have also been a number of works investigating the importance of shape vs. texture for classification decisions, both in supervised <ref type="bibr" target="#b38">(Geirhos et al., 2019;</ref><ref type="bibr">Hermann et al., 2020)</ref> and self-supervised learning <ref type="bibr" target="#b39">(Geirhos et al., 2020)</ref>. Similar to the findings in the supervised setting in <ref type="bibr" target="#b38">Geirhos et al. (2019)</ref>-that increasing the shapebias increases robustness and accuracy, we found that background augmentations increase shape-bias and also improve robustness and accuracy. While there has been much work investigating robustness properties in the supervised setting (e.g. ; <ref type="bibr" target="#b58">Hendrycks et al. ( , 2021</ref>; <ref type="bibr" target="#b43">Goodfellow et al. (2015)</ref>), the self-supervised setting has received relatively less attention. <ref type="bibr" target="#b39">Geirhos et al. (2020)</ref> characterize the robustness of several SSL models to low-level noise distortions but do not investigate other aspects of robustness nor approaches to improve semantic focus and performance. We evaluate a diverse spectrum of high performing SSL methods in 17 distribution shift settings, in addition to investigating approaches to improve robustness. Thus, our work is complementary to existing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Self-Supervised Learning. We do not make a formal distinction between self-/unsupervised learning (but see <ref type="bibr" target="#b66">Jing and Tian (2020)</ref>) and broadly discuss related work. Generally, representation learning without human-annotated labels involves solving "pretext" prediction tasks. We coarsely organize the literature as follows.</p><p>Hand-crafted pretext tasks. Early work used hand-crafted pretext tasks such as predicting image orientation (RotNet, <ref type="bibr" target="#b42">Gidaris et al. (2018)</ref>), image inpainting <ref type="bibr" target="#b86">(Pathak et al., 2016)</ref>, solving image jigsaw puzzles <ref type="bibr" target="#b83">(Noroozi and Favaro, 2016)</ref>, denoising <ref type="bibr" target="#b106">(Vincent et al., 2008)</ref> and cross-channel <ref type="bibr" target="#b129">(Zhang et al., , 2017d</ref> auto-encoding for representation learning. Combining multiple pretext tasks <ref type="bibr" target="#b31">(Doersch and Zisserman, 2017)</ref> and using larger networks  can improve performance.</p><p>Learning view invariance. While hand-crafted pretext tasks have been shown to be useful for learning representations useful for downstream tasks, their performance has been far from their supervised counterparts. Learning view-invariant representations has recently been a fruitful direction in SSL; such approaches date back to <ref type="bibr" target="#b16">Becker and Hinton (1992)</ref>. We coarsely group such works based on how trivial representations are avoided.</p><p>? Contrastive learning. Contrastive learning <ref type="bibr" target="#b48">(Hadsell et al., 2006</ref>) is a framework for learning representations from data organized into similar/dissimilar pairs. Contrastive learning prevents trivial representations through use of dissimilar pairs and has been a popular design choice in SSL <ref type="formula">(</ref> ? Clustering. A number of SSL methods have avoided trivial representations through clustering <ref type="bibr" target="#b13">(Asano et al., 2020;</ref><ref type="bibr" target="#b21">Caron et al., 2018</ref><ref type="bibr" target="#b22">Caron et al., , 2019</ref><ref type="bibr">Caron et al., , 2020</ref><ref type="bibr" target="#b65">Ji et al., 2019)</ref>. There has also been work <ref type="bibr" target="#b73">(Li et al., 2021b;</ref><ref type="bibr" target="#b131">Zhuang et al., 2019)</ref> that bridges clustering and contrastive learning approaches.</p><p>? Other. Methods such as BYOL <ref type="bibr">(Grill et al., 2020)</ref>, SimSiam <ref type="bibr" target="#b27">(Chen and He, 2020)</ref> and Barlow Twins <ref type="bibr">(Zbontar et al., 2021)</ref> are not explicitly contrastive nor based on clustering and prevent trivial representations in other ways.</p><p>While we compare performance with respect to numerous SSL methods to situate our work in literature, we note that we do not propose any new SSL methods. Rather, we improve upon the core ingredient of the best performing methods: the augmentation pipeline. We choose one SSL method from each coarse grouping of the literature to form a diverse test bed of SSL methods, so as to characterize when background augmentations can or cannot confer benefits, as well as to demonstrate the generality of our results. We show that learning background invariance improves performance, robustness and label efficiency across a diverse spectrum of high-performing SSL methods. Importantly, our extensive analyses led to insights that allowed us to improve performance beyond a plug-and-play approach. While we focus on view-invariant SSL approaches that differently augment the same image to generate views, background augmentations can also be applied to approaches that use different frames from video to generate views (e.g. <ref type="bibr">Zhuang</ref>  Analyzing and Improving SSL Augmentation Pipelines. The augmentation pipeline for most high-performing SSL methods is similar. A number of recent studies have focused on analyzing and improving this pipeline, e.g. <ref type="bibr" target="#b103">Tamkin et al. (2021)</ref> learn the augmentations jointly with the contrastive learning objective; <ref type="bibr" target="#b105">Tian et al. (2020b)</ref> use labeled data to learn color spaces which are then split to generate views and also characterize ImageNet acc. vs. augmentation strength for many augmentations. <ref type="bibr">Purushwalkam and Gupta (2020)</ref> investigate invariance to occlusion, viewpoint, and category instance and show that common SSL pipelines encourage occlusion invariance-a useful property for object recognition tasks. <ref type="bibr" target="#b105">Tian et al. (2020b)</ref> observe on a synthetic toy dataset that the background can overwhelm the foreground, but do not investigate further nor propose a solution.</p><p>We show that background augmentations improve semantic focus in representations, leading to better generalization and robustness. Of particular note, <ref type="bibr" target="#b98">Selvaraju et al. (2020)</ref> also aims to improve focus on semantically meaningful content in images; they do so by constraining the crops used in the SSL augmentation pipeline to contain the object of interest as determined by a saliency method. We also investigate the impact of constraining crops to contain the salient object, and find that it does not improve performance (Appendix C.6) on top of background augmentations. Critically, in contrast with our work, <ref type="bibr" target="#b98">Selvaraju et al. (2020)</ref> relies on a saliency detector trained using ImageNet class labels making this method not truly self-supervised. Note that <ref type="bibr" target="#b98">Selvaraju et al. (2020)</ref> also investigate using an "attention" loss computed using Grad-CAM <ref type="bibr" target="#b97">(Selvaraju et al., 2017)</ref>, which we discuss below.</p><p>"Copy-Paste" Augmentations. There is a long history of work, largely in the supervised setting, that has investigated the use of "copy-paste" augmentations in which "copied" foregrounds are "pasted" onto backgrounds, generally with the aim of generating more labeled data. We draw heavy inspiration from this literature. Copy-paste has been used for learning optical flow <ref type="bibr" target="#b33">(Dosovitskiy et al., 2015)</ref>, instance detection <ref type="bibr" target="#b36">(Dwibedi et al., 2017)</ref>, object detection <ref type="bibr" target="#b40">(Georgakis et al., 2017;</ref><ref type="bibr" target="#b35">Dvornik et al., 2018)</ref>, text localization <ref type="bibr" target="#b47">(Gupta et al., 2016)</ref> and instance segmentation <ref type="bibr" target="#b91">(Remez et al., 2018;</ref><ref type="bibr" target="#b41">Ghiasi et al., 2020)</ref>. In these works, the segmentation masks required for copying are obtained from human annotation or using networks trained in a supervised manner to generate them. This implicit or explicit reliance on human annotation has been an obstacle limiting the application of copy-paste outside of the supervised setting where it is widely used.</p><p>Recent work <ref type="bibr" target="#b130">(Zhao et al., 2021</ref>) took a first step in the SSL setting by using a heuristic saliency method to generate a mask, and applied a "copy-paste" augmentation similar to BG RM, finding improved performance on ImageNet linear classification accuracy. However, the gain achieved in their best performing SSL method (MoCo-v2+DiLo-RBD) is only +0.2% (see <ref type="table" target="#tab_4">Table 2</ref>). Thus, it remains unclear if such augmentations can significantly benefit SSL, especially high performing SSL methods-indeed, in <ref type="bibr" target="#b130">Zhao et al. (2021)</ref>, the gains rapidly and monotonically decline with the baseline SSL method's performance. Thus, not only is it unclear if copy-paste can significantly benefit SSL, it is unclear when and how such augmentations confer benefits. Further, the impact of such augmentations on downstream tasks is also unclear, though <ref type="bibr" target="#b130">Zhao et al. (2021)</ref> report small gains (?0.4-0.6 AP %) on object detection and instance segmentation tasks.</p><p>In contrast, in our work, a) we develop a completely unsupervised method to generate high quality masks and demonstrate the utility of background augmentations in conferring large performance benefits (?1-2%) across a spectrum of high performing SSL methods (e.g. we obtain a 7? larger gain of +1.4 on MoCo-v2 using BG RM), b) we systematically characterize when and how background augmentations confer benefits: it is not sufficient to merely decorrelate the foreground and background nor to disincentive focus on the background; rather, background augmentations confer benefits when there is a similarity comparison between images in view-invariant SSL, where the background maybe used as a shortcut, c) Contrary to <ref type="bibr" target="#b130">Zhao et al. (2021)</ref>, we show that using natural random backgrounds (BG Random) can result in better performance, d ) further, our insights on how background augmentations work enable us to develop a novel, more effective background augmentation method (BG Swaps), leading to even larger performance gains (+1.8) (a 9? larger gain on MoCo-v2), e) we show how benefits from background augmentations may be hidden by implementation choices, f ) Perhaps most critically, we focus on generalization in OOD settings, robustness, label and training efficiency, g) we also characterize the limitations of background augmentations.</p><p>We note that in our work, we use the term "background augmentation" rather than "copy-paste" augmentation, since our purpose is to discourage focus on the backgrounds, rather than creating more labeled data for the foreground (e.g. <ref type="bibr" target="#b41">Ghiasi et al. (2020)</ref>; <ref type="bibr" target="#b36">Dwibedi et al. (2017)</ref>). However, more broadly, even in absence of labels our work enables foreground augmentations for SSL, making it possible to create images with multiple objects in a controlled manner.</p><p>Mixing Augmentations. Background augmentations have some resemblance to mixing augmentations used in the supervised setting, e.g. mixup <ref type="bibr" target="#b124">(Zhang et al., 2018a)</ref>, CutMix <ref type="bibr" target="#b120">(Yun et al., 2019)</ref>, which mix information contained in images. In contrast with background augmentations, these methods a) mix images ignoring the semantic relevance of parts of an image and b) also mix the corresponding labels. Extending such mixing augmentations to SSL is an orthogonal improvement to our work and may be a fruitful line of inquiry for future work. Relatedly, but conversely, mixing augmentations that consider semantic relevance of parts of an image in the supervised setting, could also be a fruitful direction.</p><p>"Attention" Loss. <ref type="bibr" target="#b98">Selvaraju et al. (2020)</ref> investigate the impact of using an additional "attention" loss that encourages similarity between the Grad-CAM heatmap of the query and its saliency mask. The Grad-CAM heatmap is computed by additionally encoding the masked (background removed) key using the teacher network and computing the spatial regions in the query that the network relies on to map the masked key to the query, by back-propagation on the activations in the last convolutional layer. In contrast, our work, besides not relying on a saliency detector trained using supervised information, is much simpler-simply adding an augmentation to the data augmentation pipeline and thereby more agnostic to the specific method and yet, is highly effective.</p><p>"Hard" Instances. Our work is also related to the literature on "hard negatives", which have been explored recently in the context of contrastive SSL to improve learning <ref type="bibr">(Kalantidis et al., 2020;</ref><ref type="bibr" target="#b93">Robinson et al., 2021;</ref><ref type="bibr" target="#b20">Cai et al., 2020)</ref>. In this literature, hard negative instances are "mined" using distances in the embedding space. In a broader sense, creating negatives whose background matches the query (as we do in BG Swaps) can also be considered "hard negatives" and in a similar vein, one could consider the positive pair (q and k + ) with different backgrounds as "hard positives". Mining for hard negatives (or even hard positives) is an orthogonal improvement to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>We investigated the potential of background augmentations for self-supervised learning. We explored several variants of background augmentations, including those with constant gray backgrounds (BG RM), randomized natural backgrounds (BG Random), and a novel method that uses matched backgrounds between queries and negatives (BG Swaps). Across view-invariant SSL methods, we found that background augmentations result in sizeable performance improvements ?1-2% on ImageNet linear classification, enabling performance on par with the supervised baseline. In the limited-labels setting, we found even larger performance benefits. Across SSL methods, we found that BG Random and BG Swaps often lead to larger performance improvements than BG RM due to being more in-distribution. However, other factors such as ease of optimization (Section 4.6) also play a role. Background augmentations take a large step forward in reducing the amount of training required for competitive performance in SSL, e.g. enabling performance on par with or better than many recent SSL methods trained for 800-1000 epochs in only 100 epochs.</p><p>Interestingly, we found that background augmentations conferred no benefit in supervised training nor in RotNet, an SSL method not based on view-invariance. These results demonstrate the importance of designing augmentations tailored to the SSL setting, especially view-invariant SSL. These findings are timely and relevant to the community, since view-invariant SSL methods are currently the best performing methods across a range of architectures and downstream tasks.</p><p>As SSL methods shrink the gap to their supervised counterparts, it has become increasingly important to characterize the limitations of performant SSL methods as well as to understand their robustness and generalization properties in a more comprehensive manner. Across state-of-the-art SSL methods, we found that background augmentations enable increased model focus on semantically meaningful content and lead to improved robustness to numerous distribution shifts including ImageNet-9, natural adversarial examples, ImageNet-R, adversarial attacks, as well as natural distribution shift. Our analyses revealed an increased shape bias for SSL models trained with background augmentations, which may have driven some of the improvement, especially on the ImageNet-R data set where texture cues are OOD relative to ImageNet, requiring representations to better encode shape cues in an image for improved performance. All of these results raise the intriguing possibility that background augmentations induce representations that are (slightly) more brain-like. Future work could investigate this idea further, potentially by comparing representations with neuronal recordings <ref type="bibr" target="#b115">(Yamins et al., 2014)</ref>. Relatedly, neural networks are known to be easily fooled by objects in unusual poses <ref type="bibr" target="#b12">(Alcorn et al., 2019)</ref>, unlike humans. An interesting line of investigation for future work could be to learn robustness to unusual poses by foreground augmentations (e.g. using foreground masks to augment data with rotated objects). Indeed, such approaches have been adopted in the supervised setting (e.g. <ref type="bibr" target="#b36">Dwibedi et al., 2017;</ref><ref type="bibr" target="#b41">Ghiasi et al., 2020)</ref> when segmentation masks for foreground objects are available. Our work enables such approaches in the absence of human-annotation, opening up new possibilities beyond our application here in background augmentations.</p><p>It is worth noting that background augmentations as implemented here are specific instantiations of encouraging background invariance-we focused on extensively evaluating simple instantiations. Specifically, using a saliency method to separate foreground and background could be problematic in more complex scenes. Remarkably, though ImageNet has a large share (e.g. <ref type="bibr" target="#b102">Stock and Cisse, 2018;</ref><ref type="bibr" target="#b18">Beyer et al., 2020)</ref> of multi-object multi-class images, the simple approach we have taken here works well. More sophisticated approaches hold the potential for further improvement, e.g. one straightforward approach could be to copy foregrounds from simple images using saliency detection and paste multiple objects into images to create more complex scenes in controlled manner, either offline or on-the-fly.</p><p>There has been increasing recent interest <ref type="bibr" target="#b29">(Chen et al., 2021;</ref><ref type="bibr" target="#b24">Caron et al., 2021;</ref><ref type="bibr" target="#b72">Li et al., 2021a)</ref> in self-supervised learning for Vision Transformers (ViTs, <ref type="bibr" target="#b34">Dosovitskiy et al. (2021)</ref>). Future work could investigate whether background (or foreground) augmentations can benefit SSL for ViTs. Yet another interesting line of inquiry could be to investigate the impact of background augmentations in high performing semi-supervised learning methods (e.g. FixMatch <ref type="bibr">(Sohn et al., 2020)</ref>).</p><p>Organization of Supplementary Information. Appendix A contains implementation details and evaluation protocols. In Appendix B, we characterize the impact of foreground mask quality by systematically distorting the masks in numerous ways. Appendix C contains additional ablations. Appendix D contains additional results, including characterization of robustness to image corruptions and evaluations of models where background augmentations used masks generated by a supervised saliency method, U 2 Net. Table captions throughout the appendices have additional redundancy to increase ease of reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Implementation Details</head><p>In this Appendix, we discuss pre-training details for each of the SSL methods in our test bed and the protocols followed for downstream evaluations. We also discuss the implementation details of the unsupervised saliency detection method, DeepUSPS 2 .</p><p>General Settings. All experiments use the ResNet-50 <ref type="bibr" target="#b51">(He et al., 2016)</ref> architecture unless otherwise indicated. All specified learning rates are base learning rates for a batch size of 256 unless otherwise indicated. Learning rates are obtained by linearly scaling the base learning rates as base learning rate ? batch size/256. We closely follow the implementation details of the original works where possible. Settings not mentioned here are identical to respective original works. All models were implemented using PyTorch <ref type="bibr" target="#b85">(Paszke et al., 2019)</ref>, torchvision <ref type="bibr" target="#b79">(Marcel and Rodriguez, 2010)</ref> and NumPy <ref type="bibr" target="#b50">(Harris et al., 2020)</ref>. All cosine schedules <ref type="bibr" target="#b76">(Loshchilov and Hutter, 2017)</ref> are half-period without restarts. Binary foreground masks used in background augmentations are obtained by thresholding saliency predictions between 0-1 using a threshold value of 0.9; by default, we use our unsupervised saliency detector DeepUSPS 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pre-Training of SSL Methods</head><p>MoCo-v2. We use a larger (than the standard 256) batch size of 1024 (distributed across 32 GPUs) with a 20 epoch warmup for 220 (800) epochs in the medium (full) setting. These setting were chosen to speed up pre-training while matching (or improving upon) the reported performance at a similar number of epochs in <ref type="bibr" target="#b28">Chen et al. (2020c)</ref>.</p><p>Background Augmentations. In BG RM, backgrounds were removed in q and k + independently with p = 0.2. In BG Swaps, p pos = p neg = 0.2 and crops in RandomResizedCrop (RRC) were constrained to include FG min = 0.1 fraction of the foreground, by rejection sampling. Concretely, RRC parameters were sampled up to 10 times to satisfy constraints, defaulting to a CenterCrop if no satisfactory parameters are sampled. See Appendix C.6 for additional discussion and ablations. In BG Swaps, the background matched negatives are batched together with the positive keys during forward pass through the teacher/key network; only the positives keys are subsequently placed in the queue Q.</p><p>BYOL. We used a batch size of 4096 (distributed across 64 GPUs). Our implementation used synchronized batch-normalization layers (synced per group of 8 GPUs) using the apex 9 library. In RRC, we used a scale setting of (0.2, 1.0). We obtained similar results in the medium setting (300 epochs) when we instead used a) synchronized batch-normalization layers across 9. https://github.com/NVIDIA/apex <ref type="figure" target="#fig_0">Figure A1</ref>: Higher pre-training loss but better generalization. Background augmentations generally lead to a higher final pre-training loss as they make the objective function more "difficult", but can lead to better generalization.</p><p>all GPUs (global sync) or b) used a scale setting of (0.08, 1.0) as in <ref type="bibr">(Grill et al., 2020)</ref>, but this may be different in the full setting (1000 epochs), potentially further improving on the performance we obtained in the full setting using background augmentations.</p><p>Background Augmentations. In BG RM, p = 0.15 and FG min = 0.05, while in BG Random, p = 0.05 and FG min = 0.05.</p><p>SwAV. Pre-training was identical to original implementation <ref type="bibr">(Caron et al., 2020)</ref>. Background Augmentations. In BG RM, p = 0.25 and FG min = 0.15, while in BG Random, p = 0.2 and FG min = 0.15. We only apply background augmentation to the global views in multi-crop.</p><p>RotNet. Pre-training procedure was largely similar to the original paper <ref type="bibr" target="#b42">(Gidaris et al., 2018)</ref>. When background augmentations were used, BG RM and BG Random were applied before the default augmentations of RandomResizedCrop, RandomHorizontalFlip and Rotation. Models were pre-trained for 30 epochs with a learning rate of 0.01, with a step schedule (10, 20) and a decay factor of 0.1 using SGD with momentum=0.9, a batchsize of 192 and weight decay of 5 ? 10 ?4 .</p><p>Discussion: Loss and Background Augmentations. Background augmentations make the objective function more "difficult", leading to a higher final pre-training loss but can lead to better generalization, see <ref type="figure" target="#fig_0">Figure A1</ref>. This is consonant with previous work  finding that the loss of the pretext task is not necessarily monotonically related to generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training DeepUSPS 2</head><p>We trained a DRN-D-105 <ref type="bibr" target="#b119">(Yu et al., 2017)</ref> network via BYOL for 500 epochs to an accuracy of 73.9% with the following settings: base learning rate of 0.3, weight decay of 1 ? 10 ?6 and momentum coefficient of 0.99 for the teacher network. All other settings use the defaults for training BYOL. We then use this network as an initialization to train a saliency detector. Note that previous work instead initialized with a network trained using ImageNet class labels as well as CityScapes <ref type="bibr" target="#b30">(Cordts et al., 2016)</ref> segmentation labels. We train this network following the procedure from DeepUSPS, but in phase 1 of training, we use a learning rate of 6 ? 10 ?6 instead of the default value of 1 ? 10 ?6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Linear Evaluation on ImageNet</head><p>Linear evaluation protocol was largely similar to original work. For MoCo-v2 and SwAV, we evaluate with a larger batch size to speed up evaluation. We train the linear classifier for more epochs in the case of MoCo-v2 and BYOL to reduce variability in the results. Note that warmup is not required, but for simplicity we opted to keep the training procedure close to standard supervised training.</p><p>MoCo-v2. The linear classifier was trained for 120 epochs, with a step schedule of 60, 80, 100 and a decay factor of 0.1, with a warmup of 10 epochs, a batch size of 2048 (distributed across 32 GPUs). Parameters in the backbone were frozen to the pre-trained values.</p><p>BYOL. The linear classifier was trained for 140 epochs, with a step schedule of 80, 100, 120 and a decay factor of 0.1. We used a base learning rate of 0.2 and a batch size of 1024 distributed across 16 GPUs. Parameters in the backbone were frozen to the pre-trained values.</p><p>SwAV. The linear classifier was trained for 100 epochs with 5 warmup epochs using a batch size of 2048 (distributed across 32 GPUs) and a cosine schedule. Parameters and buffers in the backbone were frozen to the pre-trained values.</p><p>RotNet. The linear evaluation procedure was largely similar to the original paper <ref type="bibr" target="#b42">(Gidaris et al., 2018)</ref>. A linear classifier was trained on top of Conv5 layer using SGD with Nesterov momentum over 35 epochs using a batchsize of 256 and a momentum of 0.9 with a learning rate of 0.1, a step schedule of (5, 15, 25) and weight decay of 5 ? 10 ?4 .</p><p>General Settings. Following common protocol, pre-processing during training consists of RandomResizedCrop and RandomHorizontalFlip followed by normalization. The preprocessing on validation images consists of Resize to size 256 along the shorter edge of the image, followed by CenterCrop to size 224?224 and normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Background Augmentations in the Supervised Setting</head><p>By default, training followed the settings specified in Methods (Section 2). Here, we discuss implementation details for a) re-training a classifier without background augmentations and b) finetuning the whole network without background augmentations and c) longer training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Re-training a linear classifier w/o background augmentations</head><p>We trained a linear classifier from scratch on top of the frozen trunk without background augmentations using standard pre-processing and data augmentation. We used SGD with momentum of 0.9 , a batchsize of 4096 with a base learning rate of 0.01 and a cosine schedule over 40 epochs and a weight decay of 1 ? 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Finetuning w/o background augmentations</head><p>We finetuned the network without background augmentations, using standard preprocessing and data augmentation. We used SGD with momentum of 0.9 , a batchsize of 4096 with a base learning rate of 0.0005 and a cosine schedule over 20 epochs and a weight decay of 1 ? 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Painting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sculpture Embroidery</head><p>Origami Cartoon Toy <ref type="figure">Figure A2</ref>: Examples from ImageNet-Renditions. Image from <ref type="bibr" target="#b58">Hendrycks et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Longer Training</head><p>Training followed the settings specified in Methods (section 2), with the following changes: following CutMix <ref type="bibr" target="#b120">(Yun et al., 2019)</ref>, training was for 300 epochs with a step schedule of (75, 150, 225).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Evaluation in Limited-Labels Setting</head><p>For consistency with previous work, we use the same fixed splits 10 as in <ref type="bibr" target="#b26">Chen et al. (2020b)</ref> for the 1% and 10% labels settings of ImageNet training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 Linear Evaluation</head><p>For simplicity, we train a linear classifier using the same settings as in the corresponding 100% labels linear evaluation, except that we use the training data from the corresponding split (1% or 10%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.2 Semi-Supervised Evaluation</head><p>MoCo-v2/BYOL: We finetuned the network for 50 epochs with a step schedule (30, 40) with a decay factor of 0.1, with a batch size of 256 and no weight decay. For MoCo-v2, in the 1% (10%) label setting, we used a learning rate of 1.0 (0.3) for the classifier head and a learning rate of 1 ? 10 ?4 (1 ? 10 ?4 ) for the trunk. For BYOL, in the 1% (10%) label setting, we used a learning rate of 1.0 (0.1) for the classifier head and a learning rate of 1 ? 10 ?3 (1 ? 10 ?3 ) for the trunk. SwAV: we use the same settings as in the original implementation <ref type="bibr">(Caron et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Robustness Evaluations</head><p>Robustness evaluations do not involve any additional training-they use the same network (backbone and linear classifier) used for linear evaluation on ImageNet in the 100% labels setting. Note that it is common for robustness benchmarks to use the pre-trained model from torchvision 11 as the supervised baseline. We additionally report metrics using our reimplementation of the standard, stronger supervised baseline in <ref type="bibr" target="#b45">Goyal et al. (2018)</ref>. We follow 10. https://github.com/google-research/simclr/tree/master/imagenet_subsets 11. https://github.com/pytorch/vision/tree/master/torchvision/models the pre-processing protocols from respective original works; unless otherwise mentioned, this is simply the standard way that ImageNet validation images are pre-processed (Appendix A.3).  <ref type="figure">Figure A3</ref>: Examples with and without texture-shape cue conflict.</p><p>ImageNet-C. ImageNet-C <ref type="bibr" target="#b55">(Hendrycks and Dietterich, 2019)</ref> consists of 75 test data sets; 15 types of corruptions from four main categories (noise, blur, weather, digital) are applied to ImageNet validation images to generate the test images. Each corruption type has five levels of severity. We report the average performance on the four main categories of corruptions in Appendix D.4. Pre-processing steps are CenterCrop and normalization as in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Shape-Bias Evaluation</head><p>The shape-bias <ref type="bibr" target="#b38">(Geirhos et al., 2019</ref>) of a model is computed using texture-shape cue conflict stimuli 17 (the shape and texture cues in the image correspond to different ImageNet classes, e.g. see <ref type="figure">Figure A3</ref>) as the fraction of classification decisions that correspond to shape information; this computation only considers the subset of "correctly" classified imageseither the shape or texture category are correctly classified. Images are pre-processed as in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Linear Evaluation on CIFAR-10, 100</head><p>All methods were evaluated using the same protocol for fair comparison. A linear classifier was trained with SGD with momentum 0.9 for 100 epochs with a base learning rate of 0.08, a batch size of 32 and a cosine learning rate schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Linear Evaluation on Places-205</head><p>All methods were evaluated using the same protocol for fair comparison. A linear classifier was trained with SGD with momentum 0.9 for 28 epochs with a base learning rate of 1.0, a batch size of 256 and a step schedule of 7, 14, 21 and a decay factor of 0.1. Weight decay was set to 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 Object Detection and Instance Segmentation</head><p>We follow standard protocol across all SSL methods: a) VOC detection: We finetuned a Faster R-CNN <ref type="bibr" target="#b92">(Ren et al., 2015)</ref>   VOC: For MoCo-v2 and SwAV, we followed the settings from their corresponding original papers. In the case of BYOL, we followed the settings of MoCo-v2 with one deviation: we used a base learning rate of 0.08. COCO: we used the default settings of MoCo-v2 for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Characterizing the Impact of Mask Quality</head><p>While the results in Section 4.7 show that there may be diminishing gains to using higher quality masks than those provided by DeepUSPS 2 , it does not give us a clear picture of the impact of mask quality. For example, one may wonder,</p><p>? How does performance vary as a function of mask quality?</p><p>? Which SSL methods and background augmentations are more robust to mask quality?</p><p>To answer these questions, and in an attempt to gain further insight into the mechanism by which background augmentations improve representations, we systematically perturb the quality of the foreground masks. In these experiments, we use U 2 Net to generate the initial foreground masks (that we then perturb) to minimize the possibility of starting with a poor mask and maintain greater control over the quality of a perturbed mask.</p><p>We perturb the masks in numerous ways across a range of distortion strengths/levels. The distortions we consider are: a) rotation (see <ref type="figure" target="#fig_1">Figure A4</ref> for examples), b) shearing ( <ref type="figure" target="#fig_2">Figure  A5</ref>), c) translation ( <ref type="figure">Figure A6)</ref>, d ) horizontal flips and e) using bounding-box masks instead of the original mask ( <ref type="figure">Figure A7</ref>). We expect mask translation and using bounding-box masks to be particularly challenging.  <ref type="table">Table A1</ref>: Mask Distortion: Rotation. Background augmentations are robust to substantial mask noise induced via rotations of the foreground mask. Of note, BG Swaps maintains a large performance benefit even for strong distortions. We highlight the ? relative to the baselines with no background augmentations.</p><p>We characterize the dependence on mask quality for BG RM across all the view-invariant SSL methods in our test bed; in the case of MoCo-v2, we additionally include BG Swaps. We apply the respective distortion to each mask, every time it is used in a background augmentation. All experiments are in the respective medium duration settings; we report ImageNet accuracy. We make the following observations:</p><p>Low quality masks can still be beneficial. Overall, we find that there is substantial robustness to mask quality. In many instances, only a little of the foreground remains in view, yet background augmentations maintain improved performance over the baseline. We find that mask translation and using bounding-box masks are particularly challenging distortions as expected-surprisingly, some performance benefits persist (but quickly disappear with higher distortion strength).</p><p>SwAV and BG Swaps are quite robust to mask quality. We find that BG Swaps is far more robust to mask quality compared to BG RM across all variants and degrees of distortions, showcasing the robustness of this augmentation method. For example, across rotation, shearing, translation and flip distortions, the improvement due to BG Swaps is ?2-3? the improvement due to BG RM in the respective strongest distortion levels; further, when the foreground mask is replaced with a bounding-box mask, the benefit due to BG RM disappears, while BG Swaps retains a significant performance benefit.</p><p>Among the SSL methods in our test bed, SwAV appears to be most robust to mask quality, managing to maintain significant performance benefits even with strong mask distortion. We speculate that this may be linked to use of multi-crop augmentation wherein local crops are expected to be predictive of global crops. When background augmentations are applied using distorted masks, only a small part of the foreground may be featured in a view-much like a local crop in multi-crop augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Implementation Details.</head><p>Every mask was perturbed prior to background augmentations in every epoch. Rotation. Each mask was subject to random rotation sampled between (?max rot., +max rot.). Shearing. Each mask was subject to shearing independently along x and y coordinates with a      <ref type="table">Table A5</ref>: Mask Distortion: Bounding-Box Mask. BG Swaps is robust even to replacing the mask with a bounding-box mask. Notably, SwAV also manages to retain some performance benefit even in this extreme setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Ablations</head><p>Here, we report additional ablations of design choices and hyperparameter settings. We also provide additional information regarding ablations in the main text. To control for mask quality, we use U 2 Net to generate foreground masks unless otherwise indicated. We report ImageNet accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Augmentation Strength</head><p>We ablate the strength of background augmentations and find that p ? [0.1, 0.3] is generally a good setting. For MoCo-v2, as previously discussed, though BG RM is more (Out-of-Distribution) OOD than BG Random, the presence of negatives in the queue Q with (gray) backgrounds similar to q offsets this and results in better performance than BG Random across all augmentation strengths, see <ref type="table">Table A6</ref>. As discussed in Section 4.4, BG Swaps overcomes the OOD issue of BG RM by using cached random natural backgrounds in q and k + with a probability p pos and additionally includes an extra negative whose background matches q with a probability p neg . As can be seen in <ref type="table" target="#tab_35">Table A7</ref>, this results in substantially improved performance at each level of p pos over the corresponding level of BG Random. Performance is more sensitive to p pos than to p neg and p pos p neg is generally a reasonable setting. Performance as a function of augmentation strength for BYOL and SwAV are shown in Tables A8 and A9. In all of the above ablations, FG min = 0, i.e. no constraints were imposed on RRC regarding the foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Do Multiple Matched Negatives Help?</head><p>We investigated using multiple background matched negatives in BG Swaps but found that it did not confer further improvements in performance. We obtained 68.9% ImageNet accuracy using 5 matched negatives vs. 68.8% using 1 matched negative, suggesting that there may be little benefit to increasing the number of background matched negatives. In these experiments, there were no background augmentations in q, only in k + and k ? ; p pos = p neg = 0.2 and background augmented negatives matched the background of k + . Baseline (72.7) p = 0.05 p = 0.1 p = 0.15 p = 0.2 p = 0.25 Baseline (72.2) p = 0.05 p = 0.1 p = 0.15 p = 0.2 p = 0.25 p = 0.3 To answer this question, we contrasted independent background augmentations in BG Swaps in k + and k ? with coupled background augmentations in k + and k ? . We observed identical performance of 68.8% in each case. Other experiment settings as in C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BG RM</head><p>C.4 Is it Better for a Negative's Background to Match q or k + ?</p><p>We observed similar results in both cases a) background matches q (68.9%) and b) background matches k + (68.8%). Other experiment settings as in C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Order of Augmentations</head><p>As noted in Section 2.2, by default, we apply background augmentations after all other augmentations in the respective pipeline. Here, we show that background augmentation before all other augmentations produces similar results. We apply BG Random in SwAV with p = 0.1 and find corresponding accuracies of 73.4% (before) vs. 73.2% (after ). Similarly, applying BG RM with p = 0.1 in MoCo-v2, we find corresponding accuracies of 69.6% (before) vs. 69.3% (after ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Influence of Random Crop</head><p>RandomResizedCrop (RRC) is a critical part of current SSL pipelines. Indeed, replacing RRC with CenterCrop results in very poor accuracy, 26.8% for MoCo-v2, see  <ref type="table" target="#tab_15">Table A11</ref>: Impact of including more foreground in views. Constraining RRC to include more of the foreground in MoCo-v2, we see that it can be beneficial when the setting of background augmentation (BG Swaps) strength is lower than optimal, but confers little (if any) benefit on top of the optimal setting of background augmentation strength.</p><p>view. Concretely, we generate the parameters for RRC imposing one additional constraint: that FG min fraction of the foreground be present in the resulting crop. The foreground area is obtained from the corresponding binary foreground mask. Thus, no constraint corresponds to FG min = 0. Too large a value of FG min can be expected to hurt performance, since it overly constrains RRC which is useful for inducing desirable invariances (e.g. occlusion or scale invariance). Crop parameters are sampled in the standard way, defaulting to a CenterCrop if the sampled RRC parameters are rejected 10 times. We find that this strategy of imposing a foreground constraint can help when the setting of background augmentation strength is lower than optimal, but it adds little benefit (if any) on top of optimal settings, see <ref type="table" target="#tab_15">Table A11</ref>. As another example, consider SwAV, where BG RM (BG Random) results in an accuracy of 73.6% (73.7%) with FG min = 0, see <ref type="table">Table  A9</ref>. We find no benefit when we impose a constraint of FG min = 0.15, with corresponding accuracies for BG RM (BG Random) of 73.7% (73.5%), see <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Sensitivity of SwAV</head><p>Ease of Optimization. We observe that SwAV is more sensitive to high amount of background augmentations when using using BG Random, with performance degrading less gracefully than BYOL or MoCo-v2 when the strength of background augmentation is high. We linked this behavior to the difficulty of the optimization task of learning invariance to random natural backgrounds in conjunction with SwAV's objective function. As discussed in Section 4.6, this can be alleviated by increasing the capacity of the projection MLP or by warming up the background augmentations, resulting in stable performance even at very strong augmentation strength, see <ref type="figure" target="#fig_1">Figure 4</ref>. We linearly warmed up background augmentation strength over 10 epochs.</p><p>Crop Scale Ablation. Parametrizing the scale setting for local and global crops as (0.05, s) and (s, 1), the default setting uses s = 0.14. As discussed in Section 4.6, we find that increasing s can help improve performance when used in conjunction with background augmentations. For BG RM (BG Random), we used s = 0.26 (s = 0.2). We used a 5 epoch linear warmup of background augmentation strength. We found that increasing s to 0.2 for the SwAV baseline, i.e. without background augmentations, results in failure-the loss at the end of pre-training is at chance. The projection MLP capacity was set to 4096/256 in all cases.</p><p>Temperature Sensitivity. We also note that we observed high sensitivity to the temperature setting in SwAV, in contrast with MoCo-v2. Future work could investigate the source of this sensitivity, potentially further improving performance.</p><p>General Settings. All ablations discussed here (Appendix C.7, Section 4.6) use defaults for remaining settings except the experiments with the default projection MLP capacity (2048/128)-these numbers are from the ablation of augmentation strength in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 Does BN Adaptation Help?</head><p>Due to strong augmentation during SSL pre-training, the statistics of images during pretraining may be different from those in downstream application, e.g. linear evaluation on ImageNet. Intuitively, one might expect that adapting BN statistics might result in improved performance. Indeed, in the supervised setting, it has been shown <ref type="bibr" target="#b95">(Schneider et al., 2020;</ref><ref type="bibr" target="#b81">Nado et al., 2020</ref>) that adapting the batch normalization (BN) statistics under distribution shift can result in improved performance. Here, we consider linear evaluation on ImageNet with and without adaptation of BN statistics in the backbone. Specifically, for adaptation (no adaptation), we use train (eval) mode for BN layers while training the linear classifier and eval (eval) model during evaluation. Intriguingly, we find that adaptation does not necessarily result in improved performance.</p><p>Note that no supervised information is used for BN adaptation. All parameters in the backbone remain frozen to their pre-trained values. All models received full pre-training. Background augmentations used DeepUSPS 2 . <ref type="table" target="#tab_5">Tables 3, 4</ref> follow the settings in Appendix C.1 and use FG min = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.9 Additional Information for Ablations in Main Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablations in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Additional Results</head><p>Here we report additional results, including downstream evaluations of corresponding models which used U 2 Net for generating foreground masks-evaluations of these models are consistent with evaluations of corresponding DeepUSPS 2 models; we therefore skip detailed discussions of these specific results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ImageNet acc. w/o adapt. w/ adapt. We expand on results in <ref type="table">Table 10</ref> in <ref type="table" target="#tab_7">Table A14</ref> to include SEM, which was excluded in the main text to avoid clutter. These results correspond to medium duration pre-trained models. We report corresponding results for the full duration pre-trained models in <ref type="table" target="#tab_22">Table  A15</ref>. Corresponding tables for U 2 Net are <ref type="table" target="#tab_23">Tables A16 and A17</ref>; for convenience, we report also report corresponding BG-Gap in <ref type="table" target="#tab_12">Table A18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 ImageNet-v2</head><p>Here, we report evaluation on all variants of ImageNet-v2: MatchedFrequency (MF), Thresh-old0.7 (T0.7) and TopImages (TI). Background augmentations result in large performance gains across all variants. Corresponding results for U 2 Net are shown in <ref type="table" target="#tab_4">Table A20</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 ImageNet-A</head><p>We expand on results in <ref type="table" target="#tab_7">Table 14</ref> in <ref type="table" target="#tab_4">Table A22</ref> to include evaluation on Only-FG ImageNet-A. While we report these numbers for completeness, as discussed in Section 5.4, evaluation on Only-FG ImageNet-A must be interpreted with caution, since this data set is also challenging for saliency detection. Corresponding results for U 2 Net are shown in <ref type="table" target="#tab_4">Table A23</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 ImageNet-C</head><p>On ImageNet-C, we report the average performance on the four main categories of corruptions: noise, blur, weather and digital, see <ref type="table" target="#tab_4">Table A29</ref>. While background augmentations generally result in improved robustness, they appear to decrease robustness to noise corruptions; this may be due to difficulty discerning between the foreground and background due to the high frequency noise added throughout the image.    <ref type="table" target="#tab_23">Table A16</ref>: Robustness: Foreground-Background Shifts. Background augmentations result in large performance gains on ImageNet-9 (IN-9) across all SSL methods, with BG Swaps generally exhibiting similar or better performance than BG RM. We highlight the performance benefit on the variants of IN-9 especially relevant to our work. All pre-training durations correspond to respective medium settings. Note that IN-9 uses only 9 classes, so chance is ?11.1%. Similar to <ref type="table" target="#tab_7">Table A14</ref>, but U 2 Net was used for FG extraction.  <ref type="table">Table A17</ref>: Robustness: Foreground-Background Shifts. Background augmentations result in large performance gains on ImageNet-9 (IN-9) across all SSL methods, with BG Swaps generally exhibiting similar or better performance than BG RM. We highlight the performance benefit on the variants of IN-9 especially relevant to our work. All pre-training durations correspond to respective full settings. Note that IN-9 uses only 9 classes, so chance is ?11.1%. Similar to <ref type="table" target="#tab_22">Table A15</ref>, but U 2 Net was used for FG extraction. Med. 11.1 7.4 (-3.7) 4.6 (-6.5) 6.6 5.2 (-1.4) 5.1 (-1.5) 10.9 9.3 (-1.6) 9.3 (-1.6) Full 10.0 6.4 (-3.6) 4.4 (-5.6) 9.1 5.3 (-3.8) 5.0 (-4.1) 11.4 9.9 (-1.5) 10.3 (-1.1)           <ref type="table" target="#tab_4">Table A28</ref>: Background augmentations increase shape bias. SSL methods considered generally have a higher shape bias than the supervised baseline. SwAV deviates from this pattern due to multi-crop (SwAV w/o multi-crop shape bias: 27.4). Similar to <ref type="table" target="#tab_4">Table 20</ref>, but U 2 Net was used for FG extraction. <ref type="table" target="#tab_4">Table A29</ref>: Robustness: Corruptions. Background augmentations generally improve robustness to corruptions in ImageNet-C. We observe that across methods, robustness to added noise (e.g. Gaussian, Speckle) is reduced as a result of background augmentations, while there is improved robustness to blur, weather and digital corruptions. This maybe due to difficulty discerning between the foreground and background due to the high frequency noise added throughout the image. All models received full pre-training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic of Siamese SSL methods. A simplified schematic of the Siamese SSL methods in our test bed. Dashed line in MoCo-v2 denotes enqueuing the positives k + from the previous mini-batch (and dequeuing the oldest mini-batch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Wider projection MLP and warmup alleviates early optimization difficulty. (left) Wider projection MLP alleviates early optimization difficulty, improving performance and removes the gap between BG RM and BG Random. Augmentation strength: p = 0.25. (right) The gap between BG RM and BG Random increases with stronger augmentation with the default (black dashed lines) MLP capacity. In addition to increasing MLP capacity, warming up background augmentations further adds stability across a range of augmentation strengths. Notation: (MLP width/output dimension).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Examples of variations of ImageNet-9. Also shown are classification decisions from the supervised baseline. Image from.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Semantic</head><label></label><figDesc>Focus and Robustness. A number of recent works have investigated whether non-semantic features are exploited by models in supervised learning. We draw heavy inspiration from this literature, especially Xiao et al. (2021a), Sehwag et al. (2020) and Beery et al. (2018) who demonstrate the importance of backgrounds for image classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc> Chen et al., 2020c,a,b;<ref type="bibr" target="#b112">Wu et al., 2018;</ref><ref type="bibr" target="#b84">Oord et al., 2018;</ref><ref type="bibr" target="#b117">Ye et al., 2019;</ref><ref type="bibr" target="#b104">Tian et al., 2020a;</ref><ref type="bibr" target="#b80">Misra and van der Maaten, 2020;</ref><ref type="bibr" target="#b32">Dosovitskiy et al., 2014;</ref><ref type="bibr" target="#b62">Huynh et al., 2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>et al. (2020); Sermanet et al. (2018); Gordon et al. (2020); Han et al. (2019)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>in VOC 2007 + 2012 trainval for 24k iterations and evaluated in VOC 2007 test, b) COCO detection and COCO instance segmentation: We fine-tuned a Mask R-CNN (He et al., 2017) (2? schedule) in COCO 2017 train, evaluated in COCO 2017 val. All Faster/Mask R-CNN models are with the C4-backbone. We use Detectron2 (Wu et al., 2019) for all experiments 18 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A4 :</head><label>A4</label><figDesc>Mask Distortion: Rotation. Examples of varying distortion strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A5 :</head><label>A5</label><figDesc>Mask Distortion: Shearing. Examples of varying distortion strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>We show BG Swaps &gt; BG Random for MoCo-v2, see section 4.3. BG Swaps does not apply to BYOL and SwAV as they do not use negative instances.</figDesc><table><row><cell>: Background augmentations confer large performance benefits in linear</cell></row><row><cell>evaluation on ImageNet across a spectrum of SSL methods using the original</cell></row><row><cell>or reassessed labels. For shorter training, we report metrics averaged over 3 independent</cell></row><row><cell>runs, reflecting robust improvements. Number of training epochs are chosen to be consistent</cell></row><row><cell>with previously published results. We highlight performance gains due to background</cell></row><row><cell>augmentations relative to our reproductions, but also include published baseline numbers for</cell></row><row><cell>comparison. Notation: Mean?SEM (Standard Error of the Mean). Best results are in bold.</cell></row></table><note>a.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>BG RM vs. BG Random. Comparing BG RM and BG Random in MoCo-v2 controlling for presence of negatives in the queue with similar background.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablations of BG Swaps for MoCo-v2. Each component confers a performance improvement and the improvements stack on top of each other.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Supervised Setting: Longer Training. Longer training with background augmentation does not significantly improve performance in the supervised setting over the baseline. Augmentation strength: p = 0.2.</figDesc><table><row><cell>4.9 Can Background Augmentations Improve Performance in the Supervised</cell></row><row><cell>Setting?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table /><note>BG-Gap: Background augmentations decrease BG-Gap of SSL Methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Robustness: Natural Distribution Shift. Background augmentations improve performance on ImageNet-v2, a test set for ImageNet. Notably, background augmentations enable SwAV to perform on par with the standard supervised baseline (63.8%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Robustness: Rotation, Viewpoint, Background Shift. Background augmentations improve performance on ObjectNet, a challenging test set that controls object orientation, viewpoint and background.</figDesc><table><row><cell>Pre-Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Duration</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Med.</cell><cell>3.1</cell><cell cols="2">3.3?0.1 (+0.2) 3.6?0.1 (+0.5)</cell><cell>4.4</cell><cell cols="2">5.8?0.3 (+1.4) 6.1?0.1 (+1.7)</cell><cell>3.7</cell><cell cols="2">4.2?0.1 (+0.5) 4.1?0.1 (+0.4)</cell></row><row><cell>Full</cell><cell>4.2</cell><cell>4.7 (+0.5)</cell><cell>5.3 (+1.1)</cell><cell>5.3</cell><cell>7.2 (+1.9)</cell><cell>7.2 (+1.9)</cell><cell>5.2</cell><cell>6.0 (+0.8)</cell><cell>5.7 (+0.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc></figDesc><table /><note>Robustness: Natural Adversarial Examples. Background augmentations improve performance on ImageNet-A, a data set of natural adversarial examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>Robustness: Renditions. Background augmentations improve performance on ImageNet-R, a data set of ImageNet-Renditions (e.g. paintings, sculpture).</figDesc><table><row><cell>Pre-Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Duration</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Med.</cell><cell>4.5</cell><cell cols="2">6.4?0.0 (+1.9) 8.4?0.2 (+3.9)</cell><cell>10.6</cell><cell cols="2">11.9?0.4 (+1.3) 11.4?0.1 (+0.8)</cell><cell>6.0</cell><cell cols="2">6.6?0.1 (+0.6) 6.7?0.1 (+0.7)</cell></row><row><cell>Full</cell><cell>7.8</cell><cell>10.6 (+2.8)</cell><cell>13.1 (+5.3)</cell><cell>10.4</cell><cell>13.2 (+2.8)</cell><cell>13.4 (+3.0)</cell><cell>9.1</cell><cell>10.1 (+1.0)</cell><cell>10.4 (+1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 :</head><label>16</label><figDesc>Robustness: Adversarial Attack. Background augmentations increase robustness to FGSM adversarial attacks.</figDesc><table /><note>backgrounds and that background augmentations can substantially improve robustness to these natural adversarial examples. Additional results: Appendix D</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 19 .</head><label>19</label><figDesc>We</figDesc><table><row><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell></row><row><cell>baseline</cell><cell>BG RM</cell><cell cols="2">BG Swaps baseline</cell><cell>BG RM</cell><cell cols="2">BG Random baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>28.7</cell><cell cols="2">27.3 (-1.4) 25.8 (-2.9)</cell><cell>44.5</cell><cell cols="2">40.0 (-4.5) 42.1 (-2.4)</cell><cell>49.6</cell><cell>48.1 (-1.5) 48.1 (-1.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 18 :</head><label>18</label><figDesc>AP 50 AP AP 75 AP m 50 AP m AP m 75 MoCo-v2 (repro.) 82.7?0.0 57.9?0.0 64.5?0.1 61.0 41.1 44.8 57.7</figDesc><table><row><cell></cell><cell cols="3">VOC 07+12 detection</cell><cell>COCO detection</cell><cell>COCO instance seg.</cell></row><row><cell>Method</cell><cell>AP 50</cell><cell>AP</cell><cell>AP 75</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.8</cell><cell>38.4</cell></row><row><cell>MoCo-v2 + BG RM</cell><cell cols="4">82.9?0.1 58.1?0.1 65.2?0.2 61.2 41.2 44.7</cell><cell>58.0 36.0 38.6</cell></row><row><cell cols="5">MoCo-v2 + BG Swaps 82.7?0.0 57.5?0.0 63.9?0.1 61.1 41.1 44.3</cell><cell>57.6</cell><cell>35.8</cell><cell>38.3</cell></row><row><cell>BYOL (repro.)</cell><cell cols="4">82.7?0.1 56.7?0.1 63.0?0.3 61.1 40.9 44.5</cell><cell>57.6</cell><cell>35.5</cell><cell>37.8</cell></row><row><cell>BYOL + BG RM</cell><cell cols="4">83.0?0.1 57.0?0.0 64.0?0.1 61.5 41.1 44.4</cell><cell>57.9</cell><cell>35.6</cell><cell>38.0</cell></row><row><cell cols="6">BYOL + BG Random 83.1?0.2 57.6?0.1 64.7?0.1 61.7 41.4 44.7 58.4 36.0 38.3</cell></row><row><cell>SwAV (repro.)</cell><cell cols="4">82.3?0.1 55.6?0.0 61.9?0.2 61.4 40.7 43.7</cell><cell>57.6</cell><cell>35.4</cell><cell>37.4</cell></row><row><cell>SwAV + BG RM</cell><cell cols="4">82.4?0.0 55.9?0.1 62.2?0.2 61.2 40.6 44.0</cell><cell>57.6</cell><cell>35.4</cell><cell>37.4</cell></row><row><cell>SwAV + BG Random</cell><cell cols="5">82.4?0.1 55.9?0.1 62.4?0.2 61.2 41.4 44.8 58.0 36.0 38.3</cell></row></table><note>When background is relevant: Places-205. When background information is important, background augmentations can reduce downstream performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 19 :</head><label>19</label><figDesc>Detection and Instance Segmentation.</figDesc><table><row><cell>Background Augmentations result</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>BG RM BG Swaps baseline BG RM BG Random baseline BG RM BG Random</figDesc><table><row><cell>Supervised</cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell>baseline 22.1 28.8</cell><cell>31.7</cell><cell>33.4</cell><cell>27.6</cell><cell>29.8</cell><cell>31.0</cell><cell>17.0</cell><cell>17.7</cell><cell>19.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table A2 :</head><label>A2</label><figDesc>Mask Distortion: Shearing. Background augmentations are robust to substantial mask noise induced via shearing of the foreground mask. Of note, BG Swaps maintains a large performance benefit even for strong distortions. We highlight the ? relative to the baselines with no background augmentations.Figure A6: Mask Distortion: Translation. Examples of varying distortion strength.</figDesc><table><row><cell></cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell cols="2">BYOL</cell><cell cols="2">SwAV</cell></row><row><cell>Max</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Translation baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>baseline</cell><cell>BG RM</cell></row><row><cell>0%</cell><cell>67.7</cell><cell cols="2">69.3 (+1.6) 69.7 (+2.0)</cell><cell>72.7</cell><cell>73.5 (+0.8)</cell><cell>72.2</cell><cell>73.7 (+1.5)</cell></row><row><cell>5%</cell><cell>-</cell><cell cols="2">68.9 (+1.2) 69.5 (+1.8)</cell><cell>-</cell><cell>73.3 (+0.6)</cell><cell>-</cell><cell>73.3 (+1.1)</cell></row><row><cell>10%</cell><cell>-</cell><cell cols="2">68.7 (+1.0) 69.2 (+1.5)</cell><cell>-</cell><cell>73.3 (+0.6)</cell><cell>-</cell><cell>73.5 (+1.3)</cell></row><row><cell>15%</cell><cell>-</cell><cell cols="2">68.2 (+0.5) 68.7 (+1.0)</cell><cell>-</cell><cell>73.1 (+0.4)</cell><cell>-</cell><cell>73.2 (+1.0)</cell></row><row><cell>20%</cell><cell>-</cell><cell cols="2">68.0 (+0.3) 68.7 (+1.0)</cell><cell>-</cell><cell>73.1 (+0.4)</cell><cell>-</cell><cell>73.3 (+1.1)</cell></row><row><cell>25%</cell><cell>-</cell><cell cols="2">67.9 (+0.2) 68.3 (+0.6)</cell><cell>-</cell><cell>72.9 (+0.2)</cell><cell>-</cell><cell>73.0 (+0.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table A3 :</head><label>A3</label><figDesc>Mask Distortion: Translation. Background augmentations are robust to substantial mask noise induced via translation of the foreground mask. Of note, BG Swaps maintains a large performance benefit even for strong distortions. We highlight the ? relative to the baselines with no background augmentations. value uniformly sampled from (?max shear, +max shear). Translation. Each mask was subject to translation independently along the width and height with values uniformly sampled from (?max trans., +max trans.) ? max FG width and (?max trans., +max trans.) ? max FG height respectively. Horizontal Flip. Each mask was horizontally flipped with a probability 0.5 (on each use of the mask). Bounding-Box Masks. Binary foreground masks were used to generate rectangular bounding-box masks of size max FG width ? max FG height.</figDesc><table><row><cell></cell><cell>MoCo-v2</cell><cell></cell><cell cols="2">BYOL</cell><cell cols="2">SwAV</cell></row><row><cell>Random</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Horizontal Flip baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>baseline</cell><cell>BG RM</cell></row><row><cell>67.7</cell><cell cols="2">69.3 (+1.6) 69.7 (+2.0)</cell><cell>72.7</cell><cell>73.5 (+0.8)</cell><cell>72.2</cell><cell>73.7 (+1.5)</cell></row><row><cell>-</cell><cell cols="2">68.7 (+1.0) 69.4 (+1.7)</cell><cell>-</cell><cell>73.3 (+0.6)</cell><cell>-</cell><cell>73.5 (+1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table A4 :</head><label>A4</label><figDesc>Mask Distortion: Horizontal Flip. Background augmentations are robust to substantial mask noise induced via horizontal flip of the foreground mask. Of note, BG Swaps maintains a large performance benefit even for strong distortions. We highlight the ? relative to the baselines with no background augmentations.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Saliency</cell><cell cols="2">Selected as</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mask</cell><cell cols="2">"Foreground"</cell><cell></cell></row><row><cell></cell><cell cols="2">Image</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Bounding Box</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mask</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Figure A7: Mask Distortion: Bounding-Box Mask. An example of a bounding-box</cell></row><row><cell>mask.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell cols="2">BYOL</cell><cell cols="2">SwAV</cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>baseline</cell><cell>BG RM</cell></row><row><cell>Saliency mask</cell><cell>67.7</cell><cell cols="2">69.3 (+1.6) 69.7 (+2.0)</cell><cell>72.7</cell><cell>73.5 (+0.8)</cell><cell>72.2</cell><cell>73.7 (+1.5)</cell></row><row><cell>Bounding-box mask</cell><cell>-</cell><cell cols="2">67.8 (+0.1) 68.7 (+1.0)</cell><cell>-</cell><cell>73.0 (+0.3)</cell><cell>-</cell><cell>73.0 (+0.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table A7 :</head><label>A7</label><figDesc>MoCo-v2: Ablating the strength of background augmentation BG Swaps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head></head><label></label><figDesc>BYOL: Ablating the strength of background augmentations BG RM and BG Random.</figDesc><table><row><cell>RM</cell><cell>73.7</cell><cell>73.6</cell><cell>73.8</cell><cell>73.5</cell><cell>73.1</cell></row><row><cell>BG Random</cell><cell>73.7</cell><cell>73.9</cell><cell>73.5</cell><cell>73.4</cell><cell>72.7</cell></row><row><cell>Table A8:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head></head><label></label><figDesc>SwAV: Ablating the strength of background augmentations BG RM and BG Random.</figDesc><table><row><cell></cell><cell>73.1</cell><cell>73.5</cell><cell>73.5</cell><cell>73.5</cell><cell>73.6</cell><cell>73.7</cell></row><row><cell>BG Random</cell><cell>73.0</cell><cell>73.2</cell><cell>73.2</cell><cell>73.7</cell><cell>70.9</cell><cell>67.2</cell></row><row><cell cols="7">Table A9: C.3 Is it a Better Teaching Signal for Background Augmentations in the</cell></row><row><cell cols="6">Positive and Negative to be Independent or Coupled?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table A10</head><label>A10</label><figDesc>(b). Concretely, instead of RRC, we first Resize to size 256 along the shorter edge of the image, followed by a CenterCrop to size 224?224. Interestingly, using BG Swaps with CenterCrop can substantially improve performance (c) over CenterCrop alone, though the performance is still far reduced from using RRC. This raises an intriguing possibility: perhaps one role of RRC is to bootstrap learning background invariance. Future work could potentially investigate this hypothesis.One effect of RRC is that only a little of the foreground may be present in a view. We investigate the effect of including a lower bound on the amount of foreground included in a RRC CenterCrop BG Swaps ImageNet acc. Impact of RandomResizedCrop. RRC is critical for good performance in current SSL pipelines. In MoCo-v2, replacing RRC with CenterCrop significantly hurts performance, but application of BG Swaps somewhat helps compensate. Aug. Strength of BG Swaps: p pos = p neg = 0.1.</figDesc><table><row><cell>baseline</cell><cell></cell><cell></cell><cell>67.7</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>69.2</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell>26.8</cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell>49.6</cell></row><row><cell>Table A10: Baseline (67.7)</cell><cell cols="3">FG min = 0 FG min = 0.1 FG min = 0.2</cell></row><row><cell>p pos = p neg = 0.1</cell><cell>69.2</cell><cell>69.7</cell><cell>69.4</cell></row><row><cell>p pos = p neg = 0.2</cell><cell>69.7</cell><cell>69.8</cell><cell>69.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head>Table A13 :</head><label>A13</label><figDesc>Limited-Labels Setting. Background augmentations improve performance in the limited-labels setting. Linear evaluation using 100% of ImageNet labels though a standard benchmark, is a somewhat unrealistic setting. Evaluation in the more practical setting of limited-labels reveals even larger improvement in performance. We highlight performance gains due to background augmentations. Similar toTable 6, but U 2 Net was used for foreground extraction. Best (second best) results are in bold (underlined).</figDesc><table><row><cell>Data Set</cell><cell cols="2">Supervised</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell cols="2">BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell></row><row><cell></cell><cell></cell><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell cols="2">BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Original</cell><cell></cell><cell>95.6</cell><cell>92.7</cell><cell>93.8</cell><cell>94.2</cell><cell>94.9</cell><cell>95.6</cell><cell></cell><cell>96.0</cell><cell>94.1</cell><cell>95.0</cell><cell>94.9</cell></row><row><cell>Only-BG-B ?</cell><cell></cell><cell>11.4</cell><cell>6.1</cell><cell>6.1</cell><cell>3.6</cell><cell>5.4</cell><cell>4.9</cell><cell></cell><cell>6.0</cell><cell>10.9</cell><cell>8.8</cell><cell>8.3</cell></row><row><cell>Only-BG-T ?</cell><cell></cell><cell>16.3</cell><cell>14.8</cell><cell>12.9</cell><cell>9.3</cell><cell>12.7</cell><cell>11.8</cell><cell></cell><cell>11.5</cell><cell>15.8</cell><cell>16.7</cell><cell>17.6</cell></row><row><cell>No-FG</cell><cell></cell><cell>45.9</cell><cell>37.8</cell><cell>42.3</cell><cell>39.6</cell><cell>43.9</cell><cell>45.9</cell><cell></cell><cell>46.2</cell><cell>41.3</cell><cell>44.2</cell><cell>45.2</cell></row><row><cell>Only-FG ?</cell><cell></cell><cell>86.8</cell><cell>74.4</cell><cell cols="2">81.9?0.1 (+7.5) 86.1?0.4 (+11.7)</cell><cell>83.5</cell><cell cols="3">88.8?0.1 (+5.3) 87.7?0.6 (+4.2)</cell><cell>79.4</cell><cell>85.3?0.1 (+5.9) 84.3?0.2 (+4.9)</cell></row><row><cell>Mixed-Same ?</cell><cell></cell><cell>86.2</cell><cell>81.8</cell><cell cols="2">84.0?0.1 (+2.2) 87.9?0.3 (+6.1)</cell><cell>86.2</cell><cell cols="3">88.6?0.2 (+2.4) 90.1?0.1 (+3.9)</cell><cell>82.2</cell><cell>86.1?0.3 (+3.9) 86.3?0.2 (+4.1)</cell></row><row><cell>Mixed-Rand ? Mixed-Next ?</cell><cell></cell><cell>78.9 77.2</cell><cell>70.7 67.0</cell><cell cols="2">76.3?0.2 (+5.6) 84.1?0.3 (+13.4) 73.0?0.1 (+6.0) 82.2?0.4 (+15.2)</cell><cell cols="4">1% Labels 79.6 83.2?0.1 (+3.6) 85.5?0.3 (+5.9) 77.6 80.7?0.1 (+3.1) 84.0?0.1 (+6.4)</cell><cell cols="2">71.3 10% Labels 77.1?0.3 (+5.8) 77.0?0.3 (+5.7) 69.0 74.3?0.2 (+5.3) 74.4?0.2 (+5.4)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top-1</cell><cell cols="2">Top-5</cell><cell cols="2">Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell cols="3">Supervised</cell><cell></cell><cell></cell><cell>25.4</cell><cell></cell><cell>48.4</cell><cell></cell><cell>56.4</cell><cell>80.4</cell></row><row><cell></cell><cell></cell><cell cols="3">MoCo-v2 (repro.)</cell><cell></cell><cell>52.0</cell><cell></cell><cell>77.7</cell><cell></cell><cell>63.9</cell><cell>85.8</cell></row><row><cell></cell><cell></cell><cell cols="3">MoCo-v2 + BG RM</cell><cell cols="3">54.4 (+2.4)</cell><cell>78.7</cell><cell cols="3">65.2 (+1.3)</cell><cell>86.3</cell></row><row><cell></cell><cell></cell><cell cols="6">MoCo-v2 + BG Swaps 56.4 (+4.4)</cell><cell>79.8</cell><cell cols="3">65.8 (+1.9)</cell><cell>86.5</cell></row><row><cell></cell><cell>Linear</cell><cell cols="3">BYOL (repro.) BYOL + BG RM</cell><cell cols="3">57.5 60.8 (+3.3)</cell><cell>80.8 82.9</cell><cell cols="3">68.6 70.2 (+1.6)</cell><cell>88.6 89.3</cell></row><row><cell></cell><cell></cell><cell cols="3">BYOL + BG Random</cell><cell cols="7">61.0 (+3.5) 83.5 70.6 (+2.0)</cell><cell>89.6</cell></row><row><cell></cell><cell></cell><cell cols="3">SwAV (repro.)</cell><cell></cell><cell>52.8</cell><cell></cell><cell>78.4</cell><cell></cell><cell>68.3</cell><cell>88.7</cell></row><row><cell></cell><cell></cell><cell cols="3">SwAV + BG RM</cell><cell cols="3">57.6 (+4.8)</cell><cell>81.8</cell><cell cols="3">70.3 (+2.0)</cell><cell>89.8</cell></row><row><cell></cell><cell></cell><cell cols="3">SwAV + BG Random</cell><cell cols="3">56.4 (+3.6)</cell><cell>80.8</cell><cell cols="3">70.3 (+2.0)</cell><cell>89.8</cell></row><row><cell></cell><cell></cell><cell cols="3">MoCo-v2 (repro.)</cell><cell></cell><cell>54.1</cell><cell></cell><cell>81.3</cell><cell></cell><cell>67.6</cell><cell>89.4</cell></row><row><cell></cell><cell></cell><cell cols="3">MoCo-v2 + BG RM</cell><cell cols="3">55.3 (+1.2)</cell><cell>81.4</cell><cell cols="3">68.0 (+0.4)</cell><cell>89.3</cell></row><row><cell></cell><cell></cell><cell cols="6">MoCo-v2 + BG Swaps 57.7 (+3.6)</cell><cell>82.7</cell><cell cols="3">68.8 (+1.2)</cell><cell>89.6</cell></row><row><cell></cell><cell>Finetune</cell><cell cols="3">BYOL (repro.) BYOL + BG RM</cell><cell cols="3">57.3 60.5 (+3.2)</cell><cell>80.5 82.6</cell><cell cols="3">70.6 71.7 (+1.1)</cell><cell>90.0 90.6</cell></row><row><cell></cell><cell></cell><cell cols="3">BYOL + BG Random</cell><cell cols="3">60.7 (+3.4)</cell><cell cols="4">83.1 71.9 (+1.3) 90.8</cell></row><row><cell></cell><cell></cell><cell cols="3">SwAV (repro.)</cell><cell></cell><cell>54.0</cell><cell></cell><cell>78.5</cell><cell></cell><cell>70.1</cell><cell>89.9</cell></row><row><cell></cell><cell></cell><cell cols="3">SwAV + BG RM</cell><cell cols="3">54.7 (+0.7)</cell><cell>78.9</cell><cell cols="3">70.7 (+0.6)</cell><cell>90.2</cell></row><row><cell></cell><cell></cell><cell cols="3">SwAV + BG Random</cell><cell cols="3">55.7 (+1.7)</cell><cell>79.3</cell><cell cols="3">70.8 (+0.7)</cell><cell>90.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table A14 :</head><label>A14</label><figDesc>Robustness: Foreground-Background Shifts. Background augmentations result in large performance gains on ImageNet-9 (IN-9) across all SSL methods, with BG Swaps generally exhibiting similar or better performance than BG RM. We highlight the performance benefit on the variants of IN-9 especially relevant to our work. All pre-training durations correspond to respective medium settings. Note that IN-9 uses only 9 classes, so chance is ?11.1%. This table is an expanded version ofTable 10, to include SEM which were excluded in the main text to avoid clutter.</figDesc><table><row><cell>Data Set</cell><cell>Supervised</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell cols="2">BG Random baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Original</cell><cell>95.6</cell><cell>94.7</cell><cell>94.9</cell><cell>95.3</cell><cell>95.2</cell><cell>95.8</cell><cell>95.7</cell><cell>94.6</cell><cell>95.4</cell><cell>95.1</cell></row><row><cell>Only-BG-B ?</cell><cell>11.4</cell><cell>7.9</cell><cell>6.3</cell><cell>5.1</cell><cell>7.1</cell><cell>5.8</cell><cell>6.0</cell><cell>11.4</cell><cell>10.5</cell><cell>10.9</cell></row><row><cell>Only-BG-T ?</cell><cell>16.3</cell><cell>14.7</cell><cell>13.9</cell><cell>11.1</cell><cell>16.5</cell><cell>13.0</cell><cell>14.1</cell><cell>19.2</cell><cell>18.3</cell><cell>18.0</cell></row><row><cell>No-FG</cell><cell>45.9</cell><cell>42.3</cell><cell>43.5</cell><cell>42.6</cell><cell>42.7</cell><cell>46.5</cell><cell>47.5</cell><cell>46.0</cell><cell>47.4</cell><cell>43.5</cell></row><row><cell>Only-FG ?</cell><cell>86.8</cell><cell>79.7</cell><cell cols="2">85.2 (+5.5) 86.9 (+7.2)</cell><cell>81.4</cell><cell cols="2">88.5 (+7.1) 87.3 (+5.9)</cell><cell>81.9</cell><cell cols="2">84.1 (+2.2) 83.2 (+1.3)</cell></row><row><cell>Mixed-Same ?</cell><cell>86.2</cell><cell>84.9</cell><cell cols="2">85.8 (+0.9) 89.7 (+4.8)</cell><cell>86.7</cell><cell cols="2">89.2 (+2.5) 90.2 (+3.5)</cell><cell>84.3</cell><cell cols="2">86.0 (+1.7) 85.5 (+1.2)</cell></row><row><cell>Mixed-Rand ?</cell><cell>78.9</cell><cell>74.9</cell><cell cols="2">79.0 (+4.1) 85.3 (+10.4)</cell><cell>77.6</cell><cell cols="2">83.9 (+6.3) 85.8 (+8.2)</cell><cell>72.9</cell><cell cols="2">76.7 (+3.8) 76.5 (+3.9)</cell></row><row><cell>Mixed-Next ?</cell><cell>77.2</cell><cell>72.9</cell><cell cols="2">76.0 (+3.1) 82.7 (+9.8)</cell><cell>75.7</cell><cell cols="2">82.0 (+6.3) 84.1 (+8.4)</cell><cell>70.2</cell><cell cols="2">74.5 (+4.3) 73.1 (+2.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_42"><head>Table A15 :</head><label>A15</label><figDesc>Robustness: Foreground-Background Shifts. Background augmentations result in large performance gains on ImageNet-9 (IN-9) across all SSL methods, with BG Swaps generally exhibiting similar or better performance than BG RM. We highlight the performance benefit on the variants of IN-9 especially relevant to our work. All pre-training durations correspond to respective full settings. Note that IN-9 uses only 9 classes, so chance is ?11.1%.</figDesc><table><row><cell>Data Set</cell><cell>Supervised</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Original</cell><cell>95.6</cell><cell>92.7</cell><cell>94.1</cell><cell>94.4</cell><cell>94.9</cell><cell>95.8</cell><cell>95.9</cell><cell>94.1</cell><cell>94.8</cell><cell>94.7</cell></row><row><cell>Only-BG-B ?</cell><cell>11.4</cell><cell>6.1</cell><cell>7.2</cell><cell>4.2</cell><cell>5.4</cell><cell>4.9</cell><cell>5.8</cell><cell>10.9</cell><cell>8.5</cell><cell>8.7</cell></row><row><cell>Only-BG-T ?</cell><cell>16.3</cell><cell>14.8</cell><cell>13.8</cell><cell>10.7</cell><cell>12.7</cell><cell>12.0</cell><cell>12.2</cell><cell>15.8</cell><cell>16.6</cell><cell>17.2</cell></row><row><cell>No-FG</cell><cell>45.9</cell><cell>37.8</cell><cell>43.5</cell><cell>41.7</cell><cell>43.9</cell><cell>46.5</cell><cell>46.2</cell><cell>41.3</cell><cell>46.1</cell><cell>45.3</cell></row><row><cell>Only-FG ?</cell><cell>86.8</cell><cell>74.4</cell><cell>83.6?0.3 (+9.2)</cell><cell>85.4?0.1 (+11)</cell><cell>83.5</cell><cell cols="2">89.3?0.5 (+5.8) 87.6?0.4 (+4.1)</cell><cell>79.4</cell><cell cols="2">85.4?0.2 (+6.0) 85.3?0.2 (+5.9)</cell></row><row><cell>Mixed-Same ?</cell><cell>86.2</cell><cell>81.8</cell><cell cols="2">85.6?0.2 (+3.8) 88.2?0.2 (+6.4)</cell><cell>86.2</cell><cell cols="2">89.0?0.3 (+2.8) 90.4?0.3 (+4.2)</cell><cell>82.2</cell><cell cols="2">86.0?0.4 (+3.8) 86.1?0.1 (+3.9)</cell></row><row><cell>Mixed-Rand ?</cell><cell>78.9</cell><cell>70.7</cell><cell cols="2">78.2?0.2 (+7.5) 83.6?0.2 (+12.9)</cell><cell>79.6</cell><cell cols="2">83.8?0.1 (+4.2) 85.3?0.2 (+5.7)</cell><cell>71.3</cell><cell cols="2">76.7?0.1 (+5.4) 76.8?0.3 (+5.5)</cell></row><row><cell>Mixed-Next ?</cell><cell>77.2</cell><cell>67.0</cell><cell cols="2">75.2?0.1 (+8.2) 81.2?0.2 (+14.2)</cell><cell>77.6</cell><cell cols="2">81.7?0.1 (+4.1) 83.5?0.3 (+5.9)</cell><cell>69.0</cell><cell cols="2">73.8?0.4 (+4.8) 74.1?0.2 (+5.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_45"><head>Table A18 :</head><label>A18</label><figDesc>BG-Gap: Background augmentations decrease BG-Gap of SSL Methods. Similar toTable 11, but U 2 Net was used for FG extraction.</figDesc><table><row><cell>Method</cell><cell></cell><cell>ImageNet-v2 acc.</cell><cell></cell></row><row><cell></cell><cell>MF</cell><cell>T0.7</cell><cell>TI</cell></row><row><cell>Supervised</cell><cell>63.8</cell><cell>72.6</cell><cell>77.7</cell></row><row><cell>Pre-Train Duration: Medium</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo-v2 (repro.)</cell><cell>54.7</cell><cell>63.6</cell><cell>69.6</cell></row><row><cell>MoCo-v2 + BG RM</cell><cell cols="3">56.7?0.1 (+2.0) 65.7?0.1 (+2.1) 71.5?0.1 (+1.9)</cell></row><row><cell>MoCo-v2 + BG Swaps</cell><cell cols="3">57.2?0.1 (+2.5) 66.3?0.1 (+2.7) 72.0?0.2 (+2.4)</cell></row><row><cell>BYOL (repro.)</cell><cell>60.7</cell><cell>70.2</cell><cell>75.6</cell></row><row><cell>BYOL + BG RM</cell><cell cols="3">61.7?0.2 (+1.0) 71.0?0.2 (+0.8) 76.3?0.2 (+0.7)</cell></row><row><cell>BYOL + BG Random</cell><cell cols="3">62.1?0.1 (+1.4) 71.5?0.0 (+1.3) 76.7?0.1 (+1.1)</cell></row><row><cell>SwAV (repro.)</cell><cell>59.3</cell><cell>68.5</cell><cell>73.9</cell></row><row><cell>SwAV + BG RM</cell><cell cols="3">61.2?0.3 (+1.9) 70.3?0.1 (+1.8) 75.6?0.1 (+1.7)</cell></row><row><cell>SwAV + BG Random</cell><cell cols="3">60.7?0.0 (+1.4) 70.0?0.2 (+1.5) 75.3?0.1 (+1.4)</cell></row><row><cell>Pre-Train Duration: Full</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo-v2 (repro.)</cell><cell>58.9</cell><cell>67.4</cell><cell>73.1</cell></row><row><cell>MoCo-v2 + BG RM</cell><cell>59.6 (+0.7)</cell><cell>68.7 (+1.3)</cell><cell>74.2 (+1.1)</cell></row><row><cell>MoCo-v2 + BG Swaps</cell><cell>60.3 (+1.4)</cell><cell>68.8 (+1.4)</cell><cell>74.9 (+1.8)</cell></row><row><cell>BYOL (repro.)</cell><cell>61.9</cell><cell>71.2</cell><cell>76.2</cell></row><row><cell>BYOL + BG RM</cell><cell>63.4 (+1.5)</cell><cell>72.4 (+1.2)</cell><cell>77.7 (+1.5)</cell></row><row><cell>BYOL + BG Random</cell><cell>62.8 (+0.9)</cell><cell>72.4 (+1.2)</cell><cell>77.2 (+1.0)</cell></row><row><cell>SwAV (repro.)</cell><cell>61.7</cell><cell>70.8</cell><cell>76.4</cell></row><row><cell>SwAV + BG RM</cell><cell>63.8 (+2.1)</cell><cell>72.8 (+2.0)</cell><cell>77.9 (+1.5)</cell></row><row><cell>SwAV + BG Random</cell><cell>63.4 (+1.7)</cell><cell>72.3 (+1.5)</cell><cell>77.9 (+1.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46"><head>Table A19 :</head><label>A19</label><figDesc>Robustness: Natural Distribution Shift. Expanded version of Table 12. Background augmentations improve performance on all variants of ImageNet-v2. Notably, background augmentations enable SwAV to perform on par with the standard supervised baseline. Best results are in bold. Notation: MF=MatchedFrequency, T0.7=Threshold0.7, TI=TopImages. Results inTable 12correspond to MF and are included here for completeness.</figDesc><table><row><cell>Method</cell><cell></cell><cell>ImageNet-v2 acc.</cell><cell></cell></row><row><cell></cell><cell>MF</cell><cell>T0.7</cell><cell>TI</cell></row><row><cell>Supervised</cell><cell>63.8</cell><cell>72.6</cell><cell>77.7</cell></row><row><cell>Pre-Train Duration: Medium</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo-v2 (repro.)</cell><cell>54.7</cell><cell>63.6</cell><cell>69.6</cell></row><row><cell>MoCo-v2 + BG RM</cell><cell cols="3">57.0?0.1 (+2.3) 66.2?0.2 (+2.6) 71.8?0.1 (+2.2)</cell></row><row><cell>MoCo-v2 + BG Swaps</cell><cell cols="3">57.6?0.0 (+2.9) 66.5?0.2 (+2.9) 72.3?0.1 (+2.7)</cell></row><row><cell>BYOL (repro.)</cell><cell>60.7</cell><cell>70.2</cell><cell>75.6</cell></row><row><cell>BYOL + BG RM</cell><cell cols="3">62.2?0.2 (+1.5) 71.3?0.1 (+1.1) 76.5?0.1 (+0.9)</cell></row><row><cell>BYOL + BG Random</cell><cell cols="3">62.2?0.4 (+1.5) 71.5?0.1 (+1.3) 76.6?0.2 (+1.0)</cell></row><row><cell>SwAV (repro.)</cell><cell>59.3</cell><cell>68.5</cell><cell>73.9</cell></row><row><cell>SwAV + BG RM</cell><cell cols="3">61.2?0.1 (+1.9) 70.2?0.2 (+1.7) 75.4?0.1 (+1.5)</cell></row><row><cell>SwAV + BG Random</cell><cell cols="3">61.0?0.3 (+1.7) 69.9?0.2 (+1.4) 75.4?0.1 (+1.5)</cell></row><row><cell>Pre-Train Duration: Full</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCo-v2 (repro.)</cell><cell>58.9</cell><cell>67.4</cell><cell>73.1</cell></row><row><cell>MoCo-v2 + BG RM</cell><cell>60.2 (+1.3)</cell><cell>69.2 (+1.8)</cell><cell>74.6 (+1.5)</cell></row><row><cell>MoCo-v2 + BG Swaps</cell><cell>60.5 (+1.6)</cell><cell>69.3 (+1.9)</cell><cell>75.1 (+2.0)</cell></row><row><cell>BYOL (repro.)</cell><cell>61.9</cell><cell>71.2</cell><cell>76.2</cell></row><row><cell>BYOL + BG RM</cell><cell>62.9 (+1.0)</cell><cell>72.2 (+1.0)</cell><cell>77.5 (+1.3)</cell></row><row><cell>BYOL + BG Random</cell><cell>63.2 (+1.3)</cell><cell>71.9 (+0.7)</cell><cell>77.1 (+0.9)</cell></row><row><cell>SwAV (repro.)</cell><cell>61.7</cell><cell>70.8</cell><cell>76.4</cell></row><row><cell>SwAV + BG RM</cell><cell>63.7 (+2.0)</cell><cell>73.1 (+2.3)</cell><cell>78.2 (+1.8)</cell></row><row><cell>SwAV + BG Random</cell><cell>63.5 (+1.8)</cell><cell>72.4 (+1.6)</cell><cell>77.7 (+1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head>Table A20 :</head><label>A20</label><figDesc>Robustness: Natural Distribution Shift. Background augmentations improve performance on all variants of ImageNet-v2. Notably, background augmentations enable SwAV to perform on par with the standard supervised baseline. Best results are in bold. Notation: MF=MatchedFrequency, T0.7=Threshold0.7, TI=TopImages. Similar toTable A19, but U 2 Net was used for FG extraction.</figDesc><table><row><cell>Pre-Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Duration</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Med.</cell><cell>14.4</cell><cell cols="2">17.1?0.3 (+2.7) 18.5?0.3 (+4.1)</cell><cell>20.4</cell><cell cols="2">22.9?0.2 (+2.5) 22.4?0.1 (+2.0)</cell><cell>16.1</cell><cell cols="2">19.0?0.3?0.1 (+2.9) 18.3?0.2 (+2.2)</cell></row><row><cell>Full</cell><cell>17.4</cell><cell>20.2 (+2.8)</cell><cell>21.9 (+4.5)</cell><cell>20.8</cell><cell>24.3 (+3.5)</cell><cell>23.7 (+2.9)</cell><cell>19.3</cell><cell>23.1 (+3.8)</cell><cell>21.2 (+1.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_48"><head>Table A21 :</head><label>A21</label><figDesc>Robustness: Rotation, Viewpoint, Background Shift. Background augmentations improve performance on ObjectNet, a challenging test set that controls object orientation, viewpoint and background. Similar to Table 13, but U 2 Net was used for FG extraction.</figDesc><table><row><cell>Pre-Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Duration</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>ImageNet-A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Med.</cell><cell>3.1</cell><cell cols="2">3.3?0.1 (+0.2) 3.6?0.1 (+0.5)</cell><cell>4.4</cell><cell cols="2">5.8?0.3 (+1.4) 6.1?0.1 (+1.7)</cell><cell>3.7</cell><cell cols="2">4.2?0.1 (+0.5) 4.1?0.1 (+0.4)</cell></row><row><cell>Full</cell><cell>4.2</cell><cell>4.7 (+0.5)</cell><cell>5.3 (+1.1)</cell><cell>5.3</cell><cell>7.2 (+1.9)</cell><cell>7.2 (+1.9)</cell><cell>5.2</cell><cell>6.0 (+0.8)</cell><cell>5.7 (+0.5)</cell></row><row><cell cols="2">Only-FG ImageNet-A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Med.</cell><cell>2.8</cell><cell cols="2">2.8?0.0 (+0.0) 4.2?0.1 (+1.4)</cell><cell>3.2</cell><cell cols="2">4.8?0.1 (+1.6) 4.7?0.2 (+1.5)</cell><cell>2.7</cell><cell cols="2">4.0?0.1 (+1.3) 3.5?0.1 (+0.8)</cell></row><row><cell>Full</cell><cell>3.4</cell><cell>3.1 (-0.3)</cell><cell>4.7 (+1.3)</cell><cell>3.1</cell><cell>5.8 (+2.7)</cell><cell>5.1 (+2.0)</cell><cell>3.9</cell><cell>4.0 (+0.1)</cell><cell>3.8 (-0.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_49"><head>Table A22 :</head><label>A22</label><figDesc>Robustness: Natural Adversarial Examples. Background augmentations improve performance on ImageNet-A, a data set of natural adversarial examples. Interestingly, the performance of all SSL methods drops when presented with only foreground, but background augmentations provide some robustness against this distribution shift as well. Similar toTable 14, but expanded to include only FG ImageNet-A.</figDesc><table><row><cell>Pre-Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Duration</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>ImageNet-A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Med.</cell><cell>3.1</cell><cell cols="2">3.4?0.1 (+0.3) 3.9?0.0 (+0.8)</cell><cell>4.4</cell><cell cols="2">6.0?0.2 (+1.6) 6.3?0.1 (+1.9)</cell><cell>3.7</cell><cell cols="2">4.1?0.0 (+0.4) 4.1?0.1 (+0.4)</cell></row><row><cell>Full</cell><cell>4.2</cell><cell>4.8 (+0.4)</cell><cell>5.4 (+1.2)</cell><cell>5.3</cell><cell>7.4 (+2.1)</cell><cell>7.1 (+1.8)</cell><cell>5.2</cell><cell>6.1 (+0.9)</cell><cell>6.1 (+0.9)</cell></row><row><cell cols="2">Only-FG ImageNet-A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Med.</cell><cell>2.8</cell><cell cols="2">3.2?0.3 (+0.4) 4.4?0.1 (+1.6)</cell><cell>3.2</cell><cell cols="2">4.9?0.2 (+1.7) 4.8?0.2 (+1.6)</cell><cell>2.7</cell><cell cols="2">4.0?0.1 (+1.3) 3.6?0.1 (+0.9)</cell></row><row><cell>Full</cell><cell>3.4</cell><cell>4.3 (+0.9)</cell><cell>4.6 (+1.2)</cell><cell>3.1</cell><cell>5.9 (+2.8)</cell><cell>5.0 (+1.9)</cell><cell>3.9</cell><cell>4.7 (+0.8)</cell><cell>4.0 (+0.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_50"><head>Table A23 :</head><label>A23</label><figDesc>Robustness: Natural Adversarial Examples. Background augmentations improve performance on ImageNet-A, a data set of natural adversarial examples. Interestingly, the performance of all SSL methods drops when presented with only foreground, but background augmentations provide some robustness against this distribution shift as well. Similar toTable A22, but U 2 Net was used for FG extraction.</figDesc><table><row><cell>Pre-Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Duration</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Med.</cell><cell>27.7</cell><cell cols="2">31.1?0.4 (+3.4) 32.3?0.1 (+4.6)</cell><cell>36.3</cell><cell cols="2">39.7?0.1 (+3.4) 38.4?0.1 (+2.1)</cell><cell>27.9</cell><cell cols="2">32.0?0.2 (+4.1) 31.4?0.1 +3.5)</cell></row><row><cell>Full</cell><cell>30.4</cell><cell>33.4 (+3.0)</cell><cell>34.2 (+3.8)</cell><cell>34.4</cell><cell>40.5 (+6.1)</cell><cell>39.7 (+5.3)</cell><cell>29.4</cell><cell>33.6 (+4.2)</cell><cell>32.5 (+3.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_51"><head>Table A24 :</head><label>A24</label><figDesc>Robustness: Renditions. Background augmentations improve performance on ImageNet-R, a data set of ImageNet-Renditions (e.g. paintings, sculpture). Similar toTable 15, but U 2 Net was used for FG extraction.</figDesc><table><row><cell>Pre-Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Duration</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>Med.</cell><cell>4.5</cell><cell cols="2">6.0?0.1 (+1.5) 8.6?0.2 (+4.1)</cell><cell>10.6</cell><cell cols="2">11.7?0.2 (+1.1) 11.6?0.2 (+1.0)</cell><cell>6.0</cell><cell cols="2">6.4?0.0 (+0.4) 6.7?0.1 (+0.7)</cell></row><row><cell>Full</cell><cell>7.8</cell><cell>10.1 (+2.3)</cell><cell>12.6 (+4.8)</cell><cell>10.4</cell><cell>13.7 (+3.3)</cell><cell>13.5 (+3.1)</cell><cell>9.1</cell><cell>10.2 (+1.1)</cell><cell>10.5 (+1.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_52"><head>Table A25 :</head><label>A25</label><figDesc>Robustness: Adversarial Attack. Background augmentations increase robustness to FGSM adversarial attacks. Similar toTable 16, but U 2 Net was used for FG extraction.</figDesc><table><row><cell>Data Set</cell><cell></cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell></row><row><cell></cell><cell>baseline</cell><cell>BG RM</cell><cell>BG Swaps</cell><cell>baseline</cell><cell>BG RM</cell><cell cols="2">BG Random baseline</cell><cell>BG RM</cell><cell>BG Random</cell></row><row><cell>CIFAR-10</cell><cell>73.9</cell><cell cols="2">80.3 (+6.4) 77.5 (+3.6)</cell><cell>86.7</cell><cell cols="2">88.5 (+1.8) 87.9 (+1.2)</cell><cell>92.7</cell><cell>93.3 (+0.6) 93.3 (+0.6)</cell></row><row><cell>CIFAR-100</cell><cell>40.8</cell><cell cols="2">51.4 (+10.6) 46.3 (+5.5)</cell><cell>67.6</cell><cell cols="2">68.2 (+0.6) 67.1 (-0.5)</cell><cell>76.0</cell><cell>77.3 (+1.3) 76.8 (+0.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_53"><head>Table A26 :</head><label>A26</label><figDesc>CIFAR-10, 100. Background augmentations improve performance on linear evaluation on CIFAR-10 and 100. Similar to Table 17, but U 2 Net was used for FG extraction. BG Swaps 82.7?0.0 57.4?0.2 64.0?0.3 61.2 41.4 44.8 BG Random 83.2?0.1 57.4?0.1 64.1?0.2 61.7 41.4 44.7</figDesc><table><row><cell></cell><cell cols="3">VOC 07+12 detection</cell><cell>COCO detection</cell><cell cols="2">COCO instance seg.</cell></row><row><cell>Method</cell><cell>AP 50</cell><cell>AP</cell><cell>AP 75</cell><cell cols="2">AP 50 AP AP 75 AP m 50</cell><cell>AP m AP m 75</cell></row><row><cell>MoCo-v2(repro.)</cell><cell cols="4">82.7?0.0 57.9?0.0 64.5?0.1 61.0 41.1 44.8</cell><cell>57.7</cell><cell>35.8</cell><cell>38.4</cell></row><row><cell>MoCo-v2+ BG RM</cell><cell cols="4">82.6?0.1 57.6?0.1 64.5?0.2 60.9 41.2 44.8</cell><cell>57.8</cell><cell>35.9</cell><cell>38.5</cell></row><row><cell cols="6">MoCo-v2+ 58.0</cell><cell>36.0</cell><cell>38.3</cell></row><row><cell>BYOL (repro.)</cell><cell cols="4">82.7?0.1 56.7?0.1 63.0?0.3 61.1 40.9 44.5</cell><cell>57.6</cell><cell>35.5</cell><cell>37.8</cell></row><row><cell>BYOL + BG RM</cell><cell cols="4">83.0?0.1 57.0?0.1 63.9?0.2 61.5 41.1 44.3</cell><cell>57.8</cell><cell>35.5</cell><cell>37.8</cell></row><row><cell cols="6">BYOL + 57.9</cell><cell>35.7</cell><cell>37.8</cell></row><row><cell>SwAV (repro.)</cell><cell cols="4">82.3?0.1 55.6?0.0 61.9?0.2 61.4 40.7 43.7</cell><cell>57.6</cell><cell>35.4</cell><cell>37.4</cell></row><row><cell>SwAV + BG RM</cell><cell cols="4">82.6?0.0 55.8?0.1 62.0?0.1 61.2 40.6 43.8</cell><cell>57.4</cell><cell>35.1</cell><cell>37.0</cell></row><row><cell cols="5">SwAV + BG Random 82.5?0.1 56.0?0.1 62.7?0.1 61.2 40.8 44.3</cell><cell>57.7</cell><cell>35.5</cell><cell>37.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_54"><head>Table A27 :</head><label>A27</label><figDesc>Detection and Instance Segmentation. Background Augmentations result in small improvements in detection and instance segmentation tasks, likely due to extensive supervision involved in subsequent training. All VOC metrics reported are average of 3 independent runs. Similar toTable 19, but U 2 Net was used for FG extraction. BG RM BG Swaps baseline BG RM BG Random baseline BG RM BG Random</figDesc><table><row><cell>Supervised</cell><cell>MoCo-v2</cell><cell></cell><cell></cell><cell>BYOL</cell><cell></cell><cell></cell><cell>SwAV</cell><cell></cell></row><row><cell>baseline 22.1 28.8</cell><cell>32.3</cell><cell>31.9</cell><cell>27.6</cell><cell>31.7</cell><cell>29.1</cell><cell>17.0</cell><cell>20.1</cell><cell>17.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_55"><head></head><label></label><figDesc>C.Zhuang, T. She, A. Andonian, M. S. Mark, and D. Yamins. Unsupervised learning from video with deep neural embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. In setting of default MLP capacity (dashed lines), masks from U 2 Net were used to control for influence of mask quality. 4. Since we maintain the same resolution of 96?96 for the smaller crops as in the default setting and simply modify the max scale in RandomResizedCrop, compute and memory requirements stay identical. Additional details in Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">. It is more difficult to determine whether performance should increase or decrease for the No-FG variant, since this manipulation leaves a perfectly shaped cutout of the foreground on the background, which provides substantial information about the structure of the foreground even though it has been removed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">. We evaluated the shape-bias of an official SwAV model trained for 400 epochs without multi-crop from https://github.com/facebookresearch/swav.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">. We use the cue conflict stimuli released at https://github.com/rgeirhos/texture-vs-shape. 18. We use the code provided at https://github.com/facebookresearch/moco/tree/main/detection.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Xinlei Chen, Mathilde Caron, and Saining Xie for helpful discussions, and Ting-Wu Rudy Chin and Matthew Leavitt for feedback on an early draft of the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">IN-9) consists of data sets with varying amount of foreground-background signal. Variations of IN-9 (excluding the &quot;Original&quot;) involve a distribution shift in foreground-background statistics. We use the data and code 12 from the original work for evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imagenet-9 ; Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ImageNet-9</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2020) relabel the ImageNet validation images to better represent the semantic content present in the images; we use the Reassessed Labels 13 (ReaL) to evaluate ImageNet-ReaL accuracy. Our supervised (torchvision) baseline has an ImageNet-ReaL accuracy of 82</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imagenet-Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beyer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>7% (82.9%)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">2019) consists of three test data sets for ImageNet, with 10,000 images each. The three variations are a) MatchedFrequency, b)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
		<idno>ImageNet-v2. ImageNet-v2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">2019) is a test data set that controls for rotation, background, and viewpoint. It contains 50,000 images with 313 object classes. 113 of ObjectNet&apos;s classes overlap with ImageNet-we evaluate on this subset 14 . Following the original work 15 , after removing the one-pixel red border, images are resized to size 224 along the shorter edge, followed by pre-processing steps of CenterCrop and normalization as in Appendix A.3). Our supervised (torchvision) baseline has an</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Objectnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Objectnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barbu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ObjectNet accuracy of 24.4% (24.7%)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">2019b) is a data set of &quot;natural&quot; adversarial examples-images obtained by a process of adversarial filtering of natural images. It consists of 7,500 images mis-classified by the torchvision ResNet-50 pre-trained model 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-A</forename><surname>Imagenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-A (</forename><surname>Imagenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrycks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>report that a corresponding model re-trained from scratch has an accuracy of 2.2%; our supervised baseline has an accuracy of 2.4%.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ImageNet-R is a data set curated to measure generalization to various abstract visual renditions (e.g. paintings, embroidery, cartoons etc., see Figure A2 for examples) of ImageNet classes. ImageNet-R involves a shift in texture statistics and contains 30,000 images. Our supervised (torchvision) baseline has an ImageNet-R accuracy of 36</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imagenet-R</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>0% (36.1%)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">We used foolbox (Rauber et al., 2020) for ? FGSM adversarial attack with = 8/255 applied to ImageNet validation images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adversarial</forename><surname>Attack</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">for additional filtering criteria used</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See</forename><surname>Hendrycks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Table A6: MoCo-v2: Ablating the strength of background augmentations BG RM and BG Random</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baseline</surname></persName>
		</author>
		<idno>67.7) p neg = 0.1 p neg = 0.2 p neg = 0.3 p neg = 0.4 p neg = 0.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">U 2 Net Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saliency</forename><surname>Method</surname></persName>
		</author>
		<idno>30.3 30.3 (+0.0) 33.1 (+2.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206596</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4845" to="4854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-organizing neural network that discovers surfaces in random-dot stereograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Are all negatives created equal in contrastive instance discrimination?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06682</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-task Self-Supervised Visual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative Unsupervised Feature Learning with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">FlowNet: Learning Optical Flow With Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Instaboost: Boosting instance segmentation via probability map guided copy-pasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="682" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenettrained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On the surprising similarities between supervised and self-supervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narayanappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08377</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Synthesizing training data for object detection in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07836</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07177</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Watching the world go by: Representation learning from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07990</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Van Kerkwijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Del R?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>G?rard-Marchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07174</idno>
		<title level="m">Natural adversarial examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The origins and prevalence of texture bias in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Boosting contrastive self-supervised learning with false negative cancellation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11765</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Invariant Information Clustering for Unsupervised Image Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Measuring the tendency of cnns to learn surface statistical regularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11561</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Revisiting Self-Supervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Efficient Self-supervised Vision Transformers for Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09785</idno>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Prototypical Contrastive Learning of Unsupervised Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning to Detect a Salient Object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.70</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6609" to="6617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Torchvision the machine-vision package of torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Evaluating prediction-time batch normalization for robustness under covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10963</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Deepusps: Deep robust unsupervised saliency prediction with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P N</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13055</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">U2-net: Going deeper with nested u-structure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107404</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Foolbox native: Fast adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensorflow, and jax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">53</biblScope>
			<biblScope unit="page">2607</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<title level="m">Do ImageNet Classifiers Generalize to ImageNet? In International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning to segment via cut-and-paste</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>1573-1405</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Oak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14077</idno>
		<title level="m">Time for a background check! uncovering the impact of background features on deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Casting your model: Learning to localize improves self-supervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04630</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Timecontrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Viewmaker networks: Learning views for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">What Makes for Good Views for Contrastive Learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4019" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Conditional negative sampling for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">What should not be contrastive in contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L K</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Hierarchical Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<title level="m">Large batch training of convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Supervision by Fusion: Towards Unsupervised Learning of Deep Salient Object Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4048" to="4056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9029" to="9038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Distilling localization for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
