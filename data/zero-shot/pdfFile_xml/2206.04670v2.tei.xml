<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjie</forename><surname>Mai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Abed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><forename type="middle">Kader</forename><surname>Hammoud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9% to 86.1%, even outperforming state-of-theart PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7% on ScanObjectNN, surpassing PointMLP by 2.3%, while being 10? faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-theart performance with 74.9% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. The code and models are available at https://github.com/guochengqian/pointnext. * Equal contribution. ? Corresponding authors.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in 3D data acquisition have led to a surge in interest for point cloud understanding. With the rise of PointNet <ref type="bibr" target="#b28">[29]</ref> and PointNet++ <ref type="bibr" target="#b29">[30]</ref>, processing point clouds in their unstructured format using deep CNNs become possible. Subsequent to "PointNets", many point-based networks are introduced with the majority focusing on developing new and sophisticated modules to extract local structures, e.g. the pseudo-grid convolution in KPConv <ref type="bibr" target="#b42">[43]</ref> and the self-attention layer in Point Transformer <ref type="bibr" target="#b55">[56]</ref>. These newly proposed methods outperform PointNet++ by a large margin in a variety of tasks, leaving the impression that the PointNet++ architecture is too simple to learn complex point cloud representations. In this work, we revisit PointNet++, the classical and widely used network, and find that its full potential has yet to be explored, mainly due to two factors that were not present at the time of PointNet++: (1) superior training strategies and (2) effective model scaling strategies.</p><p>Through a comprehensive empirical study on various benchmarks, e.g., ScanObjecNN <ref type="bibr" target="#b43">[44]</ref> for object classification and S3DIS <ref type="bibr" target="#b0">[1]</ref> for semantic segmentation, we discover that training strategies, i.e., data augmentation and optimization techniques, play an important role in the network's performance. In fact, a large part of the performance gain of state-of-the-art (SOTA) methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b55">56]</ref> over Point-Net++ <ref type="bibr" target="#b29">[30]</ref> is due to improved training strategies that are, unfortunately, less publicized compared to <ref type="figure">Figure 1</ref>: Effects of training strategies and model scaling on PointNet++ <ref type="bibr" target="#b29">[30]</ref>. We show that improved training strategies (data augmentation and optimization techniques) and model scaling can significantly boost PointNet++ performance. The average overall accuracy and mIoU (6-fold cross-validation) are reported on ScanObjectNN <ref type="bibr" target="#b43">[44]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref>. architectural changes. For example, randomly dropping colors during training can unexpectedly boost the testing performance of PointNet++ by 5.9% mean IoU (mIoU) on S3DIS <ref type="bibr" target="#b0">[1]</ref>, as demonstrated in Tab. 5. In addition, adopting label smoothing <ref type="bibr" target="#b38">[39]</ref> can improve the overall accuracy (OA) on ScanObjectNN <ref type="bibr" target="#b43">[44]</ref> by 1.3%. These findings inspire us to revisit PointNet++ and equip it with new advanced training strategies that are widely used today. Surprisingly, as shown in <ref type="figure">Fig. 1</ref>, utilizing the improved training strategies alone improves the OA of PointNet++ by 8.2% on ScanObjectNN (from 77.9% to 86.1%), establishing a new SOTA without introducing any changes to the architecture (refer to Sec. 4.4.1 for details). For the S3DIS segmentation benchmark, the mIoU evaluated in all areas by 6-fold cross-validation can increase by 13.6% (from 54.5% to 68.1%), outperforming many modern architectures that are subsequent to PointNet++, such as PointCNN <ref type="bibr" target="#b21">[22]</ref> and DeepGCN <ref type="bibr" target="#b20">[21]</ref>.</p><p>Moreover, we observe that the current prevailing models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b55">56]</ref> for point cloud analysis have employed many more parameters than the original PointNets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Effectively expanding PointNet++ from its original small scale to a larger scale is a topic worth studying because larger models are generally expected to enable richer representations and perform better <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b54">55]</ref>. However, we find that the naive way of using more building blocks or increasing the channel size in PointNet++ only leads to an overhead in latency and no significant improvement in accuracy (see <ref type="bibr">Sec. 4.4.2)</ref>. For effective and efficient model scaling, we introduce residual connections <ref type="bibr" target="#b12">[13]</ref>, an inverted bottleneck design <ref type="bibr" target="#b35">[36]</ref>, and separable MLPs <ref type="bibr" target="#b31">[32]</ref> into PointNet++. The modernized architecture is named PointNeXt, the next version of PointNets. PointNeXt can be scaled up flexibly and outperforms SOTA on various benchmarks. As demonstrated in <ref type="figure">Fig. 1</ref>, PointNeXt improves the original PointNet++ by 20.4% mIoU (from 54.5% to 74.9%) on S3DIS [1] 6-fold and achieves 9.8% OA gains on ScanObjecNN <ref type="bibr" target="#b43">[44]</ref>, surpassing SOTA Point Transformer <ref type="bibr" target="#b55">[56]</ref> and PointMLP <ref type="bibr" target="#b27">[28]</ref>. We summarize our contributions next:</p><p>? We present the first systematic study of training strategies in the point cloud domain and show that PointNet++ strikes back (+8.2% OA on ScanObjectNN and +13.6% mIoU on S3DIS) by simply adopting improved training strategies alone. The improved training strategies are general and can be easily applied to improve other methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b27">28]</ref>. ? We propose PointNeXt, the next version of PointNets. PointNeXt is scalable and surpasses SOTA on all tasks studied, including object classification <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49]</ref>, semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>, and part segmentation <ref type="bibr" target="#b52">[53]</ref>, while being faster than SOTA in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary: A Review of PointNet++</head><p>Our PointNeXt is built upon PointNet++ <ref type="bibr" target="#b29">[30]</ref>, which uses a U-Net <ref type="bibr" target="#b34">[35]</ref> like architecture with an encoder and a decoder, as visualized in <ref type="figure">Figure 2</ref>. The encoder part hierarchically abstracts features of point clouds using a number of set abstraction (SA) blocks, while the decoder gradually interpolates the abstracted features by the same number of feature propagation blocks. The SA block consists of a subsampling layer to downsample the incoming points, a grouping layer to query neighbors for each point, a set of shared multilayer perceptrons (MLPs) to extract features, and a reduction layer to aggregate features within the neighbors. The combination of the grouping layer, MLPs, and the reduction layer is formulated as:</p><formula xml:id="formula_0">x l+1 i = R j:(i,j)?N h ? [x l j ; p l j ? p l i ] ,<label>(1)</label></formula><p>where R is the reduction layer (e.g. max-pooling) that aggregates features for point i from its neighbors denoted as {j : (i, j) ? N }. p l i , x l i , x l j are the input coordinates, the input features, and the features of neighbor j in the l th layer of the network, respectively. h ? denotes the shared MLPs that take the concatenation of x l j and the relative coordinates (p l j ? p l i ) as input. Note that, since PointNet++ with single-scale grouping that uses one SA block per stage is the default architecture used in the original paper <ref type="bibr" target="#b29">[30]</ref>, we refer to it as PointNet++ throughout and use it as our baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology: From PointNet++ to PointNeXt</head><p>In this section, we present how to modernize the classical architecture PointNet++ <ref type="bibr" target="#b29">[30]</ref> into PointNeXt, the next version of PointNet++ with SOTA performance. Our exploration mainly focuses on two aspects: (1) training modernization to improve data augmentation and optimization techniques, and (2) architectural modernization to probe receptive field scaling and model scaling. Both aspects have important impact on the model's performance, but were under-explored by previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Modernization: PointNet++ Strikes Back</head><p>We conduct a systematic study to quantify the effect of each data augmentation and optimization technique used by modern point cloud networks <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b55">56]</ref> and propose a set of improved training strategies. The potential of PointNet++ can be unveiled by adopting our proposed training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Data Augmentation</head><p>Data augmentation is one of the most important strategies to boost the performance of a neural network; thus we start our modernization from there. The original PointNet++ used simple combinations of data augmentations from random rotation, scaling, translation, and jittering for various benchmarks <ref type="bibr" target="#b29">[30]</ref>. Recent methods adopt stronger augmentations than those used in PointNet++. For example, KPConv <ref type="bibr" target="#b42">[43]</ref> randomly drops colors during training, Point-BERT <ref type="bibr" target="#b53">[54]</ref> uses a common point resampling strategy to randomly sample 1, 024 points from the original point cloud for data scaling, while RandLA-Net <ref type="bibr" target="#b14">[15]</ref> and Point Transformer <ref type="bibr" target="#b55">[56]</ref> load the entire scene as input in segmentation tasks. In this paper, we quantify the effect of each data augmentation through an additive study.</p><p>We start our study with PointNet++ <ref type="bibr" target="#b29">[30]</ref> as the baseline, which is trained with the original data augmentations and optimization techniques. We remove each data augmentation to check whether it is necessary or not. We add back the useful augmentations but remove the unnecessary ones. We then systematically study all the data augmentations used in the representative works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref>, including data scaling such as point resampling <ref type="bibr" target="#b53">[54]</ref> and loading the entire scene as input <ref type="bibr" target="#b14">[15]</ref>, random rotation, random scaling, translation to shift point clouds, jittering to add independent noise to each point, height appending <ref type="bibr" target="#b42">[43]</ref> (i.e., appending the measurement of each point along the gravity direction of objects as additional input features), color auto-contrast to automatically adjust color contrast <ref type="bibr" target="#b55">[56]</ref>, and color drop that randomly replaces colors with zero values. We verify the effectiveness of data augmentation incrementally and only keep the augmentations that give a better validation accuracy. At the end of this study, we provide a collection of data augmentations for each task that allow for the highest boost in the model's performance. Sec. 4.4.1 presents and analyzes in detail the uncovered findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Optimization Techniques</head><p>Optimization techniques including loss functions, optimizers, learning rate schedulers, and hyperparameters are also vital to the performance of a neural network. PointNet++ uses the same optimization techniques throughout its experiments: CrossEntropy loss, Adam optimizer <ref type="bibr" target="#b15">[16]</ref>, exponential learning rate decay (Step Decay), and the same hyperparmeters. Owing to the development of machine learning theory, modern neural networks can be trained with theoretically better optimizers (e.g. AdamW <ref type="bibr" target="#b26">[27]</ref> vs. Adam <ref type="bibr" target="#b15">[16]</ref>) and more advanced loss functions (CrossEntropy with label smoothing <ref type="bibr" target="#b38">[39]</ref>). Similarly to our study on data augmentations, we also quantify the effect of each modern optimization technique on PointNet++. We first perform a sequential hyperparameter search for the learning rate and weight decay. We then conduct an additive study on label smoothing, optimizer, and learning rate scheduler. We discover a set of improved optimization techniques that further </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture Modernization: Small Modifications ? Big Improvements</head><p>In this subsection, we modernize PointNet++ <ref type="bibr" target="#b29">[30]</ref> into the proposed PointNeXt. The modernization consists of two aspects: (1) receptive field scaling and (2) model scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Receptive Field Scaling</head><p>The receptive field is a significant factor in the design space of a neural network <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7]</ref>. There are at least two ways to scale the receptive field in point cloud processing: (1) adopting a larger radius to query the neighborhood, and (2) adopting a hierarchical architecture. Since the hierarchical architecture has been adopted in the original PointNet++, we mainly study <ref type="bibr" target="#b0">(1)</ref> in this subsection. Note that the radius of PointNet++ is set to an initial value r that doubles when the point cloud is downsampled. We study a different initial value in each benchmark and discover that the radius is dataset-specific and can have significant influence on performance. This is elaborated in Sec. 4.4.2.</p><p>Furthermore, we find that the relative coordinates ? p = p l j ?p l i in Eq. (1) make network optimization harder, leading to a decrease in performance. Thus, we propose relative position normalization (? p normalization) to divide relative position by the neighborhood query radius:</p><formula xml:id="formula_1">x l+1 i = R j:(i,j)?N h ? [x l j ; (p l j ? p l i )/r l ] .<label>(2)</label></formula><p>Without normalization, values of relative positions (? p = p l j ? p l i ) are considerably small (less than the radius), requiring the network to learn a larger weight to apply on ? p . This makes the optimization non-trivial, especially since weight decay is used to reduce the weights of the network and thus tends to ignore the effects of relative position. The proposed normalization alleviates this issue by rescaling and in the meantime reduces the variance of ? p among different stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Model Scaling</head><p>PointNet++ is a relatively small network, where the encoder consists of only 2 stages in the classification architecture and 4 stages for segmentation. Each stage consists of only 1 SA block, and each block contains 3 layers of MLP. The model sizes of PointNet++ for both classification and segmentation are less than 2M, which is much smaller compared to modern networks that typically use more than 10M parameters <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>. Interestingly, we find that neither appending more SA blocks nor using more channels leads to a noticeable improvement in accuracy, while causing a significant drop in throughput (refer to Sec. 4.4.2), mainly due to vanishing gradient and overfitting. Therefore, in this subsection, we study how to scale up PointNet++ in an effective and efficient way.</p><p>We propose an Inverted Residual MLP (InvResMLP) block to be appended after the first SA block, per stage, for effective and efficient model scaling. InvResMLP is built on the SA block and is illustrated at the bottom middle of <ref type="figure">Fig. 2</ref>. There are three differences between InvResMLP and SA.</p><p>(1) A residual connection between the input and the output is added to alleviate the vanishing gradient problem <ref type="bibr" target="#b12">[13]</ref>, especially when the network goes deeper. (2) Separable MLPs are introduced to reduce computation and reinforce pointwise feature extraction. While all 3 layers of MLPs in the original SA block are computed on the neighborhood features, InvResMLP separates the MLPs into a single layer computed on the neighborhood features (between the grouping and reduction layers) and two layers for point features (after reduction), as inspired by MobileNet <ref type="bibr" target="#b13">[14]</ref> and ASSANet <ref type="bibr" target="#b31">[32]</ref>. <ref type="formula">(3)</ref> The inverted bottleneck design <ref type="bibr" target="#b35">[36]</ref> is leveraged to expand the output channels of the second MLP by 4 times to enrich feature extraction. Appending InvResMLP blocks is proven to significantly improve performance compared to the appending of the original SA blocks (see Sec. 4.4.2).</p><p>In addition to InvResMLP, we present three changes in the macro architecture. (1) We unify the design of PointNet++ encoder for classification and segmentation, i.e., scaling the number of SA blocks for classification from 2 to 4 while keeping the original number (4 blocks) for segmentation at each stage. <ref type="bibr" target="#b1">(2)</ref> We utilize a symmetric decoder in which its channel size is changed to match the encoder. <ref type="formula">(3)</ref> We add a stem MLP, an additional MLP layer inserted at the beginning of the architecture, to map the input point cloud to a higher dimension.</p><p>In summary, we present PointNeXt, the next version of PointNets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b51">52]</ref>, modified from PointNet++ by incorporating the proposed InvResMLP and the aforementioned macro-architectural changes. The architecture of PointNeXt is illustrated in <ref type="figure">Fig. 2</ref>. We denote the channel size of the stem MLP as C and the number of InvResMLP blocks as B. A larger C leads to an increase in the width of the network (i.e., width scaling), while a larger B leads to an increase in the depth of the network (i.e., depth scaling). Note that when B = 0, only one SA block and no InvResMLP blocks are used at each stage. The number of MLP layers in the SA block is set to 2, and a residual connection is added inside each SA block. When B = 0, InvResMLP blocks are appended after the original SA block. The number of MLP layers in the SA block in this case is set to 1 to save computation cost. The configuration of our PointNeXt family is summarized as follows:</p><formula xml:id="formula_2">? PointNeXt-S: C = 32, B = 0 ? PointNeXt-B: C = 32, B = (1, 2, 1, 1) ? PointNeXt-L: C = 32, B = (2, 4, 2, 2) ? PointNeXt-XL: C = 64, B = (3, 6, 3, 3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate PointNeXt on five standard benchmarks: S3DIS <ref type="bibr" target="#b0">[1]</ref> and ScanNet <ref type="bibr" target="#b4">[5]</ref> for semantic segmentation, ScanObjectNN <ref type="bibr" target="#b43">[44]</ref> and ModelNet40 <ref type="bibr" target="#b48">[49]</ref> for object classification, and ShapeNetPart <ref type="bibr" target="#b2">[3]</ref> for object part segmentation.</p><p>Experimental Setups. We train PointNeXt using CrossEntropy loss with label smoothing <ref type="bibr" target="#b38">[39]</ref>, AdamW optimizer <ref type="bibr" target="#b26">[27]</ref>, an initial learning rate lr = 0.001, weight decay 10 ?4 , with Cosine Decay, and a batch size of 32, with a 32G V100 GPU, for all tasks, unless otherwise specified. The best model on the validation set is selected for testing. For S3DIS segmentation, point clouds are voxel downsampled with a voxel size of 0.04m following common practice <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b55">56]</ref>. PointNeXt is trained with an initial lr = 0.01, for 100 epochs (training set is repeated by 30 times), using a fixed number of points (24, 000) per batch with a batch size of 8 as input. During training, the input points are obtained by querying the nearest neighbors of a random point in each iteration. Following Point Transformer <ref type="bibr" target="#b55">[56]</ref>, we evaluate PointNeXt using the entire voxel-downsampled scene as input. For ScanNet scene segmentation, we follow the Stratified Transformer <ref type="bibr" target="#b16">[17]</ref> and train PointNeXt with multi-step learning rate decay and decay at [70,90] epochs with a decay rate of 0.1 without label smoothing. The voxel size is set to 0.02m and input number of points in training is set to 64, 000. We train the model for 100 epochs (training set is repeated for 6 times) with a batch size of 2 per GPU with 8 GPUs. For ScanOb-jectNN classification, PointNeXt is trained with a weight decay of 0.05 for 250 epochs. Following Point-BERT <ref type="bibr" target="#b53">[54]</ref>, the number of input points is set to 1, 024, where the points are randomly sampled during training and uniformly sampled during testing (denoted as point resampled augmentation). For ModelNet40 classification, PointNeXt is trained similarly as ScanObjectNN but for 600 epochs. For ShapeNetPart part segmentation, we train PointNeXt using a batch size of 8 per GPU with 4 GPUs, and Poly FocalLoss <ref type="bibr" target="#b17">[18]</ref> as criterion, for 400 epochs. Following PointNet++, 2,048 randomly sampled points with normals are used as input for training and testing. The details of data augmentations used in S3DIS, ScanNet, ScanObjectNN, ModelNet40 and ShapeNetPart are detailed in Sec. 4.4.1.  <ref type="bibr">8 46</ref> For all experiments except ShapeNetPart segmentation, we do not conduct any voting [23] 2 , since it is more standard to compare the performance without using any ensemble methods as suggested by SimpleView <ref type="bibr" target="#b8">[9]</ref>. However, we found that the performance in ShapeNetPart of nearly all models is quite close to each other, where it is hard to achieve state-of-the-art IoUs without voting. We also provide model parameters (Params.) and inference throughput (instances per second) for comparison. The throughput of all methods is measured using 128 ? 1024 (batch size 128, number of points 1024) as input in ScanObjectNN and ModelNet40 and 64 ? 2048 in ShapeNetPart. In S3DIS, 16 ? 15, 000 points are used to measure throughput following <ref type="bibr" target="#b31">[32]</ref>, since some methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b19">20]</ref> could not process the whole scene due to memory constraints. The throughput of all methods is measured using an NVIDIA Tesla V100 32GB GPU and a 32 core Intel Xeon @ 2.80GHz CPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D Semantic Segmentation in S3DIS and</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Object Classification in ScanObjectNN and ModelNet40</head><p>ScanObjectNN <ref type="bibr" target="#b43">[44]</ref> contains about 15, 000 real scanned objects that are categorized into 15 classes with 2, 902 unique object instances. Due to occlusions and noise, ScanObjectNN poses significant challenges to existing point cloud analysis methods. Following PointMLP <ref type="bibr" target="#b27">[28]</ref>, we experiment on PB_T50_RS, the hardest and most commonly used variant of ScanObjectNN. As reported in Tab. 2, the proposed PointNeXt-S surpasses existing methods by non-trivial margins in terms of both OA and mAcc, while using much fewer model parameters and running much faster. Built upon PointNet++ <ref type="bibr" target="#b29">[30]</ref>, PointNeXt achieves significant improvements over the originally reported performance of PointNet++, i.e. +9.8% OA and +10.4% mACC. This demonstrates the efficacy of the proposed training and model scaling strategies. PointNeXt also outperforms SOTA PointMLP <ref type="bibr" target="#b27">[28]</ref> (i.e. +2.3% OA, +1.9% mACC), while running 10? faster. This shows that PointNeXt is a simple, yet effective, and efficient baseline. Note that we did not experiment with upscaled variants of PointNeXt on this benchmark, since we found that the performance had saturated using PointNeXt-S mostly due to the limited scale of the dataset.</p><p>ModelNet40 <ref type="bibr" target="#b48">[49]</ref> was a commonly used 3D object classification dataset, which has 40 object categories, each of which contains 100 unique CAD models. However, recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> show an increasing interest in the real-world scanned dataset ScanObejectNN compared to this synthesized dataset. Following this trend, we mainly benchmarked PointNeXt in ScanObjectNN.</p><p>Here, we also provide our results in ModelNet40. Tab. 2 shows that advanced training strategies improve PointNet++ from 91.9% OA to 92.8% OA without any architecture change. PointNeXt-S (C = 32) outperforms the original reported PointNet++ by 1.3% OA, while being faster. Note that PointNeXt-S with a larger width C = 64 can achieve a higher overall accuracy (94.0%). to that of the SOTA CurveNet <ref type="bibr" target="#b49">[50]</ref> and outperforms a large number of representative networks, such as KPConv <ref type="bibr" target="#b42">[43]</ref> and ASSANet <ref type="bibr" target="#b31">[32]</ref> in terms of both instance mean IoU (ins. mIoU) and throughput. Due to the small scale of ShapeNetPart, the model would overfit after being depth scaled. However, we find by increasing the width from 32 to 64 instead, PointNeXt can outperform CurveNet, while being over 4? faster. It is also worth highlighting that PointNeXt with an even larger width (C = 160) reaches 87.0% Ins. mIoU, whereas the performance of point-based methods has saturated below this value for years. We highlight that we used voting only in ShapeNetPart by averaging the results of 10 randomly scaled input point clouds, with scaling factors equal to [0.8,1.2]. Without voting, we notice a performance drop around 0.5 instance mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">3D Object Part Segmentation in ShapeNetPart</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation and Analysis</head><p>Tab. 4 and Tab. 5 present additive studies for the proposed training and scaling strategies in ScanObjectNN <ref type="bibr" target="#b43">[44]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref>, respectively. We adopt the original PointNet++ as the baseline. In ScanObjectNN, PointNet++ was trained by <ref type="bibr" target="#b43">[44]</ref> with CrossEntropy loss, Adam optimizer, a learning rate 1e-3, a weight decay of 1e-4, a step decay of 0.7 for every 20 epochs, and a batch size of 16, for 250 epochs, while using random rotation and jittering as data augmentations. The official PointNet++ did not conduct experiments in S3DIS dataset. We refer to the widely used reimplementation <ref type="bibr" target="#b51">[52]</ref>, where PointNet++ was trained with the same settings as ScanObjectNN except that only random rotation was used as augmentation. Note that for all experiments, we train all our models for 250 epochs in ScanObjectNN and for 100 epochs in S3DIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Training Strategies</head><p>Data augmentation is the first aspect that we study to modernize PointNet++. We draw four conclusions based on observations in Tab. 4 and 5.</p><p>(1) Data scaling improves performance for both classification and segmentation tasks. For example, point resampling is shown to boost the performance by 2.5% OA in ScanObjectNN. Taking the entire scene as input instead of using the block or sphere subsampled input as done in PointNet++ <ref type="bibr" target="#b29">[30]</ref> and other previous works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> improves the segmentation result by 1.1% mIoU. (2) Height appending improves performance, especially for object classification. Height appending makes the network aware of the actual size of the objects, thus leading to an increase in accuracy (+1.1% OA). (3) Color drop is a strong augmentation that significantly improves the performance of tasks where colors are available. Adopting color drop alone adds 5.9% mIoU in S3DIS area 5. We hypothesize that color drop forces the network to focus more on the geometric relationships between points, which in turn improves performance. (4) Larger models favor stronger data augmentation. Whereas random rotation drops the performance of PointNet++ by 0.3% mIoU in S3DIS (2 nd row in Tab. 5 data augmentation part), it is shown to be beneficial for larger-scale models (e.g. raises 1.5% mIoU on PointNeXt-B). Another example in ScanObjectNN shows that the removal of random jittering also adds 1.1% OA. In general, with the improved data augmentations, the OA of PointNet++ in ScanObjectNN and the mIoU in S3DIS area 5 are increased by 5.8% and 9.5%, respectively.  Optimization techniques involve loss functions, optimizers, learning rate schedulers, and hyperparameters. As shown in Tab. 4 and 5, Label Smoothing, AdamW <ref type="bibr" target="#b26">[27]</ref> optimizer, and Cosine Decay consistently boost performance in both classification and segmentation tasks. This reveals that the more developed optimization methods such as label smoothing and AdamW are generally good for optimizing a neural network. Compared to Step Decay, Cosine Decay is also easier to tune (usually only the initial and minimum learning rates are required) and can achieve a performance similar to</p><p>Step Decay. Regarding hyperparameters, using a learning rate greater than that used in PointNet++ improves the segmentation performance in S3DIS.</p><p>In general, our training strategies consisted of stronger data augmentation and modern optimization techniques can increase the performance of PointNet++ from 77.9% to 86.1% OA in ScanObjectNN dataset, impressively surpassing SOTA PointMLP by 0.7%. The mIoUs in S3DIS area 5 and S3DIS 6-fold (illustrated in <ref type="figure">Fig. 1</ref>) are boosted by 11.7 and 13.6 absolute percentage points, respectively. Our observations imply that a significant portion of the performance gap between classical PointNet++ and SOTA is due to the training strategies. Generalize to other networks. Although the training strategies are proposed for PointNet++ <ref type="bibr" target="#b29">[30]</ref>, we find that they can be applied to other methods such as PointNet <ref type="bibr" target="#b28">[29]</ref>, DGCNN <ref type="bibr" target="#b45">[46]</ref>, and PointMLP <ref type="bibr" target="#b27">[28]</ref>, and also improve their performance. Such generalizability is validated in ScanObjectNN <ref type="bibr" target="#b43">[44]</ref>. As shown in Tab. 6, the OA of the representative methods can all be improved when equipped with our training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Model Scaling</head><p>Receptive field scaling includes both radius scaling and normalizing ? p defined in Eqn. <ref type="bibr" target="#b1">(2)</ref>, which are also validated in Tab. 4 and 5. The radius is dataset specific, while down-scaling the radius from 0.2 to 0.15 improves 0.3% OA in ScanObjectNN, keeping the radius the same as 0.1 achieves the best performance in S3DIS. Regarding normalizing ? p , it improves the performance in ScanObjectNN and S3DIS by 0.3 OA and 0.4 mIoU, respectively. Furthermore, in Tab. 7, we show that normalizing ? p has a larger impact (2.3 mIoU in S3DIS dataset) on the bigger model PointNext-XL.</p><p>Model scaling scales PointNet++ by the proposed InvResMLP and some macro-architectural changes (see Sec. 3.2.2). In Tab. 4, we show that PointNeXt-S using the stem MLP, the symmetric decoder, and the residual connection in the SA block improves 1.0% OA in ScanObjectNN. Performance in the large-scale S3DIS dataset can be further unveiled (from 63.8% to 70.5% mIoU) by up-scaling PointNeXt-S using more blocks of the proposed InvResMLP, as demonstrated in Tab. 5. Furthermore, in Tab. 7, we ablate each component of the proposed InvResMLP block and different stage ratios in S3DIS area 5 using the best-performed model PointNeXt-XL as the baseline. As observed, each architectural change indeed contributes to increased performance. Among all changes, the residual connection is the most essential, without which the mIoU will drop from 70.5% to only 64.0%. The separable MLPs increase 3.9% mIoU while speeding up the network 3 times. Removing the inverted bottleneck from the baseline leads to a drop of 1.5% mIoU with less than a 1% gain in speed. Adding more blocks inside each stage after removing inverted bottleneck can improve its performance to 69.7 ? 0.3 but is still lower than the baseline. Tab. 7 also shows the performance of naive width scaling that increases the width of PointNet++ from 32 to 256 to match the throughput of PointNeXt-XL, naive depth scaling to append more SA blocks in PointNet++ to obtain the same number of blocks of PointNext-XL whose B = <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref>, and naive compound scaling to double the width of the naive depth scaled model to the same width as PointNeXt-XL (C = 64). Our proposed model scaling strategy achieves much higher performance than these naive scaling strategies, while being much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Point-based methods process point clouds directly using their unstructured format compared to voxel-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref> and multi view-based methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>. PointNet <ref type="bibr" target="#b28">[29]</ref>, the pioneering work of point-based methods, proposes to model the permutation invariance of points with shared MLPs by restricting feature extraction to be pointwise. PointNet++ <ref type="bibr" target="#b29">[30]</ref> is presented to improve PointNet by capturing local geometric structures. Currently, most point-based methods focus on the design of local modules. <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b30">31]</ref> rely on graph neural networks. <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42]</ref> project point clouds onto pseudo grids to allow for regular convolutions. <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> adaptively aggregate neighborhood features through weights determined by the local structure. In addition, very recent methods leverage Transformer-like networks <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b16">17]</ref> to extract local information through self-attention. Our work does not follow this trend in local module design. In contrast, we shift our attention to another important but largely under-explored aspect, i.e., the training and scaling strategies.</p><p>Training strategies are studied recently in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26]</ref> on image classification. In the point cloud domain, SimpleView <ref type="bibr" target="#b8">[9]</ref> is the first work to show that training strategies have a large impact on the performance of a neural network. However, SimpleView simply adopts the same training strategies as DGCNN <ref type="bibr" target="#b45">[46]</ref>. On the contrary, we conducted a systematic study to quantify the effect of each data augmentation and optimization technique, and propose a set of improved training strategies that boost the performance of PointNet++ <ref type="bibr" target="#b29">[30]</ref> and other representative works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Model scaling can significantly improve the performance of a network, as shown in pioneering works in various domains <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b20">21]</ref>. Compared to PointNet++ <ref type="bibr" target="#b29">[30]</ref> that uses parameters less than 2M, most current prevailing networks consist of parameters greater than 10 M, such as KPConv <ref type="bibr" target="#b42">[43]</ref> (15M) and PointMLP <ref type="bibr" target="#b27">[28]</ref> (13M). In our work, we explore model scaling strategies that can scale up PointNet++ in an effective and efficient manner. We offer practical suggestions on scaling technologies that improve performance, namely using residual connections and an inverted bottleneck design, while maintaining throughput by using separable MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>In this paper, we demonstrate that with improved training and scaling strategies, the performance of PointNet++ can be increased to exceed the current state of the art. More specifically, we quantify the effect of each data augmentation and optimization technique that are widely used today, and propose a set of improved training strategies. These strategies can be easily applied to boost the performance of PointNet++ and other representative works. We also introduce the Inverted Residual MLP block into PointNet++ to develop PointNeXt. We demonstrate that PointNeXt has superior performance and scalability over PointNet++ on various benchmarks while maintaining high throughput. This work aims to guide researchers toward paying more attention to the effects of training and scaling strategies and motivate future work in this direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Strategies Comparison</head><p>In this section, we summarize the training strategies used in representative point-based methods such as DGCNN <ref type="bibr" target="#b45">[46]</ref>, KPConv <ref type="bibr" target="#b42">[43]</ref>, PointMLP <ref type="bibr" target="#b27">[28]</ref>, Point Transformer <ref type="bibr" target="#b55">[56]</ref>, Stratified Transformer <ref type="bibr" target="#b16">[17]</ref>, PointNet++ <ref type="bibr" target="#b29">[30]</ref>, and our PointNeXt on S3DIS <ref type="bibr" target="#b0">[1]</ref> in Tab. I, on ScanObjectNN <ref type="bibr" target="#b43">[44]</ref> in Tab. II, on ScanNet <ref type="bibr" target="#b4">[5]</ref> in Tab. III, and on ShapeNetPart <ref type="bibr" target="#b52">[53]</ref> in Tab. IV, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Results</head><p>We provide qualitative results of PointNeXt-XL for S3DIS ( <ref type="figure">Fig. II)</ref> and PointNeXt-S (C = 160) for ShapeNetPart <ref type="figure">(Fig. III)</ref>. The qualitative results of PointNet++ trained with the original training strategies are also included in the figures for comparison. On both datasets, PointNeXt produces predictions closer to the ground truth compared to PointNet++. More specifically, on S3DIS shown in <ref type="figure">(Fig. II)</ref>, PointNeXt is able to segment hard classes, including doors (1 st , 3 rd , and 4 th rows), clutter (1 st and 3 rd rows), chairs (2 nd row), and the board (4 th row), while PointNet++ fails to segment properly to some extent. On ShapeNetPart <ref type="figure">(Fig. III)</ref>, PointNeXt precisely segments wings of an airplane (1 st row), microphone of an earphone(2 nd row), body of a motorbike(3 rd row), fin of a rocket(4 th row), and bearing of a skateboard (5 th row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Classification Architecture</head><p>As illustrated in <ref type="figure">Fig. I</ref>, the classification architecture shares the same encoder as the segmentation one. The output features of the encoder are passed to a global pooling layer (i.e. global max-pooling) to acquire a global shape representation for classification. Note that the points are only downsampled by a factor of 2 in each stage, since the number of input points in classification tasks is usually small, e.g. 1024 or 2048 points.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Societal Impact</head><p>We do not see an immediate negative societal impact from our work. We notice that the way we discover the improved training and scaling strategies may consume a little more computing resources and affect the environment. Nevertheless, the improved training and scaling strategies will make researchers pay more attention to aspects other than architectural changes, which in the long term makes research in computer vision more diverse and generally better.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure I :Figure II :Figure III :</head><label>IIIIII</label><figDesc>PointNeXt architecture for classification. The classification architecture shares the same encoder as the segmentation architecture. Qualitative comparisons of PointNet++ (2 nd column), PointNeXt (3 rd column), and Ground Truth (4 th column) on S3DIS semantic segmentation. The input point cloud is visualized with original colors in the 1 st column. Differences between PointNet++ and PointNeXt are highlighted with red dash circles. Zoom-in for details. Qualitative comparisons of PointNet++ (left), PointNeXt (middle), and Ground Truth (right) on ShapeNetPart part segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>3D semantic segmentation in S3DIS (evaluation by 6-Fold or in Area 5) and ScanNet V2. For PointNeXt in S3DIS Area 5, the average results without voting in three random runs are reported. The improvements of PointNeXt over the original performance reported by PointNet++<ref type="bibr" target="#b29">[30]</ref> are highlighted in green color. PointNet++ (ours) denotes PointNet++ trained using our improved data augmentation and optmization techniques. Methods are in chronological order. XL (ours) 74.9 (+20.<ref type="bibr" target="#b3">4)</ref> 90.3 (+9.3) 70.5?0.3(+17.0) 90.6?0.1(+7.6) 71.5(+18.0) 71.2(+15.5) 41.6 84.</figDesc><table><row><cell>Method</cell><cell cols="2">S3DIS 6-Fold mIoU OA</cell><cell>mIoU</cell><cell cols="2">S3DIS Area-5 OA</cell><cell cols="2">ScanNet V2 Val mIoU Test mIoU</cell><cell cols="3">Params. FLOPs Throughput</cell></row><row><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>M</cell><cell>G</cell><cell>(ins./sec.)</cell></row><row><cell>PointNet [29]</cell><cell>47.6</cell><cell>78.5</cell><cell>41.1</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">3.6 35.5</cell><cell>162</cell></row><row><cell>PointCNN [22]</cell><cell>65.4</cell><cell>88.1</cell><cell>57.3</cell><cell></cell><cell>85.9</cell><cell>-</cell><cell>45.8</cell><cell>0.6</cell><cell>-</cell><cell>-</cell></row><row><cell>DGCNN [46]</cell><cell>56.1</cell><cell>84.1</cell><cell>47.9</cell><cell></cell><cell>83.6</cell><cell>-</cell><cell>-</cell><cell>1.3</cell><cell>-</cell><cell>8</cell></row><row><cell>DeepGCN [21]</cell><cell>60.0</cell><cell>85.9</cell><cell>52.5</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.6</cell><cell>-</cell><cell>3</cell></row><row><cell>KPConv [43]</cell><cell>70.6</cell><cell>-</cell><cell>67.1</cell><cell></cell><cell>-</cell><cell>69.2</cell><cell>68.6</cell><cell>15.0</cell><cell>-</cell><cell>30</cell></row><row><cell>RandLA-Net [15]</cell><cell>70.0</cell><cell>88.0</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>64.5</cell><cell>1.3</cell><cell>5.8</cell><cell>159</cell></row><row><cell>BAAF-Net [33]</cell><cell>72.2</cell><cell>88.9</cell><cell>65.4</cell><cell></cell><cell>88.9</cell><cell>-</cell><cell>-</cell><cell>5.0</cell><cell>-</cell><cell>10</cell></row><row><cell cols="2">Point Transformer [56] 73.5</cell><cell>90.2</cell><cell>70.4</cell><cell></cell><cell>90.8</cell><cell>70.6</cell><cell>-</cell><cell>7.8</cell><cell>5.6</cell><cell>34</cell></row><row><cell>CBL [41]</cell><cell>73.1</cell><cell>89.6</cell><cell>69.4</cell><cell></cell><cell>90.6</cell><cell>-</cell><cell>70.5</cell><cell>18.6</cell><cell>-</cell><cell>-</cell></row><row><cell>PointNet++ [30]</cell><cell>54.5</cell><cell>81.0</cell><cell>53.5</cell><cell></cell><cell>83.0</cell><cell>53.5</cell><cell>55.7</cell><cell>1.0</cell><cell>7.2</cell><cell>186</cell></row><row><cell>PointNet++ (ours)</cell><cell cols="4">68.1(+13.6) 87.6(+6.2) 63.2?0.4(+9.7)</cell><cell cols="2">87.5?0.2(+4.5) 57.2(+3.7)</cell><cell>-</cell><cell>1.0</cell><cell>7.2</cell><cell>186</cell></row><row><cell>PointNeXt-S (ours)</cell><cell cols="4">68.0(+13.5) 87.4(+6.4) 63.4?0.8(+9.9)</cell><cell cols="3">87.9?0.3(+4.9) 64.5(+11.0) -</cell><cell>0.8</cell><cell>3.6</cell><cell>227</cell></row><row><cell>PointNeXt-B (ours)</cell><cell cols="7">71.5(+17.0) 88.8(+7.8) 67.3?0.2(+13.8) 89.4?0.1(+6.4) 68.4(+14.9) -</cell><cell>3.8</cell><cell>8.9</cell><cell>158</cell></row><row><cell>PointNeXt-L (ours)</cell><cell cols="7">73.9(+19.4) 89.8(+8.8) 69.0?0.5(+15.5) 90.0?0.1(+7.0) 69.4(+15.9) -</cell><cell cols="2">7.1 15.2</cell><cell>115</cell></row><row><cell>PointNeXt-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We also provide the results of PointNeXt in S3DIS area 5 in the Tab. 1 with mean?std in three random runs, where PointNeXt achieves similar improvements as the 6-fold experiments.ScanNet<ref type="bibr" target="#b4">[5]</ref>, another well-known large-scale segmentation dataset, contains 3D indoor scenes of various rooms with 20 semantic categories. We follow the public training, validation, and test splits, with 1201, 312 and 100 scans, respectively. For PointNet++, we use the results reported from the Stratified Transformer<ref type="bibr" target="#b16">[17]</ref> for comparison. As shown in Tab. 1, we improve PointNet++ from 53.5%</figDesc><table /><note>ScanNet S3DIS [1] (Stanford Large-Scale 3D Indoor Spaces) is a challenging benchmark composed of 6 large-scale indoor areas, 271 rooms, and 13 semantic categories in total. The standard 6-fold cross-validation results in S3DIS are reported in Tab. 1. Note that the official PointNet++ [30] did not conduct experiments in S3DIS. Here, we use the results reported by PointCNN [22] for comparison. Our PointNeXt-S, the smallest variant, outperforms PointNet++ by 13.5%, 6.4%, and 10.2% in terms of mean IoU (mIoU), overall accuracy (OA), and mean accuracy (mAcc), respectively, while being faster in terms of throughput. The increased speed is due to the reduced number of layers in the SA block for PointNeXt-S (see Sec. 3.2.2). With the proposed model scaling, the performance of PointNeXt can be gradually boosted. For example, PointNeXt-L outperforms SOTA Point Transformer [56] by 0.4% in mIoU while being 3? faster. Note that Point Transformer utilizes most of the improved training strategies of ours. PointNeXt-XL, the extra large variant, achieves mIoU/OA/mAcc of 74.9%/90.3%/83.0%, while running faster than Point Transformer. As a limitation, our PointNeXt-XL consists of more parameters and is more computationally expensive in terms of FLOPs, mainly due to channel expansion (?4) in the inverted bottleneck and doubled initial channel size (C = 64).mIou to 57.2% mIoU in the validation set by adopting the improved training strategies (detailed in supplementary material). PointNeXt-S further gains +11.0 in val mIoU over the original PointNet++ mostly due to the use of a smaller radius (0.1m ? 0.05m) and relative position normalization. The performance in ScanNet improves steadily with the increase in model sizes. Our largest variant,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>3D object classification in ScanObjectNN and ModelNet40. Averaged results in three random runs using 1024 points as input without normals and without voting are reported.</figDesc><table><row><cell></cell><cell cols="2">ScanObjectNN (PB_T50_RS)</cell><cell cols="2">ModelNet40</cell><cell cols="3">Params. FLOPs Throughput</cell></row><row><cell>Method</cell><cell>OA (%)</cell><cell>mAcc (%)</cell><cell>OA (%)</cell><cell>mAcc (%)</cell><cell>M</cell><cell>G</cell><cell>(ins./sec.)</cell></row><row><cell>PointNet [29]</cell><cell>68.2</cell><cell>63.4</cell><cell>89.2</cell><cell>86.2</cell><cell>3.5</cell><cell>0.9</cell><cell>4212</cell></row><row><cell>PointCNN [22]</cell><cell>78.5</cell><cell>75.1</cell><cell>92.2</cell><cell>88.1</cell><cell>0.6</cell><cell>-</cell><cell>44</cell></row><row><cell>DGCNN [46]</cell><cell>78.1</cell><cell>73.6</cell><cell>92.9</cell><cell>90.2</cell><cell>1.8</cell><cell>4.8</cell><cell>402</cell></row><row><cell>DeepGCN [20]</cell><cell>-</cell><cell>-</cell><cell>93.6</cell><cell>90.9</cell><cell>2.2</cell><cell>3.9</cell><cell>263</cell></row><row><cell>KPConv [43]</cell><cell>-</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>14.3</cell><cell>-</cell><cell>-</cell></row><row><cell>ASSANet-L [32]</cell><cell>-</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>118.4</cell><cell>-</cell><cell>153</cell></row><row><cell>SimpleView [9]</cell><cell>80.5?0.3</cell><cell>-</cell><cell>93.0?0.4</cell><cell>90.5?0.8</cell><cell>0.8</cell><cell>-</cell><cell>-</cell></row><row><cell>MVTN [12]</cell><cell>82.8</cell><cell>-</cell><cell>93.5</cell><cell>92.2</cell><cell>3.5</cell><cell>1.8</cell><cell>236</cell></row><row><cell cols="2">Point Cloud Transformer [11] -</cell><cell>-</cell><cell>93.2</cell><cell>-</cell><cell>2.9</cell><cell>2.3</cell><cell>-</cell></row><row><cell>CurveNet [50]</cell><cell>-</cell><cell>-</cell><cell>93.8</cell><cell>-</cell><cell>2.0</cell><cell>-</cell><cell>22</cell></row><row><cell>PointMLP [28]</cell><cell>85.4?1.3</cell><cell>83.9?1.5</cell><cell>94.1</cell><cell>91.3</cell><cell>13.2</cell><cell>31.3</cell><cell>191</cell></row><row><cell>PointNet++ [30]</cell><cell>77.9</cell><cell>75.4</cell><cell>91.9</cell><cell>-</cell><cell>1.5</cell><cell>1.7</cell><cell>1872</cell></row><row><cell>PointNet++ (ours)</cell><cell cols="2">86.1?0.7(+8.2) 84.2?0.9(+8.8)</cell><cell cols="2">92.8?0.1(+0.9) 89.9?0.8</cell><cell>1.5</cell><cell>1.7</cell><cell>1872</cell></row><row><cell>PointNeXt-S (ours)</cell><cell cols="4">87.7?0.4(+9.8) 85.8?0.6(+10.4) 93.2?0.1(+1.3) 90.8?0.2</cell><cell>1.4</cell><cell>1.6</cell><cell>2040</cell></row></table><note>PointNeXt-XL outperforms PointNet++ by 18.0% mIoU in validation and achieves 71.2% mIoU in testing, beating the recent methods Point Transformer [56] and CBL [41].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Part segmentation in ShapeNetPart.</figDesc><table><row><cell>ShapeNetPart [53] is a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>widely-used dataset for object-level part segmenta-tion. It consists of 16, 880</cell><cell>Method PointNet [29] DGCNN [46]</cell><cell>ins. mIoU 83.7 85.2</cell><cell>cls. mIoU 80.4 82.3</cell><cell cols="3">Params. FLOPs Throughput 3.6 4.9 1184 1.3 12.4 147</cell></row><row><cell>models from 16 different shape categories, 2-6 parts for each category, and</cell><cell cols="2">KPConv [43] CurveNet [50] ASSANet-L [32] Point Transformer [56] 86.6 86.4 86.8 86.1</cell><cell>85.1 --83.7</cell><cell>---7.8</cell><cell>----</cell><cell>44 97 640 297</cell></row><row><cell>50 part labels in total. As shown in Tab. 3, our PointNeXt-S with default</cell><cell>PointMLP [28] Stratifiedformer [17] PointNet++ [30] PointNeXt-S</cell><cell cols="2">86.1 86.6 85.1 86.7?0.0(+1.6) 84.4?0.2(+2.5) 84.6 85.1 81.9</cell><cell>--1.0 1.0</cell><cell>--4.9 4.5</cell><cell>270 398 708 782</cell></row><row><cell>width (C = 32) obtains a performance comparable</cell><cell>PointNeXt-S (C=64) PointNeXt-S (C=160)</cell><cell cols="2">86.9?0.1(+1.8) 84.8?0.5(+2.9) 87.0?0.1(+1.9) 85.2?0.1(+3.3)</cell><cell>3.7 22.5</cell><cell>17.8 110.2</cell><cell>331 76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Additive study of sequentially applying training and scaling strategies for classification on ScanOb-jectNN. We use light green, purple, yellow, and pink background colors to denote data augmentation, optimization techniques, receptive field scaling, and model scaling, respectively.</figDesc><table><row><cell>Improvements</cell><cell>OA (%)</cell><cell>?</cell></row><row><cell>PointNet++</cell><cell>77.9</cell><cell>-</cell></row><row><cell>+ Point resampling</cell><cell cols="2">81.4 ? 0.6 +2.5</cell></row><row><cell>? Jittering</cell><cell cols="2">82.5 ? 0.4 +1.1</cell></row><row><cell>+ Height appending</cell><cell cols="2">83.6 ? 0.4 +1.1</cell></row><row><cell>+ Random scaling</cell><cell cols="2">83.7 ? 0.2 +0.1</cell></row><row><cell>+ Label Smoothing</cell><cell cols="2">85.0 ? 0.5 +1.3</cell></row><row><cell>+ Adam ? AdamW</cell><cell cols="2">85.6 ? 0.1 +0.6</cell></row><row><cell cols="3">+ Step Decay ? Cosine Decay 86.1 ? 0.7 +0.5</cell></row><row><cell>+ Radius 0.2 ? 0.15</cell><cell cols="2">86.4 ? 0.3 +0.3</cell></row><row><cell>+ Normalizing ?p (Eqn. (2))</cell><cell cols="2">86.7 ? 0.3 +0.3</cell></row><row><cell>+ Scale up (PointNeXt-S)</cell><cell cols="2">87.7 ? 0.4 +1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Step Decay ? Cosine Decay 63.2 ? 0.4 +0.7</figDesc><table><row><cell cols="3">Additive study of sequentially apply-</cell></row><row><cell cols="3">ing training and scaling strategies for segmen-</cell></row><row><cell cols="3">tation on S3DIS area 5. +/? denote adopt-</cell></row><row><cell>ing/removing the strategy.</cell><cell></cell><cell></cell></row><row><cell>Improvements</cell><cell>mIoU (%)</cell><cell>?</cell></row><row><cell>PointNet++</cell><cell>51.5</cell><cell>-</cell></row><row><cell>+ Entire scene as input</cell><cell cols="2">52.6 ? 0.5 +1.1</cell></row><row><cell>? Rotation</cell><cell cols="2">52.9 ? 0.6 +0.3</cell></row><row><cell>+ Height appending</cell><cell cols="2">53.4 ? 0.4 +0.5</cell></row><row><cell>+ Color drop</cell><cell cols="2">59.3 ? 0.7 +5.9</cell></row><row><cell>+ Color auto-contrast</cell><cell cols="2">61.0 ? 0.4 +0.7</cell></row><row><cell>+ lr = 0.001 ? 0.01</cell><cell cols="2">61.5 ? 0.5 +0.5</cell></row><row><cell>+ Label Smoothing</cell><cell cols="2">61.9 ? 0.1 +0.4</cell></row><row><cell>+ Adam ? AdamW</cell><cell cols="2">62.5 ? 0.6 +0.6</cell></row><row><cell>+ + Normalize ?p</cell><cell cols="2">63.6 ? 0.4 +0.4</cell></row><row><cell>+ Scale down (PointNeXt-S)</cell><cell cols="2">63.4 ? 0.8 -0.2</cell></row><row><cell>+ Scale up (PointNeXt-B)</cell><cell cols="2">65.8 ? 0.5 +2.4</cell></row><row><cell>+ Rotation</cell><cell cols="2">67.3 ? 0.2 +1.5</cell></row><row><cell>+ Scale up (PointNeXt-L)</cell><cell cols="2">69.0 ? 0.5 +1.7</cell></row><row><cell>+ Scale up (PointNeXt-XL)</cell><cell cols="2">70.5 ? 0.3 +1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The</figDesc><table><row><cell></cell><cell cols="2">generalizability of im-</cell></row><row><cell cols="3">proved training strategies. OA on</cell></row><row><cell cols="3">ScanObjectNN of networks trained with</cell></row><row><cell cols="3">improved training strategies is reported.</cell></row><row><cell>Method</cell><cell>ours</cell><cell>?</cell></row><row><cell>PointNet [29]</cell><cell cols="2">74.4 ? 0.9 +6.2</cell></row><row><cell>DGCNN [46]</cell><cell cols="2">86.0 ? 0.5 +7.9</cell></row><row><cell cols="3">PointMLP [28] 87.1 ? 0.7 +1.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablate architectural changes on S3DIS area 5. ? denotes removing from baseline. TP denotes throughput.</figDesc><table><row><cell>Ablate</cell><cell>mIoU</cell><cell>?</cell><cell>TP</cell></row><row><cell cols="2">baseline (PointNeXt-XL) 70.5 ? 0.3</cell><cell>-</cell><cell>45</cell></row><row><cell>? normalizing ?p</cell><cell>68.2 ? 0.7</cell><cell>-2.3</cell><cell>45</cell></row><row><cell>? residual connection</cell><cell>64.0 ? 1.0</cell><cell>-6.5</cell><cell>45</cell></row><row><cell>? stem MLP</cell><cell>70.1 ? 0.4</cell><cell>-0.4</cell><cell>46</cell></row><row><cell>? Separable MLPs</cell><cell>66.6 ? 0.8</cell><cell>-3.9</cell><cell>15</cell></row><row><cell>? Inverted bottleneck</cell><cell>69.0 ? 0.4</cell><cell>-1.5</cell><cell>48</cell></row><row><cell>? Inverted bottleneck</cell><cell>69.7 ? 0.3</cell><cell>-0.8</cell><cell>43</cell></row><row><cell>stage ratio ? (1:1:1:1)</cell><cell>69.8 ? 0.6</cell><cell>-0.7</cell><cell>52</cell></row><row><cell>stage ratio ? (2:1:1:1)</cell><cell>69.4 ? 0.4</cell><cell>-1.1</cell><cell>41</cell></row><row><cell>stage ratio ? (1:1:2:1)</cell><cell>69.9 ? 0.6</cell><cell>-0.6</cell><cell>47</cell></row><row><cell>stage ratio ? (1:1:1:2)</cell><cell>69.5 ? 0.4</cell><cell>-1.0</cell><cell>48</cell></row><row><cell>stage ratio ? (1:3:1:1)</cell><cell>70.1 ? 0.4</cell><cell>-0.4</cell><cell>39</cell></row><row><cell>naive width scaling</cell><cell cols="3">59.4 ? 0.1 -11.1 43</cell></row><row><cell>naive depth scaling</cell><cell>63.4 ? 0.5</cell><cell>-7.1</cell><cell>53</cell></row><row><cell>naive compound scaling</cell><cell>62.3 ? 1.2</cell><cell>-8.2</cell><cell>24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Limitation. Even though PointNeXt-XL is one of the largest models among all representative pointbased networks<ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b55">56]</ref>, its number of parameters (44M) is still below that of small networks in image classification such as Swin-S<ref type="bibr" target="#b24">[25]</ref> (50M), ConNeXt-S<ref type="bibr" target="#b25">[26]</ref> (50M), and ViT-B<ref type="bibr" target="#b7">[8]</ref> (87M), and is far from their large variants, including Swin-L (197M), ConvNeXt-XL (350M), and ViT-L (305M). In this work, we do not push the model size further, mainly due to the smaller-scale nature of point cloud datasets compared to their larger image counterparts, such as ImageNet<ref type="bibr" target="#b5">[6]</ref>. Moreover, our work is limited to existing modules since the focus is not on introducing new architectural changes.Naive width scaling increases the channel size of PointNet++ from 32 to 256 to match the throughput of the baseline model, PointNeXt-XL. Naive depth scaling refers to appending more SA blocks (B = (3, 6, 3, 3), the same as PointNext-XL) in PointNet++. Furthermore, naive compound scaling doubles the width of naive depth scaled model to the same as PointNeXt-XL (C = 64). Compared to the PointNet++ trained with improved training strategies (63.2% mIoU, 186 ins./sec.), naive depth scaling (63.4% mIoU, 53 ins. / sec.) and naive width scaling (59.4% mIoU, 43 ins./sec.) only lead to a large overhead in throughput with insignificant improvement in accuracy. In contrast, our proposed model scaling strategy achieves much higher performance than the naive scaling strategies while being much faster. This can be observed by comparing PointNeXt-XL (70.5% mIoU, 45 ins./sec.) to the naive compound scaled PointNet++ (62.3% mIoU, 24 ins./sec.).</figDesc><table><row><cell>PointNeXt: Revisiting PointNet++ with</cell></row><row><cell>Improved Training and Scaling Strategies</cell></row><row><cell>-Supplementary Material -</cell></row><row><cell>In this appendix, we provide additional content to complement the main manuscript:</cell></row><row><cell>? Appendix A: A detailed description of Tab. 7.</cell></row><row><cell>? Appendix B: Comparisons of training strategies for prior representative works and PointNeXt.</cell></row><row><cell>? Appendix C: Qualitative comparisons on S3DIS and ShapeNetPart.</cell></row><row><cell>? Appendix D: The architecture of PointNeXt for classification.</cell></row><row><cell>? Appendix E: Societal impact.</cell></row><row><cell>A Detailed Description for Manuscript Tab. 7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table I :</head><label>I</label><figDesc>Training strategies used in different methods for S3DIS segmentation.</figDesc><table><row><cell>Method</cell><cell cols="5">DGCNN KPConv PointTransformer PointNet++ PointNeXt (Ours)</cell></row><row><cell>Epochs</cell><cell>101</cell><cell>500</cell><cell>100</cell><cell>32</cell><cell>100</cell></row><row><cell>Batch size</cell><cell>12</cell><cell>10</cell><cell>16</cell><cell>16</cell><cell>8</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>SGD</cell><cell>SGD</cell><cell>Adam</cell><cell>AdamW</cell></row><row><cell>LR</cell><cell cols="2">1 ? 10 ?3 1 ? 10 ?2</cell><cell>0.5</cell><cell>1 ? 10 ?3</cell><cell>0.01</cell></row><row><cell>LR decay</cell><cell>step</cell><cell>step</cell><cell>multi step</cell><cell>step</cell><cell>cosine</cell></row><row><cell>Weight decay</cell><cell>0</cell><cell>10 ?3</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell></row><row><cell>Label smoothing ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell>Entire scene as input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random rotation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random scaling</cell><cell></cell><cell>[0.8,1.2]</cell><cell>[0.9,1.1]</cell><cell></cell><cell>[0.9,1.1]</cell></row><row><cell>Random translation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random jittering</cell><cell></cell><cell>0.001</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Height appending</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Color drop</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell>Color auto-contrast</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Color jittering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU (%)</cell><cell>56.1</cell><cell>70.6</cell><cell>73.5</cell><cell>54.5</cell><cell>74.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table II :</head><label>II</label><figDesc>Training strategies used in different methods for ScanObecjectNN classification.</figDesc><table><row><cell>Method</cell><cell cols="4">DGCNN PointMLP PointNet++ PointNeXt (Ours)</cell></row><row><cell>Epochs</cell><cell>250</cell><cell>200</cell><cell>250</cell><cell>250</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>32</cell><cell>16</cell><cell>32</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>SGD</cell><cell>Adam</cell><cell>AdamW</cell></row><row><cell>LR</cell><cell>1 ? 10 ?3</cell><cell>0.01</cell><cell>10 ?3</cell><cell>2 ? 10 ?3</cell></row><row><cell>LR decay</cell><cell>step</cell><cell>cosine</cell><cell>step</cell><cell>cosine</cell></row><row><cell>Weight decay</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>0.05</cell></row><row><cell>Label smoothing ?</cell><cell>0.2</cell><cell>0.2</cell><cell></cell><cell>0.3</cell></row><row><cell>Point resampling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random rotation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random scaling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random translation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random jittering</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Height appending</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OA (%)</cell><cell>78.1</cell><cell>85.7</cell><cell>77.9</cell><cell>87.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table III :</head><label>III</label><figDesc>Training strategies used in different methods for ScanNet segmentation.</figDesc><table><row><cell>Method</cell><cell cols="5">KPConv PointTransformer Stratified Transformer PointNet++ PointNeXt (Ours)</cell></row><row><cell>Epochs</cell><cell>500</cell><cell>100</cell><cell>100</cell><cell>200</cell><cell>100</cell></row><row><cell>Batch size</cell><cell>10</cell><cell>16</cell><cell>8</cell><cell>32</cell><cell>2</cell></row><row><cell>Optimizer</cell><cell>SGD</cell><cell>SGD</cell><cell>AdamW</cell><cell>Adam</cell><cell>AdamW</cell></row><row><cell>LR</cell><cell>1 ? 10 ?2</cell><cell>5 ? 10 ?1</cell><cell>6 ? 10 ?3</cell><cell>1 ? 10 ?3</cell><cell>1 ? 10 ?3</cell></row><row><cell>LR decay</cell><cell>step</cell><cell>multi step</cell><cell>multi step with warm up</cell><cell>step</cell><cell>multi step</cell></row><row><cell>Weight decay</cell><cell>10 ?3</cell><cell>10 ?4</cell><cell>5 ? 10 ?2</cell><cell>10 ?4</cell><cell>10 ?4</cell></row><row><cell>Entire scene as input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random rotation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random scaling</cell><cell>[0.9,1.1]</cell><cell>[0.9,1.1]</cell><cell>[0.8,1.2]</cell><cell></cell><cell>[0.8,1.2]</cell></row><row><cell>Random translation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random jittering</cell><cell>0.001</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Height appending</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Color drop</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.2</cell></row><row><cell>Color auto-contrast</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Color jittering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test mIoU (%)</cell><cell>68.6</cell><cell>-</cell><cell>73.7</cell><cell>55.7</cell><cell>71.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table IV :</head><label>IV</label><figDesc>Training strategies used in different methods for ShapeNetPart segmentation.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="7">DGCNN KPConv PointNet++ PointNeXt (Ours)</cell></row><row><cell>Epochs</cell><cell></cell><cell></cell><cell>201</cell><cell>500</cell><cell></cell><cell>201</cell><cell></cell><cell></cell><cell>300</cell></row><row><cell>Batch size</cell><cell></cell><cell></cell><cell>16</cell><cell>16</cell><cell></cell><cell>32</cell><cell></cell><cell></cell><cell>8</cell></row><row><cell>Optimizer</cell><cell></cell><cell></cell><cell>Adam</cell><cell>SGD</cell><cell></cell><cell cols="2">Adam</cell><cell></cell><cell>AdamW</cell></row><row><cell>LR</cell><cell></cell><cell></cell><cell cols="3">3 ? 10 ?3 1 ? 10 ?2</cell><cell cols="2">1 ? 10 ?3</cell><cell></cell><cell>0.001</cell></row><row><cell>LR decay</cell><cell></cell><cell></cell><cell>step</cell><cell>step</cell><cell></cell><cell>step</cell><cell></cell><cell></cell><cell>multi step</cell></row><row><cell cols="2">Weight decay</cell><cell></cell><cell>0.0</cell><cell>10 ?3</cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell>10 ?4</cell></row><row><cell cols="3">Label smoothing ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Random rotation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Random scaling</cell><cell></cell><cell></cell><cell cols="2">[0.9,1.1]</cell><cell></cell><cell></cell><cell></cell><cell>[0.8,1.2]</cell></row><row><cell cols="3">Random translation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Random jittering</cell><cell></cell><cell></cell><cell>0.001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.001</cell></row><row><cell>Normal Drop</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Height appending</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU (%)</cell><cell></cell><cell></cell><cell>85.2</cell><cell>86.4</cell><cell></cell><cell>85.1</cell><cell></cell><cell></cell><cell>87.0</cell></row><row><cell>MLP</cell><cell>Set Abstraction</cell><cell>InvResMLP</cell><cell>Set Abstraction</cell><cell>InvResMLP</cell><cell>Set Abstraction</cell><cell>InvResMLP</cell><cell>Set Abstraction</cell><cell>InvResMLP</cell><cell>Global Pooling</cell></row><row><cell>[N,32]</cell><cell cols="2">[N/2,64]</cell><cell cols="2">[N/4,128]</cell><cell cols="2">[N/8,256]</cell><cell cols="2">[N/16,512]</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The voting strategy combines results by using randomly augmented points as input to enhance performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The authors would like to thank the reviewers of NeurIPS'22 for their constructive suggestions. This work was supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research through the Visual Computing Center (VCC) funding, as well as, the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Revisiting resnets: Improved training and scaling strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3809" to="3820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Pct: Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mvtn: Multi-view transformation network for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stratified transformer for 3d point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Polyloss: A polynomial expansion perspective of classification loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqi</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drago</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="6437" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on X -transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8887" to="8896" />
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="326" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking network design and local geometry in point cloud: A simple residual MLP framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Haoxi Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pu-gcn: Point cloud upsampling using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="11683" to="11692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Assanet: An anisotropic separable set abstraction for efficient point cloud representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic segmentation for real point cloud scenes via bilateral augmentation and adaptive fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1757" to="1767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Surface representation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Haoxi Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrastive boundary learning for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<editor>Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung</editor>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Walk in the cloud: Learning curves for point clouds shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="915" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="https://github.com/yanx27/Pointnet_Pointnet2_pytorch" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arcewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Point-bert: Pre-training 3d point cloud transformers with masked point modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lulu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12104" to="12113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
