<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLEXIBLE STYLE IMAGE SUPER-RESOLUTION USING CONDITIONAL OBJECTIVE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><forename type="middle">Ho</forename><surname>Park</surname></persName>
							<email>seungho@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">INMC Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country>08826 Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Visual Display Division</orgName>
								<address>
									<addrLine>Samsung Electronics</addrLine>
									<settlement>Suwon-si</settlement>
									<country>16677 Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Su</forename><surname>Moon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Visual Display Division</orgName>
								<address>
									<addrLine>Samsung Electronics</addrLine>
									<settlement>Suwon-si</settlement>
									<country>16677 Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Cho</surname></persName>
							<email>nicho@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">INMC Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country>08826 Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FLEXIBLE STYLE IMAGE SUPER-RESOLUTION USING CONDITIONAL OBJECTIVE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image restoration ? Multi-task learning ? Single image super-resolution</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have significantly enhanced the performance of single-image super-resolution (SR) using convolutional neural networks (CNNs). While there can be many high-resolution (HR) solutions for a given input, most existing CNN-based methods do not explore alternative solutions during the inference. A typical approach to obtaining alternative SR results is to train multiple SR models with different loss weightings and exploit the combination of these models. Instead of using multiple models, we present a more efficient method to train a single adjustable SR model on various combinations of losses by taking advantage of multi-task learning. Specifically, we optimize an SR model with a conditional objective during training, where the objective is a weighted sum of multiple perceptual losses at different feature levels. The weights vary according to given conditions, and the set of weights is defined as a style controller. Also, we present an architecture appropriate for this training scheme, which is the Residual-in-Residual Dense Block equipped with spatial feature transformation layers. At the inference phase, our trained model can generate locally different outputs conditioned on the style control map. Extensive experiments show that the proposed SR model produces various desirable reconstructions without artifacts and yields comparable quantitative performance to stateof-the-art SR methods. Code and trained models will be available at https://github.com/seunghosnu/FxSR</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finding a high-resolution (HR) counterpart from a given low-resolution (LR) image is referred to as single image super-resolution (SISR). The SISR is an ill-posed problem in that infinitely many HR images correspond to a single LR image. Despite such ill-posedness, recent convolutional neural networks (CNNs) are shown to map an LR to a plausible HR <ref type="bibr" target="#b0">[1]</ref>. SRCNN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> first showed the effectiveness of a CNN for SISR, and various CNN architectures have been proposed for better performance afterward <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Earlier works used mean square error (MSE) as a loss function to train the network. However, since it tends to produce blurry HR outputs, researchers are finding new loss functions to generate more realistic outputs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Specifically, perceptual losses <ref type="bibr" target="#b18">[19]</ref> are introduced to optimize the super-resolution (SR) model in the feature space instead of pixel space. Ledig et al. <ref type="bibr" target="#b19">[20]</ref> proposed to use adversarial loss <ref type="bibr" target="#b20">[21]</ref> in combination with the perceptual loss to encourage the network to favor perceptually superior solutions residing in the manifold of natural images. More recently, Wang et al. <ref type="bibr" target="#b21">[22]</ref> investigated class-conditional SR. It employed Spatial Feature Transform (SFT) capable of altering an SR network's behavior conditioned on semantic segmentation probability maps. However, since most of the existing methods calculate perceptual losses on an entire image in the same feature space, the results tend to be monotonous and unnatural. For this reason, Rad et al. <ref type="bibr" target="#b22">[23]</ref> optimized SR models with a targeted objective function that penalizes images at different semantics using the corresponding terms. But, since the segmentation label needs to be fed to the SR network to calculate the targeted perceptual loss, the users cannot easily adjust the objective function. In summary, most early SR networks provide a designated HR output among many possible ones, not allowing us to explore more plausible arXiv:2201.04898v3 [cs.CV] 8 Mar 2022</p><p>Flexible Style Image Super-Resolution using Conditional Objective outputs at the test phase. To alleviate this problem, Lugmayr et al. <ref type="bibr" target="#b23">[24]</ref> proposed the SRFlow using a normalizing flow method capable of learning the conditional distribution of the output given the low-resolution input. As a result, it can learn to predict diverse photo-realistic high-resolution images. Though great strides have been made, the natural and flexible reconstruction of local regions is still challenging. As stated previously, there can be diverse HR solutions for a given LR, meaning that one LR input can be restored to different HR results depending on the context and situation. Particularly because of various shapes and textures in the real world, the one-to-many problem becomes even more serious if the SR network's capacity is not large enough. To solve this problem, first, the SR model should be able to generate more diverse styles of HR reconstruction while keeping consistency with the given LR image. Second, the recovery style needs to be locally controlled. Third, training and storing too many redundant SR models with different parameters should be avoided. Achieving these requirements would enable us to explore various HR solutions for each region effectively. In this respect, some recent methods made it possible to continuously generate and adjust intermediate results between two objective functions, i.e., perception and distortion functions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. However, there can be some improvements in these approaches, as they defined just two objective functions and controlled the entire image, not the local regions needing adjustment.</p><p>In this paper, we attempt locally adjustable HR generation by exploring the SR model optimization, focusing on the development of conditional objectives that can generate various reconstruction styles. The proposed objective consists of the weighted sum of several perceptual losses from different feature levels. The weights vary according to the condition, which is the recovery style information in our work. Experiments show that training an SR model with our multi-level perceptual losses generates various recovery styles effectively, which also enables us to finely control the styles of local regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Loss Functions for SISR</head><p>The choice of the objective function affects the recovery style and reconstruction performance. For instance, adversarial loss <ref type="bibr" target="#b20">[21]</ref> encourages an SR network to generate perception-oriented solutions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Perceptual losses <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19]</ref> are proposed to optimize SR models by minimizing the error in the feature space instead of pixel space. Dovovitskiy et al. <ref type="bibr" target="#b17">[18]</ref> and Ledig et al. <ref type="bibr" target="#b19">[20]</ref> proposed to use adversarial loss in combination with the perceptual loss to encourage the network to favor solutions that look more like natural images. With these loss functions, the overall visual quality of reconstruction is significantly improved <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Recently, some studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> proposed to use GAN with losses based on perceptual quality assessment metric. Another perceptual loss is proposed in <ref type="bibr" target="#b22">[23]</ref>, using different levels of features according to semantic segmentation labels such as objects, boundaries, and backgrounds. In these approaches, once an SR model is trained, a fixed HR is produced for the LR input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Conditioning</head><p>The feature normalization techniques generally change networks' behavior based on the input properties. The representative normalization methods may be batch normalization (BN) <ref type="bibr" target="#b38">[39]</ref> and instance normalization (IN) <ref type="bibr" target="#b39">[40]</ref>. The IN normalizes a single image while the BN does a whole batch of images. Conditional Instance Normalization (CIN) has also been introduced in <ref type="bibr" target="#b40">[41]</ref>, which uses the learned representations to model multiple styles simultaneously. Huang et al. <ref type="bibr" target="#b41">[42]</ref> proposed adaptive instance normalization (AdaIN) to adjust features to arbitrary new styles. Perez et al. <ref type="bibr" target="#b42">[43]</ref> proposed Feature-wise Linear Modulation, called FiLM, as a general-purpose conditioning method for neural networks. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. Inspired by these works, Wang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a spatial feature transformation (SFT) layer to modulate the features of some intermediate layers in a single network conditioned on semantic segmentation probability maps. Our approach is partially inspired by the above feature normalization methods, which can alter the behavior of deep CNNs to influence the output. In terms of network architecture, we use the Residual-in-Residual Dense Block (RRDB) <ref type="bibr" target="#b43">[44]</ref> equipped with SFT layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Continuous Imagery Effect Transition</head><p>Since the restored image's perceived quality is relatively subjective, and the perception-oriented methods sometimes generate artifacts, users may wish to control the reconstruction result according to the preferences or image characteristics. In recent years, there have been some tunable models that produce intermediate images between the goals of two different objective functions. Specifically, these methods start by training several separate models and then propose different ways of interpolating between them, specifically by directly interpolating the output pixels or network weights <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b25">26]</ref>, or by using specialized adaptor blocks in the networks <ref type="bibr" target="#b26">[27]</ref>. They considered trade-off relationships between two objectives, such as perception-distortion balance in SR, noise reduction vs. detail preservation in denoising and style transfer <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. However, these methods have some limitations: the number of objective functions is two, and they cannot adjust local regions, i.e., the algorithm is equally applied to the entire region of an image. It is also inefficient that they have to train and store multiple separate models. On the other hand, Bahat et al. <ref type="bibr" target="#b45">[46]</ref> proposed an explorable SR framework that enables local restoration control. However, users have to manually edit the texture in a few steps through a user interface. For easier and more effective quality control, we propose a controllable SR model that can produce various recovery styles for each region with a simple adjustment method. Besides, we can generate intermediate results between two or more different styles at fine control levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-task Learning</head><p>Learning one task at a time is a typical methodology in machine learning because it is hard to simultaneously optimize multiple objectives due to model capacity limitation or conflicting losses. For this reason, such multi-objective problems are commonly scalarized by a linear combination of the losses, with weights defining the trade-off between the loss term <ref type="bibr" target="#b46">[47]</ref>. On the other hand, Multi-task Learning (MTL) is an inductive transfer mechanism whose goal is to improve generalization performance by leveraging useful domain-specific information contained in multiple related tasks <ref type="bibr" target="#b47">[48]</ref>. Specifically, since the MTL networks use shared layers trained in parallel on all the tasks, what is learned for each task can help others to learn better when tasks are closely related <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>. Recently, Dosovitskiy et al. <ref type="bibr" target="#b49">[50]</ref> proposed loss-conditional training of deep networks for MTL that can improve model efficiency by exploiting the redundancy of multiple related models. They demonstrate style-transfer trained in this way and utilize feature-wise linear modulation <ref type="bibr" target="#b42">[43]</ref> that affects the whole image style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Targeted Perceptual Loss</head><p>In general, the choice of feature space significantly influences perceptual reconstruction performance and the styles. For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows the effect of choosing different feature spaces in computing the perceptual loss. In this paper, four different layers, ReLU 2-2, ReLU 3-4, ReLU 4-4, and ReLU 5-4 of the VGG-19 network <ref type="bibr" target="#b50">[51]</ref> are considered, denoted as VGG22, VGG34, VGG44, and VGG54, respectively. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, while the low-level feature space VGG22 seems more suitable for reconstructing simple edges with less distortion and over-sharpening, the midand high-level feature spaces of VGG44 are more appropriate for recovering complex textures. Therefore, it is difficult to determine a single feature space that works best for the entire image. In our work, we use more than two feature spaces at the same time to train a flexible SR (FxSR) model capable of generating various reconstruction styles. We define two kinds of FxSR models, namely FxSR-PD (perception-distortion) and FxSR-DS (diversity). The FxSR-PD is the main model in our work, which controls the output style between the distortion-oriented and perception-oriented by <ref type="figure">Figure 2</ref>: The architecture of our proposed flexible SR network. We use the RRDB equipped with SFT as a basic block ( <ref type="figure">Figure 3</ref>(c)). The condition branch takes a style map for reconstruction style as input. This map is used to control the recovery styles of edges and textures for each region through SFT layers.</p><p>(a) RB with SFT <ref type="bibr" target="#b21">[22]</ref> (b) Residual-in-Residual Dense Block (RRDB) <ref type="bibr" target="#b43">[44]</ref> (c) The proposed Basic Block (RRDB equipped with SFT layer) <ref type="figure">Figure 3</ref>: RRDB with SFT for basic blocks combining the reconstruction loss (for distortion) and VGG22 feature loss (for perception), along with the adversarial loss. The FxSR-DS uses the same architecture as the FxSR-PD but is trained with different losses, including all the VGG features stated above. Hence, the aim of FxSR-DS is to produce diverse styles of outputs related to different VGG features rather than to control between distortion and perception. Unlike previous works where there is no control data, we adjust the network by applying different objective functions for each local region through a style control map 1 . As a result, we can explore various HR solutions that are generated using multiple objective functions and thus reconstruct an image with the desired style or an image closer to the original HR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed SR with Flexible Style</head><p>Given a single LR image I LR , SISR is to estimate an HR image? HR , which is as similar as possible to its corresponding HR counterpart I HR . Most of the current CNN-based methods use feed-forward networks to directly learn a mapping function G ? parameterized by ? as?</p><formula xml:id="formula_0">HR = G ? I LR .<label>(1)</label></formula><p>To optimize G ? on the training samples, we design a specific objective function O as</p><formula xml:id="formula_1">? * = arg min ? E Z?P Z O ? HR , I HR<label>(2)</label></formula><p>where Z = I LR , I HR is sampled from given a training distribution of pairs P Z . Many recent studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b51">52]</ref> use perceptual loss and adversarial loss for designing O to recover realistic textures. Although these losses greatly improve the perceptual quality, the generated textures tend to be monotonous and unnatural <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>. To further improve the restoration performance, Wang et al. <ref type="bibr" target="#b21">[22]</ref> used semantic segmentation probability maps as the categorical prior ? and reformulated (1) as?</p><formula xml:id="formula_2">HR ? = G ? I LR ?).<label>(3)</label></formula><p>However, the perceptual loss was applied to the entire region of images, like in previous works. Specifically, the same level of features was used both on simple edges and complex textures, which has a limitation in restoring images composed of various types of objects. In addition, once model training is completed, there is no way to adjust the SR results without retraining. Hence, instead, we propose a novel method to apply different objectives to each region for reconstructing desired images or images closer to the original. Specifically, the proposed flexible SR model is optimized with a conditional objective, which is a weighted sum of several perceptual losses corresponding to different feature levels, where each weight changes depending on the style map. Formally, our objective is described as:</p><formula xml:id="formula_3">I HR T = G ? I LR T),<label>(4)</label></formula><formula xml:id="formula_4">? * = arg min ? E t?Pt E Z?P Z O ? HR T , I HR |T<label>(5)</label></formula><p>where T is a map delivering spatially varying style control. That is, the map T is an LR-sized matrix, which is fed to the condition network to change the SR styles. Since the purpose of training is to let the network learn various styles corresponding to given control parameters, we feed various T randomly to the network during the training. Specifically, we feed a flat map T = t ? 1 during the training, where 1 is the matrix with all the elements 1, and t is a variable related to the feature combinations, which will be detailed in the following subsection. For training with various feature combinations, we change t randomly at each epoch. At the inference, if we feed a flat map as defined above, the network will deliver an SR style globally corresponding to the t. If we wish to control the styles locally, we feed a spatially varying map, which will be demonstrated in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed Network Architecture</head><p>An overview of the architecture is shown in <ref type="figure">Figure 2</ref>. The generator network G ? consists of two streams, an SR branch and a condition branch. The SR branch is built with basic blocks consisting of RRDB equipped with the SFT layers <ref type="bibr" target="#b21">[22]</ref>, which take the shared conditions as input and modulate feature maps by applying the affine transformation. This structure is shown in <ref type="figure">Figure 3</ref>(c), where the residual block with SFT <ref type="bibr" target="#b21">[22]</ref> and RRDB <ref type="bibr" target="#b43">[44]</ref> are also shown in Figures 3(a) and (b) for comparison. The SFT layer learns a mapping function that outputs a modulation parameter based on a style condition T. This modulation layer allows the SR branch to optimize the changing objective during the training and also to generate SR results with spatially different styles according to the style map. The condition branch is used to produce shared intermediate style conditions that can be broadcasted to all the SFT layers for efficiency. As in the study of <ref type="bibr" target="#b21">[22]</ref>, all the convolution layers in the condition branch are restricted to use 1 ? 1 kernels to avoid the interference of different regions. For discriminator network, we use VGG network <ref type="bibr" target="#b50">[51]</ref> that contains ten convolution layers gradually decreasing the spatial dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Proposed Loss Function</head><p>We combine multiple losses to train our SR model. The conditional objective consists of three terms, namely pixel-wise reconstruction loss, adversarial loss, and proposed conditional perceptual loss:</p><formula xml:id="formula_5">O T = ? rec (T) ? L rec + ? adv (T) ? L adv + ? per ? L per (T)<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">? rec (T) = ? rec_o + (? ? w rec (T)),<label>(7)</label></formula><p>? adv (T) = ? adv_o ? w adv (T).</p><p>The notations will be explained one by one below. First, the reconstruction loss is calculated as:</p><formula xml:id="formula_8">L rec = E ? HR ? I HR 1 .<label>(9)</label></formula><p>We use the adversarial loss using Relativistic average Discriminator RaD <ref type="bibr" target="#b52">[53]</ref> that performs better for learning sharper edges and more detailed textures compared to standard GAN <ref type="bibr" target="#b20">[21]</ref>. While the standard version estimates the probability that one input image I is real and natural, the RaD predicts the probability that a real image I HR is relatively more realistic than a fake one? HR . In addition, for adversarial training, RaD benefits from the gradients from both? HR and I HR , while only? HR takes effect in the standard version. Specifically, the adversarial and the discriminator losses are:</p><formula xml:id="formula_9">L adv = ?E?HR log D ? HR ? E I HR log 1 ? D I HR (10) L dis = ?E I HR log D I HR ? E?HR log 1 ? D ? HR<label>(11)</label></formula><p>where D I HR = sigmoid C I HR ? E?HR C ? HR</p><p>D ? HR = sigmoid C ? HR ? E I HR C I HR <ref type="bibr" target="#b12">(13)</ref> where C (?) represents the output logit of discriminator. The conditional perceptual loss is a weighted sum of multiple perceptual losses in different levels of feature spaces:</p><formula xml:id="formula_11">L per (T) = l w l (T) ? L l ,<label>(14)</label></formula><p>where L l denotes the distance in each feature space, l ? {V GG12,V GG22, ? ? ? , V GG54}, and the weights w l changes according to T. Precisely, the distance L l is defined as</p><formula xml:id="formula_12">L l = E ? l (? HR ) ? ? l (I HR )) 2<label>(15)</label></formula><p>where ? l denotes feature maps in the feature space l. The weights w rec , w adv , and w l are functions of t as described in <ref type="figure" target="#fig_1">Figure 4</ref>, where t is a random variable having uniform distribution in [0, 1] during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation details</head><p>This subsection explains how we design the combination of feature losses depending on the change of t. The left column of <ref type="figure" target="#fig_1">Figure 4</ref> shows the weight function for FxSR-PD (using only VGG22 for perceptual loss), and the right for FxSR-DS (using more feature spaces for diversity). When t=0, the figure shows that FxSR-PD corresponds to distortion-oriented SR (perceptual and adversarial losses are zero). When the value of t approaches 1, then it becomes perception-oriented (weight for the reconstruction loss becomes zero, while adversarial and perceptual losses grow to 1). In the case of the right column, various feature distances are involved in the perceptual loss, and hence FxSR-DS can deliver diverse styles. Specifically, note that t = 1 corresponds to a perception-oriented SR with VGG54 as the feature space. Also, even when t approaches 0, the FxSR-DS still produces perception-oriented SR results of different Whole image HR 4? FxSR-PD t = 0.0 t = 0.3 t = 0.6 t = 1.0 <ref type="figure">Figure 5</ref>: Changes in the result of FxSR-PD 4? SR according to t on DIV2K validation set <ref type="bibr" target="#b30">[31]</ref> .</p><p>styles corresponding to VGG22, unlike the FxSR-PD that is distortion-oriented at t = 0. Regarding the style control, as stated previously, we use a uniform map T = t ? 1 at the training phase. That is, a flat map is fed to the condition branch, with its intensity t randomly changing during the training. Since the SR network is a fully convolutional neural network, it inherits the local connectivity property that the local image and the map region determine the output pixel. Hence, SR models trained with uniform maps can handle spatially varying cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the experiment, we compare our FxSR-PD and FxSR-DS with several state-of-the-art SR methods on benchmark datasets. We start the section with a description of the datasets and evaluation methods. Next, we present the comparison results. We also provide examples of local style control and validate the effectiveness of our approach for compressed images. Finally, we report complexity analysis for the proposed methods.  <ref type="figure">Figure 8</ref>: 4? SR performance comparison of state-of-the-art and proposed methods evaluated by the (a) NIQE <ref type="bibr" target="#b58">[59]</ref> and (b) BRISQUE <ref type="bibr" target="#b59">[60]</ref> for DIV2K according to condition parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>For the experiments, we train the FxSR with DIV2K <ref type="bibr" target="#b30">[31]</ref> dataset, which contains 800 training images, 100 validation images, and 100 test images. We use BSDS100, General100, and DIV2K 100 validation images as our test datasets. We also use JPEG-compressed images for training and testing FxSR models to show that our proposed method is still effective on the real-world compressed LR images. The scaling factors of 4? and 8? are tried for experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Method</head><p>To evaluate the perceptual distance to the Ground Truth, we report LPIPS <ref type="bibr" target="#b55">[56]</ref> as default <ref type="bibr" target="#b62">[63]</ref>, and additionally use DISTS <ref type="bibr" target="#b56">[57]</ref> as structure and texture similarity in some cases. PSNR and SSIM <ref type="bibr" target="#b53">[54]</ref> are reported as fidelity-oriented   <ref type="figure" target="#fig_0">Figure 11</ref>: Visual comparison with state-of-the-art perception-driven SR methods on DIV2K validation set <ref type="bibr" target="#b30">[31]</ref>. The proposed method produces competitive results compared to other modern techniques and can also generate reconstructed images of various styles of LR images.</p><p>metrics. Furthermore, we report the no-reference metric NIQE <ref type="bibr" target="#b55">[56]</ref>. Since the consistency with the LR image is also an important factor, we report the LR-PSNR, computed as the PSNR between the downsampled SR image and the original LR. To measure the meaningful diversity of SR methods that can actively sample from the space of plausible super-resolutions, we also report the SR-Diversity score, which is used for the evaluation protocol on the Super-Resolution Space Challenge learning track in the NTIRE Challenge 2021 <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. Specifically, we sample 11 images and densely calculate LPIPS <ref type="bibr" target="#b55">[56]</ref> metric between the samples and the ground truth. To obtain the local best score, we pixel-wisely select the best score out of the 11 samples and take the full image's average. The global best score is calculated by averaging the whole image's score and selecting the best. Then, the diversity score is calculated as follows:</p><formula xml:id="formula_13">score = (globalbest ? localbest)/(globalbest) ? 100.<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training Method</head><p>For the scaling factor 4?, sub-images are cropped with the sizes of 320 ? 320 with a stride of 160 and 80 ? 80 with 40, for the HR and LR training images, respectively. For the scaling factor 8?, the LR sub-images are cropped to the size of 40 ? 40 with a 20 strides. Then, the batch image pairs for each iteration of training are randomly cropped from these sub-images. The HR batch size is 128 ? 128 and the LR batch sizes are 32 ? 32 and 16 ? 16 for scaling factors of 4? and 8?, respectively.</p><p>For the optimization, we use initial learning rate of 10 ?4 . The learning rate is halved after 5K, 10K, 20K, and 30K iterations. Adam <ref type="bibr" target="#b65">[66]</ref> with ? 1 = 0.9 and ? 2 = 0.99 is used for both generator and discriminator training. We use pre-trained RRDB <ref type="bibr" target="#b43">[44]</ref> and ESRGAN <ref type="bibr" target="#b43">[44]</ref> models to optimize the proposed FxSR models. While fine-tuning FxSR-PD and FxSR-DS, ? rec_o , ? adv_o and ? per are set to be 1 ? 10 ?2 , 5 ? 10 ?3 and 1.0 respectively, but ? is set differently to 1 ? 10 and 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8? SR comparison Whole image</head><p>HR RRDB <ref type="bibr" target="#b43">[44]</ref> ESRGAN <ref type="bibr" target="#b43">[44]</ref> SRFlow t=0.9 FxSR t=0.8 <ref type="figure" target="#fig_0">Figure 12</ref>: Visual comparison for 8? SR results on DIV2K validation set <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Flexible SR for Perception-Distortion (FxSR-PD)</head><p>By adjusting a single parameter t, the FxSR-PD model can generate various SR results for the trade-offs between distortion and perception objective at the inference phase, as shown in <ref type="figure">Figure 5</ref>. It shows that t = 0 generates blurry outputs as the FxSR objective is distortion-oriented, and t = 1 generates sharp textures as the FxSR becomes perception-oriented. Also, the t between 0 and 1 generates different trade-offs, with less or more distortions, and more or less blurriness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative Comparison</head><p>We compare our method quantitatively with distortion-oriented methods such as RRDB <ref type="bibr" target="#b43">[44]</ref>, and perception-oriented methods such as SRGAN <ref type="bibr" target="#b19">[20]</ref>, ESRGAN <ref type="bibr" target="#b43">[44]</ref>, SFTGAN <ref type="bibr" target="#b21">[22]</ref>, NatSR <ref type="bibr" target="#b60">[61]</ref>, SPSR <ref type="bibr" target="#b61">[62]</ref> and SRFlow <ref type="bibr" target="#b23">[24]</ref>. For the 4? SR, we use pre-trained models provided by the authors, while for the non-provided 8? SR, we used the author's code to train the RRDB <ref type="bibr" target="#b43">[44]</ref> and ESRGAN <ref type="bibr" target="#b43">[44]</ref> models. The results are presented in Figures from 6 to 10 and <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure">Figures from 6 to 8</ref> show the performance comparison of 4? SR results according to t, evaluated by the distortion-oriented (PSNR, SSIM <ref type="bibr" target="#b53">[54]</ref>, MS-SSIM <ref type="bibr" target="#b54">[55]</ref>), perception-oriented (LPIPS <ref type="bibr" target="#b55">[56]</ref>, DISTS <ref type="bibr" target="#b56">[57]</ref>, VIF <ref type="bibr" target="#b57">[58]</ref>), and non-reference perception-oriented metrics (NIQE <ref type="bibr" target="#b58">[59]</ref>, BRISQUE <ref type="bibr" target="#b59">[60]</ref>), respectively. In <ref type="figure">Figure 6</ref>, we can see that the scores of the distortion-oriented metrics improve as t approaches 0, whereas in <ref type="figure">Figures 7 and 8</ref>, the scores of the perception-oriented metrics improve as t approaches 1.</p><p>Since there is a trade-off between the distortion-oriented metrics and the perception-oriented metrics, it is necessary to evaluate the performance of the SR models in a perception-distortion 2D plane <ref type="bibr" target="#b66">[67]</ref>, as shown in <ref type="figure">Figure 9</ref>. The vertical axis denotes perceptual loss LPIPS <ref type="bibr" target="#b55">[56]</ref>, and the horizontal axis the PSNR (distortion-oriented measure). Hence, the lower left part is the desired place where both MSE and perceptual loss are low <ref type="bibr" target="#b66">[67]</ref>, and we can see that our method is comparable to others in this respect. Note that the RRDB <ref type="bibr" target="#b43">[44]</ref> and ESRGAN <ref type="bibr" target="#b43">[44]</ref> are the results of using distortion-oriented and perception-oriented loss, respectively. Others drawn in solid lines are adjustable methods. Pixel interpolation (Pix-Interp) and network weight interpolation (Net-Interp) methods utilize two differently trained models, i.e., the RRDB and ESRGAN stated above. The number of parameters for each method is also provided for complexity comparison. More details about complexity analysis will be provided in Section IV.F.</p><p>Since various metrics examined in <ref type="figure">Figures 6-8</ref> have different characteristics and performance, we present additional performance comparisons for the perception-distortion plane with these metrics in <ref type="figure" target="#fig_0">Figure 10</ref>. These comparisons show trends similar to those in <ref type="figure">Figure 9</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the evaluation of FxSR-PD and other SR methods for the specific t values. The proposed FxSR-PD obtains the best PSNR and SSIM at t = 0 among perception-oriented methods and the best LPIPS values at t = 0.8 for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative Comparison</head><p>Visual comparison between our proposed FxSR-PD and other state-of-the-art methods for 4? and 8? are shown in <ref type="figure" target="#fig_0">Figures 11 and 12</ref>, respectively. We can see that our FxSR-PD provides stronger edges and fine details than the distortion-oriented method RRDB <ref type="bibr" target="#b43">[44]</ref>, and other perception-oriented ones. Also, there are fewer artifacts in our method compared to others.</p><p>Whole image HR 4? FxSR-DS t = 0.0 t = 0.3 t = 0.6 t = 1.0 <ref type="figure" target="#fig_0">Figure 13</ref>: Changes in the result of FxSR-DS 4? SR according to t on DIV2K validation set <ref type="bibr" target="#b30">[31]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Flexible SR for Diverse Styles (FxSR-DS)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Diverse Style HR Generation</head><p>Unlike the FxSR-PD that attempts flexible trade-offs between perception and distortion, the FxSR-DS aims to generate various styles of HR textures with perceptually high scores for all t values. As shown in <ref type="figure">Figures from 7 to 8</ref>, the FxSR-DS scores better overall with a relatively narrow dynamic range regarding the perception-oriented metrics other than VIF <ref type="bibr" target="#b57">[58]</ref>. On the other hand, it scores relatively lower for distortion-oriented metrics as in <ref type="figure">Figure 6</ref>. The loss terms and their weights for the conditional objective of the FxSR-DS model are described in <ref type="figure" target="#fig_1">Figure 4</ref>. Different from FxSR-PD with one perceptual loss term, four perceptual loss terms at different feature levels are used. In <ref type="figure" target="#fig_0">Figure 13</ref>, we can see that the SR results for different t values have different types of styles that are clearly distinct from each other. While <ref type="figure">Figure 5</ref> shows the trade-off results between perception and distortion, <ref type="figure" target="#fig_0">Figure 13</ref> visualizes our method's scalability to generate various styles of textures by employing more feature spaces into the loss.    <ref type="table" target="#tab_0">Table 1</ref> is the evaluation of SR results for a specific t value, while <ref type="table" target="#tab_1">Table 2</ref> is the average of all of the SR results for 11 different t values, from 0 to 1, with the step size of 0.1. Specifically, in <ref type="table" target="#tab_1">Table 2</ref>, the FxSR-DS generally scores the best mean LPIPS and Local best (L-best) LPIPS, while the FxSR-PD achieves the best Global best (G-best) LPIPS score. This proves that the perceptually distinct diverse SR results generated by FxSR-DS in <ref type="figure" target="#fig_0">Figure 13</ref> are of high quality in terms of perception-oriented metrics. Since Local Best LPIPS is the maximum performance of the SR model in terms of perceptual measurement, the proposed FxSR-DS shows an improvement of about 2.7% compared to the SRFlow. Figure  <ref type="figure" target="#fig_0">Figure 15</ref>: Comparison of the SR results of the conventional method (a), which applies one objective to the entire image, and the FxSR-PD method, which applies different objectives for each area (clothes and letters) through a local map. We can see that the proposed FxSR-PD in (b) can more accurately produce the locally intended and suitable SR results without side effects such as blurry textures and broken characters.  <ref type="figure" target="#fig_0">Figure 16</ref>: Comparison of the SR results of the conventional method (a), which applies one objective to the entire image, and the FxSR-DS method, which applies different objectives for each area (buildings and trees) through a local map. We can see that the proposed FxSR-DS in (b) can more accurately produce the locally intended and suitable SR results without side effects such as blurry tree textures and overshoot around the edges. . <ref type="figure" target="#fig_0">Figure 18</ref>: An example of applying a user-created depth map to enhance the perspective feeling with the sharper and richer textured foreground and the background with more reduced camera noise than the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Quantitative Comparison</head><p>14 also demonstrates that while the FxDR-PD scores better G-best LPIPS compared to FxDR-DS, the FxDR-DS scores rather superior L-best LPIPS than FxSR-PD. Meanwhile, the SRFlow <ref type="bibr" target="#b23">[24]</ref> produces the highest diversity, which learns the sample distribution during training while the proposed models are trained to optimize objectives in the training distribution of objective. However, it is also important to note that the diversity scores are normalized by the G-best</p><p>Whole image HR SRResNet <ref type="bibr" target="#b19">[20]</ref> SRGAN <ref type="bibr" target="#b19">[20]</ref> SRGAN-CA FxSR-CA  as Eqn. <ref type="bibr" target="#b15">16</ref>. This means that the higher the G-best LPIPS, that is, the lower the absolute perceptual quality level, the higher the diversity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Per-pixel Style Control</head><p>In this section, we demonstrate some examples of applying local style control. First, <ref type="figure" target="#fig_0">Figure 15</ref> is an example where the LR image has both text and texture areas. In the conventional methods for the SR of <ref type="figure" target="#fig_0">Figure 15</ref>(a), multiple SR models are trained with one objective each. Then a model is selected, and the entire image is optimized with the model's objective. If the SR model 0 is selected, which is RRDB <ref type="bibr" target="#b43">[44]</ref> representing the distortion-oriented model, the textures of the clothes are blurred while the text edges are restored without artifacts. Conversely, suppose we select the SR model N ? 1, which is ESRGAN <ref type="bibr" target="#b43">[44]</ref> representing the perception-oriented model. In that case, some characters in the text area are broken while the textures of the clothes are naturally restored. On the other hand, the proposed FxSR-PD in <ref type="figure" target="#fig_0">Figure 15</ref>(b) can restore both the textures of clothes and characters at the same time by applying different objectives to each area through the locally-manipulated style map.</p><p>As the second example, let us consider the structural edges of the building and textures of the tree area in <ref type="figure" target="#fig_0">Figure 16</ref>. In a typical approach of using multiple SR models in <ref type="figure" target="#fig_0">Figure 16(a)</ref>, when the SR model 0 (RRDB) is selected, the structural edges of the building are restored without artifacts, but the tree textures are blurred. Conversely, if the SR model N-1 (ESRGAN) is chosen, the overshoot side-effect occurs around the edges. As shown in <ref type="figure" target="#fig_0">Figure 16(b)</ref>, similar to the previous example, when a properly adjusted local style map is fed along with the input image, the proposed model FxSR-DS can restore both the tree textures and building edges naturally.</p><p>The next is an example of enhancing the perspective feeling when depth information is available, as shown in <ref type="figure" target="#fig_0">Figure  17</ref>. Precisely, input image and depth map pairs used in this example are from the Make3D data set <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. When the distance map is used as T in our FxSR, the foreground region is super-resolved in a perception-oriented way (with emphasized texture), and the background region in distortion-oriented (somewhat blurry). Depth information obtained by some equipment such as Kinect <ref type="bibr" target="#b69">[70]</ref> and Time-of-Flight (ToF) camera <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>, or depth estimation algorithms <ref type="bibr" target="#b72">[73]</ref> can be used. It is also possible for users to directly generate a depth map from an input image using image editing S/W, as shown in <ref type="figure" target="#fig_0">Figure 18</ref>. This makes the foreground clearer with sharp details and avoids the unnaturalness of the background becoming as sharp as the foreground. In addition, the camera noise in the background can be reduced. As seen in the examples so far, the proposed method can be used for most cases in various fields that require different processing for each area for a specific purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Compressed LR Image Restoration</head><p>Since real-world SR is challenging due to unknown degradation and various noise <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>, we also validate the effectiveness of our method for compressed inputs in <ref type="figure" target="#fig_0">Figure 19</ref>. Unlike previous experiments, FxSR and SRGAN <ref type="bibr" target="#b19">[20]</ref> are re-trained using LR images compressed with JPEG quality factor 90, called FxSR-CA (compression artifacts) and SRGAN-CA. We can see that while compression artifacts are amplified in the results of SRResNet <ref type="bibr" target="#b19">[20]</ref> and SRGAN <ref type="bibr" target="#b19">[20]</ref> trained with clean images, the proposed FxSR-CA, generates different style and details according to the change of t. To test the effectiveness of the proposed method for the case of real-world compressed images, two videos 2 which are filmed, edited and copyrighted by Milosh Kitchovitch are used by courtesy of him. Details of the video are provided in the <ref type="table" target="#tab_3">Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Complexity Analysis</head><p>We compare the running time, computation costs, and storage size of our methods with other SR methods in <ref type="table" target="#tab_4">Table 4</ref>. We measure the complexity for the SR 4? processing of one 128 ? 128 LR input image on the environment of NVIDIA RTX3090 GPU. According to <ref type="table" target="#tab_4">Table 4</ref>, ESRGAN with high-complexity RRDB architecture in <ref type="figure">Figure 3</ref>(b) requires about 10 times the number of Mult-Add and Run-time than SRGAN. Compared to ESRGAN, FxSR with the proposed RRDBs with SFT in <ref type="figure">Figure 3</ref>(c) has almost the same number of Mult-Adds and parameter size, but the Forward Pass Size is about 4 times, and the run-time is also increased by 4 times due to the additional memory usage related to the SFT layers. However, it needs to be noted that we use a single network for diverse output generation, whereas the existing methods need at least two networks for producing varying outputs. This is specifically observed in <ref type="figure">Figure  9</ref>, where it is observed that the FxSR requires less or comparable parameters than the network/image interpolation methods that use multiple ESRGAN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation Study</head><p>The goal of classic multi-objective optimization is to find a set of solutions as close as possible to Pareto optimal front and as diverse as possible <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>. To investigate the performance depending on network architecture and complexity, we observe the change in the perception and distortion (PD) curve while training two versions of FxSR-PD using 16 RBs with SFT in <ref type="figure">Figure 3</ref>(a), and 23 RRDBs with SFT in <ref type="figure">Figure 3</ref>(b), respectively. As the number of training iterations increases, the PD curve of FxSR-PD converges to the desired place (lower left), and at the same time, the possible SR range on the curves is also expanded as shown in <ref type="figure" target="#fig_10">Figures 20(a)</ref> and (b). However, after a certain amount of iterations, the performance does not improve further. <ref type="figure" target="#fig_10">Figure 20</ref>(c) shows the performance comparison between the two FxSR-PD versions at the 250,000th iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1">Benefits of FxSR</head><p>A single FxSR model can produce different styles corresponding to employed feature losses and is also able to generate intermediate results between the different styles. Moreover, we can control the local regions differently by feeding a control map to the network. Hence, we can have more natural SR outputs by focusing on the foreground or salient regions more than the backgrounds, using user-edited or automatically generated segmentation/depth/saliency maps. Also, we can remedy unnaturally generated regions by controlling the parameters as the post-processing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2">Limitations of FxSR</head><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, our method can generate comparable or superior results to the existing methods in terms of perceptual quality. But it shows a lower diversity score than the SRFlow because flat control maps are tried in this experiment. Hence, we need more studies on effective control map generation along with other feature spaces and their combinations to increase diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.3">Future works</head><p>We have used a one-dimensional control parameter t for adjusting SR styles in this work. By defining more than one-dimensional SR style space with various style objectives, we can explore the n-dimensional SR spaces, possibly producing more diverse styles. Also, we may consider expanding the work to the image denoising and deblurring to control the degree of restoration locally. Furthermore, leveraging meta-learning would make it possible to improve adaptation to new samples and target objectives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The effect of choosing different layers when estimating perceptual losses on different regions, e.g., on edge and texture regions, where the losses correspond to MSE, ReLU 2-2 (VGG22), and ReLU 4-4 (VGG44) of the VGG-19 network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The left column shows the weight functions for FxSR-PD. t = 0 corresponds to distortion-oriented SR (only MSE loss) and t = 1 perception-oriented (with adversarial and perceptual loss from VGG22). The right column shows the weight functions for FxSR-DS, where more perceptual losses are used to expand the HR styles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>4? SR performance comparison of state-of-the-art and proposed methods evaluated by the metric (a) PSNR, (b) SSIM<ref type="bibr" target="#b53">[54]</ref>, and (c) MS-SSIM<ref type="bibr" target="#b54">[55]</ref> for DIV2K according to condition parameters. 4? SR performance comparison of state-of-the-art and proposed methods evaluated by the metric (a) LPIPS<ref type="bibr" target="#b55">[56]</ref>, (b) DISTS<ref type="bibr" target="#b56">[57]</ref>, and (c) VIF<ref type="bibr" target="#b57">[58]</ref> for DIV2K according to condition parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Performance comparison of the state-of-the-arts and proposed method (FxSR-PD: Perception-Distortion Flexible SR) for DIV2K 4? SR. 4? SR P-D performance comparison of state-of-the-art and proposed methods evaluated by the metric (a) NIQE [59] Vs. PSNR, (b) SSIM [54] Vs. LPIPS<ref type="bibr" target="#b55">[56]</ref>, and (c) SSIM<ref type="bibr" target="#b53">[54]</ref> Vs. DISTS<ref type="bibr" target="#b56">[57]</ref> for DIV2K according to condition parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 14 :</head><label>14</label><figDesc>(a) The FxSR-PD result with G-Best LPIPS (b) The G-Best LPIPS map (0.1355) (c) The L-Best LPIPS map (0.1289) (d) The FxSR-DS result with G-Best LPIPS (e) The G-Best LPIPS map (0.1404) (f) The L-Best LPIPS map (0.1168) On the left are the SR results of FxSR-PD (top) and FxSR-DS (bottom) for DIV2K 0858, corresponding to t values with Global Best (G-Best) LPIPIS among 11 samples, respectively. In the middle are the LPIPS maps of the SR results on the left. On the right are the Local Best (L-Best) LPIPS maps generated by selecting the highest score per pixel from 11 samples. The brighter the pixel, the higher the LPIPS value and the greater the perceptual difference from the ground truth. Each number in parentheses is the average LPIPS value for the entire image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( b )</head><label>b</label><figDesc>The proposed method of using single FxSR-PD model trained on the training distribution of objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Flexible</head><label></label><figDesc>Style Image Super-Resolution using Conditional Objective (a) The conventional method of using multiple SR models trained separately for a different objective each. (b) The proposed method of using single FxSR-DS model trained on the training distribution of objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 17 :</head><label>17</label><figDesc>Depth-adaptive FxSR. T-maps is the modified version of the depth map of an image from the Make3D dataset<ref type="bibr" target="#b67">[68]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>For compressed LR images, FxSR-CA can generate different styles of textures without amplifying artifacts. The intensity and style of textures change according to t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 19 :</head><label>19</label><figDesc>The SR Results for compressed LR images. Two feature space (VGG44 and VGG54) and 16 RBs with SFT are used for FxSR-CA model. LR Images are extracted from "Amazing Place" video title that is encoded by VP9 codec at 0.3Mbps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 20 :</head><label>20</label><figDesc>Convergence of diversity curve of the proposed FxSR-PD model as the number of training iteration increase, using (a) 16 RBs with SFT and (b) using 23 RRDBs with SFT. (c) The performance comparison between two FxSR-PD version at the 250,000th iteration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ESRGAN [44]SFTGAN [22] NatSR [61] SPSR [62] SRFlow t=0.9 FxSR t=0.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">8?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RRDB</cell><cell>SRGAN</cell><cell>ESR-</cell><cell>SFT-</cell><cell>NatSR</cell><cell>SPSR</cell><cell>SRFlow</cell><cell>SRFlow</cell><cell>FxSR-</cell><cell>FxSR-</cell><cell>RRDB</cell><cell>ESR-</cell><cell>SRFlow</cell><cell>SRFlow</cell><cell>FxSR-</cell><cell>FxSR-</cell></row><row><cell>Dataset Metric</cell><cell>[44]</cell><cell>[20]</cell><cell>GAN</cell><cell>GAN</cell><cell>[61]</cell><cell>[62]</cell><cell>? =0.0</cell><cell>? =0.9</cell><cell>PD</cell><cell>PD</cell><cell>[44]</cell><cell>GAN</cell><cell>? =0.0</cell><cell>? =0.9</cell><cell>PD</cell><cell>PD</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[44]</cell><cell>[22]</cell><cell></cell><cell></cell><cell>[24]</cell><cell>[24]</cell><cell>t=0.0</cell><cell>t=0.8</cell><cell></cell><cell>[44]</cell><cell>[24]</cell><cell>[24]</cell><cell>t=0.0</cell><cell>t=0.8</cell></row><row><cell cols="2">BSD PSNR? 26.53</cell><cell cols="9">24.13 23.95 24.09 25.13 24.16 26.23 24.66 26.38 24.77</cell><cell>23.56</cell><cell cols="5">20.23 23.37 21.66 23.60 21.93</cell></row><row><cell cols="17">100 SSIM? 0.7438 0.6454 0.6463 0.6460 0.6780 0.6531 0.7293 0.6580 0.7380 0.6817 0.5700 0.4350 0.5428 0.4632 0.5728 0.5039</cell></row><row><cell cols="2">LRPSNR? 51.52</cell><cell cols="9">39.32 41.35 40.92 42.26 40.99 50.81 49.86 52.48 49.24</cell><cell>45.82</cell><cell cols="5">24.81 52.39 51.09 47.12 42.41</cell></row><row><cell cols="17">LPIPS? 0.3575 0.1777 0.1615 0.1710 0.2115 0.1613 0.3635 0.1833 0.3433 0.1572 0.5571 0.3582 0.5303 0.3238 0.5079 0.3129</cell></row><row><cell cols="17">DISTS? 0.2005 0.1288 0.1160 0.1224 0.1436 0.1165 0.1943 0.1372 0.1921 0.1160 0.2956 0.2096 0.3183 0.2068 0.2753 0.1972</cell></row><row><cell>NIQE?</cell><cell>5.35</cell><cell>3.18</cell><cell>3.53</cell><cell>3.23</cell><cell>3.67</cell><cell>3.23</cell><cell>6.83</cell><cell>3.51</cell><cell>5.10</cell><cell>3.30</cell><cell>6.23</cell><cell>3.15</cell><cell>12.82</cell><cell>3.68</cell><cell>5.49</cell><cell>4.58</cell></row><row><cell cols="2">General PSNR? 30.30</cell><cell cols="9">27.54 27.53 27.04 28.61 27.65 29.72 27.83 29.94 28.44</cell><cell>25.38</cell><cell cols="5">21.51 25.09 23.45 25.42 24.00</cell></row><row><cell cols="17">100 SSIM? 0.8696 0.7998 0.7984 0.7861 0.8259 0.7995 0.8574 0.7951 0.8629 0.8229 0.7081 0.5674 0.6806 0.6063 0.7097 0.6534</cell></row><row><cell cols="2">LRPSNR? 53.96</cell><cell cols="9">41.44 41.93 40.05 45.06 42.31 50.65 49.59 52.22 49.82</cell><cell>44.78</cell><cell cols="5">25.19 48.95 47.59 44.28 41.36</cell></row><row><cell cols="17">LPIPS? 0.1665 0.0962 0.0881 0.1084 0.1118 0.0865 0.1731 0.0962 0.1519 0.0784 0.3403 0.2494 0.3194 0.2341 0.2924 0.2058</cell></row><row><cell cols="17">DISTS? 0.1321 0.0955 0.0845 0.1166 0.1099 0.0857 0.1276 0.1022 0.1205 0.0831 0.2362 0.1852 0.2488 0.1899 0.2134 0.1716</cell></row><row><cell>NIQE?</cell><cell>6.56</cell><cell>4.35</cell><cell>4.65</cell><cell>4.38</cell><cell>4.71</cell><cell>4.37</cell><cell>7.02</cell><cell>5.18</cell><cell>6.05</cell><cell>4.54</cell><cell>7.18</cell><cell>4.40</cell><cell>11.92</cell><cell>4.89</cell><cell>6.09</cell><cell>5.46</cell></row><row><cell cols="2">DIV2K PSNR? 29.48</cell><cell cols="9">26.63 26.64 26.56 27.82 26.71 29.05 27.08 29.24 27.51</cell><cell>25.50</cell><cell cols="5">21.37 25.09 23.04 25.60 23.56</cell></row><row><cell cols="17">SSIM? 0.8444 0.7625 0.7640 0.7578 0.7931 0.7614 0.8290 0.7558 0.8383 0.7890 0.6951 0.5533 0.6589 0.5728 0.6989 0.6241</cell></row><row><cell cols="2">LRPSNR? 53.72</cell><cell cols="9">40.87 42.61 40.40 44.64 42.57 51.02 49.96 53.30 50.54</cell><cell>46.05</cell><cell cols="5">25.21 51.28 50.26 46.96 42.66</cell></row><row><cell cols="17">LPIPS? 0.2537 0.1263 0.1154 0.1449 0.1523 0.1099 0.2513 0.1201 0.2390 0.1028 0.4245 0.2841 0.4033 0.2719 0.3857 0.2403</cell></row><row><cell cols="17">DISTS? 0.1261 0.0613 0.0530 0.0858 0.0766 0.0493 0.1139 0.0622 0.1169 0.0513 0.2203 0.1293 0.2342 0.1386 0.1953 0.1190</cell></row><row><cell>NIQE?</cell><cell>4.42</cell><cell>2.57</cell><cell>2.79</cell><cell>2.92</cell><cell>2.91</cell><cell>2.74</cell><cell>5.16</cell><cell>3.50</cell><cell>4.11</cell><cell>2.81</cell><cell>5.15</cell><cell>2.53</cell><cell>7.15</cell><cell>3.54</cell><cell>4.41</cell><cell>3.61</cell></row><row><cell>Param.</cell><cell>16.7M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3M</cell></row></table><note>Comparison with state-of-the-art SR methods on benchmarks. In 4? and 8?, the 1st and the 2nd best performances are highlighted in red and blue, respectively.1.5M 16.7M 53.7M 4.8M 24.8M 39.5M 39.5M 18.3M 18.3M 16.7M 16.7M 50.8M 50.8M 18.3M 18.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art SR methods on DIV2K in terms of low resolution consistency, photo-realism and meaningful diversity. The numbers in the parentheses are the relative performances, i.e., the best value is set to 1, and the others are divided by the best value.</figDesc><table><row><cell></cell><cell>SR</cell><cell>LR-</cell><cell>Mean</cell><cell>G-best</cell><cell>L-best</cell><cell>Div.</cell></row><row><cell></cell><cell>Model</cell><cell>PSNR?</cell><cell>LPIPS?</cell><cell>LPIPS?</cell><cell>LPIPS?</cell><cell>score?</cell></row><row><cell></cell><cell>SRFlow [24]</cell><cell>50.55 (0.99)</cell><cell>0.1765 (1.54)</cell><cell>0.1153 (1.14)</cell><cell>0.0905 (1.03)</cell><cell>23.12 (1.00)</cell></row><row><cell>4?</cell><cell>DNI [26] FxSR-PD</cell><cell>44.37 (0.87) 51.16 (1.00)</cell><cell>0.1968 (1.72) 0.1253 (1.10)</cell><cell>0.1114 (1.10) 0.1010 (1.00)</cell><cell>0.1003 (1.14) 0.0926 (1.05)</cell><cell>10.01 (0.43) 8.98 (0.39)</cell></row><row><cell></cell><cell>FxSR-DS</cell><cell>44.49 (0.87)</cell><cell>0.1144 (1.00)</cell><cell>0.1018 (1.01)</cell><cell>0.0880 (1.00)</cell><cell>13.66 (0.59)</cell></row><row><cell></cell><cell>SRFlow [24]</cell><cell>50.78 (1.00)</cell><cell>0.3261 (1.32)</cell><cell>0.2613 (1.19)</cell><cell>0.2066 (1.08)</cell><cell>21.88 (1.00)</cell></row><row><cell>8?</cell><cell>FxSR-PD</cell><cell>44.76 (0.88)</cell><cell>0.2477 (1.00)</cell><cell>0.2192 (1.00)</cell><cell>0.1996 (1.04)</cell><cell>9.11 (0.42)</cell></row><row><cell></cell><cell>FxSR-DS</cell><cell>37.77 (0.74)</cell><cell>0.2477 (1.00)</cell><cell>0.2206 (1.01)</cell><cell>0.1912 (1.00)</cell><cell>13.39 (0.61)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>compares with DNI<ref type="bibr" target="#b25">[26]</ref> and SRFlow<ref type="bibr" target="#b23">[24]</ref> in terms of LRPSNR (low-resolution PSNR), LPIPS and Diversity metrics which are evaluation protocol on the Ntire 2021 Challenge<ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref> stated previously.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The details of video we used for SR performance comparison.</figDesc><table><row><cell>Title</cell><cell cols="2">Resolution Bitrate/Codec</cell></row><row><cell>Amazing Place 2018</cell><cell>640?360</cell><cell>319kbps/VP9</cell></row><row><cell>Amazing Place 2019</cell><cell>640?360</cell><cell>301kbps/VP9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of the running time, the computational costs and the size of SR models for super-resolution 4?, when the size of LR input images is 128 ? 128.</figDesc><table><row><cell></cell><cell cols="3">Run Time Mult-Add Param Size</cell><cell>Forward</cell></row><row><cell></cell><cell>(msec)</cell><cell># (G)</cell><cell>(MB)</cell><cell>Pass (MB)</cell></row><row><cell>SRGAN [20]</cell><cell>0.014</cell><cell>1.51</cell><cell>41.63</cell><cell>585.11</cell></row><row><cell>ESRGAN [44]</cell><cell>0.138</cell><cell>16.69</cell><cell>293.97</cell><cell>2061.50</cell></row><row><cell>FxSR</cell><cell>0.501</cell><cell>18.30</cell><cell>320.20</cell><cell>8432.78</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the rest of the paper, we will refer the style control map as just style map or a map T.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">URLs of Amazing Place 2018 and Amazing Place 2019: https://www.youtube.com/watch?v=37IqCYVUhcs, https: //www.youtube.com/watch?v=g5hA2qo2EFc</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel training method and a network structure for the SISR, enabling us to explore various regionwise HR outputs. From this, we can flexibly reconstruct the images between perception-oriented and distortion-oriented ones. This is achieved by defining a conditional objective function with the weights related to the perceptual losses in various feature space levels. Also, our network is designed to modulate the network's intermediate features to change the operation according to these control inputs. As a result, we can generate an image with a desired restoration style for each area. Experiments show that the proposed FxSR yields state-of-the-art perceptual quality and higher PSNR than other perception-oriented methods. Also, we can find many solutions by controlling a single parameter at the inference phase. We will release our code for further research and comparisons.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3106" to="3121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Drfn: Deep recurrent fusion network for single-image super-resolution with large factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="337" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A flexible deep cnn framework for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bobkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Steinbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1055" to="1068" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A single-image super-resolution method based on progressive-iterative approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1407" to="1422" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mrfn: Multi-receptive-field network for fast and accurate single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1042" to="1054" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate and efficient image super-resolution via global-local adjusting dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1924" to="1937" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coarse-to-fine cnn for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1489" to="1502" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>date: 02-05-2016 Through 04-05-2016</idno>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Srobb: Targeted perceptual loss for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Basler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2710" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="715" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cfsnet: Toward a controllable feature space for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4139" to="4148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep network interpolation for continuous imagery effect transition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1692" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic-net: Tuning the objective without re-training for synthesis tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3214" to="3222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to super-resolve blurry face and text images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-world super-resolution using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kheradmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1760" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Super-resolution with deep convolutional sufficient statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Progressive perception-oriented network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">546</biblScope>
			<biblScope unit="page" from="769" to="786" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ntire 2020 challenge on perceptual extreme super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Akita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ooba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hrishikesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Puthussery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2045" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Why are deep representations good perceptual quality features?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Tursun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Didyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aim 2019 challenge on real-world image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Investigating loss functions for extreme super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1705" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep objective quality assessment driven single image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2957" to="2971" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR, 07-09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<meeting>the 32nd International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno>abs/1610.07629</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FiLM: Visual Reasoning with a General Conditioning Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New Orleans, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="63" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modulating image restoration with continual levels via adaptive feature modification layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explorable super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2713" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A model of inductive bias learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">You only train once: Loss-conditional training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image quality assessment: Unifying structure and texture similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Natural and realistic single image super-resolution with explicit natural manifold discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8114" to="8123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Structure-preserving super resolution with gradient guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7766" to="7775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning for real-world super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3408" to="3416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Learning the super-resolution space challenge, ntire 2021 at cvpr</title>
		<ptr target="https://github.com/andreas128/NTIRE21_Learning_SR_Space" />
		<imprint>
			<date type="published" when="2022-01" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ntire 2021 learning the super-resolution space challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chudasama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prajapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Upla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="596" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">3-d depth reconstruction from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;11</title>
		<meeting>the 24th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Algorithms for 3d shape scanning with a depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1039" to="1050" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">3d shape scanning with a time-of-flight camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1173" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep learning for monocular depth estimation: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="page" from="14" to="33" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Ntire 2020 challenge on real-world image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kheradmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Michelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prajapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Umer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2058" to="2076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kalarot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Korkmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Makwana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badhwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1985" to="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Correction filter for single image super-resolution: Robustifying off-theshelf deep super-resolvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1425" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Simusr: A simple but strong baseline for unsupervised image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1953" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Frequency separation for real-world super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3599" to="3608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Unsupervised learning for real-world super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3408" to="3416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multi-objective optimisation using evolutionary algorithms: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-objective Evolutionary Optimisation for Product Design and Manufacturing</title>
		<meeting><address><addrLine>London; London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Pareto multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<idno>pp. 12 060-12 070</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
