<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Coreference Resolution in Visual Dialog using Neural Module Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Coreference Resolution in Visual Dialog using Neural Module Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual dialog entails answering a series of questions grounded in an image, using dialog history as context. In addition to the challenges found in visual question answering (VQA), which can be seen as oneround dialog, visual dialog encompasses several more. We focus on one such problem called visual coreference resolution that involves determining which words, typically noun phrases and pronouns, co-refer to the same entity/object instance in an image. This is crucial, especially for pronouns (e.g., 'it'), as the dialog agent must first link it to a previous coreference (e.g., 'boat' ), and only then can rely on the visual grounding of the coreference 'boat' to reason about the pronoun 'it'. Prior work (in visual dialog) models visual coreference resolution either (a) implicitly via a memory network over history, or (b) at a coarse level for the entire question; and not explicitly at a phrase level of granularity. In this work, we propose a neural module network architecture for visual dialog by introducing two novel modules-Refer and Exclude-that perform explicit, grounded, coreference resolution at a finer word level. We demonstrate the effectiveness of our model on MNIST Dialog, a visually simple yet coreference-wise complex dataset, by achieving near perfect accuracy, and on VisDial, a large and challenging visual dialog dataset on real images, where our model outperforms other approaches, and is more interpretable, grounded, and consistent qualitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of Visual Dialog <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">44]</ref> involves building agents that 'see' (i.e. understand an image) and 'talk' (i.e. communicate this understanding in a dialog). Specifically, it requires an agent to answer a sequence of questions about an image, requiring it to reason about both the image and the past dialog history. For instance, in <ref type="figure" target="#fig_1">Fig. 1</ref>, to answer 'What color is it?', the agent needs to reason about the history to know what 'it' refers to and the image to find out the color. This generalization of visual question answering (VQA) <ref type="bibr" target="#b7">[8]</ref> to dialog takes a step closer to real-world applications (aiding visually impaired users, intelligent home  , boat (brown) and dragon head (green), and stores them in a pool for future coreference resolution in the dialog (right). When asked 'Q1: Is the boat on water?', it identifies that the boat (known entity) and water (unknown entity) are crucial to answer the question. It then grounds the novel entity water in the image (blue), but resolves boat by referring back to the pool and reusing the available grounding from the caption, before proceeding with further reasoning. Thus, our model explicitly resolves coreferences in visual dialog. assistants, natural language interfaces for robots) but simultaneously introduces new modeling challenges at the intersection of vision and language. The particular challenge we focus on in this paper is that of visual coreference resolution in visual dialog. Specifically, we introduce a new model that performs explicit visual coreference resolution and interpretable entity tracking in visual dialog.</p><p>It has long been understood <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">50]</ref> that humans use coreferences, different phrases and short-hands such as pronouns, to refer to the same entity or referent in a single text. In the context of visually grounded dialog, we are interested in referents which are in the image, e.g. an object or person. All phrases in the dialog which refer to the same entity or referent in the image are called visual coreferences. Such coreferences can be noun phrases such as 'a dragon head', 'the head', or pronouns such as 'it' <ref type="figure" target="#fig_1">(Fig. 1</ref>). Especially when trying to answer a question that contains an anaphora, for instance the pronoun 'it', which refers to its full form (the antecedent) 'a dragon head', it is necessary to resolve the coreference on the language side and ground it to the underlying visual referent. More specifically, to answer the question 'What color is it?' in <ref type="figure" target="#fig_1">Fig. 1</ref>, the model must correctly identify which object 'it' refers to, in the given context. Notice that a word or phrase can refer to different entities in different contexts, as is the case with 'it' in this example. Our approach to explicitly resolve visual coreferences is inspired from the functionality of variables or memory in a computer program. In the same spirit as how one can refer back to the contents of variables at a later time in a program without explicitly re-computing them, we propose a model which can refer back to entities from previous rounds of dialog and reuse the associated information; and in this way resolve coreferences.</p><p>Prior work on VQA <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4]</ref> has (understandably) largely ignored the problem of visual coreference resolution since individual questions asked in isolation rarely contain coreferences. In fact, recent empirical studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b16">17]</ref> suggest that today's vision and language models seem to be exploiting dataset-level statistics and perform poorly at grounding entities into the correct pixels. In contrast, our work aims to explicitly reason over past dialog interactions by referring back to previous references. This allows for increased interpretability of the model. As the dialog progresses ( <ref type="figure" target="#fig_1">Fig. 1)</ref>, we can inspect the pool of entities known to the model, and also visualize which entity a particular phrase in the question has been resolved to. Moreover, our explicit entity tracking model has benefits even in cases that may not strictly speaking require coreference resolution. For instance, by explicitly referring 'dragon' in Q3 ( <ref type="figure" target="#fig_1">Fig. 1</ref>) back to a known entity, the model is consistent with itself and (correctly) grounds the phrase in the image. We believe such consistency in model outputs is a strongly desirable property as we move towards human-machine interaction in dialog systems.</p><p>Our main technical contribution is a neural module network architecture for visual dialog. Specifically, we propose two novel modules-Refer and Excludethat perform explicit, grounded, coreference resolution in visual dialog. In addition, we propose a novel way to handle captions using neural module networks at a word-level granularity finer than a traditional sentence-level encoding. We show quantitative benefits of these modules on a reasoning-wise complicated but visually simple MNIST dialog dataset <ref type="bibr" target="#b41">[41]</ref>, where achieve near perfect accuracy. On the visually challenging VisDial dataset <ref type="bibr" target="#b12">[13]</ref>, our model not only outperforms other approaches but also is more interpretable by construction and enables word-level coreference resolution. Furthermore, we qualitatively show that our model is (a) more interpretable (a user can inspect which entities were detected and tracked as the dialog progresses, and which ones were referred to for answering a specific question), (b) more grounded (where the model looked to answer a question in the dialog), (c) more consistent (same entities are considered across rounds of dialog).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We discuss: (a) existing approaches to visual dialog, (b) related tasks such as visual grounding and coreference resolution, and (c) neural module networks. Visual Dialog. Though the origins of visual dialog can be traced back to <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b15">16]</ref>, it was largely formalized by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">44]</ref> who collected human annotated datasets for the same. Specifically, <ref type="bibr" target="#b12">[13]</ref> paired annotators to collect free-form naturallanguage questions and answers, where the questioner was instructed to ask questions to help them imagine the hidden scene (image) better. On the other hand, dialogs from <ref type="bibr" target="#b44">[44]</ref> are more goal driven and contain yes/no questions directed towards identifying a secret object in the image. The respective follow up works used reinforcement learning techniques to solve this problem <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">43]</ref>. Other approaches to visual dialog include transferring knowledge from a discriminatively trained model to a generative dialog model <ref type="bibr" target="#b29">[30]</ref>, using attention networks to solve visual coreferences <ref type="bibr" target="#b41">[41]</ref>, and more recently, a probabilistic treatment of dialogs using conditional variational autoencoders <ref type="bibr" target="#b32">[33]</ref>. Amongst these, <ref type="bibr" target="#b41">[41]</ref> is the closest to this work, while <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> are complementary. To solve visual coreferences, <ref type="bibr" target="#b41">[41]</ref> relies on global visual attentions used to answer previous questions. They store these attention maps in a memory against keys based on textual representations of the entire question and answer, along with the history. In contrast, operating at a finer word-level granularity within each question, our model can resolve different phrases of a question, and ground them to different parts of the image, a core component in correctly understanding and grounding coreferences. E.g., 'A man and woman in a car. Q: Is he or she driving?', which requires resolving 'he' and 'she' individually to answer the question. Grounding language in images and video. Most works in this area focus on the specific task of localizing a textual referential expression in the image <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b50">50]</ref> or video <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b6">7]</ref>. Similar to these works, one component of our model aims to localize words and phrases in the image. However, the key difference is that if the phrase being grounded is an anaphora (e.g., 'it', 'he', 'she', etc.), our model first resolves it explicitly to a known entity, and then grounds it by borrowing the referent's visual grounding. Coreference resolution. The linguistic community defines coreference resolution as the task of clustering phrases, such as noun phrases and pronouns, which refer to the same entity in the world (see, for example, <ref type="bibr" target="#b9">[10]</ref>). The task of visual coreference resolution links the coreferences to an entity in the visual data. For example, <ref type="bibr" target="#b37">[37]</ref> links character mentions in TV show descriptions with their occurrence in the video, while <ref type="bibr" target="#b24">[25]</ref> links text phrases to objects in a 3D scene. Different from these works, we predict a program for a given natural language question about an image, which then tries to resolve any existing coreferences, to then answer the question. An orthogonal direction is to generate language while jointly grounding and resolving coreferences -e.g., <ref type="bibr" target="#b40">[40]</ref> explore this for movie descriptions. While out of scope for this work, it is an interesting direction for future work in visual dialog, especially when generating questions. Neural Module Networks <ref type="bibr" target="#b5">[6]</ref> are an elegant class of models where an instancespecific architecture is composed from neural 'modules' (or building blocks) that are shared across instances. The high-level idea is inspired by 'options' or subtasks in hierarchical RL. They have been shown to be successful for visual question answering in real images and linguistic databases <ref type="bibr" target="#b4">[5]</ref> and for more complex reasoning tasks in synthetic datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref>. For this, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref> learn program prediction and module parameters jointly, end-to-end. Within this context, our work generalizes the formulation in <ref type="bibr" target="#b19">[20]</ref> from VQA to visual dialog by introducing a novel module to perform explicit visual coreference resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Recall that visual dialog <ref type="bibr" target="#b12">[13]</ref> involves answering a question Q t at the current round t, given an image I, and the dialog history (including the image caption)</p><formula xml:id="formula_0">H = ( C H0 , (Q 1 , A 1 ) H1 , ? ? ? , (Q t?1 , A t?1 )</formula><p>Ht?1 ), by ranking a list of 100 candidate an-  Towards this end, our model first identifies relevant words or phrases in the current question that refer to entities in the image (typically objects and attributes). The model also predicts whether each of these has been mentioned in the dialog so far. Next, if these are novel entities (unseen in the dialog history), they are localized in the image before proceeding, and for seen entities, the model predicts the (first) relevant coreference in the conversation history, and retrieves its corresponding visual grounding. Therefore, as rounds of dialog progress, the model collects unique entities and their corresponding visual groundings, and uses this reference pool to resolve any coreferences in subsequent questions.</p><formula xml:id="formula_1">swers A t = {A (1) t , ? ? ? , A</formula><p>Our model has three broad components: (a) Program Generation (Sec. 3.3), where a reasoning pathway, as dictated by a program, is predicted for the current question Q t , (b) Program Execution (Sec. 3.4), where the predicted program is executed by dynamically connecting neural modules <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref> to produce a context vector summarizing the semantic information required to answer Q t from the context (I, H), and lastly, (c) Answer Decoding (Sec. 3.4), where the context vector c t is used to obtain the final answer? t . We begin with a general characterization of neural modules used for VQA in Sec. 3.1 and then discuss our novel modules for coreference resolution (Sec. 3.2) with details of the reference pool. After describing the inner working of the modules, we explain each of the above three components of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Modules for Visual Question Answering</head><p>The main technical foundation of our model is the neural module network (NMN) <ref type="bibr" target="#b5">[6]</ref>. In this section, we briefly recap NMNs and more specifically, the attentional modules from <ref type="bibr" target="#b19">[20]</ref>. In the next section, we discuss novel modules we propose to handle additional challenges in visual dialog.</p><p>For a module m, let x vis and x txt be the input image and text embeddings, respectively. In particular, the image embeddings x vis are spatial activation maps of the image I from a convolutional neural network. The text embedding x txt is computed as a weighted sum of embeddings of words in the question Q t using the soft attention weights ? predicted by a program generator for module m (more details in Sec. 3.3). Further, let {a i } be the set of n m single-channel spatial maps corresponding to the spatial image embeddings, where n m is the number of attention inputs to m. Denoting the module parameters with ? m , a neural module m is essentially a parametric function</p><formula xml:id="formula_2">y = f m (x vis , x txt , {a i } nm i=1 ; ? m ).</formula><p>The output from the module y can either be a spatial image attention map (denoted by a) or a context vector (denoted by c), depending on the module. The output spatial attention map a feeds into next level modules while a context vector c is used to obtain the final answer A t . The upper part of Tab. 1 lists modules we adopt from prior work, with their functional forms. We shortly summarize their behavior. A Find module localizes objects or attributes by producing an attention over the image. The Relocate module takes in an input image attention and performs necessary spatial relocations to handle relationships like 'next to', 'in front of ', 'beside', etc. Intersection or union of attention maps can be obtained using And and Or, respectively. Finally, Describe, Exist, and Count input an attention map to produce the context vector by describing an attribute, checking for existence, or counting, respectively, in the given input attention map. As noted in <ref type="bibr" target="#b19">[20]</ref>, these modules are designed and named for a potential 'atomic' functionality. However, we do not enforce this explicitly and let the modules discover their expected behavior by training in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Modules for Coreference Resolution</head><p>We now introduce novel components and modules to handle visual dialog. Reference Pool (P ref ). The role of the reference pool is to keep track of entities seen so far in the dialog. Thus, we design P ref to be a dictionary of key-value pairs (x txt , a) for all the Find modules instantiated while answering previous questions (Q i ) t?1 i=1 . Recall that Find localizes objects/attributes specified by x txt , and thus by storing each output attention map y, we now have access to all the entities mentioned so far in the dialog with their corresponding visual groundings. Interestingly, even though x txt and y are intermediate outputs from our model, both are easily interpretable, making our reference pool a semantic dictionary. To the best of our knowledge, our model is the first to attempt explicit, interpretable coreference resolution in visual dialog. While <ref type="bibr" target="#b41">[41]</ref>   nor do their keys lend similar interpretability as ours.</p><formula xml:id="formula_3">With P ref = {(x (i) p , a (i) p )} i as input to Refer, we can now resolve references in Q t .</formula><p>Refer Module. This novel module is responsible for resolving references in the question Q t and ground them in the conversation history H. To enable grounding in dialog history, we generalize the above formulation to give the module access to a pool of references P ref of previously identified entities. Specifically, Refer only takes the text embedding x txt and the reference pool P ref as inputs, and resolves the entity represented by x txt in the form of a soft attention ? over Q t . in this section after introducing P ref . For the example shown in <ref type="figure" target="#fig_3">Fig. 2</ref>, ? for Refer attends to 'it', indicating the phrase it is trying to resolve.</p><p>At a high level, Refer treats x txt as a 'query' and retrieves the most likely match from P ref as measured by some similarity with respect to keys {x</p><formula xml:id="formula_4">(i) p } i in P ref .</formula><p>The associated image attention map of the best match is used as the visual grounding for the phrase that needed resolution (i.e. 'it' ). More concretely, we first learn a scoring network which when given a query x txt and a possible candidate x (i) p , returns a scalar value s i indicating how likely these text features refer to the same entity (1). To enable Refer to consider the sequential nature of dialog when assessing a potential candidate, we additionally provide ? i t, a measure of the 'distance' of a candidate x (i) p from x txt in the dialog history, as input to the scoring network. ? i t is formulated as the absolute difference between the round of x txt (current round t) and the round when x (i) p was first mentioned. Collecting these scores from all the candidates, we apply a softmax function to compute contributionss i from each entity in the pool <ref type="bibr" target="#b1">(2)</ref>. Finally, we weigh the corresponding attention maps via these contributions to obtain the visual grounding a out for x txt (3). </p><formula xml:id="formula_5">s i = MLP([x txt , x (i) p , ? i t]) (1) s i = Softmax(s i ) (2) a out = |P ref | i=1s i a (i) p<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Program Generation</head><p>A program specifies the network layout for the neural modules for a given question Q t . Following <ref type="bibr" target="#b19">[20]</ref>, it is serialized through the reverse polish notation (RPN) <ref type="bibr" target="#b10">[11]</ref>. This serialization helps us convert a hard, structured prediction problem into a more tractable sequence prediction problem. In other words, we need a program predictor to output a series of module tokens in order, such that a valid layout can be retrieved from it. There are two primary design considerations for our predictor. First, in addition to the program, our predictor must also output a soft attention ? ti , over the question Q t , for every module m i in the program. This attention is responsible for the correct module instantiation in the current context. For example, to answer the question 'What color is the cat sitting next to the dog?', a Find module instance attending to 'cat' qualitatively serves a different purpose than one attending to 'dog'. This is implemented by using the attention over Q t to compute the text embedding x txt that is directly fed as an input to the module during execution. Second, to decide whether an entity in Q t has been seen before in the conversation, it must be able to 'peek' into the history H. Note that this is unique to our current problem and does not exist in <ref type="bibr" target="#b19">[20]</ref>. To this effect, we propose a novel augmentation of attentional recurrent neural networks <ref type="bibr" target="#b8">[9]</ref> with memory <ref type="bibr" target="#b46">[46]</ref> to address both the requirements <ref type="figure" target="#fig_3">(Fig. 2)</ref>. The program generation proceeds as follows. First, each of the words in Q t are embedded to give {w ti } T i=1 , where T denotes the number of tokens in Q t . We then use a question encoder, a multi-layer LSTM, to process w ti 's, resulting in a sequence of hidden states {? ti } T i=1 <ref type="bibr" target="#b3">(4)</ref>. Notice that the last hidden state h T is the question encoding, which we denote with q t . Next, each piece of history (H i ) t?1 i=0 is processed in a similar way by a history encoder, which is a multi-layer LSTM akin to the question encoder. This produces encodings (h i ) t?1 i=0 (5) that serve as memory units to help the program predictor 'peek' into the conversation history. Using the question encoding q t , we attend over the history encodings (h i ) t?1 i=0 , and obtain the history vector? t (6). The history-agnostic question encoding q t is then fused with the history vector? t via a fully connected layer to give a history-aware question encodingq t <ref type="bibr" target="#b6">(7)</ref>, which is fed into the program decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Encoder</head><formula xml:id="formula_6">{?ti} = LSTM({wti})<label>(4)</label></formula><p>qt =?tT</p><p>History Memor?</p><formula xml:id="formula_7">hi = LSTM(hi) (5) ?ti = Softmax(q T t?i ) ht = t?1 i=0 ?ti?i (6) qt = MLP([qt,?t])<label>(7)</label></formula><p>Program Decoder u</p><formula xml:id="formula_8">(j) ti = Linear([?tj, dti]) u (j) ti = v T tanh(? (j) ti ) ? (j) ti = Softmax(u (j) ti ) eti = T j=1 ? (j) ti? tj (8) eti = MLP([eti, dti]) (9) p(mi|{m k } i?1 k=1 , Qt, H) = Softmax(?ti)<label>(10)</label></formula><p>The decoder is another multi-layer LSTM network (with hidden states {d ti }) which, at every time step i, produces a soft attention map ? ti over the input sequence (Q t ) <ref type="bibr" target="#b8">[9]</ref>. This soft attention map for each module is used to compute the corresponding text embedding, x txt = j ? (j) ti w tj . Finally, to predict a module token m i at time step i, a weighted sum of encoder hidden states e ti (8) and the history-aware question vectorq t are combined via another fully-connected layer (9), followed by a softmax to give a distribution P (m i |{m k } i?1 k=1 , Q t , H) over the module tokens <ref type="bibr" target="#b9">(10)</ref>. During training, we minimize the cross-entropy loss L prog Q between this predicted distribution and the ground truth program tokens. <ref type="figure" target="#fig_3">Fig.  2</ref> outlines the schematics of our program generator. Modules on captions. As the image caption C is also a part of the dialog (history H 0 at round 0), it is desirable to track entities from C via the coreference pool P ref . To this effect, we propose a novel extension of neural module networks to captions by using an auxiliary task that checks the alignment of a (caption, image) pair. First, we learn to predict a program from C, different from those generated from Q t , by minimizing the negative log-likelihood L prog C , akin to L prog Q , of the ground truth caption program. Next, we execute the caption program on two images I + = I and I ? (a random image from the dataset), to produce caption context vectors c + C and c ? C , respectively. Note that c + C and c ? C are different from the context vector c t produced from execution of the question program. Finally, we learn a binary classifier on top to output classes +1/ ? 1 for c + C and c ? C , respectively, by minimizing the binary cross entropy loss L aux C . The intuition behind the auxiliary task is: to rightly classify aligned (C, I + ) from misaligned (C, I ? ), the modules will need to localize and focus on salient entities in the caption. These entities (specifically, outputs from Find in the caption program) are then collected in P ref for explicit coreference resolution on Q t .</p><p>Entities in answers. Using an analogous argument as above, answers from the previous rounds {A i } t?1 i=1 could have entities necessary to resolve coreferences in Q t . For example, 'Q: What is the boy holding? A: A ball. Q: What color is it?' requires resolving 'it' with the 'ball' mentioned in the earlier answer. To achieve this, at the end of round t ? 1, we encode H t?1 = (Q t?1 , A t?1 ) as h ref t using a multi-layer LSTM, obtain the last image attention map a fed to the last module in the program that produced the context vector c t , and add (h ref , a) as an additional candidate to the reference pool P ref . Notice that h ref contains the information about the answer A t?1 in the context of the question Q t?1 , while a denotes the image attention which was the last crucial step in arriving at A t?1 in the earlier round. In resolving coreferences in Q t , if any, all the answers from previous rounds now become potential candidates by virtue of being in P ref .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Model Components</head><p>Program Execution. This component takes the generated program and associated text features x txt for each participating module, and executes it. To do so, we first deserialize the given program from its RPN to a hierarchical module layout. Next, we arrange the modules dynamically according to the layout, giving us the network to answer Q t . At this point, the network is a simple feed-forward neural network, where we start the computation from the leaf modules and feed outputs activations from modules at one layer as inputs into modules at the next layer (see <ref type="figure" target="#fig_3">Fig. 2</ref>). Finally, we feed a context vector c t produced from the last module into the next answer decoding component. Answer Decoding. This is the last component of our model that uses the context vector c t to score answers from a pool of candidates A t , based on their correctness. The answer decoder: (a) encodes each candidate A (i) t ? A t with a multi-layer LSTM to obtain o (i) t , (b) computes a score via a dot product with the context vector, i.e., c T t o (i) t , and (c) applies a softmax activation to get a distribution over the candidates. During training, we minimize the negative loglikelihood L dec A of the ground truth answer A gt t . At test time, the candidate with the maximum score is picked as A t . Using nomenclature from <ref type="bibr" target="#b12">[13]</ref>, this is a discriminative decoder. Note that our approach is not limited to a discriminative decoder, but can also be used with a generative decoder (see supplement). Training Details. Our model components have fully differentiable operations within them. Thus, to train our model, we combine the supervised loss terms from both program generation {L prog Q , L prog C , L aux C } and answer decoding {L dec A }, and minimize the sum total loss L total .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first show results on the synthetic MNIST Dialog dataset <ref type="bibr" target="#b41">[41]</ref>, designed to contain complex coreferences across rounds while being relatively easy textually and visually. It is important to resolve these coreferences accurately in order to do well on this dataset, thus stress testing our model. We then experiment with a large visual dialog dataset on real images, VisDial <ref type="bibr" target="#b12">[13]</ref>, which offers both linguistic and perceptual challenge in resolving visual coreferences and grounding them in the image. Implementation details are in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MNIST Dialog Dataset</head><p>Dataset. The dialogs in the MNIST dialog dataset <ref type="bibr" target="#b41">[41]</ref> are grounded in images composed from a 4 ? 4 grid of MNIST digits <ref type="bibr" target="#b25">[26]</ref>. Digits in the grid have four attributes-digit class (0 ? 9), color, stroke, and background color. Each dialog has 10 question-answer pairs, where the questions are generated through language templates, and the answers are single words. Further, the questions are designed to query attributes of target digit(s), count digits with similar attributes, etc., all of which need tracking of the target digits(s) by resolving references across dialog rounds. Thus, coreference resolution plays a crucial part in the reasoning required to answer the question, making the MNIST dataset both interesting and challenging <ref type="figure" target="#fig_4">(Fig. 3)</ref>. The dataset contains 30k training, 10k validation, and 10k test images, with three 10-round dialogs for each image. Models and baselines. Taking advantage of single-word answers in this dataset, we simplify our answer decoder to be a N -way classifier, where N is the number of possible answers. Specifically, the context vector c t now passes through a fully connected layer of size N , followed by softmax activations to give us a distribution over possible answer classes. At training time, we minimize the cross-entropy L dec A of the predicted answer distribution with the ground truth answer, at every round. Note that single-word answers also simplify evaluation as answer accuracy can now be used to compare different models. We further simplify our model by removing the memory augmentation to the program generator, i.e.,q t = q t (7), and denote it as CorefNMN. In addition to the full model, we also evaluate an ablation, CorefNMN\Seq, without ? i t that additionally captured sequential nature of dialog (see Refer description). We compete against the explicit reasoning model (NMN) <ref type="bibr" target="#b19">[20]</ref> and a comprehensive set of baselines AMEM, image-only (I), and question-only (Q), all from <ref type="bibr" target="#b41">[41]</ref>. Supervision. In addition to the ground truth answer, we also need program supervision for questions to learn the program generation. For each of the 5 'types' of questions, we manually create one program which we apply as supervision for all questions of the corresponding type.   Intuitively, phrases with multiple potential referents, more often than not, refer to the most recent referent, as seen in <ref type="figure" target="#fig_1">Fig. 1, where '</ref>it' has to be resolved to the closest referent in history. <ref type="figure" target="#fig_4">Fig. 3</ref> shows a qualitative example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VisDial v0.9 Dataset</head><p>Dataset. The VisDial dataset <ref type="bibr" target="#b12">[13]</ref> is a crowd-sourced dialog dataset on COCO images <ref type="bibr" target="#b27">[28]</ref>, with free-form answers. The publicly available VisDial v0.9 contains 10-round dialogs on around 83k training images, and 40k validation images. VisDial was collected from pairs of human workers, by instructing one of them to ask questions in a live chat interface to help them imagine the scene better.  <ref type="table">Table 3</ref>: Retrieval performance on the validation set of VisDial v0.9 <ref type="bibr" target="#b12">[13]</ref> (discriminative models) using VGG <ref type="bibr" target="#b42">[42]</ref> features (except last row). Higher the better for mean reciprocal rank (MRR) and recall@k (R@1, R@5, R@10), while lower the better for mean rank. Our CorefNMN model outperforms all other models across all metrics.</p><p>Thus, the dialogs contain a lot of coreferences in natural language, which need to be resolved to answer the questions accurately. Models and baselines. In addition to the CorefNMN model described in Sec. 3, we also consider ablations without the memory network augmented program generator (CorefNMN\Mem) or the auxiliary loss L aux C to train modules on captions (CorefNMN\L aux C ), and without both (CorefNMN\Mem\L aux C ). As strong baselines, we consider: (a) neural module network without history <ref type="bibr" target="#b19">[20]</ref> with answer generation, (b) the best discriminative model based on memory networks MN-QIH-D from <ref type="bibr" target="#b12">[13]</ref>, (c) history-conditioned image attentive encoder (HCIAE-D-MLE) <ref type="bibr" target="#b28">[29]</ref>, and (d) Attention-based visual coreference model (AMEM+SEQ-QI) <ref type="bibr" target="#b41">[41]</ref>. We use ImageNet pretrained VGG-16 <ref type="bibr" target="#b42">[42]</ref> to extract x vis , and also ResNet-152 <ref type="bibr" target="#b18">[19]</ref> for CorefNMN. Further comparisons are in supplement. Evaluation. Evaluation in visual dialog is via retrieval of the ground truth answer A gt t from a pool of 100 candidate answers A t = {A</p><p>(1) t , ? ? ? A (100) t }. These candidates are ranked based the discriminative decoder scores. We report Recall@k for k = {1, 5, 10}, mean rank, and mean reciprocal rank (MRR), as suggested by <ref type="bibr" target="#b12">[13]</ref>, on the set of 40k validation images (there is not test available for v0.9). Supervision. In addition to the ground truth answer A gt t at each round, our model gets program supervision for Q t , to train the program generator. We automatically obtain (weak) program supervision from a language parser on questions (and captions) <ref type="bibr" target="#b20">[21]</ref> and supervision to predict for Refer from an offthe-shelf text coreference resolution tool 4 , based on <ref type="bibr" target="#b11">[12]</ref>. For questions that are a part of coreference chain, we replace Find with Refer in the parser supervised program. Our model predicts everything from the questions at test time. Results. We summarize our observations from Tab. 3 below: (a) Our CorefNMN outperforms all other approaches across all the metrics, highlighting the impor- Next, to answer Q1, it localizes 'boat' and 'water', both of which are 'unseen', and rightly answers with Yes. The ground truth rank (1 for Q1) is shown in the brackets. Additionally, it also registers these two entities in P ref for coreference resolution in future dialog. For Q2, it refers the phrase 'the head' to the referent registered as C-1, indicated by attention on the bar above Refer. tance of explicitly resolving coreferences for visual dialog. Specifically, our R@k (k = 1, 2, 5) is at least 1 point higher than the best prior work (AMEM+SEQ-QI), and almost 2 points higher than NMN. (b) Removing memory augmentation (CorefNMN\Mem) hurts performance uniformly over all metrics, as the model is unable to peek into history to decide when to resolve coreferences via the Refer module. Modules on captions seems to have varied effect on the full model, with decrease in R@1, but marginal increase or no effect in other metrics. (c) <ref type="figure" target="#fig_5">Fig. 4</ref> illustrates the interpretable and grounded nature of our model.   <ref type="table">Table 4</ref>: Retrieval performance on the test-standard split of VisDial v1.0 dataset <ref type="bibr" target="#b12">[13]</ref> (discriminative models). Higher the better for mean reciprocal rank (MRR), recall@k (R@1, R@5, R@10), and normalized discounted cumulative gain (NDGC) while lower the better for mean rank. Our CorefNMN model outperforms all other models across all metrics, except neural module baseline (NMN) on NDGC.</p><p>crowd-sourced dialogs between pairs of humans were collected similar to v0.9. The 10k images are further split into 2k validation (val v1.0) and 8k test sets (test-std v1.0). Dense candidate option annotations, which indicate the correctness of each candidate in the pool, were also collected for these 10k images. Each image in the val v1.0 split is associated with a 10?round dialog, while an image in test-std v1.0 has a variable-round dialog. Additional Metrics and Models. Just as in the previous version (v0.9), the performance on the VisDial v1.0 dataset is benchmarked using standard retrieval metrics like Recall@k (k = {1, 5, 10}), mean reciprocal rank (MRR) and mean rank. Further, Das et al. <ref type="bibr" target="#b12">[13]</ref> also propose to use normalized discounted cumulative gain (NDCG) to score the sorted pool of candidate answers, to evaluate on VisDial v1.0 5 . Intuitively, NDCG penalizes accurate answers that appear lower in the sorted pool based on a logarithmic weighting scheme, normalizing for the number of accurate answers across instances. We train our CorefNMN model on train v1.0 and report numbers on test-std v1.0 split. We compare against LF-QIH-D, HRE-QIH-D, and MN-QIH-D from <ref type="bibr" target="#b12">[13]</ref>, and out-of-the-box neural module network (NMN) <ref type="bibr" target="#b19">[20]</ref>. The LF-QIH-D, HRE-QIH-D, and MN-QIH-D use VGG <ref type="bibr" target="#b42">[42]</ref> image features, while the neural module based models (CorefNMN and NMN) use ResNet-152 <ref type="bibr" target="#b18">[19]</ref> features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>Performance on VisDial v1.0 is given in Tab. 4. Our CorefNMN outperforms all other approaches on all metrics, except the neural module baseline (NMN) on the NDCG metric. We note that recent state-of-the-art, as reported on the leaderboard 6 , has reached up to 0.578 (NDCG) from team DL-61, but the approach and other details (e.g. features, use of an ensemble) are not fully known and unpublished at this point in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduced a novel model for visual dialog based on neural module networks that provides an introspective reasoning about visual coreferences. It explicitly links coreferences and grounds them in the image at a word-level, rather than implicitly or at a sentence-level, as in prior visual dialog work. Our CorefNMN outperforms prior work on both the MNIST dialog dataset (close to perfect accuracy), and on VisDial dataset, while being more interpretable, grounded, and consistent by construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of Supplement</head><p>The supplement is organized as follows:</p><p>-Sec. A shows the results of our model using a discriminative decoder with image features extracted using ImageNet pretrained ResNet-152 <ref type="bibr" target="#b18">[19]</ref>, showing superior performance of our explicit coreference model CorefNMN,</p><p>-Sec. B details our experiments with a generative answer decoder,</p><p>-Implementation details for our experiments are given in Sec. C, and -Schematics of our novel Refer module are in <ref type="figure">Fig. 5, Fig. 6</ref> visualizes the auxiliary task used to run modules on captions, a novel way to handle captions at a fine word-level granularity, and <ref type="figure" target="#fig_8">Fig. 7</ref> shows another qualitative example from VisDial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discriminative Decoder Experiments</head><p>Comparisons with ResNet-152 features. As mentioned in Sec. 4.2 of the main paper, the models trained on VisDial v0.9 used an ImageNet pretrained VGG <ref type="bibr" target="#b42">[42]</ref> to extract the image features x vis . In this section, we present results where a pretrained ResNet-152 <ref type="bibr" target="#b18">[19]</ref> was used to obtain the image features for our CorefNMN model, in Tab. 5. For a fair comparison, we obtained performance metrics from the authors for few of these baselines (MNQIH-G, LF-QH-G), and retrain NMN with ResNet-152 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Generative Decoder Experiments</head><p>The main paper describes a discriminative answer decoder (Sec.3.4) and presents results for the same (Sec. 4.2) on VisDial v0.9 dataset. As a reminder, a discriminative decoder takes the context vector c t as an input, and scores candidate answers according to their correctness. In other words, a discriminative decoder needs to be presented with a list of candidate answers and cannot 'generate' novel answers. We now introduce a generative answer decoder and present results on VisDial v0.9 dataset. Generative Answer Decoder is a language model composed of a multi-layer LSTM with c t as its initial state. During training, we minimize the negative loglikelihood L dec A of the ground truth answer A gt t with respect to the model. At test time, we use the decoder to score all candidates in the answer pool A t by model log-likelihood, and rank them accordingly. Note that this ranking of candidate answers is done to comply with the evaluation protocol of VisDial v0.9, and is not a limitation of generative answer decoder, unlike the discriminative one. Thus, the generative answer decoder can potentially be used to generate novel answers to a given question in the visual dialog via language generation.  <ref type="table">Table 5</ref>: Retrieval performance on the validation set of VisDial dataset v0.9 <ref type="bibr" target="#b12">[13]</ref> (discriminative models with ResNet-152 <ref type="bibr" target="#b18">[19]</ref> features). Higher the better for mean reciprocal rank (MRR) and recall@k (R@1, R@5, R@10), while lower the better for mean rank. Our CorefNMN model outperforms all other models across all metrics. *indicates numbers obtained from authors for models retrained on ResNet-152 features.</p><p>Models and baselines. We denote our model, as described in Sec. <ref type="bibr">3, with</ref> CorefNMN to indicate that the model uses history H. We also consider ablations which do not have the memory network augmented program generator (CorefNMN\Mem), or, the auxiliary loss L aux C to train modules on captions (CorefNMN\L aux C ), and a combination of both (CorefNMN\Mem\L aux C ). As strong baselines, we consider: (a) neural module network without history <ref type="bibr" target="#b19">[20]</ref> with answer generation, (b) the best generative model based on memory networks MN-QIH-G from <ref type="bibr" target="#b12">[13]</ref>, in addition to their LF-QIG-G and HRE-QIH-G models, and (c) history-conditioned image attentive encoder (HCIAE-G-MLE) <ref type="bibr" target="#b28">[29]</ref>. We do not consider the HCIAE-G-DIS model with perceptual loss <ref type="bibr" target="#b28">[29]</ref> as its contribution is complementary to our model. Our model uses ImageNet pretrained ResNet-152 <ref type="bibr" target="#b18">[19]</ref> to extract features for images x vis , while some of these baseline models originally use VGG-16 <ref type="bibr" target="#b42">[42]</ref> features. For a fair comparison, we obtained performance metrics from the authors for few of these baselines (MN-QIH-G, LF-QH-G), and report the others (HCIAE-G-MLE) as is. Results. We summarize our observations from Tab. 6 below: (a) Our CorefNMN outperforms all other approaches according to R@5, R@10, and mean rank metrics, highlighting the importance of explicitly resolving coreferences for visual dialog. Specifically, our mean rank of 15.69 is a 4% improvement over the NMN baseline. (b) However, the NMN baseline has a higher R@1, and perhaps as a result, the best MRR. A possible reason could be due to the noisy supervision from the out-of-domain, automatic, text-based coreference tool, as we entirely rely on it to predict Refer, the module responsible for coreference resolution.   <ref type="table">Table 6</ref>: Retrieval performance on the validation set of VisDial v0.9 <ref type="bibr" target="#b12">[13]</ref> (generative models). Higher the better for mean reciprocal rank (MRR) and recall@k (R@1, R@5, R@10), while lower the better for mean rank. Our CorefNMN model outperforms all other models in R@5, R@10, and mean rank. However, the neural module network baseline (NMN) has the best R@1, and perhaps as a result, the best MRR as well. *indicates numbers obtained from authors for models retrained on ResNet-152 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>All our models are implemented with Tensorflow v1.0 <ref type="bibr" target="#b1">[2]</ref>. To optimize, we use Adam <ref type="bibr" target="#b23">[24]</ref> with a learning rate of 0.0001. Gradients at each iteration are clamped to [?2.0, 2.0] to avoid gradient explosion. To preprocess text, we follow <ref type="bibr" target="#b12">[13]</ref>, i.e., we lowercase all questions and answers, and tokenize using the Python NLTK framework <ref type="bibr" target="#b0">[1]</ref>. We then construct a dictionary of all words that appear at least five times in the training set. Specific model hyperparameters for each experiment are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 MNIST Dialog Dataset</head><p>Due to the synthetic nature, the text in the dialog is of low variability and is made up of a small vocabulary. Thus, we only use a single-layered LSTM with a hidden size of 64 to encode both questions and history. For each word in our vocabulary of size 73, we learn embeddings with 32 dimensions. We learn a similar dimensional embedding for each of our modules. To extract image features, we design a convolutional neural network (CNN) with the same architecture as <ref type="bibr" target="#b41">[41]</ref>. Specifically, our CNN has four 3 ? 3 convolutional layers, each followed by a batch norm, ReLU non-linearity, and a 2 ? 2 max pool layer. While the first two convolutional layers have 32 feature channels, the last two have 64 channels each. To pick the best model, we use early stoppage on the provided validation set of 10k images. <ref type="figure">Fig. 5</ref>: Visualization how our refer module accesses the coreference pool, to understand "it" in this example. For notation see main paper Section 3. xp can be seen as the keys to retrieve attentions ap from the coreference pool P ref . The yellow and and pink lines symbolize the sentences/questions, and red is the text attention. Cyan boxes symbolize the image, with red being the spatial attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 VisDial Dataset</head><p>Each LSTM used in our VisDial experiments has two layers and with a hidden size of 1000. To represent images, we use convolutional features before the final mean pooling from a ImageNet pre-trained ResNet-152 <ref type="bibr" target="#b18">[19]</ref> model. Further, we also add two additional dimensions indicating the X (columns) and Y (rows) locations respectively, to facilitate the model in handling spatial reasoning. With a large vocabulary of around 8k words, our word and module embeddings are 300 dimensional vectors. We also initialize our word embeddings with GloVe <ref type="bibr" target="#b35">[35]</ref>. We pick the best model via early stopping using mean reciprocal rank metric on a subset of 3k images, set aside from the 83k training images of VisDial v0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Document Changelog</head><p>To help the readers track changes to this document, a brief changelog describing the revisions is provided below: v0: ECCV 2018 camera-ready version (not on arXiv). v1: Initial arXiv version: Added experiments on VisDial v1.0 dataset. <ref type="figure">Fig. 6</ref>: Flow diagram for the auxiliary task that measures the alignment between a caption and image as a binary classification task. By predicting a program for the caption, we propose a novel way to process captions at a finer word-level. Sec. 3.3 of the main paper motivates the use of modules on captions via the auxiliary task. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Work partially done as an intern at Facebook AI Research arXiv:1809.01816v1 [cs.CV] 6 Sep 2018</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Our model begins by grounding entities in the caption (C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>As a key component for building better visual dialog agents, our model explicitly resolves visual coreferences in the current question, if any.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our model architecture. The question Qt (orange bar) is encoded along with the history H through a memory augmented question encoder, using which a program (Refer Describe) is decoded. For each module in the program, an attention ?ti over Qt is also predicted, used to compute the text feature xtxt. For Qt, attention is over 'it' for Refer and 'What color' for Describe, respectively (orange bars with red attention). Refer module uses the coreference pool P ref , a dictionary of all previously seen entities with their visual groundings, resolves 'it', and borrows the referent's visual grounding (boat in this case). Finally, Describe extracts the 'color' to produce ct used by a final decoder to pick the answer At from the candidate pool At.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of explicit coreference resolution reasoning of our model on the MNIST dialog dataset. For each question, a program and corresponding attentions (?'s) over question words (hot matrix on the left) is predicted. A layout is unpacked from the program, and modules are connected to form a feed-forward network used to answer the question, shown in green to indicate correctness. We also visualize output attention maps (right) from each participating module. Specifically, in Q1 and Q2, Find localizes all violet digits and 4's, respectively (indicated by the corresponding ?). In Q2, Refer resolves 'them' and borrows the visual grounding from previous question. nature of NMN prohibiting it from capturing the statistic dataset priors. (b) Our CorefNMN outperforms all other models with near perfect accuracy of 99.3%. Examining the failure cases reveals that most of the mistakes made by CorefNMN was due to misclassifying qualitatively hard examples from the original MNIST dataset. (c) Factoring the sequential nature of the dialog additionally in the model is beneficial, as indicated by the 10.6% improvement in CorefNMN, and 7.2% in AMEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Example to demonstrate explicit coreference resolution by our CorefNMN model. It begins by grounding 'dragon head' from the caption C (shown on top), and saves it in the coreference pool P ref (right). At this point however, it does not consider the entity 'boat' important, and misses it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 3</head><label>3</label><figDesc>VisDial v1.0 Dataset Dataset. Das et al.[13] recently released VisDial v1.0 dataset. Specifically, Vis-Dial v1.0 comprises of: (a) A re-organization of train and validation splits from v0.9 to form the new train v1.0. Thus, train v1.0 now contains 120k images with 10?round dialogs for each images, resulting in a total of 1.2 million questionanswer pairs. (b) An additional 10k COCO-like images from Flickr, on which</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative example on VisDial dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>maintains a dictionary similar to P ref , they do not consider word/entity level coreferences</figDesc><table><row><cell>Name</cell><cell>Inputs</cell><cell>Output</cell><cell>Function</cell></row><row><cell></cell><cell cols="3">Neural Modules for VQA [20]</cell></row><row><cell>Find</cell><cell>xvis, xtxt</cell><cell>attention</cell><cell>y = conv2(conv1(xvis W xtxt))</cell></row><row><cell>Relocate</cell><cell>a, xvis, xtxt</cell><cell cols="2">= W1sum(a xvis) y = conv2(conv1(xvis) ? W2xtxt) attention?</cell></row><row><cell>And</cell><cell>a1, a2</cell><cell>attention</cell><cell>y = min{a1, a2}</cell></row><row><cell>Or</cell><cell>a1, a2</cell><cell>attention</cell><cell>y = max{a1, a2}</cell></row><row><cell>Exist</cell><cell>a, xvis, xtxt</cell><cell>context</cell><cell>y = W T vec(a)</cell></row><row><cell>Describe</cell><cell>a, xvis, xtxt</cell><cell>context</cell><cell>y = W T 1 (W2sum(a xvis) W3xtxt)</cell></row><row><cell>Count</cell><cell>a, xvis, xtxt</cell><cell>context</cell><cell>y = W T 1 ([vec(a), max{a}, min{a}])</cell></row><row><cell></cell><cell cols="3">Neural Modules for Coreference resolution (Ours)</cell></row><row><cell>Not</cell><cell>a</cell><cell>attention</cell><cell>y = normL 1 (1 ? a)</cell></row><row><cell>Refer</cell><cell>xtxt, P ref</cell><cell>attention</cell><cell>(see text for details, (3))</cell></row><row><cell>Exclude</cell><cell>a, xvis, xtxt</cell><cell>attention</cell><cell></cell></row></table><note>y = And[Find[xvis, xtxt], Not[a]]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Neural modules used in our work for visual dialog, along with their inputs, outputs, and function formulations. The upper portion contains modules from prior work used for visual question answering, while the bottom portion lists our novel mod- ules designed to handle additional challenges in visual dialog.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Not Module. Designed to focus on regions of the image not attended by the input attention map a, it outputs y = norm L1 (1?a), where norm L1 (.) normalizes the entries to sum to one. This module is used in Exclude, described next.Exclude Module. To handle questions like 'What other red things are present?', which seek other objects/attributes in the image than those specified by an input attention map a, we introduce yet another novel module -Exclude. It is constructed using Find, Not, and And modules as y = And[Find[x txt , x vis ], Not[a]], where x txt is the text feature input to the Exclude module, for example, 'red things'. More explicitly, Find first localizes all objects instances/attributes in the image. Next, we focus on regions of the image other than those specified by a using Not[a]. Finally, the above two outputs are combined via And to obtain the output y of the Exclude module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The type of question is provided with the question. Note that our model needs program supervision only while training, and uses predictions from program generator at test time.Results. Tab. 2 shows the results on MNIST dataset. The following are the key observations: (a) The text-only Q (36.6%) and image-only I (20.2%) do not perform well, perhaps as expected as MNIST Dialog needs resolving strong coreferences to arrive at the correct answer. For the same reason, NMN<ref type="bibr" target="#b19">[20]</ref> has a low accuracy of 23.8%. Interestingly, Q outperforms NMN by around 13% (both use question and image, but not history), possibly due to the explicit reasoning</figDesc><table><row><cell>Model</cell><cell>Acc.</cell></row><row><cell>I [41]</cell><cell>20.2</cell></row><row><cell>Q [41]</cell><cell>36.6</cell></row><row><cell cols="2">AMEM\Seq [41] 89.2</cell></row><row><cell>AMEM [41]</cell><cell>96.4</cell></row><row><cell>NMN [20]</cell><cell>23.8</cell></row><row><cell cols="2">CorefNMN\Seq 88.7</cell></row><row><cell>CorefNMN</cell><cell>99.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Answer</figDesc><table><row><cell>accuracy</cell></row><row><cell>on MNIST Dialog dataset.</cell></row><row><cell>Higher the better. Our</cell></row><row><cell>CorefNMN outperforms all</cell></row><row><cell>other models with a near</cell></row><row><cell>perfect accuracy on test set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>c) Our novel way of handling captions using modules (indicated L aux C ) boosts the full model, while the hurting the ablation without the memory augmentation. That is, CorefNMN\Mem\L aux C is better than CorefNMN\Mem across all metrics, while CorefNMN\L aux C is uniformly worse than CorefNMN.</figDesc><table><row><cell>Model</cell><cell>MRR</cell><cell>R@1</cell><cell>R@5</cell><cell cols="2">R@10 Mean</cell></row><row><cell>MN-QIH-G [13] (VGG)</cell><cell>0.526</cell><cell>42.29</cell><cell>62.85</cell><cell>68.88</cell><cell>17.06</cell></row><row><cell>HCIAE-G-MLE[30](VGG)</cell><cell>0.539</cell><cell>44.06</cell><cell>63.55</cell><cell>69.24</cell><cell>16.01</cell></row><row><cell>LF-QIH-G* [13]</cell><cell>0.515</cell><cell>41.04</cell><cell>61.63</cell><cell>67.54</cell><cell>17.32</cell></row><row><cell>HRE-QIH-G* [13]</cell><cell>0.523</cell><cell>42.26</cell><cell>62.20</cell><cell>67.95</cell><cell>16.96</cell></row><row><cell>MN-QIH-G* [13]</cell><cell>0.527</cell><cell>42.60</cell><cell>62.58</cell><cell>68.52</cell><cell>17.21</cell></row><row><cell>NMN[20]</cell><cell cols="2">0.542 45.05</cell><cell>63.27</cell><cell>69.28</cell><cell>16.34</cell></row><row><cell>CorefNMN\Mem</cell><cell>0.531</cell><cell>43.67</cell><cell>62.31</cell><cell>68.27</cell><cell>16.73</cell></row><row><cell>CorefNMN\Mem\L aux C</cell><cell>0.537</cell><cell>44.26</cell><cell>63.02</cell><cell>69.01</cell><cell>16.47</cell></row><row><cell>CorefNMN\L aux C</cell><cell>0.533</cell><cell>43.62</cell><cell>63.08</cell><cell>69.12</cell><cell>16.39</cell></row><row><cell>CorefNMN</cell><cell>0.535</cell><cell>43.66</cell><cell cols="2">63.54 69.93</cell><cell>15.69</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/huggingface/neuralcoref</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://visualdialog.org/challenge/2018 6 https://evalai.cloudcv.org/web/challenges/challenge-page/103/leaderboard/298</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by NSF, AFRL, DARPA, Siemens, Google, Amazon, ONR YIPs and ONR Grants N00014-16-1-{2713,2793}, N000141210903. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.nltk.org/" />
		<title level="m">NLTK</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/,softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Learning to Compose Neural Networks for Question Answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrapping path-based pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of a logical machine using parenthesis-free notation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Burks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Wright</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2001990" />
	</analytic>
	<monogr>
		<title level="j">Mathematical Tables and Other Aids to Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="53" to="57" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06585</idno>
		<title level="m">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual Turing Test for computer vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Logic and conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
		<ptr target="http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf" />
	</analytic>
	<monogr>
		<title level="m">Syntax and Semantics</title>
		<editor>Cole, P., Morgan, J.L.</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
	<note>Speech Acts</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual semantic search: Retrieving videos via complex textual queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical Question-Image Co-Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Flipdial: A generative model for two-way visual dialogue</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating expressions that refer to visible objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1137" />
		<imprint>
			<date type="published" when="2013-06" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1174" to="1184" />
			<pubPlace>Atlanta, Georgia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Linking people in videos with &quot;their&quot; names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grounding Action Descriptions in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generating descriptions with grounded and co-referenced people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Endto-end optimization of goal-driven and visually grounded dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Procedures as a representation for data in a computer program for understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DTIC Document</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Understanding Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Academic Press, Inc</publisher>
			<pubPlace>Orlando, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Grounded language learning from videos described with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Yin and Yang: Balancing and Answering Binary Visual Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
