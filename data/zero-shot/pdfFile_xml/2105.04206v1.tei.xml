<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You Only Learn One Representation: Unified Network for Multiple Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
							<email>ihyeh@emc.com.tw</email>
							<affiliation key="aff1">
								<orgName type="institution">Elan Microelectronics Corporation</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan</forename><forename type="middle">Mark</forename><surname>Liao</surname></persName>
							<email>liao@iis.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">You Only Learn One Representation: Unified Network for Multiple Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People "understand" the world via vision, hearing, tactile, and also the past experience. Human experience can be learned through normal learning (we call it explicit knowledge), or subconsciously (we call it implicit knowledge). These experiences learned through normal learning or subconsciously will be encoded and stored in the brain. Using these abundant experience as a huge database, human beings can effectively process data, even they were unseen beforehand. In this paper, we propose a unified network to encode implicit knowledge and explicit knowledge together, just like the human brain can learn knowledge from normal learning as well as subconsciousness learning. The unified network can generate a unified representation to simultaneously serve various tasks. We can perform kernel space alignment, prediction refinement, and multi-task learning in a convolutional neural network. The results demonstrate that when implicit knowledge is introduced into the neural network, it benefits the performance of all tasks. We further analyze the implicit representation learnt from the proposed unified network, and it shows great capability on catching the physical meaning of different tasks. The source code of this work is at : https:// github.com/ WongKinYiu/ yolor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As shown in <ref type="figure">Figure 1</ref>, humans can analyze the same piece of data from various angles. However, a trained convolutional neural network (CNN) model can usually only fulfill a single objective. Generally speaking, the features that can be extracted from a trained CNN are usually poorly adaptable to other types of problems. The main cause for the above problem is that we only extract features from neurons, and implicit knowledge, which is abundant in CNN, is not used. When the real human brain is operating, the aforementioned implicit knowledge can effectively assist the brain to perform various tasks.</p><p>Implicit knowledge refers to the knowledge learned in a subconscious state. However, there is no systematic def- <ref type="bibr">Figure 1:</ref> Human beings can answer different questions from the same input. Our aim is to train a single deep neural network that can serve many tasks.</p><p>inition of how implicit learning operates and how to obtain implicit knowledge. In the general definition of neural networks, the features obtained from the shallow layers are often called explicit knowledge, and the features obtained from the deep layers are called implicit knowledge. In this paper, we call the knowledge that directly correspond to observation as explicit knowledge. As for the knowledge that is implicit in the model and has nothing to do with observation, we call it as implicit knowledge.</p><p>We propose a unified network to integrate implicit knowledge and explicit knowledge, and enable the learned model to contain a general representation, and this general representation enable sub-representations suitable for various tasks. <ref type="figure" target="#fig_0">Figure 2.</ref>(c) illustrates the proposed unified network architecture.</p><p>The way to construct the above unified networks is to combine compressive sensing and deep learning, and the main theoretical basis can be found in our previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. In <ref type="bibr" target="#b15">[16]</ref>, we prove the effectiveness of reconstructing residual error by extended dictionary. In <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, we use sparse coding to reconstruct feature map of a CNN and make it more robust. The contribution of this work are summarized as follows:</p><p>1. We propose a unified network that can accomplish various tasks, it learns a general representation by integrating implicit knowledge and explicit knowledge, and one can complete various tasks through this general representation. The proposed network effectively improves the performance of the model with a very small amount of additional cost (less than one ten thousand of the amount of parameters and calculations.) 2. We introduced kernel space alignment, prediction refinement, and multi-task learning into the implicit knowledge learning process, and verified their effectiveness. 3. We respectively discussed the ways of using vector, neural network, or matrix factorization as a tool to model implicit knowledge, and at the same time verified its effectiveness. 4. We confirmed that the proposed implicit representation learned can accurately correspond to a specific physical characteristic, and we also present it in a visual way. We also confirmed that if operators that conform to the physical meaning of an objective, it can be used to integrate implicit knowledge and explicit knowledge, and it will have a multiplier effect. 5. Combined with state-of-the-art methods, our proposed unified network achieved comparable accuracy as Scaled-YOLOv4-P7 <ref type="bibr" target="#b14">[15]</ref> on object detection and the inference speed has been increased 88%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We conduct a review of the literature related to this research topic. This literature review is mainly divided into three aspects: (1) explicit deep learning: it will cover some methods that can automatically adjust or select features based on input data, (2) implicit deep learning: it will cover the related literature of implicit deep knowledge learning and implicit differential derivative, and (3) knowledge modeling: it will list several methods that can be used to integrate implicit knowledge and explicit knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Explicit deep learning</head><p>Explicit deep learning can be carried out in the following ways. Among them, Transformer <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref> is one way, and it mainly uses query, key, or value to obtain self-attention. Non-local networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref> is another way to obtain attention, and it mainly extracts pair-wise attention in time and space. Another commonly used explicit deep learning method <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> is to automatically select the appropriate kernel by input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Implicit deep learning</head><p>The methods that belong to the category of implicit deep learning are mainly implicit neural representations <ref type="bibr" target="#b10">[11]</ref> and deep equilibrium models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref>. The former is mainly to obtain the parameterized continuous mapping representation of discrete inputs to perform different tasks, while the latter is to transform implicit learning into a residual form neural networks, and perform the equilibrium point calculation on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Knowledge modeling</head><p>As for the methods belonging to the category of knowledge modeling, sparse representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> and memory networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12]</ref> are mainly included. The former uses exemplar, predefined over complete, or learned dictionary to perform modeling, while the latter relies on combining various forms of embedding to form memory, and enable memory to be dynamically added or changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">How implicit knowledge works?</head><p>The main purpose of this research is to conduct a unified network that can effectively train implicit knowledge, so first we will focus on how to train implicit knowledge and inference it quickly in the follow-up. Since implicit representation z i is irrelevant to observation, we can think of it as a set of constant tensor Z = {z 1 , z 2 , ..., z k }. In this section we will introduce how implicit knowledge as constant tensor can be applied to various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Manifold space reduction</head><p>We believe that a good representation should be able to find an appropriate projection in the manifold space to which it belongs, and facilitate the subsequent objective tasks to succeed. For example, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, if the target categories can be successfully classified by the hyperplane in the projection space, that will be the best outcome. In the above example, we can take the inner product of the projection vector and implicit representation to achieve the goal of reducing the dimensionality of manifold space and effectively achieving various tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Kernel space alignment</head><p>In multi-task and multi-head neural networks, kernel space misalignment is a frequent problem, <ref type="figure" target="#fig_2">Figure 4</ref>.(a) illustrates an example of kernel space misalignment in multitask and multi-head NN. To deal with this problem, we can perform addition and multiplication of output feature and implicit representation, so that Kernel space can be translated, rotated, and scaled to align each output kernel space of neural networks, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>.(b). The above mode of operation can be widely used in different fields, such as the feature alignment of large objects and small objects in feature pyramid networks (FPN) <ref type="bibr" target="#b7">[8]</ref>, the use of knowledge distillation to integrate large models and small models, and the handling of zero-shot domain transfer and other issues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">More functions</head><p>In addition to the functions that can be applied to different tasks, implicit knowledge can also be extended into many more functions. As illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>, through introducing addition, one can make neural networks to predict the offset of center coordinate. It is also possible to introduce multiplication to automatically search the hyperparameter set of an anchor, which is very often needed by an anchor-based object detector. Besides, dot multiplication and concatenation can be used, respectively, to perform multi-task feature selection and to set pre-conditions for subsequent calculations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implicit knowledge in our unified networks</head><p>In this section, we shall compare the objective function of conventional networks and the proposed unified networks, and to explain why introducing implicit knowledge is important for training a multi-purpose network. At the same time, we will also elaborate the details of the method proposed in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Formulation of implicit knowledge</head><p>Conventional Networks:</p><p>For the object function of conventional network training, we can use (1) to express as follows:</p><formula xml:id="formula_0">y = f ? (x) + minimize (1)</formula><p>where x is observation, ? is the set of parameters of a neural network, f ? represents operation of the neural network, is error term, and y is the target of given task.</p><p>In the training process of a conventional neural network, usually one will minimize to make f ? (x) as close to the target as possible. This means that we expect different observations with the same target to be a single point in the sub space obtained by f ? , as illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>.(a). In other words, the solution space we expect to obtain is discriminative only for the current task t i and invariant to tasks other than t i in various potential tasks,</p><formula xml:id="formula_1">T \ t i , where T = {t 1 , t 2 , ..., t n }.</formula><p>For general purpose neural network, we hope that the obtained representation can serve all tasks belonging to T . Therefore, we need to relax to make it possible to find solution of each task at the same time on manifold space, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>.(b). However, the above requirements make it impossible for us to use a trivial mathematical method, such as maximum value of one-hot vector, or threshold of Euclidean distance, to get the solution of t i . In order to solve the problem, we must model the error term to find solutions for different tasks, as shown in <ref type="figure" target="#fig_4">Figure  6</ref>.(c). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified Networks:</head><p>To train the proposed unified networks, we use explicit and implicit knowledge together to model the error term, and then use it to guide the multi-purpose network training process. The corresponding equation for training is as follows:</p><formula xml:id="formula_2">y = f ? (x) + + g ? ( ex (x), im (z)) minimize + g ? ( ex (x), im (z))<label>(2)</label></formula><p>where ex and im are operations which modeling, respectively, the explicit error and implicit error from observation x and latent code z. g ? here is a task specific operation that serves to combine or select information from explicit knowledge and implicit knowledge.</p><p>There are some existing methods to integrate explicit knowledge into f ? , so we can rewrite <ref type="formula" target="#formula_2">(2)</ref> into <ref type="formula" target="#formula_3">(3)</ref>.</p><formula xml:id="formula_3">y = f ? (x) g ? (z)<label>(3)</label></formula><p>where represents some possible operators that can combine f ? and g ? . In this work, the operators introduced in Section 3 will be used, which are addition, multiplication, and concatenation. If we extend derivation process of error term to handling multiple tasks, we can get the following equation:</p><formula xml:id="formula_4">F (x, ?, Z, ?, Y, ?) = 0<label>(4)</label></formula><p>where Z = {z 1 , z 2 , ..., z T } is a set of implicit latent code of T different tasks. ? are the parameters that can be used to generate implicit representation from Z. ? is used to calculate the final output parameters from different combinations of explicit representation and implicit representation. For different tasks, we can use the following formula to obtain prediction for all z ? Z.</p><formula xml:id="formula_5">d ? (f ? (x), g ? (z), y) = 0<label>(5)</label></formula><p>For all tasks we start with a common unified representation f ? (x), go through task-specific implicit representation g ? (z), and finally complete different tasks with taskspecific discriminator d ? . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Modeling implicit knowledge</head><p>The implicit knowledge we proposed can be modeled in the following ways:</p><formula xml:id="formula_6">Vector / Matrix / Tensor: z<label>(6)</label></formula><p>Use vector z directly as the prior of implicit knowledge, and directly as implicit representation. At this time, it must be assumed that each dimension is independent of each other. Neural Network:</p><formula xml:id="formula_7">Wz (7)</formula><p>Use vector z as the prior of implicit knowledge, then use the weight matrix W to perform linear combination or nonlinearization and then become an implicit representation. At this time, it must be assumed that each dimension is dependent of each other. We can also use more complex neural network to generate implicit representation. Or use Markov chain to simulate the correlation of implicit representation between different tasks. Matrix Factorization:</p><formula xml:id="formula_8">Z T c<label>(8)</label></formula><p>Use multiple vectors as prior of implicit knowledge, and these implicit prior basis Z and coefficient c will form implicit representation. We can also further do sparse constraint to c and convert it into sparse representation form. In addition, we can also impose non-negative constraint on Z and c to convert them into non-negative matrix factorization (NMF) form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>Assuming that our model dos not have any prior implicit knowledge at the beginning, that is to say, it will not have any effect on explicit representation f ? (x). When the combining operator ? {addition, concatenation}, the initial implicit prior z ? N (0, ?), and when the combing operator is multiplication, z ? N (1, ?). Here, ? is a very small value which is close to zero. As for z and ?, they both are trained with backpropagation algorithm during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Inference</head><p>Since implicit knowledge is irrelevant to observation x, no matter how complex the implicit model g ? is, it can be reduced to a set of constant tensor before the inference phase is executed. In other words, the formation of implicit information has almost no effect on the computational complexity of our algorithm. In addition, when the above operator is multiplication, if the subsequent layer is a convolutional layer, then we use (9) below to integrate. When one encounters an addition operator, and if the previous layer is a convolutional layer and it has no activation function, then one use (10) shown below to integrate.</p><formula xml:id="formula_9">x (l+1) = ?(W l (g ? (z)x l ) + b l ) = ?(W l (x l ) + b l ), where W l = W l g ? (z)<label>(9)</label></formula><formula xml:id="formula_10">x (l+1) = W l (x l ) + b l + g ? (z) = W l (x l ) + b l , where b l = b l + g ? (z)<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our experiments adopted the MSCOCO dataset <ref type="bibr" target="#b8">[9]</ref>, because it provides ground truth for many different tasks, including 1 object detection, 2 instance segmentation, 3 panoptic segmentation, 4 keypoint detection, 5 stuff segmentation, <ref type="bibr" target="#b5">6</ref> image caption, 7 multi-label image classification, and 8 long tail object recognition. These data with rich annotation content can help train a unified network that can support computer vision-related tasks as well as natural language processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>In the experimental design, we chose to apply implicit knowledge to three aspects, including 1 feature alignment for FPN, 2 prediction refinement, and 3 multi-task learning in a single model. The tasks covered by multi-task learning include 1 object detection, 2 multi-label image classification, and 3 feature embedding. We choose YOLOv4-CSP <ref type="bibr" target="#b14">[15]</ref> as the baseline model in the experiments, and introduce implicit knowledge into the model at the position pointed by the arrow in <ref type="figure" target="#fig_6">Figure 8</ref>. All the training hyper-parameters are compared to default setting of Scaled-YOLOv4 <ref type="bibr" target="#b14">[15]</ref>. In Section 5.2, 5.3, and 5.4, we use the simplist vector implicit representation and addition operator to verify the positive impact on various tasks when implicit knowledge is introduced. In Section 5.5, we will use different operators on different combinations of explicit knowledge and implicit knowledge, and discuss the effectiveness of these combinations. In Section 5.6, we shall model implicit knowledge by using different approaches. In Section 5.7, we analyze the model with and without introduce implicit knowledge. Finally in Section 5.8, we shall train object detectors with implicit knowledge and then compare the performance with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Feature alignment for FPN</head><p>We add implicit representation into the feature map of each FPN for feature alignment, and the corresponding experiment results are illustrated in <ref type="table" target="#tab_0">Table 1</ref>. From these results shown in <ref type="table" target="#tab_0">Table 1</ref> we can say: After using implicit representation for feature space alignment, all performances, including AP S , AP M , and AP L , have been improved by about 0.5%, which is a very significant improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Prediction refinement for object detection</head><p>Implicit representations are added to YOLO output layers for prediction refinement. As illustrated in <ref type="table" target="#tab_1">Table 2</ref>, we see that almost all indicator scores have been improved. <ref type="figure" target="#fig_7">Figure 9</ref> shows how the introduction of implicit representation affects the detection outcome. In the object detection case, even we do not provide any prior knowledge for implicit representation, the proposed learning mechanism can still automatically learn (x, y), (w, h), (obj), and (classes) patterns of each anchor.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Canonical representation for multi-task</head><p>When one wants to train a model that can be shared by many tasks at the same time, since the joint optimization process on loss function must be executed, multiple parties often pull each other during the execution process. The above situation will cause the final overall performance to be worse than training multiple models individually and then integrating them. In order to solve the above problem, we propose to train a canonical representation for multitasks. Our idea is to augment the representation power by introducing implicit representation to each task branch, and the effects it causes are listed in <ref type="table" target="#tab_2">Table 3</ref>. As the data illustrated in <ref type="table" target="#tab_2">Table 3</ref>, without the introduction of implicit representation, some index scores improved after multi-task training, and some dropped. After introducing implicit representation to joint detection and classification (JDC), in the model category corresponding to + iJDC, we can clearly see that the overall index score has increased significantly, and it has surpassed the performance of single-task training model. Compared to when implicit representation was not introduced, the performance of our model on mediumsized objects and large-sized objects has also been improved by 0.3% and 0.7%, respectively. In the experiment of joint detection and embedding (JDE), because of the characteristic of implicit representation implied by feature alignment, the effect of improving the index score is more significant. Among the index scores corresponding to JDE and + iJDE listed in <ref type="table" target="#tab_2">Table 3</ref>, all index scores of + iJDE surpass the index that does not introduce implicit representation. Among them, the AP for large objects even increased by 1.1%.   <ref type="table" target="#tab_3">Table 4</ref> shows the experimental results of using different operators shown in <ref type="figure" target="#fig_8">Figure 10</ref> to combine explicit representation and implicit representation. In the implicit knowledge for feature alignment experiment, we see that addition and concatenation both improve performance, while multiplication actually degrades performance. The experimental results of feature alignment are in full compliance with its physical characteristics, because it must deal with the scaling of global shift and all individual clusters. In the implicit knowledge for prediction refinement experiment, since the operator of concatenation ill change the dimension of output, we only compare the effects of using addition and multiplication operators in the experiment. In this set of experiments, the performance of applying multiplication is better than that of applying addition. Analyzing the reason, we found that center shift uses addition decoding when executing prediction, while anchor scale uses multiplication decoding. Because center coordinate is bounded by grid, the impact is minor, and the artificially set anchor owns a larger optimization space, so the improvement is more significant. Based on the above analysis, we designed two other set of experiments -{? iFA * , ? iPR * }. In the first set of experiments -? iFA * , we split feature space into anchor cluster level for combination with multiplication, while in the second set of experiments -? iPR * , we only performed multiplication refinement on width and height in prediction. The results of the above experiments are illustrated in <ref type="table" target="#tab_4">Table 5</ref>. From the figures shown in <ref type="table" target="#tab_4">Table 5</ref>, we find that after corresponding modifications, the scores of various indices have been comprehensively improved. The experiment shows that when we designing how to combine explicit and implicit knowledge, we must first consider the physical meaning of the combined layers to achieve a multiplier effect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Implicit modeling with different operators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Modeling implicit knowledge in different ways</head><p>We tried to model implicit knowledge in different ways, including vector, neural networks, and matrix factorization. When modeling with neural networks and matrix factorization, the default value of implicit prior dimension is twice that of explicit representation dimension. The results of this set of experiments are shown in <ref type="table" target="#tab_5">Table 6</ref>. We can see that whether it is to use neural networks or matrix factorization to model implicit knowledge, it will improve the overall effect. Among them, the best results have been achieved by using matrix factorization model, and it upgrades the performance of AP, AP 50 , and AP 75 by 0.2%, 0.4%, and 0.5%, respectively. In this experiment, we demonstrated the effect of using different modeling ways. Meanwhile, we confirmed the potential of implicit representation in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Analysis of implicit models</head><p>We analyze the number of parameters, FLOPs, and learning process of model with/w/o implicit knowledge, and show the results in <ref type="table" target="#tab_6">Table 7</ref> and <ref type="figure" target="#fig_9">Figure 11</ref>, respectively. From the experimental data, we found that in the model with implicit knowledge set of experiments, we only increased the amount of parameters and calculations by less than one ten thousandth, which can significantly improve the performance of the model, and the training process can also converge quickly and correctly.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Implicit knowledge for object detection</head><p>Finally, we compare the effectiveness of the proposed method with object detection's state-of-the-art methods. The benefits of introducing implicit knowledge are shown in <ref type="table" target="#tab_7">Table 8</ref>. For the entire training process, we follow the scaled-YOLOv4 <ref type="bibr" target="#b14">[15]</ref> training process, that is, train from scratch 300 epochs first, and then fine-tune 150 epochs. <ref type="table" target="#tab_8">Table 9</ref> illustrates the comparisons with the state-of-theart methods. One thing worth noting is that our proposed method does not have additional training data and annotations. By introducing the unified network of implicit knowledge, we still achieve results that are sufficient to match the state-of-the-art methods.   <ref type="bibr" target="#b14">[15]</ref> 55.5% 73.4% 60.8% 16 EfficientDet <ref type="bibr" target="#b12">[13]</ref> 55.1% 74.3% 59.9% 6.5 SwinTransformer <ref type="bibr" target="#b9">[10]</ref> 57.7% ---CenterNet2 <ref type="bibr" target="#b25">[26]</ref> 56.4% 74.0% 61.6% -CopyPaste <ref type="bibr" target="#b5">[6]</ref> 57.3% ---* pre. : large dataset image classification pre-training. * seg. : training with segmentation ground truth. * add. : training with additional images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we show how to construct a unified network that integrates implicit knowledge and explicit knowledge, and prove that it is still very effective for multi-task learning under the single model architecture. In the future, we shall extend the training to multi-modal and multi-task, as shown in <ref type="figure" target="#fig_0">Figure 12</ref>.   <ref type="figure" target="#fig_1">Figure A3</ref>: Models in this paper can be mapped to three four of architecture topology. Due to Stem C, D, and E contain two downsampling modules, models used those stem blocks has no Stage B1 in the backbone, for same reason Stem F has no Stage B1 and B2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>? YOLOv4-CSP belongs to Topology 1, the architecture is described in Scaled-YOLOv4 paper.</p><p>? YOLOv4-CSP-fast is modified from YOLOv4-CSP, we replace Stem A in YOLOv4-CSP by Stem B to form YOLOv4-CSP-fast.</p><p>? YOLOv4-CSP-SSS belongs to Topology 2, Stem C is used in this model. The topology after Stage B2 is as same as YOLOv4-CSP, and width scaling factor and depth scaling factor are set as 0.5 and 0.33, respectively. We then using SiLU activation to replace all Mish activation in the model. ? YOLOv4-CSP-SSSS is modified from YOLOv4-CSP-SSS, Stem C in YOLOv4-CSP-SSS is replaced by Stem F in this model. Due to the stem block contains three down-sampling modules, YOLOv4-CSP-SSSS belongs to topology IV.</p><p>? YOLOv4-P6-light belongs to Topology 3, it uses Stem D and base channels are set as {128, 256, 384, 512, 640}. To optimize the gradient propagation, we apply CSP fusion first in B* stages and the repeat number of B2 to B6 are set as {3, 7, 7, 3, 3}.</p><p>? YOLOR-P6 has same architecture as YOLOv4-P6-light, we replace all Mish activation in YOLOv4-P6-light by SiLU activation.</p><p>? YOLOR-W6 is wider YOLOR-P6, base channels are set as {128, 256, 512, 768, 1024}.</p><p>? YOLOR-E6 expands the width of YOLOR-W6, the width scaling factor is set as 1.25, and all of convolution down-sampling modules are replaced by CSP convolution.</p><p>? YOLOR-D6 is deeper YOLOR-E6, the repeat number of B2 to B6 are set as {3, 15, 15, 7, 7}   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Multi-purpose NN architectures. (a) distinct models for distinct tasks; (b) shared backbone, different heads for different tasks; and (c) our proposed unified network: one representation with explicit knowledge and implicit knowledge for serving multiple tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Manifold space reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Kernel space alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>More functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Modeling error term.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>We proposed to use three different ways for modeling implicit knowledge. The top row shows the formation of these three different modeling approaches, and the bottom row shows their corresponding mathematical attributes. (a) Vector: single base, and each dimension is independent with another dimensions; (b) Neural Network: single or multiple basis, and each dimension is dependent to another dimensions; and (c) Matrix factorization: multiple basis, and each dimension is independent with another dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Value of learned implicit representation for prediction refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Implicit modeling with (a) addition, (b) multiplication, and (c) concatenation operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Learning curve of model with/w/o implicit knowledge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Multimodal unified netwrok.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A1 :</head><label>A1</label><figDesc>We use four kind of down-sampling modules in this work, including (a) discrete wavelet transform (DWT): https://github.com/fbcotter/pytorch wavelets, (b) re-organization (ReOrg): https://github.com/AlexeyAB/darknet/ issues/4662#issuecomment-608886018, (c) convolution, and (d) CSP convolution used in CSPNet: https://github.com/ WongKinYiu/CrossStagePartialNetworks/tree/pytorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A2 :</head><label>A2</label><figDesc>We use down-sampling modules in Figure A1 to form stem blocks: (a) Stem A is used in YOLOv4-CSP, (b) Stem B is used in YOLOv4-CSP-fast, (c) Stem C is used in YOLOv4-CSP-SSS, (d) Stem D is proposed by 10.5281/zenodo.4679653 and called focus layer, it is used in YOLOv4-P6-light, YOLOR-P6, and YOLOR-W6, (e) Stem E is used in YOLOR-E6 and YOLOR-D6, and (f) Stem F is used in YOLOv4-CSP-SSSS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of feature alignment.</figDesc><table><row><cell>Model</cell><cell>AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell cols="6">baseline 47.8% 66.3% 52.1% 30.1% 52.5% 62.0%</cell></row><row><cell>+ iFA</cell><cell cols="5">47.9% 66.6% 52.3% 30.6% 53.1% 62.6%</cell></row></table><note>* baseline is YOLOv4-CSP-fast, tested on 640?640 input resolution.* FA: feature alignment.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of prediction refinement.</figDesc><table><row><cell>Model</cell><cell>AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell cols="6">baseline 47.8% 66.3% 52.1% 30.1% 52.5% 62.0%</cell></row><row><cell>+ iPR</cell><cell cols="5">47.8% 66.5% 52.1% 30.3% 53.3% 61.5%</cell></row></table><note>* baseline is YOLOv4-CSP-fast, tested on 640?640 input resolution.* PR: prediction refinement.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of multi-task joint learning. 0% 66.8% 52.3% 30.0% 53.0% 62.7% JDC 47.7% 66.8% 51.9% 30.8% 52.4% 61.6% + iJDC 48.1% 67.1% 52.2% 31.1% 52.7% 62.3% JDE 48.1% 66.7% 52.4% 30.7% 53.2% 61.9% + iJDE 48.3% 66.8% 52.6% 30.7% 53.4% 63.0%</figDesc><table><row><cell>Model</cell><cell>AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell cols="2">baseline 48.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* baseline is YOLOv4-CSP [15], tested on 640?640 input resolution.* JD{C, E}: joint detection &amp; {clssification, embedding}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of different operators.</figDesc><table><row><cell>Model</cell><cell>AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell cols="6">baseline 47.8% 66.3% 52.1% 30.1% 52.5% 62.0%</cell></row><row><cell>+ iFA</cell><cell cols="5">47.9% 66.6% 52.3% 30.6% 53.1% 62.6%</cell></row><row><cell>? iFA</cell><cell cols="5">47.4% 65.8% 51.6% 29.6% 52.2% 62.1%</cell></row><row><cell>? iFA</cell><cell cols="5">47.8% 66.5% 52.2% 30.3% 52.9% 62.3%</cell></row><row><cell>+ iPR</cell><cell cols="5">47.8% 66.5% 52.1% 30.3% 53.3% 61.5%</cell></row><row><cell>? iPR</cell><cell cols="5">48.0% 66.7% 52.3% 29.8% 53.4% 61.8%</cell></row></table><note>* baseline is YOLOv4-CSP-fast, tested on 640?640 input resolution.* {+, ?, ?}: {addition, multiplication, concatenation}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of different operators.</figDesc><table><row><cell>Model</cell><cell>AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val</cell></row></table><note>L baseline 47.8% 66.3% 52.1% 30.1% 52.5% 62.0% ? iFA* 47.9% 66.6% 52.0% 30.5% 52.6% 62.3% ? iPR* 48.1% 66.5% 52.1% 30.1% 53.3% 61.9%* baseline is YOLOv4-CSP-fast, tested on 640?640 input resolution.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of different modeling approaches.</figDesc><table><row><cell>Model</cell><cell>AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell cols="6">baseline 47.8% 66.3% 52.1% 30.1% 52.5% 62.0%</cell></row><row><cell>+ iFA</cell><cell cols="5">47.9% 66.6% 52.3% 30.6% 53.1% 62.6%</cell></row><row><cell>+ wiFA</cell><cell cols="5">47.8% 66.4% 52.0% 30.8% 52.8% 61.9%</cell></row><row><cell>+ icFA</cell><cell cols="5">48.0% 66.7% 52.6% 30.3% 53.2% 62.5%</cell></row></table><note>* baseline is YOLOv4-CSP-fast, tested on 640?640 input resolution.* {i, wi, ic}: {vector, neural network, matrix factorization}, see 4.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Information of model with/without implicit knowledge. baseline 1 is YOLOv4-CSP-fast, tested on 640?640 input resolution. baseline 2 is YOLOv4-P6-light, tested on 1280?1280 input resolution. implicit {1, 2} are baseline {1, 2} with + iFA, ? iPR.</figDesc><table><row><cell>Model</cell><cell>AP val</cell><cell># parameters</cell><cell>MFLOPs</cell></row><row><cell cols="2">baseline 1 47.8%</cell><cell>52908989</cell><cell>117517.2952</cell></row><row><cell cols="4">implicit 1 48.0% 52911546 (+0.005%) 117519.4372 (+0.002%)</cell></row><row><cell cols="2">baseline 2 51.4%</cell><cell>37262204</cell><cell>326256.1624</cell></row><row><cell cols="4">implicit 2 51.9% 37265016 (+0.008%) 326264.7304 (+0.003%)</cell></row></table><note>***</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Benefit from implicit knowledge. 9% 69.8% 56.8% 36.0% 56.3% 65.0% fine-tuned implicit 52.5% 70.5% 57.6% 37.1% 57.2% 65.4% baseline is YOLOv4-P6-light, tested on 1280?1280 input resolution.</figDesc><table><row><cell>Model</cell><cell>AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell>baseline</cell><cell cols="5">51.4% 69.5% 56.4% 35.2% 55.8% 64.6%</cell></row><row><cell>implicit</cell><cell>51.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>** implicit is baseline with + iFA, ? iPR.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparion of state-of-the-art.</figDesc><table><row><cell>Method</cell><cell cols="2">pre. seg. add. AP test AP test 50 AP test 75 FPS V 100</cell></row><row><cell>YOLOR (ours)</cell><cell>55.4% 73.3% 60.6%</cell><cell>30</cell></row><row><cell>ScaledYOLOv4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A1 :</head><label>A1</label><figDesc>Lightweight models with implicit knowledge. 8% 57.8% 42.1% 21.3% 44.0% 52.4% Y4-SSS 640 712 17.9G 9291738 39.3% 58.1% 42.5% 21.7% 44.4% 52.8% +0.018% +0.5% +0.3% +0.4% +0.4% +0.4% +0.4% 8% 55.1% 39.7% 18.9% 41.4% 50.8% +0.018% +0.5% +0.3% +0.4% +1.2% +0.7% +1.1% 3% 56.5% 40.5% 21.1% 42.7% 47.7% +0.023% +0.6% +1.1% +0.7% -1.1% +0.8% +1.5% Y4: YOLOv4-CSP, U5R5: 10.5281/zenodo.4679653; FPS: model inference only. ? YOLOR-Y4-SSSS get 0.1% better AP than U5R5-S with 39% faster inference speed.</figDesc><table><row><cell>Model</cell><cell cols="2">YOLOR Size FPS T itanRT X batch32</cell><cell cols="3">FLOPs # parameters AP val AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell cols="8">Y4-SSS 38.Y4-SSSS 640 720 17.9G 9290077 640 806 16.1G 9205693 36.3% 54.8% 39.3% 17.7% 40.7% 49.7%</cell></row><row><cell cols="8">Y4-SSSS 36.U5R5-S 640 791 16.1G 9207354 640 569 17.0G 7266973 36.7% 55.4% 39.8% 22.2% 41.9% 46.2%</cell></row><row><cell>U5R5-S</cell><cell>640</cell><cell>563</cell><cell>17.0G</cell><cell>7268634</cell><cell>37.</cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A2 :</head><label>A2</label><figDesc>Large models with implicit knowledge.</figDesc><table><row><cell>Model</cell><cell>Size</cell><cell>FPS T itanRT X batch32</cell><cell>FLOPs # parameters AP val</cell><cell>AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell>YOLOR-P6</cell><cell>1280</cell><cell>72</cell><cell>326.2G</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A3 :</head><label>A3</label><figDesc>More comparison. ST: SwinTransformer, C2: CenterNet2, Y4: YOLOv4-CSP, P2: PP-YOLOv2.</figDesc><table><row><cell>Model</cell><cell>Size</cell><cell>FPS V /R</cell><cell cols="3">FLOPs # parameters AP test</cell><cell>AP test 50</cell><cell>AP test 75</cell><cell>AP test S</cell><cell>AP test M</cell><cell>AP test L</cell></row><row><cell>YOLOR-P6</cell><cell>1280</cell><cell>49 / 48</cell><cell>326G</cell><cell>37M</cell><cell>52.6%</cell><cell>70.6%</cell><cell>57.6%</cell><cell>34.7%</cell><cell>56.6%</cell><cell>64.2%</cell></row><row><cell>YOLOR-P6D</cell><cell>1280</cell><cell>49 / 48</cell><cell>326G</cell><cell>37M</cell><cell>53.0%</cell><cell>71.0%</cell><cell>58.0%</cell><cell>35.7%</cell><cell>57.0%</cell><cell>64.6%</cell></row><row><cell>YOLOR-W6</cell><cell>1280</cell><cell>47 / 44</cell><cell>454G</cell><cell>80M</cell><cell>54.1%</cell><cell>72.0%</cell><cell>59.2%</cell><cell>36.3%</cell><cell>57.9%</cell><cell>66.1%</cell></row><row><cell>YOLOR-E6</cell><cell>1280</cell><cell>37 / 27</cell><cell>684G</cell><cell>116M</cell><cell>54.8%</cell><cell>72.7%</cell><cell>60.0%</cell><cell>36.9%</cell><cell>58.7%</cell><cell>66.9%</cell></row><row><cell>YOLOR-D6</cell><cell>1280</cell><cell>30 / 22</cell><cell>937G</cell><cell>152M</cell><cell>55.4%</cell><cell>73.3%</cell><cell>60.6%</cell><cell>38.0%</cell><cell>59.2%</cell><cell>67.1%</cell></row><row><cell>ST-L (HTC++)</cell><cell>-</cell><cell>-</cell><cell>1470G</cell><cell>284M</cell><cell>57.7%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>C2 (R2)</cell><cell>1560</cell><cell>-/ 5</cell><cell>-</cell><cell>-</cell><cell>56.4%</cell><cell>74.0%</cell><cell>61.6%</cell><cell>38.7%</cell><cell>59.7%</cell><cell>68.6%</cell></row><row><cell>Y4-P5</cell><cell>896</cell><cell>41 / -</cell><cell>328G</cell><cell>71M</cell><cell>51.8%</cell><cell>70.3%</cell><cell>56.6%</cell><cell>33.4%</cell><cell>55.7%</cell><cell>63.4%</cell></row><row><cell>Y4-P6</cell><cell>1280</cell><cell>30 / -</cell><cell>718G</cell><cell>128M</cell><cell>54.5%</cell><cell>72.6%</cell><cell>59.8%</cell><cell>36.8%</cell><cell>58.3%</cell><cell>65.9%</cell></row><row><cell>Y4-P7</cell><cell>1536</cell><cell>16 / -</cell><cell>1639G</cell><cell>287M</cell><cell>55.5%</cell><cell>73.4%</cell><cell>60.8%</cell><cell>38.4%</cell><cell>59.4%</cell><cell>67.7%</cell></row><row><cell>P2</cell><cell>640</cell><cell>50* / -</cell><cell>-</cell><cell>-</cell><cell>50.3%</cell><cell>69.0%</cell><cell>55.3%</cell><cell>31.76%</cell><cell>53.9%</cell><cell>62.4%</cell></row><row><cell>Model</cell><cell>Size</cell><cell>FPS V /R</cell><cell cols="2">FLOPs # parameters</cell><cell>AP val</cell><cell>AP val 50</cell><cell>AP val 75</cell><cell>AP val S</cell><cell>AP val M</cell><cell>AP val L</cell></row><row><cell>ST-T (MRCNN)</cell><cell>-</cell><cell>15.3 / -</cell><cell>745G</cell><cell>86M</cell><cell>50.5%</cell><cell>69.3%</cell><cell>54.9%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-S (MRCNN)</cell><cell>-</cell><cell>12.0 / -</cell><cell>838G</cell><cell>107M</cell><cell>51.8%</cell><cell>70.4%</cell><cell>56.3%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-B (MRCNN)</cell><cell>-</cell><cell>11.6 / -</cell><cell>982G</cell><cell>145M</cell><cell>51.9%</cell><cell>70.9%</cell><cell>56.5%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-B (HTC++)</cell><cell>-</cell><cell>-</cell><cell>1043G</cell><cell>160M</cell><cell>56.4%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-L (HTC++)</cell><cell>-</cell><cell>-</cell><cell>1470G</cell><cell>284M</cell><cell>57.1%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>C2 (DLA)</cell><cell>640</cell><cell>-/ 38</cell><cell>-</cell><cell>-</cell><cell>49.2%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note>** HTC: Hybrid Task Cascade, R2: Res2Net, MRCNN: Mask R-CNN, DLA: Deep Layer Aggregation.* FPS: end-to-end batch one inference speed of V100/TitanRTX, FPS value with * indicates speed of model inference only.? YOLOR-P6D means joint train YOLOR-P6 model with YOLOR-D6 model.? YOLOR-D6 get 0.9% better AP than Y4-P6 with almost same inference speed.? YOLOR-D6 get 88% faster inference speed than Y4-P7 with almost same AP.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>The authors wish to thank National Center for Highperformance Computing (NCHC) for providing computational and storage resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. K-Svd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GCNet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshop (ICCV Workshop)</title>
		<meeting>the IEEE International Conference on Computer Vision Workshop (ICCV Workshop)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07177</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scaled-YOLOv4: Scaling cross stage partial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust face verification via bayesian sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seksan</forename><surname>Mathulaprangsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hao</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Jia</forename><surname>Shiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-San</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ching</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognition and retrieval of sound events using sparse coding convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andri</forename><surname>Santoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seksan</forename><surname>Mathulaprangsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Chin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Hsien</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ching</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="589" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sound events recognition and retrieval using multi-convolutional-channel sparse coding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Chiang</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ching</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andri</forename><surname>Santoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seksan</forename><surname>Mathulaprangsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Chin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Hsien</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1875" to="1887" />
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Implicit feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13563</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shankar Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="210" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Probabilistic two-stage detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Y4</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.5281/zenodo.4679653</idno>
		<title level="m">FPS: model inference only</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">? Y4-P6 get better AP than U5R5-X6 with 24% less computation and 11% fewer #parameters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Yolor-</surname></persName>
		</author>
		<title level="m">E6 get better AP than Y4-P6 with 5% less computation, 10% fewer #parameters, and 15% faster inference speed</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
