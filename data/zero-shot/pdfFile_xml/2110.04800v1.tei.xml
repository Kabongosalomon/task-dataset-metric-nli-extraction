<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised 3D Face Reconstruction via Conditional Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">MPI for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised 3D Face Reconstruction via Conditional Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a conditional estimation (CEST) framework to learn 3D facial parameters from 2D single-view images by self-supervised training from videos. CEST is based on the process of analysis by synthesis, where the 3D facial parameters (shape, reflectance, viewpoint, and illumination) are estimated from the face image, and then recombined to reconstruct the 2D face image. In order to learn semantically meaningful 3D facial parameters without explicit access to their labels, CEST couples the estimation of different 3D facial parameters by taking their statistical dependency into account. Specifically, the estimation of any 3D facial parameter is not only conditioned on the given image, but also on the facial parameters that have already been derived. Moreover, the reflectance symmetry and consistency among the video frames are adopted to improve the disentanglement of facial parameters. Together with a novel strategy for incorporating the reflectance symmetry and consistency, CEST can be efficiently trained with in-the-wild video clips. Both qualitative and quantitative experiments demonstrate the effectiveness of CEST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reconstructing 3D faces from single-view 2D images has been a longstanding problem in computer vision. The common approach represents the 3D face as a combination of its shape, as represented by the 3D coordinates of a number of points on its surface called vertices, and its texture, as represented by the reflectances of red, green and blue at these vertices <ref type="bibr" target="#b3">[4]</ref>. The problem then becomes learning a regression model between the 2D images, and vertices and their reflectances.</p><p>The regression itself may be learned using training data where both, the 2D images and the corresponding 3D parameters are available. However, these data are scarce, and even the ones that are available generally only have shape information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46]</ref>; the ones that do have other parameters are usually captured in a controlled environment <ref type="bibr" target="#b21">[22]</ref> or are synthetic <ref type="bibr" target="#b32">[33]</ref>, which is not representative of real-world images. Consequently, there is great interest in self-supervised learning methods, which learn the regression model from natural in-the-wild 2D images or videos,   without explicit access to 3D training data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>The problem is complicated by the fact that the actual image formation depends not only on the shape and texture of the face, but also the illumination (the intensity and direction of the incident light), and other factors such as the viewpoint (incorporating the orientation of the face and the position of the camera), etc. Thus, the learned regression model must also account for these factors. To this end, the general approach is one where shape, reflectance, illumination and viewpoint parameters are all extracted from the 2D image. The regression model that extracts these facial parameters are learned through self-supervision: the extracted facial parameters are recombined to render the original 2D image, and the model parameters are learned to minimize the reconstruction error.</p><p>The solution, however, remains ambiguous because a 2D image may be obtained from different combinations of shape, texture, illumination and viewpoint. To ensure that the self-supervision provides meaningful disentanglement, the manner in which the facial parameters are recombined to reconstruct the 2D image are based on the actual physics of image formation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33]</ref>. To further reduce potential ambiguities, regularizations are necessary. Reflectance symmetry has been proposed as a regularizer <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>, wherein the reflectance of a face image and its mirror reflec-tion are assumed to be identical. Smoothness has also been employed to regularize the shape and reflectance <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38]</ref>. Additional regularization may be obtained by considering correspondences between multiple images of the same face <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>, particularly when they are obtained under near identical conditions such as the sequence of images from a video. The approach in <ref type="bibr" target="#b36">[37]</ref> has considered reflectance consistency, where reflectances of all image frames in a video clip are assumed to be similar.</p><p>In all of these prior works, the target parameters, namely the shape, reflectance, illumination and viewpoint parameters are all individually estimated, without considering their direct influences on one another, although they are jointly optimized. In effect, at inference time they assume that the estimate of, e.g. the reflectance, is conditionally independent of the estimated shape or viewpoint, given the original 2D image. The coupling among the four is only considered during (self-supervised) training, where they must all combine to faithfully recreate the input 2D image <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37]</ref>. This is illustrated in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>.</p><p>In reality, 2D images are reduced-dimensional projections, and thus imperfect representations of the full threedimensional structure of the face, and the aspects of reflectance and illumination imprinted in them are not independent of the underlying shape of the object or the viewpoint they were captured from. Therefore, the captured 2D image represents a joint interaction among viewpoint, shape, reflectance and illumination. Consequently, the statistical estimates of any of these four factors may not, in fact, be truly conditionally independent of one another given only the 2D image (although, given the entire 3D model they might have been). Thus, modelling all of these variables as being conditionally independent effectively represents a lost opportunity since, by predicting them individually, the constraints they impose on one another are ignored. Optimization-based approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref> do attempt to capture the dependence by iteratively estimating shape and reflectance from one another. However, these methods require correspondence information of the image sequence in a video and suffer from costly inference.</p><p>In this paper, we propose a novel learning-based framework based on conditional estimation (CEST). CEST explicitly considers the statistical dependency of the various 3D facial parameters (shape, viewpoint, reflectance and illumination) upon one another, when derived from single 2D image. The specific form of the dependencies adopted in this paper is shown in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. We note that the CEST framework is very general and allows us to consider any other dependency structures. Our paper serves as one of the many potential choices that work well in practice. To this end, we present a specific, and intuitive, solution in CEST, where the viewpoint, facial shape, facial reflectance, and illumination are predicted sequentially and conditionally. In this context, the prediction of facial shape is conditioned on the input image and the derived viewpoint; the prediction of facial reflectance is conditioned on the input image, derived viewpoint and facial shape; and so forth.</p><p>As before, learning remains self-supervised, through comparison of re-rendered 2D images obtained with the estimated 3D face parameters to the original images. As additional regularizers, we also employ reflectance symmetry constraints <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>, and reflectance consistency constraints (across frames in a short video clip) <ref type="bibr" target="#b36">[37]</ref>. These are included in the form of cross-frame reconstruction error terms, the number of which increases quadratically with the number of video frames considered together for selfsupervision. To address the dramatically increased number of reconstruction terms, we propose a stochastic optimization strategy to improve training efficiency.</p><p>We present ablation studies and comparisons to state-ofthe-art methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37]</ref> to evaluate CEST. We show that CEST produces better reflectance and structured illumination, leading to more realistic rendered faces with fine facial details, compared to all other tested methods. It also achieves better shape estimation accuracy on AFLW2000-3D <ref type="bibr" target="#b48">[49]</ref> and MICC <ref type="bibr" target="#b0">[1]</ref> datasets than current state-of-the-art self-supervised and fully supervised approaches. Overall, our contributions can be summarized as follows: ? We propose CEST, a conditional estimation framework for 3D face reconstruction that explicitly considers the statistical dependencies among 3D face parameters. ? We propose a specific design for the decomposition of conditional estimation, where the viewpoint, shape, reflectance, and the illumination are derived sequentially. ? We propose a stochastic optimization strategy to efficiently incorporate reflectance symmetry and consistency constraints into CEST. As the number of video frames increase, the computational complexity of CEST is increased linearly, rather than quadratically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular 3D face reconstruction by self-supervised learning. Many research studies published recently aim to learn 3D facial parameters from a single image in a selfsupervised manner. In <ref type="bibr" target="#b28">[29]</ref>, the authors propose a coarse-tofine framework to improve the details in reconstructed 3D faces. Ayush et al. <ref type="bibr" target="#b38">[39]</ref> present a model-based deep convolutional face autoencoder (MoFA) to fit a 3DMM to shape, reflectance, and illuminance. InverseFaceNet <ref type="bibr" target="#b19">[20]</ref> trains a direct regression model on a synthetic training corpus that is generated by self-supervised bootstrapping. SfSNet <ref type="bibr" target="#b32">[33]</ref> combines labeled synthetic and unlabeled real-world images in learning, and produces accurate depth map, and reflectance and shade disentanglement. To better characterize facial details, 3DMM is generalized to a nonlinear model in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. <ref type="bibr" target="#b47">[48]</ref> uses mesh convolutions for 3D faces, lead- ing to a light-weight model with competitive performance. <ref type="bibr" target="#b33">[34]</ref> incorporates the multi-view consistency from geometry, pixel, and depth as constraints. However, these approaches generally do not consider correspondences across frames in a video. FML <ref type="bibr" target="#b36">[37]</ref> is the first self-supervised framework that incorporates video clues in training. The shape and reflectance for each video frame are approximated by averaging the shapes and reflectances in a video clip. However, models trained on the averaged representations may not work well for a single image if the number of multi-frame images is large, due to the large gap between averaged and isolated images. On the contrary, CEST uses representations from single images. More importantly, it uses conditional estimation for predicting the facial parameters, and does not assume conditional independence between them, an often unrealistic assumption employed in the previously mentioned approaches.</p><p>Optimization-based 3D face reconstruction. <ref type="bibr" target="#b17">[18]</ref> proposes to fit a template model to photo-collections by updating the viewpoint, geometry, lighting, and texture iteratively. <ref type="bibr" target="#b34">[35]</ref> fits a face model to detected 3D landmarks, and refines the texture and geometry details. <ref type="bibr" target="#b10">[11]</ref> learns facial subspaces for identity and expression variations with a parametric shape prior. <ref type="bibr" target="#b9">[10]</ref> considers 3D face reconstruction as a global variational energy minimization problem, and estimates dense low-rank 3D shapes for video frames.</p><p>While these approaches can be considered conditional estimation, they focus on deriving 3D facial parameters from video, and are not relevant to the problem of deriving them from single-frame images, the problem addressed in our work. For CEST, video clips are viewed as consistent collections of images used to better learn the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The CEST Framework</head><p>In this work, we adopt a common practice from 3D Morphable Model (3DMM) <ref type="bibr" target="#b3">[4]</ref>, which represents a 3D face as a combination of shape and reflectance. The shape comprises a collection of vertices S = [S(1); S(2); ...; S(K)] ? R K?3 , where K is the number of vertices and S(i) = [S(i, 1), S(i, 2), S(i, 3)] denotes the xyz coordinates in the Cartesian coordinate system. The typology for S is consistent for different faces. The reflectance comprises a collec-</p><formula xml:id="formula_0">tion of pixel values R = [R(1); R(2); ...; R(K)] ? R K?3 . Each row R(i) = [R(i, 1), R(i, 2), R(i, 3)</formula><p>] comprises the pixel values (i.e., RGB) at position S(i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>The problem of 3D face reconstruction from a 2D image is that of obtaining estimates of the shape S, reflectance R, viewpoint v and illumination , given an input image I. Statistically, we aim to estimate the most likely values for these variables, given the input image:</p><formula xml:id="formula_1">S,R,v,? = arg max S,R,v, P (S, R, v, |I)<label>(1)</label></formula><p>The challenges of this estimation are twofold: first P (S, R, v, |I) must be modelled, and second, arg max S,R,v, P (S, R, v, |I) must be computed. Modelling P (S, R, v, |I) directly is a challenging problem, and the problem must be factored down. Prior approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> have decomposed this problem by assuming that shape, reflectance, viewpoint and illumination are all conditionally independent, given the image, i.e. P (S, R, v, |I) = P (S|I)P (R|I)P (v|I)P ( |I). This leads to simplified estimates where each of the variables can be independently estimated, i.e.? = arg max S P (S|I), R = arg max R P (R|I), etc. As we have discussed earlier, the conditional independence assumption is questionable, since the conditioning variable, I, is a lower-dimensional projection of the 3D face that entangles the four variables.</p><p>In CEST we explicitly model the conditional dependence, as shown in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. Specifically we decompose the joint probability as</p><formula xml:id="formula_2">P (S, R, v, |I) = P (v|I)P (S|I, v)P (R|I, v, S)P ( |I, v, S, R)<label>(2)</label></formula><p>Coupling the variables in this manner results in a complication: even factored as above, maximizing the joint probability with respect to S, R, v, and must be jointly performed, since the variables are coupled. We approximate it instead with the following sequential estimate, based on the sequential decomposition above:</p><formula xml:id="formula_3">v = arg max v P (v|I)? = arg max S P (S|I,v) R = arg max R P (R|I,v,?)? = arg max P ( |I,v,?,R)<label>(3)</label></formula><p>The second challenge is that of actually computing the arg max operations in Equation 3. Rather than attempting to model the probability distributions explicitly and maximizing them, we will, instead, model the estimators in </p><formula xml:id="formula_4">R = fr(I,v,?; ?r)? = f (I,v,?,R; ? )<label>(4)</label></formula><p>The problem of learning to estimate the 3D facial parameters thus effectively reduces to that of estimating the parameters ? v , ? s , ? r and ? l .</p><p>Using the common approach, we formulate the learning process for these parameters through an autoencoder. f v (), f s (), f r () and f () are, together, viewed as the learnable encoder in the autoencoder, which estimate v, S, R and respectively. The decoder is a deterministic differentiable renderer R() with no learnable parameters, which reconstructs the original input I from the values derived by the encoder as? = R(S, R, v, ). The parameters of the encoder are learned to minimize the error between? and I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Facial Parameters Inference</head><p>Viewpoint. We first predict the viewpoint parameters from the given image, using a function f v (I;</p><formula xml:id="formula_5">? v ) : I ? v ? R 7 .</formula><p>Here v is used to parameterize the weak perspective transformation <ref type="bibr" target="#b35">[36]</ref>, including 3D spatial rotation (SO(3)), the translation (xyz coordinates), and the scaling factor.</p><p>Shape. The prediction of shape is conditioned on the given image I and the predicted v. Since the same face captured with different viewpoints should be correspond to the same facial shape, it is beneficial to exclude as much viewpoint information from the image I as possible before the shape prediction. With the predicted v, we can align the image to its canonical view in 2D plane, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> and Appendix A.1. The cropped image is denoted by</p><formula xml:id="formula_6">I ? v. A function f s (I ? v; ? s ) : I ? v ? ? ? R 228?1</formula><p>with learnable parameter ? s is constructed to predict the shape coefficients ?. The shape coefficients ? are defined by a statistical model of 3D facial shape:</p><formula xml:id="formula_7">S =S + U ?,<label>(5)</label></formula><p>where S ? R 3K?1 is the vectorized S, andS ? R 3K?1 is the mean shape. U ? R 3K?228 is the PCA basis from Basel Face Model (BFM) <ref type="bibr" target="#b26">[27]</ref> and 3DFFA <ref type="bibr" target="#b48">[49]</ref> for identity <ref type="figure">Figure 3</ref>: Illustration of generating the UV map of the illuminated texture. and expression variation, respectively.S and U are fixed during the training and testing of CEST. With the predicted ?, the shape S can be obtained using equation 5.</p><formula xml:id="formula_8">UV unwrapping wrapping (b) Conditional Estimation Model: P(R | I,v,S ) (a) Independent Estimation Model: P(R | I ) Input image Input image Qi Qj Qk Q'i Q'j Q'k UV space (x', y') (x, y) image space R R A T I, v, S fr</formula><p>Reflectance. Previous approaches usually predict the reflectance coefficients in a predefined model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>, unwrapped UV map of reflectance <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13]</ref>, or graph representation of the reflectance <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48]</ref> from the image directly. In CEST, we adopt the UV map representation for reflectance. However, the prediction of the reflectance is conditioned not only on the given image I, but also on the predicted viewpoint v and shape S.</p><p>The process is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. We first compute the image-coordinate facial shape Q ? R K?2 by projecting the world-coordinate facial shape S with viewpoint v using weak perspective transformation. The details of the transformation are given in Appendix A.2, since it is a standard formulation, and not a contribution of this paper. Next, we construct an intermediate representation, i.e. UV map of the illuminated texture T <ref type="bibr" target="#b35">[36]</ref>, which is obtained by unwrapping the given image I based on the predicted face shape Q. Subsequently, the UV map of reflectance A is predicted from the illuminated texture T by a reflectance function f r (T ; ? r ). The reflectance R can be recovered from A by UV wrapping.</p><p>The basic idea for computing the T is illustrated in <ref type="figure">Fig.  3</ref>. For each T (x , y ) (the pixel values at position (x , y )), we trace its corresponding position (x, y) in I. The illuminated texture can be simply obtained by T (x , y ) = I(x, y), where bilinear interpolation is used for inferring the pixel values of I at position (x, y) if x or y is not an integer. The computation of (x, y) is as follows. First, the canonical face shapeS is mapped to the UV space by cylinder unwrapping. We determine the triangle enclosing the point (x , y ) on a grid based on the vertex connectivity, which is provided by the 3DMM. The triangle is represented by its three vertices Q (i), Q (j), and Q (k). Since the topology of the facial shape in image space and UV space are the same, the vertices in these two space have one-to-one correspondence. We could easily get the corresponding vertices Q(i), Q(j), and Q(k). Now the position (x, y) can be computed by x = ? 1 Q(i, 1) + ? 2 Q(j, 1) + ? 3 Q(k, 1) and y = ? 1 Q(i, 2) + ? 2 Q(j, 2) + ? 3 Q(k, 2), where the ?s are the coefficients computed by Q (i), Q (j), Q (k), and (x , y ) in barycentric coordinate system <ref type="bibr" target="#b5">[6]</ref>. The computation details are included in Appendix A.3. For the invisible triangles (caused by self-occlusion), we simply ignore them.</p><p>With the illuminated texture T , the UV map of the reflectance A can be produced by a function f r (T ; ? r ), where ? r is the learnable parameters. It is worth noting that the input (T ) and output (A) of f r are spatially aligned in UV space, so the learning process can be greatly facilitated. Subsequently, the reflectance R is obtained by a wrapping function R = ?(A) <ref type="bibr" target="#b35">[36]</ref>, which has no learnable parameters, as shown in A.4.</p><p>Illumination. Following the previous studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>, we assume the distant smooth illumination and purely Lambertian surface properties <ref type="bibr" target="#b1">[2]</ref>. Spherical Harmonics (SH) <ref type="bibr" target="#b27">[28]</ref> are employed to approximate the incident radiance at a surface. We use 3 SH bands, leading to 9 SH coefficients. The illumination function is defined as f (I, T , A; ? ) : (I, T , A) ? ? R 9?1 , which takes the given image, illuminated texture map and UV map of reflectance as input, and produces the illumination parameters.</p><p>So far, the 3D face model parameters R, S, v, and are predicted, and we are able to recombine them and render the image by the expert-designed rendering module, i.e.? = R(S, R, v, ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Objectives for Self-Supervised Learning</head><p>The functions f s , f r , f v , and f are modelled by convolutional neural networks (CNNs) with learnable parameters ? s , ? r , ? v , and ? , respectively. Since all the learning modules and expert-designed renderer are differentiable, the proposed framework is end-to-end trainable. The learning objective is to minimize the differences between the original image I and the rendered image?. Following the practices in previous work, the learning objective does not include the pixels in nonface region, e.g. hair, sunglasses, scarf, etc. We identify if a pixel belongs to face or nonface region by a face segmentation network f seg , which is trained on CelebAMask-HQ dataset <ref type="bibr" target="#b22">[23]</ref> with the segmentation labels provided in the dataset. Once trained, f seg is fixed during the training and testing of CEST. We denote the effective face region as a mask M , so the pixel at position (x, y) is included in reconstruction if M (i, j) = 1, and excluded if M (i, j) = 0. The photometric loss can be written as</p><formula xml:id="formula_9">L ph = E(I, S, R, v, , M ) = M ? I ? M ?? 1 = M ? I ? M ? R(S, R, v, ) 1 ,<label>(6)</label></formula><p>where ? 1 measure the 1 distance and ? denotes the element-wise multiplication. However, if we simply optimize L ph , CEST will learn a degraded solution, where the reflectance A simply copies the pixel values from T , and yields an isotropic radiator, radiating the same intensity of radiation in all directions. In this case, CEST does not learn semantically disentangled facial parameters, but leads to a perfect reconstruction for?.</p><p>To avoid this, we adopt the symmetry and consistency constraints for reflectance. The facial reflectance is assumed to be horizontally symmetric and consistent in a video clip. Suppose I i and I j are two face images from the same video clip. One of the possible solutions is to add the regularization terms R i ? R i , R j ? R j , and R i ?R j to the learning objective, where R i and R j are the horizontally flipped versions of R i and R j . However, it is difficult to tune loss weights to balance the reconstruction and regularization terms. Instead, we adopt an alternative solution by constructing additional reconstruction terms as constraints <ref type="bibr" target="#b44">[45]</ref>. The learning objective for reconstructing I i and I j can be written as</p><formula xml:id="formula_10">L ph = E(I i , S i , R i , v i , i , M i ) + E(I j , S j , R j , v j , j , M j ) + E(I i , S i , R j , v i , i , M i ) + E(I j , S j , R i , v j , j , M j ) + E(I i , S i , R i , v i , i , M i ) + E(I j , S j , R j , v j , j , M j ) + E(I i , S i , R j , v i , i , M i ) + E(I j , S j , R i , v j , j , M j )<label>(7)</label></formula><p>Stochastic optimization. As can be seen, the number of reconstruction terms is increased dramatically. From n frames of the same video, 2n 2 reconstruction terms can be constructed. This is not scalable. To address this problem, we propose to optimize the learning objective in a stochastic way. For each training iteration, only a subset of the reconstruction terms are optimized. Specifically, a set of video frames {I 1 , I 2 , ..., I N } are randomly sampled from different videos. The frames are grouped by videos, labeled as ? = {? 1 , ? 2 , ..., ? N }. For any I i , instead of enumerating all the possible reflectances and obtaining numerous reconstruction terms, we randomly select some other frame from the same video, denoted as I j (under the condition of ? j = ? i ), and use R j and R i to construct two reconstruction terms for I i . With this strategy, the number of reconstruction terms is reduced from O(n 2 ) to O(n). Formally, the learning objective can be written as</p><formula xml:id="formula_11">L ph = 1 N N i=1,? j =? i E(I i , S i , R j , v i , i , M i ) + E(I i , S i , R i , v i , i , M i ) .<label>(8)</label></formula><p>To stabilize the training of CEST, we use 2D key points via</p><formula xml:id="formula_12">L kp = 1 N N kp N i=1 N kp j=1 Q i (k j ) ? q i (j) 1 where q(j)</formula><p>is the set of detected 2D key points on image, and k j is the index of the vertex associating to the 2D key point. We also regularize the energies of shape coefficients with</p><formula xml:id="formula_13">L rg = 1 N N i=1 ? i 2</formula><p>2 . An off-the-shelf landmark detector <ref type="bibr" target="#b6">[7]</ref> is used to produce N kp = 68 key points for a detected face. The total loss consists of the following terms:</p><formula xml:id="formula_14">L = L ph + ? 1 L kp + ? 2 Lrg<label>(9)</label></formula><p>where ? 1 and ? 2 are hyperparameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We qualitatively and quantitatively evaluate CEST with ablation experiments and comparisons to state-of-the-art methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9]</ref>. In ablation experiments, we compare CEST to the independent version of CEST (IEST) where facial parameters are estimated in a uncoupled way, and other variants trained with different constraints. Qualitative results include the predicted shape, reflectance, illumination, reconstructed face, etc. We also show the relighted faces, which are obtained by illuminating reflectances with different illuminations. Quantitative results evaluate the qualities of the predicted shape and rendered face. The metrics we used are normalized mean error (NME) <ref type="bibr" target="#b15">[16]</ref> and photometric error for shape and rendered face, respectively. NME is defined as the average per-vertex Euclidean distance between the predicted and targeted point clouds normalized by the outer 3D interocular distance. Photometric error is the mean absolute errors between pixel values in the original images and reconstruction images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>For fair comparison, we train two separate CEST models with VoxCeleb1 <ref type="bibr" target="#b25">[26]</ref> and 300W-LP <ref type="bibr" target="#b48">[49]</ref> respectively. Vox-Celeb1 is a video dataset collected from the Internet. The videos of speakers are captured in different in-the-wild scenarios. A subset of 4,727 videos of 267 persons are used in the training, leading to 6,279,609 video frames. The faces in video frames are cropped to the size of 256 ? 256 based on the detected facial key points using <ref type="bibr" target="#b6">[7]</ref>. 300W-LP is a synthetic image dataset, containing 122,450 images provided with dense landmarks. Since we focus on self-supervised learning, we only use a sparse set of 68 sparse landmarks as a regularization in training.</p><p>Training. The network architectures are given in Appendix B.1. For the training with VoxCeleb1, the minibatch consists of 128 video frames from 32 clips. For each video clip, we randomly selected 4 video frames. The training is completed at 50K iterations. For the training with 300W-LP, the minibatch consisted 128 randomly selected images, and the total iteration is 20K. For both models, we used Adam <ref type="bibr" target="#b20">[21]</ref> optimizer with learning rate of 0.001. ? 1 and ? 2 are 1 and 0.1 unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>The results of ablation study are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. We first present the original and reconstructed image (overlay) for comparison, following by the reflectance, illuminated texture, facial shape (geometry), and illumination in canonical view. More ablations can be found in Appendix B.2.</p><p>CEST and IEST. IEST is trained with the same settings as CEST, except the facial parameters are estimated independently from image during training and testing. The results are shown in <ref type="figure" target="#fig_4">Fig. 4 (a)</ref> and (b), respectively. We can see that CEST produces realistic overlay, disentangled reflectance and illumination, and geometry with personal characteristics and expressions. Compared to CEST, IEST achieves reasonable results, but the reflectances are not as detailed as those from CEST, resulting in inferior overlays and illuminated textures. It validates our hypothesis that the coupled estimation can better formulate the problem and facilitate the learning.</p><p>Reflectance symmetry and consistency constraints.  <ref type="bibr" target="#b24">[25]</ref> constraint, only consistency constraints, and without the two constraints, and show their results in <ref type="figure" target="#fig_4">Fig. 4 (c)</ref>, (d), and (e), respectively. Compared (a) and (c) we observe that the reflectance symmetry constraint leads to better reflectance and illumination separation. This is because the horizontally flipped video frames can provide more illumination variations to the training set, enabling CEST to learn to model different illuminations properly. On the other hand, if the reflectance consistency in video clip is not used, the decomposition of reflectance and illumination is not performed well. Some illumination remains around the eyes region in the reflectance (see the right hand side of the <ref type="figure" target="#fig_4">Fig. 4  (d)</ref>). Lastly, if we do not use any constraints on reflectance, CEST learns the degraded solution ( <ref type="figure" target="#fig_4">Fig. 4(e)</ref>), where the reflectance simply copies the pixel values from the image, and illumination is an isotropic radiator, radiating the same intensity of radiation in all directions. Moreover, we note that the degraded solution also affects the learned facial shape, which has less personal characteristics in <ref type="figure" target="#fig_4">Fig. 4</ref> (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We train multiple variants of CEST with only symmetry</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head><p>In this section, we compare CEST to most relevant stateof-art methods with qualitative results. More qualitative results are included in Appendix B.3.</p><p>Comparison to MoFA <ref type="bibr" target="#b38">[39]</ref>. MoFA is a fully modelbased framework. Its representation power is limited by the linear 3DMM model. In addition, all facial parameters from MoFA are independently predicted from the original image. On the contrary, we use a model-free method for reflectance, and the whole inference process is based on coupled estimation. We visualize the overlay, reflectance, geometry, illumination , as well as the errors between input and rendered image (overlays) in <ref type="figure" target="#fig_5">Fig. 5</ref>. As can be observed, results from MoFA suffer from out-of-subspace reflectance variations. Compared to MoFA, we obtain comparable shape, but significantly better reflectance, illumination, and rendered face by capturing more details.</p><p>Comparison to N3DMM <ref type="bibr" target="#b41">[42]</ref>. N3DMM generalizes 3DMM model to a nonlinear space and improves the quality of rendered faces. However, N3DMM also infers the reflectance from the input image only, and uses too many heuristic constraints, e.g. reflectance constancy, shape smoothness, supervised pretraining, etc. So their models can only capture low-frequency variations on reflectance. For example, in <ref type="figure" target="#fig_6">Fig. 6 (b)</ref> the lip stick is missing in the reflectance, and the skin colors in reflectances are almost identical for different persons. These limitations lead to higher reconstruction error. In contrast, our results produce realistic reconstruction, with more accurate reflectance and illumination, as well as lower reconstructed error <ref type="figure" target="#fig_6">(Fig. 6</ref>). Comparison to FML <ref type="bibr" target="#b36">[37]</ref>. FML properly incorporates video clues in training and can render realistic faces. However, its reconstructed reflectances are prone to an average skin color. In comparison, CEST yields more accurate skin color (see <ref type="figure" target="#fig_7">Fig. 7 (a)</ref>, (c), and (e)) by incorporating the learned shape and viewpoint in the estimation of reflectance. Qualitative results clearly show that our results have more reasonable disentanglement between reflectance and illumination. They also contribute to better visual quality of rendered faces. Notably, there are considerable differences in the eye and nose regions from the overlay in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>Relighting. Since CEST predicts the reflectances of faces, they can be easily re-lighted with different lighting conditions. <ref type="figure" target="#fig_8">Fig. 8</ref> shows the re-lit faces in canonical view. In particular, the last two target faces are under harsh lighting, which also examines the illumination removal ability of CEST. The re-lit results again validate that CEST is capable of estimating well-disentangled facial parameters and capturing the reflectance and illumination variations in realworld face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overlay</head><p>Reflectance Geometry Illumination  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative Results</head><p>We first perform quantitative evaluations on the AFLW2000-3D dataset, including 2,000 unconstrained face images with large pose variations. The ground truth of AFLW2000-3D is given by the results from 3DMM fitting, which may be somewhat noisy. The second evaluation is on MICC Florence 3D Face dataset, which consists of highresolution 3D scans from 53 subjects. We follow the practices in <ref type="bibr" target="#b15">[16]</ref> to render 2,550 testing images using the provided 3D scans. Each subject is rendered in 20 difference poses using a pitch of -15, 20 or 25 degrees and a yaw of -80, -40, 0, 40 or 80 degrees.</p><p>In order to compare with previous work, NME is computed based on a set of 19,618 vertices defined by <ref type="bibr" target="#b15">[16]</ref> in  their evaluation. The point correspondences are determined by the iterative closest point (ICP) algorithm <ref type="bibr" target="#b2">[3]</ref>. We compute the cumulative errors distribution (CED) curves and compare it to current prevailing methods such as 3DDFA <ref type="bibr" target="#b48">[49]</ref>, DeFA <ref type="bibr" target="#b23">[24]</ref>, and PRN <ref type="bibr" target="#b8">[9]</ref> on AFLW2000-3D. For MICC, we compare CEST to 3DDFA <ref type="bibr" target="#b48">[49]</ref>, VRN <ref type="bibr" target="#b15">[16]</ref>, and PRN <ref type="bibr" target="#b8">[9]</ref>. The results are given in <ref type="figure" target="#fig_10">Fig. 9</ref>. CEST achieves 3.37 and 3.14 NME on AFLW2000-3D and MICC datasets, respectively. More interestingly, our method performs better than the fully supervised techniques for shape estimation, e.g. 3DDFA (5.37 on AFLW2000-3D and 6.38 on MICC) and PRN (3.96 on AFLW2000-3D and 3.76 on MICC). Additionally, our method can also estimate facial reflectance and illumination, while both 3DDFA and PRN can not. Compared to N3DMM on MICC dataset, CEST achieves slightly lower NME <ref type="bibr">(3.14 vs. 3.20)</ref>. Notably, N3DMM uses dense landmarks for supervised pretraining while CEST only uses the 68 sparse landmarks. More quantitative comparisons can be found in Appendix B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We have proposed a conditional estimation framework, called CEST, for 3D face reconstruction from single-view images. CEST addresses the reconstruction problem with a more general formulation, which does not assume conditional independence. We have also proposed a specific decomposition for the conditional probability of different 3D facial parameters. Together with the reflectance symmetry and consistency constraints, CEST can be trained efficiently with video datasets. Both qualitative and quantitative results prove that the conditional estimation is useful. CEST is able to produce high quality and well-disentangled facial parameters for single-view images.</p><p>The proposed CEST can be improved from many aspects. Firstly, more accurate and unambiguous facial parameters can be obtained by exploring the temporal information in video. Second, the performance of shape estimation can be boosted by a more advanced morphable model, which also benefits the subsequent estimations of other facial parameters. Moreover, adding perceptual loss could also be an effective way to improve the visual quality of the facial parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Network Architecture</head><p>We use standard encoder networks for viewpoint, shape and illumination predictions, and a network similar to U-Net <ref type="bibr" target="#b29">[30]</ref> for reflectance prediction. The detailed configurations are given in <ref type="table" target="#tab_2">Table 1</ref>. Parameter d is 7 for viewpoint network f v and 9 for illumination network f . Conv 3 /2,1 denotes convoluitonal layer with kernel size of 3, where the stride and padding are 2 and 1, respectively. Each convolutional layer is followed by a Batch Normalization (BN) <ref type="bibr" target="#b14">[15]</ref> layer and Rectified Linear Units (ReLU). Bilinear interpolation is adopted for the upsampling operation. Specifically, in <ref type="table" target="#tab_2">Table 1</ref>, the layers in brackets are residual blocks. In <ref type="table" target="#tab_3">Table 2</ref>, we use shortcut to connect the feature maps of encoder and decoder, but different from U-Net, we use addition rather than concatenation to integrate information in the feature maps. For those encoder output shapes in brackets (e.g., "[128 ? 128 ? 64]"), the feature map will be added as a shortcut to the decoder feature map (also with the same brackets).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. More Ablation Studies</head><p>We perform more ablations for different settings of CEST. We explore the averaged representations, an approach adopted in <ref type="bibr" target="#b36">[37]</ref>, for reflectance consistency, where the averaged reflectance of a video clip is used to reconstruct the 3D face in each video frame. Here, we fix the size of minibatch, i.e. 128, but vary the number of images from each video clip to 2, 4, and 8. Results are shown in <ref type="figure" target="#fig_1">Fig. 10 (b)</ref>, (c), and (d), respectively. As we can see, there are still some illumination in the reflectance, indicating that the averaged representation may not be a good strategy for learning disentangled facial parameters. <ref type="figure" target="#fig_1">Fig. 11</ref> shows the results from CEST trained with reflectance consistency across video. The performance is comparable to those from CEST trained with default setting (reflectance consistency across video clip). It shows that consistency constraint can be generalized to longer videos if the recording environments are not changed dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. More Qualitative Comparisons</head><p>In this section, we show more comparisons to the state-of-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. Since there is no publicly available implementations for these methods, we compare to the results presented in their papers.</p><p>Overall, CEST produces more stable and reasonable geometries, detailed reflectances, and realistic reconstructions of the 3D faces. As shown in <ref type="figure" target="#fig_1">Fig. 12 (a) (b)</ref>, <ref type="figure" target="#fig_1">Fig. 15, Fig. 16, and Fig. 17</ref>, the facial shapes predicted by CEST are more accurate in facial expressions and lip closure. In addition, the predicted reflectances show more personal characteristics, but less remaining illumination, as illustrated in <ref type="figure" target="#fig_1">Fig. 13 and Fig. 16</ref>. Lastly, CEST yields faithful 3D reconstructions, capturing more details than the other methods (see <ref type="figure" target="#fig_1">Fig 14 and Fig 15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>CEST MoFA Tran16 Reflectance Geometry Reflectance Geometry Reflectance Geometry <ref type="figure" target="#fig_1">Figure 13</ref>: Comparisons to MoFA <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b42">[43]</ref>.</p><p>Input CEST MoFA Thies16 Reconstruction Geometry Geometry Geometry Reconstruction Reconstruction <ref type="figure" target="#fig_1">Figure 14</ref>: Comparisons to MoFA <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b39">[40]</ref>.</p><p>Input CEST MoFA Richardson16 Reconstruction Geometry Geometry Geometry Reconstruction Reconstruction <ref type="figure" target="#fig_1">Figure 15</ref>: Comparisons to MoFA <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b28">[29]</ref>. Our estimated shapes show more accurate expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Reflectance Geometry CEST CEST Booth17</p><p>Booth17 FML FML <ref type="figure" target="#fig_1">Figure 16</ref>: We compare CEST to FML <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Challenging Cases</head><p>We present some examples with dark skin in <ref type="figure" target="#fig_1">Fig. 18</ref>. Although most people in the training set (VoxCeleb) are Caucasian, CEST still produces reasonable illumination and albedo for these examples. One limitation is that the reconstruction of the non-lambertian surface is inaccurate, e.g. eyes with unusual gaze directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Photometric Error</head><p>We compare CEST, IEST, FML <ref type="bibr" target="#b36">[37]</ref> and Garrido <ref type="bibr" target="#b11">[12]</ref> on overlay face reconstruction. To measure the quality of the overlay images, we compute the average photometric error (R,G,B pixel values are from 0 to 255) between the input face image and Input CEST Tewari17 Sela17 FML19 Richardson17</p><p>MGCNet RingNet <ref type="figure" target="#fig_1">Figure 17</ref>: We compare the estimated shapes from CEST to those from <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and <ref type="bibr" target="#b33">[34]</ref> (from left to right). Our estimated shapes are more stable and accurate.  the overlay face image. We experiment on 1,000 images in CelebA dataset <ref type="bibr" target="#b24">[25]</ref>. <ref type="table" target="#tab_6">Table 3</ref> shows that the conditional estimation is beneficial for reconstructing the 3D face, and the proposed CEST outperforms existing methods by a large margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>P</head><label></label><figDesc>(S ,R ,v , l | I ) = P(S | I ) P(R | I ) P(v | I ) P(l | I ) P(S ,R ,v , l | I ) = P(v | I ) P(S | I ,v) P(R | I ,v ,S ) P(l | I ,v ,S ,R ) llumination ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Conventional 3D face reconstruction and our CEST framework. The dotted lines separate the modules used for inference of the 3D parameters from those used for training with self-supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The overall training pipeline of the proposed CEST framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Equation 3 as parametric functions: v = fv(I; ?v)? = fs(I,v; ?s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Ablations. (a) CEST with two constraints. (b) Uncoupled CEST with two constraints. (c) CEST with only reflectance consistency constraint. (d) CEST with reflectance symmetry constraint (the number of video frames is 1). (e) CEST with no constraint on reflectance. Errata: the row (d) in the ICCV 2021 version mistakenly uses the same results as the row (c). We have fixed it in this version. Please refer to our latest arXiv version for up-to-date results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparisons with MoFA. (a) and (c) are results from CEST. (b) and (d) are results from MoFA. Images are from CelebA dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons with nonlinear 3DMM. (a), (c), and (e) are results from CEST. (b), (d), and (f) are results from N3DMM. Images are from AFLW2000-3D dataset<ref type="bibr" target="#b48">[49]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Comparisons with FML. (a), (c), and (e) are results from CEST. (b), (d), and (f) are results from FML. Images are from the video frames in VoxCeleb1 dataset [26]Re-lit Input</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Lighting transfer results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) CED Curves on AFLW2000-3D dataset (b) CED Curves on MICC dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>CED curves on AFLW2000-3D and MICC datasets. For example, a point at (4, 63) means 63% of images have NME less than 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :Figure 11 :Figure 12 :</head><label>101112</label><figDesc>Ablations. (a) CEST with default settings. (b), (c) and (d) Averaged reflectance is used in training and the number of images from each video clips are 2, 4, and 8, respectively. Errata: we mistakenly use the wrong image in the ICCV 2021 version. We have replaced it with the correct one in this version. Please refer to our latest arXiv version for up-to-date results. Ablations. (a) CEST with default settings. (b) Reflectance consistency is applied to videos, not video clips. Comparisons to [38]. (a) and (c) Results from CEST. (b) and (d) Results from [38].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 :</head><label>18</label><figDesc>Some challenging examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The proposed CEST makes no assumption on dependency.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Training &amp; Inference</cell><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Viewpoint</cell><cell cols="2">Viewpoint v</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Shape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Shape S</cell><cell></cell><cell cols="2">Renderer</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Reflectance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original Image</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reflectance R</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Overlay Image</cell></row><row><cell></cell><cell></cell><cell cols="2">Illumination</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Illumination ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">(a) Most existing methods assume conditional independence.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Training &amp; Inference</cell><cell>Training</cell><cell></cell></row><row><cell>(b) Original Image</cell><cell>Viewpoint</cell><cell>Viewpoint v</cell><cell>Shape</cell><cell>Shape S</cell><cell>Reflectance</cell><cell>Reflectance R</cell><cell>Illumination</cell><cell>I</cell><cell>Renderer</cell><cell>Overlay Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The detailed CNNs architectures of viewpoint, illumination, and shape networks.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Reflectance Network</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">U-Net Encoder (?)</cell><cell></cell><cell cols="2">U-Net Decoder (?)</cell></row><row><cell>Encoder Layer</cell><cell>Act.</cell><cell>Output shape</cell><cell>Decoder Layer</cell><cell>Act.</cell><cell>Output shape</cell></row><row><cell>Input</cell><cell>-</cell><cell>256 ? 256 ? 3</cell><cell>Output</cell><cell>-</cell><cell>256 ? 256 ? 3</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>Conv 3 ? 3 /1,1</cell><cell>Tanh</cell><cell>256 ? 256 ? 3</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>256 ? 256 ? 3</cell></row><row><cell>Conv 4 ? 4 /2,1</cell><cell>BN + ReLU</cell><cell>128 ? 128 ? 64</cell><cell>Upsample (2?)</cell><cell>-</cell><cell>256 ? 256 ? 64</cell></row><row><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[128 ? 128 ? 64]</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[128 ? 128 ? 64]</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>128 ? 128 ? 64</cell></row><row><cell>Conv 4 ? 4 /2,1</cell><cell>BN + ReLU</cell><cell>64 ? 64 ? 64</cell><cell>Upsample (2?)</cell><cell>-</cell><cell>128 ? 128 ? 64</cell></row><row><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[64 ? 64 ? 64]</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[64 ? 64 ? 64]</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>64 ? 64 ? 64</cell></row><row><cell>Conv 4 ? 4 /2,1</cell><cell>BN + ReLU</cell><cell>32 ? 32 ? 128</cell><cell>Upsample (2?)</cell><cell>-</cell><cell>64 ? 64 ? 128</cell></row><row><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[32 ? 32 ? 128]</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[32 ? 32 ? 128]</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>32 ? 32 ? 128</cell></row><row><cell>Conv 4 ? 4 /2,1</cell><cell>BN + ReLU</cell><cell>16 ? 16 ? 128</cell><cell>Upsample (2?)</cell><cell>-</cell><cell>32 ? 32 ? 128</cell></row><row><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[16 ? 16 ? 128]</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[16 ? 16 ? 128]</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>16 ? 16 ? 128</cell></row><row><cell>Conv 4 ? 4 /2,1</cell><cell>BN + ReLU</cell><cell>8 ? 8 ? 256</cell><cell>Upsample (2?)</cell><cell>-</cell><cell>16 ? 16 ? 256</cell></row><row><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[8 ? 8 ? 256]</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>[8 ? 8 ? 256]</cell></row><row><cell>Conv 4 ? 4 /2,1</cell><cell>BN + ReLU</cell><cell>4 ? 4 ? 256</cell><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>8 ? 8 ? 256</cell></row><row><cell>Conv 3 ? 3 /1,1</cell><cell>BN + ReLU</cell><cell>4 ? 4 ? 256</cell><cell>Upsample (2?)</cell><cell>-</cell><cell>8 ? 8 ? 256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The detailed CNNs architectures of reflectance networks. Note that, the layers in the decoder (from input to output) are listed from bottom to top.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Photometric errors obtained by different methods.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Image Cropping</head><p>The viewpoint v comprises the scale factor v 1 , 3D spatial rotation parameters [v <ref type="bibr" target="#b1">2</ref> , v 3 , v 4 ], and 3D translation parameters [v <ref type="bibr" target="#b4">5</ref> , v 6 , v 7 ]. The original image I is cropped to its canonical view in 2D plane with viewpoint v. The cropping is given by (I ? v)(x , y ) = I(x, y), where the transformation from (x , y ) to (x, y) is formulated in the following.</p><p>Bilinear interpolation is used if x or y is not an integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Weak Perspective Transformation</head><p>The 3D spatial rotation is represented by a rotation vector w = [v 2 ; v 3 ; v 4 ] ? R 3?1 : the unit vector u = w w 2 is the axis of rotation, and the magnitude ? = w 2 is the rotation angle. The weak perspective transformation is used to project the world-coordinate facial shape S to image-coordinate Q, as formulated in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Barycentric Coefficients</head><p>Given the vertices of a triangle (Q(i), Q(j), Q(k)) and its enclosing grid point (x, y) on image. The barycentric coefficients can be computed by</p><p>The barycenteric coefficients ? 1 , ? 2 , and ? 3 are in the range of [0, 1] if the grid point (x, y) is in the triangle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Wrapping Function</head><p>The wrapping function ? : A ? R 256?256?3 ? R ? R K?3 is defined as R(i) = A(U (i, 1), U (i, 2)), where i is the index for the vertices of a 3D face. R(i) and A(U (i, 1), U (i, 2)) are 3-dimensional vectors. U ? R K?2 is the coordinates of shape in UV space from 3DMM <ref type="bibr" target="#b3">[4]</ref>. Again, bilinear interpolation is used if U (i, 1) or U (i, 2) is not an integer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The florence 2d/3d hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding</title>
		<meeting>the 2011 joint ACM workshop on Human gesture and behavior understanding</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="79" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="218" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yannis Panagakis, and Stefanos Zafeiriou. 3d face morphable models&quot; in-thewild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the area of a triangle in barycentric coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bottema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crux Mathematicorum</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="228" to="231" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1272" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3d face rigs from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Corrective 3d reconstruction of lips from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1155" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Aaron S Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d face reconstruction from a single image using a single reference face shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="394" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face reconstruction in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On learning associations of faces and voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Hijung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Kaspar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="276" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inversefacenet: Deep monocular inverse face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4625" to="4634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Avatarme: Realistically renderable 3d facial reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Lattas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Triantafyllou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5549" to="5558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1619" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A signal-processing framework for inverse rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to regress 3d face shape and expression from an image without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sfsnet: Learning shape, reflectance and illuminance of facesin the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6296" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised monocular 3d face reconstruction by occlusion-aware multiview geometry consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XV 16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic acquisition of high-fidelity facial performances using monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fml: Face model learning from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">On learning 3d face morphable model from in-the-wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05562</idno>
		<title level="m">3d dense face alignment via graph convolution networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probably symmetric deformable 3d objects from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhou</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th international conference on automatic face and gesture recognition (FGR06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaun</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dense 3d face decoding over 2500fps: Joint texture &amp; shape convolutional mesh decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
