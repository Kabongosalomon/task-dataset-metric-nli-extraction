<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GIT: A Generative Image-to-text Transformer for Vision and Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
							<email>zhengyang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
							<email>xiaowei.hu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
							<email>lindsey.li@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
							<email>ce.liu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Cloud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
						</author>
						<title level="a" type="main">GIT: A Generative Image-to-text Transformer for Vision and Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on numerous challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at https://github.com/microsoft/GenerativeImage2Text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In VL pre-training, multi-task pre-training has been widely used to empower the network with multiple or enhanced capabilities. For example, MLM and ITM are widely adopted pre-training tasks <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b98">99]</ref>. Recently, the image-text contrastive loss has also been added in <ref type="bibr" target="#b121">[122,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b104">105]</ref>. Since most VL tasks can be formulated as the text generation task <ref type="bibr" target="#b13">[14]</ref>, a single generation model can be pre-trained to support various downstream tasks. The input and output texts are usually carefully designed to pre-train such a generation model. For example in <ref type="bibr" target="#b13">[14]</ref>, the text is properly masked as the network input and the goal is to recover the masked text span. SimVLM <ref type="bibr" target="#b109">[110]</ref> randomly splits a text sentence into the input and the target output. In these methods, a multi-modal transformer encoder is utilized to incorporate the text inputs before decoding the output.</p><p>For image representation, Faster RCNN has been used in most existing approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26</ref>] to extract the region features. Recently, a growing interest is in dense representation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b61">62]</ref> from the feature map, which requires no bounding box annotations. Meanwhile, it is easy to train the entire network in an end-to-end way. In addition to the representation from the feature map, object tags <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> are leveraged to facilitate the transformer to understand the context, especially the novel objects. For scene-text-related tasks, OCR is invoked to generate the scene text as additional network input, e.g., in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b119">120]</ref>. For the text prediction, A transformer network is typically used, which can incorporate the cross-attention module to fuse the image tokens, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b121">122]</ref>, or only the self-attention modules where the image tokens are concatenated with the text tokens, e.g., <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Along the direction of scaling on VL tasks, LEMON [40]  studies the behavior of the detector-based captioning model with MLM. CoCa [122] studies different model sizes, but on the same pre-training data. In this paper, we present a comprehensive study on 9 various benchmarks (3 in main paper and 6 in supplementary materials, image/video captioning &amp; QA tasks) with 3 different model sizes and 3 different pre-training data scales (9 data points for each benchmark).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generative Image-to-text Transformer</head><p>With large-scale image-text pairs, our goal is to pre-train a VL model which is simple yet effective to benefit image/video captioning and QA tasks. As the input is the image and the output is the text, the minimal set of components could be one image encoder and one text decoder, which are the only components of our GIT as illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The image encoder is based on the contrastive pre-trained model <ref type="bibr" target="#b122">[123]</ref>. The input is the raw image and the output is a compact 2D feature map, which is flattened into a list of features. With an extra linear layer and a layernorm layer, the image features are projected into D dimensions, which are the input to the text decoder. We use the image encoder pre-trained with contrastive tasks because recent studies show superior performance with such image encoder, e.g. <ref type="bibr" target="#b122">[123,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b2">3]</ref>. In Sec 4.6 and supplementary materials, we also observe the VL performance boosts significantly with a stronger image encoder. This is consistent with the observation in object detection-based approaches, e.g. in <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b126">127]</ref>. The concurrent work of CoCa [122] unifies the contrastive task and the generation task. as one pre-training phase. Our approach is equivalent to separating the two tasks sequentially: (i) using the contrastive task to pre-train the image encoder followed by (ii) using the generation task to pre-train both the image encoder and text decoder.</p><p>The text decoder is a transformer module to predict the text description. The transformer module consists of multiple transformer blocks, each of which is composed of one self-attention layer and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A white marble taj mahal is reflected in a pool.</p><p>A star wars movie poster with a Darth Vader helmet.</p><p>A Marilyn Monroe photo with a black background.</p><p>A person holding a five dollar bill with a picture of abraham lincoln on the front.</p><p>A bowl of chinese food called mapo tofu.</p><p>A poster for the national administrative professionals day is shown.</p><p>A chevron competitive profile matrix is shown on a table.</p><p>A red apple with a green label that says fuji 94131. A baseball player with the blue jays on his jersey is about to hit a ball. blurry text occluded text <ref type="figure">Figure 1</ref>: Example captions generated by GIT. The model demonstrates strong capability of recognizing scene text, tables/charts, food, banknote, logos, landmarks, characters, products, etc.</p><p>Tremendous advances have been made in recent years on vision-language (VL) pre-training, especially based on the large-scale data of image-text pairs, e.g., CLIP <ref type="bibr" target="#b87">[88]</ref>, Florence <ref type="bibr" target="#b122">[123]</ref>, and SimVLM <ref type="bibr" target="#b109">[110]</ref>. The learned representation greatly boosts the performance on various downstream tasks, such as image captioning <ref type="bibr" target="#b69">[70]</ref>, visual question answering (VQA) <ref type="bibr" target="#b30">[31]</ref>, and image-text retrieval.</p><p>During pre-training, Masked Language Modeling (MLM) and Image-Text Matching (ITM) tasks have been widely used <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b53">54]</ref>. However, these losses are different from the downstream tasks, and task-specific adaptation has to be made. For example, ITM is removed for image captioning <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b65">66]</ref>, and an extra randomly initialized multi-layer perceptron is added for VQA <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b65">66]</ref>. To reduce this discrepancy, recent approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b107">108]</ref> have attempted to design unified generative models for pre-training, as most VL tasks can be cast as generation problems. These approaches typically leverage a multi-modal encoder and a text decoder with careful design on the text input and the text target. To further push the frontier of this direction, we present a simple Generative Image-to-text Transformer, named GIT, which consists only of one image encoder and one text decoder. The pre-training task is just to map the input image to the entire associated text description with the language modeling objective. Despite its simplicity, GIT achieves new state of the arts across numerous challenging benchmarks with a large margin, as summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The image encoder is a Swin-like vision transformer <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b122">123]</ref> pre-trained on massive image-text pairs based on the contrastive task <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b122">123]</ref>. This eliminates the dependency on the object detector, which is used in many existing approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. To extend it to the video domain, we simply extract the features of multiple sampled frames and concatenate them as the video representation. The text decoder is a transformer network to predict the associated text. The entire network is trained with the language modeling task. For VQA, the input question is treated as a text prefix, and the answer is generated in an auto-regressive way. Furthermore, we present a new generation-based scheme for ImageNet classification, where the predicted labels come directly from our generative model without pre-defining the vocabulary.</p><p>The approach is simple, but the performance is surprisingly impressive after we scale up the pretraining data and the model size. <ref type="figure">Fig. 1</ref> shows captions generated by the GIT fine-tuned with TextCaps.</p><p>The samples demonstrate the model's strong capability of recognizing and describing scene text, tables, charts, food, banknote, logos, landmarks, characters, celebrities, products, etc., indicating that our GIT model has encoded rich multi-modal knowledge about the visual world.</p><p>Our main contributions are as follows.</p><p>? We present GIT, which consists of only one image encoder and one text decoder, pre-trained on 0.8 billion image-text pairs with the language modeling task.</p><p>? We demonstrate new state-of-the-art performance over numerous tasks on image/video captioning and QA <ref type="table" target="#tab_0">(Table 1)</ref>, without the dependency on object detectors, object tags, and OCR. On TextCaps, we surpass the human performance for the first time. This implies that a simple network architecture can also achieve strong performance with scaling. ? We demonstrate that GIT pre-trained on the image-text pairs is capable of achieving new state-ofthe-art performance even on video tasks without video-dedicated encoders. ? We present a new scheme of generation-based image classification. On ImageNet-1K, we show a decent performance (88.79% top-1 accuracy) with our GIT.  one feed-forward layer. The text is tokenized and embedded into D dimensions, followed by an addition of the positional encoding and a layernorm layer. The image features are concatenated with the text embeddings as the input to the transformer module. The text begins with the [BOS] token, and is decoded in an auto-regressive way until the [EOS] token or reaching the maximum steps. The seq2seq attention mask as in <ref type="figure" target="#fig_2">Fig. 3</ref> is applied such that the text token only depends on the preceding tokens and all image tokens, and image tokens can attend to each other. This is different from a unidirectional attention mask, where not every image token can rely on all other image tokens.  mask is applied to the transformer. If (i, j) is 1, the i-th output can depend on the j-th input; otherwise, not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>??</head><p>Instead of well initializing the image encoder, we randomly initialize the text decoder. This design choice is highly motivated from the experiment studies of <ref type="bibr" target="#b105">[106]</ref>, in which the random initialization shows similar performance, compared with the BERT initialization. This could be because the BERT initialization cannot understand the image signal, which is critical for VL tasks. Without dependency of the initialization, we can easily explore different design choices. The concurrent work of Flamingo <ref type="bibr" target="#b2">[3]</ref> employs a similar architecture of image encoder + text decoder, but their decoder is pre-trained and frozen to preserve the generalization capability of the large language model. In our GIT, all parameters are updated to better fit the VL tasks.</p><p>An alternative architecture is the cross-attention-based decoder to incorporate the image signals instead of concatenation with selfattention. Empirically as shown in supplementary material (Appendix G.2), with large-scale pre-training, we find the self-attentionbased decoder achieves better performance overall, while in smallscale setting, the cross-attention-based approach wins. A plausible explanation is that with sufficient training, the decoder parameters can well process both the image and the text, and the image tokens can be better updated with the self-attention for text generation. With cross-attention, the image tokens cannot attend to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training</head><p>For each image-text pair, let I be the image, y i , i ? {1, ? ? ? , N } be the text tokens, y 0 be the [BOS] token and y N +1 be the [EOS] token. We apply the language modeling (LM) loss to train the model.</p><p>That is,</p><formula xml:id="formula_0">l = 1 N + 1 N +1 i=1 CE(y i , p(y i |I, {y j , j = 0, ? ? ? , i ? 1)}),<label>(1)</label></formula><p>where CE is the cross-entropy loss with label smoothing of 0.1.</p><p>An alternative choice is MLM, which predicts typically 15% of input tokens in each iteration. To predict all tokens, we have to run at least 1/0.15 = 6.7 epochs. For LM, each iteration can predict all tokens, which is more efficient for large-scale pre-training data. In <ref type="bibr" target="#b39">[40]</ref>, the ablation studies also show that LM can achieve better performance with limited epochs. In our large-scale training, the number of epoch is only 2 due to computational resource limitation, and thus we choose LM. Meanwhile, most of the recent large-scale language models are also based on LM, e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Without the image input, the model is reduced to a decoder-only language model, similar to GPT3 <ref type="bibr" target="#b6">[7]</ref> in the architecture wise. Thus, this design also enables the possibility to leverage the text-only data to enrich the decoding capability with a scaled-up decoder. We leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-tuning</head><p>For the image captioning task, as the training data format is the same as that in pre-training, we apply the same LM task to fine-tune our GIT.</p><p>For visual question answering, the question and the ground-truth answer are concatenated as a new special caption during the fine-tuning, but the LM loss is only applied on the answer and the [EOS] tokens. During inference, the question is interpreted as the caption prefix and the completed part is the prediction. Compared with the existing approaches <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b60">61]</ref> for VQAv2 <ref type="bibr" target="#b30">[31]</ref>, our model is generative without pre-defining the candidate answers, even in inference. This imposes more challenges as the model has to predict at least two correct tokens: one for the answer and another for <ref type="bibr">[EOS]</ref>. In contrast, the existing work pre-collects the answer candidate, recasts the problem as a classification problem, and only needs to predict once. However, considering the benefit of the free-form answer, we choose the generative approach. Due to difficulty of the generative model, we observe slightly worse performance on VQAv2 than the discriminative existing work. For the scene-text related VQA tasks, existing approaches <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b38">39]</ref> typically leverages the OCR engine to generate the scene text and use dynamic pointer network to decide the current output token should be OCR or the general text. Here, our approach depends on no OCR engine, and thus no dynamic pointer network. Empirically, we find the model gradually learns how to read the scene text with large-scale pre-training, and our model achieves new SoTA performance on these tasks.</p><p>Our model is not specifically designed for the video domain, but we find our model can also achieve competitive or even new SOTA performance with a simple architecture change. That is, we sample multiple frames from each video clip, and encode each frame via the image encoder independently. Afterwards, we add a learnable temporal embedding (initialized as zeros), and concatenate the features from sampled frames. The final representation is used in a similar way as the image representation for captioning and question answering.</p><p>We also apply our generation model to the image classification task, where the class names are interpreted as image captions, and our GIT is fine-tuned to predict the result in an auto-regressive way. This is different from existing work which normally pre-defines the vocabulary and uses a linear layer (with softmax) to predict the likelihood of each category. This new generation-based scheme is beneficial when new data and new categories are added to the existing dataset. In this case, the network can continuously train on the new data without introducing new parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setting</head><p>We collect 0.8B image-text pairs for pre-training, which include COCO <ref type="bibr" target="#b69">[70]</ref>, Conceptual Captions (CC3M) <ref type="bibr" target="#b93">[94]</ref>, SBU <ref type="bibr" target="#b81">[82]</ref>, Visual Genome (VG) <ref type="bibr" target="#b55">[56]</ref>, Conceptual Captions (CC12M) <ref type="bibr">[8]</ref>, ALT200M <ref type="bibr" target="#b39">[40]</ref>, and an extra 0.6B data following a similar collection procedure in <ref type="bibr" target="#b39">[40]</ref>. The image encoder is initialized from the pre-trained contrastive model <ref type="bibr" target="#b122">[123]</ref>. The hidden dimension (D) is 768. The text decoder consists of 6 randomly-initialized transformer blocks. The total number of model parameters is 0.7 billion. The learning rates of the image encoder and the decoder are 1e ?5 and 5e ?5 , respectively, and follow the cosine decay to 0. The total number of epochs is 2. During inference, the beam size is 4 and the length penalty <ref type="bibr" target="#b110">[111]</ref> is 0.6 by default.</p><p>Supplementary materials show results on two smaller model variants (GIT B and GIT L ) and one even larger model (GIT2). When comparing with existing approaches, the reference numbers are the best one reported in the corresponding paper unless explicitly specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Image Captioning and Question Answering</head><p>We comprehensively evaluate the captioning performance on the widely-used Karpathy split <ref type="bibr" target="#b52">[53]</ref> of COCO <ref type="bibr" target="#b69">[70]</ref>, the COCO test set, nocaps <ref type="bibr" target="#b1">[2]</ref> which focuses on novel objects, TextCaps <ref type="bibr" target="#b95">[96]</ref> which focuses on scene-text understanding, and VizWiz-Captions <ref type="bibr" target="#b34">[35]</ref> which focuses on the real use case by the vision-impaired people. The results in CIDEr <ref type="bibr" target="#b100">[101]</ref> are shown in <ref type="table" target="#tab_3">Table 2</ref>. From the results, we can see our model achieves the new SOTA performance on all these metrics except on COCO Karpathy test. On nocaps, compared with CoCa <ref type="bibr" target="#b121">[122]</ref>, our model is much smaller in the model size (0.7B vs 2.1B), but achieves higher performance (123.0 vs 120.6 in CIDEr). On Textcaps, our solution outperforms the previous SOTA (TAP <ref type="bibr" target="#b119">[120]</ref>) by a breakthrough margin (28.5 points in CIDEr), and also surpasses the human performance for the first time. Results on other metrics (e.g. BLEU@4 <ref type="bibr" target="#b83">[84]</ref>) and the other details are in supplementary materials.</p><p>On VQA, the evaluation benchmarks include VQAv2 <ref type="bibr" target="#b30">[31]</ref>, TextVQA <ref type="bibr" target="#b96">[97]</ref>, VizWiz-VQA <ref type="bibr" target="#b33">[34]</ref>. ST-VQA <ref type="bibr" target="#b5">[6]</ref>, and OCR-VQA <ref type="bibr" target="#b80">[81]</ref>. Before fine-tuning the model, we run an intermediate fine-tuning on the combination of the training data of VQAv2, TextVQA, ST-VQA, OCR-VQA, VizWiz-VQA, Visual Genome QA <ref type="bibr" target="#b55">[56]</ref>, GQA <ref type="bibr" target="#b43">[44]</ref>, and OK-VQA <ref type="bibr" target="#b78">[79]</ref>. To avoid data contamination, we remove the duplicate images of the test and validation set of the target benchmarks. As illustrated in <ref type="table" target="#tab_4">Table 3</ref>, we achieve new SOTA on VizWiz-VQA and OCR-VQA, and same performance with prior SOTA of LaTr <ref type="bibr" target="#b4">[5]</ref> on ST-VQA. Compared with the concurrent work of Flamingo <ref type="bibr" target="#b2">[3]</ref>, we achieve higher accuracy (+5.4) on TextVQA and lower (-3.29) on VQAv2. Note that Flamingo's model size is 80B, which is 114 times of ours (0.7B). On VQAv2, we observe that our model performs worse in 1.5 points than the discriminative model of Florence <ref type="bibr" target="#b122">[123]</ref>, which shares the same image encoder. The reason might be the increased difficulty of the generative model. That is, each correct answer requires at least two correct predictions (answer and [EOS]; 2.2 on average), while the discriminative model requires only one correct prediction. In <ref type="bibr" target="#b109">[110]</ref>, the ablation study also shows the better performance by around 1 point than the discriminative counterpart. Another reason could be that the model of Florence for VQA leverages RoBerta <ref type="bibr" target="#b72">[73]</ref> as the text encoder, which implicitly uses the text-only data to improve the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Video Captioning and Question Answering</head><p>On the video captioning task, the performance is evaluated on MSVD <ref type="bibr" target="#b8">[9]</ref> with the widely-used splits from <ref type="bibr" target="#b101">[102]</ref>, MSRVTT <ref type="bibr" target="#b113">[114]</ref>, YouCook2 <ref type="bibr" target="#b130">[131]</ref> (results in supplementary materials.) VATEX <ref type="bibr" target="#b108">[109]</ref>, and TVC <ref type="bibr" target="#b58">[59]</ref> (results in supplementary materials.). On VATEX, the performance is evaluated on both the public test and private test (evaluated on the server). Video QA is evaluated on MSVD-QA <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b8">9]</ref>, MSRVTT-QA <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b113">114]</ref>, and TGIF-Frame <ref type="bibr" target="#b46">[47]</ref>, which are all open-ended tasks. The results are shown in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_6">Table 5</ref> for captioning and QA, respectively. Although our model is not dedicated for video tasks, our model achieve new SOTA on MSRVD, MSRVTT, and VATEX for captioning and on MSVD-QA and TGIF-Frame for QA. For example on VATEX private test, our results are even better (93.8 vs 86.5) than CLIP4Caption++ <ref type="bibr" target="#b99">[100]</ref>, which relies on model ensemble and additional subtitle input. This is also better than Flamingo [3] (84.2) with 80B parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Image Classification</head><p>We fine-tune GIT on ImageNet-1k. Each category is mapped to a unique class name, and the prediction is correct only if it is exactly matched with the ground-truth label subject to more or fewer whitespaces 2 . As shown in <ref type="table">Table 6</ref>, our approach can achieve descent accuracy without pre-defining the vocabulary. Compared with Florence <ref type="bibr" target="#b122">[123]</ref> (same image encoder), our approach is worse in about 1.2 points. The reason might be similar to the case on VQAv2. That is, the generative approach needs to predict more tokens correctly to make one correct prediction, which increases the difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on Scene Text Recognition</head><p>The task <ref type="bibr" target="#b31">[32]</ref> aims to read scene text directly from the image. We evaluate our model in two settings. One is the GIT fine-tuned on TextCaps. The prediction is considered correct if the caption contains the ground-truth scene text word. The other is to fine-tune the model on two large scene text datasets: MJSynth (MJ) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> and SynthText (ST) <ref type="bibr" target="#b32">[33]</ref>, where the ground-truth scene text is used as the caption. The prediction is correct if the output is the exact match to the ground-truth. Following the established setup, we evaluate on six standard benchmarks, including ICDAR 2013 (IC13) <ref type="bibr" target="#b51">[52]</ref>,  60.3 All-in-one <ref type="bibr" target="#b102">[103]</ref> 66.3 VIOLET <ref type="bibr" target="#b26">[27]</ref> 68.9 MERLOT <ref type="bibr" target="#b124">[125]</ref> 69.5 GIT 72.8 (a) MSVD-QA <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b8">9]</ref> (b) MSRVTT-QA <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b113">114]</ref> (c) TGIF-Frame <ref type="bibr" target="#b46">[47]</ref> ICDAR 2015 (IC15) <ref type="bibr" target="#b50">[51]</ref>, IIIT 5K-Words (IIIT) <ref type="bibr" target="#b79">[80]</ref>, Street View Text (SVT) <ref type="bibr" target="#b106">[107]</ref>, Street View Text-Perspective (SVTP) <ref type="bibr" target="#b85">[86]</ref>, and CUTE80 (CUTE) <ref type="bibr" target="#b90">[91]</ref>. The average accuracy is reported in <ref type="table" target="#tab_8">Table 7</ref>. The accuracy on individual test sets is in supplementary materials. Our TextCaps-fine-tuned captioning model achieves an 89.9 accuracy, which demonstrates the strong scene text comprehension capability of our captioning model. After fine-tuning the model on the standard MJ+ST datasets, GIT achieves 92.9 that surpasses the prior arts <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> of 91.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis</head><p>Model and data scaling. To study the trending with data scales, we construct two smaller pretraining datasets: one is the combination of COCO, SBU, CC3M and VG, leading to 4M images or 10M image-text pairs; the other is to further combine CC12M, leading to about 14M images or 20M image-text pairs. When pre-training on small-scale datasets, we use 30 epochs rather than 2 epochs as on the 0.8B data. For the network structure, we name our model as Huge and replace the image encoder with ViT-B/16 and ViT-L/14 from CLIP <ref type="bibr" target="#b87">[88]</ref> as Base and Large, respectively. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the results on COCO, TextCaps, and VizWiz-QA. On COCO, the base model benefits from 4M to 14M, but the performance drops with 0.8B data. The 14M data are more similar to COCO than the majority of the noisy 0.8B data. Meanwhile, the Base model with limited capacity may not be able to benefit effectively from large-scale data. Similar observations are also reported in <ref type="bibr" target="#b54">[55]</ref> for ImageNet-1k classification. On TextCaps and VizWiz-QA, all model variants benefit significantly from more pre-training data. Also, a larger backbone improves more especially with 0.8B data.</p><p>Here, we scale the image encoder. Empirically, we find it is difficult to effectively scale up the text decoder. Preliminary results are shown in <ref type="table" target="#tab_9">Table 8</ref>, which shows a larger decoder shows no improvement. The reason might be that it is difficult to effectively train with limited amount of text by LM. Another plausible reason is that the image encoder is responsible for object recognition, and <ref type="table">Table 6</ref>: Results on ImageNet-1k classification task. Our approach takes the class name as the caption and predict the label in an auto-regressive way without pre-defining the vocabulary.    the decoder is responsible for organizing the object terms in a natural language way. The latter task might be easy since most of the descriptions follow similar patterns, e.g. object + verb + subject, and thus a small decoder is enough during end-to-end training. Larger decoders increase the learning difficulty, which might degrade the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vocabulary Method</head><p>Flamingo <ref type="bibr" target="#b2">[3]</ref> shows a larger decoder improves the performance. However, their decoder is pre-trained and frozen during the VL pre-training, which avoids the problem of how to effectively train the decoder. In LEMON <ref type="bibr" target="#b39">[40]</ref>, the transformer can be scaled up to 32 layers. The reason could be that LEMON uses MLM, instead of LM, which might be more difficult to train.</p><p>Scene text in pre-training data. To understand the capability of scene text comprehension, we examine the pre-training dataset and study how many image-text pairs contain the scene text. We first run the Microsoft Azure OCR API 3 against all images in CC12M and 500K images in the web crawled images. The OCR result is compared with the associated text. It is considered matched only if the text contains an OCR result that is longer than 5 characters. It is estimated that 15% of CC12M and 31% of the downloaded images contain scene text descriptions. As the training task is to predict the texts, the network gradually learns to read the scene text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In the paper, we design and train a simple generative model, named GIT, to map the input image to the associated text description on large-scale image-text pairs. On image/video captioning and question answering tasks, our model achieves new state-of-the-art performance across numerous benchmarks and surpasses the human performance on TextCaps for the first time. For the image classification, we apply the generation task to predict the label name directly. The strategy is different from the existing work with a pre-defined and fixed vocabulary, and is beneficial especially when new category data are added. Limitations. We focus on the pretraining-and-finetuning strategy to improve the absolute performance. Empirically, we find it is not easy to control the generated caption and the model lacks zero-shot and few-shot capabilities, which we leave as future work.</p><p>Societal impact. Compared with the existing work, our model clearly improves the performance and be more appropriate to help visually-impaired people. The model is pre-trained on large-scale data, and the data are not guaranteed to contain no toxic language, which may poison the output.</p><p>Although we observe few such instances qualitatively, special care should be taken to deploy the model in practice and more research exploration is required to control the output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Data Preprocessing</head><p>We follow <ref type="bibr" target="#b104">[105]</ref> to preprocess the pre-training data. That is, make sure the shorter length of the image no larger than 384 and the longer side no larger than 640 while maintaining the aspect ratio. Meanwhile, all images are re-saved with quality being 90 in the JPEG format. This results in 39 terabytes. No such preprocessing is applied on the fine-tuning dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Platform</head><p>The data are stored in Azure Blob Storage 4 , and the training is conducted on A100 provisioned by Azure Machine Learning 5 . The code is in python with packages including Pytorch 6 DeepSpeed 7 , Transformers 8 , maskrcnn-benchmark 9 , CLIP 10 , OSCAR <ref type="bibr" target="#b10">11</ref> , and VirTex [19] 12 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Network</head><p>In the main paper, we present the results of our GIT. Here, we construct two smaller model variants, named GIT B and GIT L on smaller pre-training dataset. As shown in <ref type="table" target="#tab_10">Table 9</ref>, GIT B uses CLIP/ViT-B/16 <ref type="bibr" target="#b87">[88]</ref> as the image encoder and is pre-trained on 10M image-text pairs or 4M images, which is a combination of COCO, SBU, CC3M and VG. GIT L uses CLIP/ViT-L/14 <ref type="bibr" target="#b87">[88]</ref> as the image encoder and is pre-trained on 20M image-text pairs or 14M images, which is a combination of the 10M image-text pairs with CC12M.</p><p>The three model variants share the same pre-training hyperparameters. The learning rate is warmed up in the first 500 iterations, and then follows cosine decay to 0. The learning rate is 1e ?5 for the image encoder and is multiplied by 5 for the randomly initialized text decoder. The batch size is 4096. Parameters are updated by AdamW <ref type="bibr" target="#b74">[75]</ref> with ? 1 = 0.9 and ? 2 = 0.999. The number of epochs is 2.</p><p>As the performance exhibits no signs of plateau, we further scale up the model size to 5.1B and the number of pretraining images to 10.5B (12.9B image-text pairs). The image encoder is scaled to 4.8B based on DaViT <ref type="bibr" target="#b20">[21]</ref> and is pre-trained with the UniCL <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b122">123]</ref> task. The text decoder is enlarged to 0.3B, the hyperparameters (number of transformer layers, hidden dimension, etc) of which follow BERT-Large <ref type="bibr" target="#b19">[20]</ref>. The model is named as GIT2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Implementation of the Data Loader</head><p>A challenging problem is to implement the data loader efficiently as the total data size (39TB for the 0.8B images) is much larger than the local disk size (around 7TB). As the data are stored in Azure Storage, we download the data to the local disk before reading it rather than directly from the cloud.</p><p>Considering the data scale may increase even larger in the future, we should make sure each operation is independent to the dataset size. In the meanwhile, the data downloading should be overlapped with the GPU computing, such that the data are always locally available when needed. The solution is outlined as follows.</p><p>1. The image-text pairs are evenly split among C compute nodes. Each node only accesses the corresponding part.</p><p>2. Each node consumes the data trunk by trunk. Each trunk is 2 20 image-text pairs except the last which may have fewer than 2 20 data.</p><p>3. The data in each trunk is randomly shuffled. We shuffle the data in the trunk level such that the cost is not related with the dataset size, and hence it can be applied to even larger dataset.</p><p>4. The shuffled trunk data are split evenly among the GPUs within the node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>One extra process on each node (launched by local rank = 0) is created to pre-fetch at most 7 future trunks. As each trunk is designed for all ranks in one node, it is not required for other ranks to launch the pre-fetching process, which avoids the race condition.</p><p>6. Local storage contains at most 12 trunk data, and the oldest will be removed.</p><p>Empirically, we observe almost no 13 time cost on the data loading during model training and the speed is also stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results on Image Captioning</head><p>On each task, the model is fine-tuned with 10 epochs. The batch size is 512 and the learning rate is 2.5e ?6 . SCST <ref type="bibr" target="#b89">[90]</ref> follows the same hyperparameters if performed.</p><p>COCO <ref type="figure" target="#fig_14">Fig. 10</ref> shows the complete results including GIT B and GIT L on COCO Karpathy split <ref type="bibr" target="#b52">[53]</ref>. For the base-sized and large-sized models, our model achieves competitive performance with existing approaches but with a simplified architecture. We observe that UniversalCaptioner <ref type="bibr" target="#b15">[16]</ref> achieves much better performance. As a strong image encoder of CLIP/ViT-L with 0.3B parameters is used in UniversalCaptioner for both the base and large model, effectively, the model size is much larger than those in respective categories. In the meanwhile, both UniversalCaptioner <ref type="bibr" target="#b15">[16]</ref> and OFA <ref type="bibr" target="#b107">[108]</ref> use more data than our approach within base/large-sized model sizes. <ref type="figure">Fig. 11</ref> shows the full results on the COCO test set.</p><p>nocaps. The main paper presents the overall performance on nocaps. <ref type="table" target="#tab_0">Table 12</ref> contains the complete results for each sub domain and other model variants. <ref type="figure" target="#fig_6">Fig. 5</ref> shows random <ref type="bibr" target="#b13">14</ref> prediction examples on the nocaps validation set. To visualize the novel concept recognition capability, we also collect sample images whose prediction contains at least one word not in the COCO training set, as illustrated in <ref type="figure" target="#fig_8">Fig. 6</ref>. As we can see, the model can well identify the novel object without the object tags as the network input.</p><p>TextCaps. No SCST <ref type="bibr" target="#b89">[90]</ref> is performed. <ref type="table" target="#tab_0">Table 13</ref> shows full results. <ref type="figure" target="#fig_11">Fig. 7</ref> shows predictions on random validation images. We also manually group the predictions according to different scenarios, as illustrated in <ref type="figure" target="#fig_12">Fig. 8 and 9</ref>. In <ref type="figure" target="#fig_6">Fig. 8, (1-5)</ref> show examples on which the model describes the digital time displayed on screens, which is correct most of the time. Pred: a hotel room with two beds in a room.</p><p>Pred: a pile of candy on a white background.</p><p>Pred: a group of cars parked on the side of a building.</p><p>Pred: a blue and white spotted stingray laying on the sand.</p><p>Pred: a green medical truck parked in a parking lot.</p><p>Pred: a red drum set and two guitars in a room.</p><p>Pred: a red truck parked on the side of a street.</p><p>Pred: a white lighthouse with a cloudy sky.</p><p>Pred: a lion laying in the grass next to a log.</p><p>Pred: a glass of coffee and a glass of milk.</p><p>Pred: a group of bread buns sitting on a cooling rack.</p><p>Pred: a bunch of oysters cooking on a grill.</p><p>Pred: a close up of a cat sitting in the grass.</p><p>Pred: a spider on its web with a spider in it.</p><p>Pred: a stack of pancakes sitting on a plate with syrup.</p><p>Pred: a blue crown on a stand on a black background.</p><p>Pred: a group of blue flowers in a field of tall grass.</p><p>Pred: two books sitting on top of a table.</p><p>Pred: a wooden cabinet sitting on top of a wooden floor.</p><p>Pred: a bar or pub with stools and a table.</p><p>Pred: a pool with a bed next to it in a yard.</p><p>Pred: a couple of cars parked in a showroom.</p><p>Pred: a close up of a white ceiling fan.</p><p>Pred: a group of dogs in the snow. Pred: a large starfish and fish swimming in the water.</p><p>Pred: a blue starfish sitting on top of a coral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pred:</head><p>a small dragonfly sitting on top of a blade of grass.</p><p>Pred: a black and white lemur sitting in a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pred:</head><p>a green roulette table in a room with chairs.</p><p>Pred: a couple of barbies on a cake.</p><p>Pred: a lion head door knocker on a wooden door.</p><p>Pred: a group of lipsticks sitting next to each other.</p><p>Pred: a bug sitting on top of a yellow dandelion.</p><p>Pred: a banjo sitting on top of a table with a giveaway sign.</p><p>Pred: a group of violins hanging on a wall.</p><p>Pred: a photocopier machine sitting on top of a white background.</p><p>Pred: a closet with a white kallax shelves and clothes.</p><p>Pred: a row of jeans stacked up with the date of september.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pred:</head><p>a small chipmunk eating nuts on the floor.</p><p>Pred: a close up of a wasp nest on a green leaf.</p><p>Pred: a sonicare electric toothbrush in a package.</p><p>Pred: a blue and white spotted stingray laying on the sand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pred:</head><p>a blue dragonfly sitting on a green leaf.</p><p>Pred: a blue sports car with a steering wheel and a gear shift.</p><p>Pred: a black and white skunk standing on the ground.  Pred: a green bay packers football field with the goal posts.</p><p>Pred: two computer monitors are on a desk, one of which is called ipad hacks.</p><p>Pred: a black and red poster that says end police impunity.</p><p>Pred: a collection of tin boxes on a shelf with one that says star trek.</p><p>Pred: a baseball player with the number 37 stands on the mound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pred: a baseball player with the number 35 on his jersey</head><p>Pred: a black and white photo of a floppy disk that says you're an asshole.</p><p>Pred: an orange train with the number 16433r on the side.</p><p>Pred: a billboard for the fifth elephant machine learning and analytics conference. Pred: four iphones are lined up on a carpet.</p><p>Pred: a grocery store aisle full of canned peas.</p><p>Pred: a white lenovo laptop with a black screen on it.</p><p>Pred: a green street sign for troublesome valley road.</p><p>Pred: a cd cover that says " rarities of piano music at schloss vor kusum ".</p><p>Pred: a white shelf with a picture of a dog on it.</p><p>Pred: a poster for the game dynasty warriors gundam reborn.</p><p>Pred: the cover of the book the energy glut by ian roberts and phil edwards.    A person wearing a number 19 jersey runs across a field.</p><p>A player with the number 7 on her jersey is running to first base.</p><p>A baseball player with the number 34 on his jersey.</p><p>A soccer player with the number 6 on his jersey kicks the ball.</p><formula xml:id="formula_1">(1) (2) (3) (4)<label>(5)</label></formula><p>A small train ride with a sign that says " old timer ".</p><p>A baseball player with the blue jays on his jersey is about to hit a ball.</p><p>A boat that says " desert belle " on it.</p><p>A bottle of asahi dry black beer next to a glass.</p><p>A display of legos in a store with a neighborhood health sign in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7) (6) (8) (9) (10)</head><p>A man is writing on a white board that says pwm = pulse width modulation.</p><p>A white board with a drawing of pythagorean theorem and a right angle.</p><p>A white board with a drawing of a dinosaur and the words do not erase.</p><p>A white board with welcome minis 2013 written on it.</p><p>A white board with a drawing of a cat and the words i'm smart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(11) (12) (13) (14) (15)</head><p>A road sign that says viaduct lookout ( deaths corner ) 300 m.</p><p>A drawing of a whale with the words " the largest in the world, even this creature you can see ".</p><p>An advertisement for telbru call rates yet for as low as $ 0. 25 per minute.</p><p>A colorful poster that says to be happy make other people happy.</p><p>A red sign that says ensure you always wear your emergency escape set. A book is open to the page and the title says " moderni politici sopa i delitti e le pene ".</p><p>A person reading a book that says house on the prairie.</p><p>A book is open to a page titled brisbane in motion moving pictures.</p><p>A book by stephen r. covey titled the 8th habit.</p><p>A page of a book with a quote about christ's love compels us to be broken for the good of others -to live gracious hospitality and generosity.  A delta plane is parked on the tarmac at an airport.</p><p>A tesla model s car is parked in a dirt road.</p><p>A microsoft store in the mall.</p><p>A white bentley convertible drives down a road with a license plate number 006m377.</p><p>A group of people are outside of a store called xiaomi.</p><p>(1) (2) (3) (4)</p><p>A white marble taj mahal is reflected in a pool.</p><p>A golden gate bridge with a city in the background.</p><p>A temple of heaven with a blue sign on it.</p><p>The colosseum is lit up at night with a fence in the background.</p><p>A sydney opera house is on the water with a city in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(6) (7) (8) (9) (10)</head><p>A paella is cooked in a pan with a lemon and shrimp.</p><p>A bowl of pad thai with a chicken and some sprouts.</p><p>A bowl of chinese food called mapo tofu.</p><p>A beef wellington on a cutting board with a knife.</p><p>A plate of caprese salad with tomatoes and basil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(13) (12) (11) (14) (15)</head><p>A marilyn monroe photo with a black background.</p><p>A poster for the movie the matrix.</p><p>Bart simpson is shown in a scene from the simpsons.</p><p>A star wars movie poster with a Darth Vader helmet.</p><p>Elon Musk in a black jacket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(19) (17) (18) (16) (20)</head><p>A red apple with a green label that says fuji 94131.</p><p>A yellow and red apple with a honeycrisp sticker on it.</p><p>A bunch of orange peppers with a logo that says whole foods market.</p><p>A package of whole baby bella mushrooms from food lion.</p><p>A bag of mayan sweets premium sweet onions.  recognizing scene text in languages such as Arabic, Japanese, Korean, and Chinese. <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> provide examples of recognizing scene text in stylized fonts. As shown in <ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref><ref type="bibr" target="#b24">(25)</ref>, GIT also performs well in reading curved scene text, which is generally considered a challenging case in scene text recognition studies. In <ref type="figure" target="#fig_6">Fig. 9, samples (1-5)</ref> show examples of reading numbers on jerseys. As shown in (6-10), we observe that GIT has a strong ability in inferring occluded scene text, based on both visual and text context information. For example, "blue jays" is a baseball team name in sample <ref type="formula">(6)</ref>, "asahi" is a beer brand in sample <ref type="bibr" target="#b8">(9)</ref>, and the occluded letter could be letter "t" in sample <ref type="bibr">(8)</ref>. <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> provide examples of reading hand-written scene text. <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> demonstrate GIT's ability in reading long pieces of scene texts. GIT works well in organizing scene text words into a fluent and informative sentence. <ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref><ref type="bibr" target="#b24">(25)</ref> show the challenging case of describing a book page, where the model needs to recognize and select the key information to describe. For example in sample <ref type="bibr" target="#b23">(24)</ref>, GIT covers the name and author of the book in the image.</p><p>In addition to the scene text captioning ability, we observe that the TextCaps-fine-tuned GIT is knowledgeable and can produce diverse and informative captions. We group the representative captions in <ref type="figure" target="#fig_14">Fig. 10</ref>. Samples (1-5) contain the descriptions of logos, such as "delta," "tesla," "oneplus," etc. GIT also shows the capability of describing landmarks, e.g., "taj mahal," "golden gate bridge," "temple of heaven," "Colosseum," and "Sydney opera house" in <ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref>. Samples <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> show examples on food images, such as "mapo tofu," "pad thai," "paella," "beef wellington," and "caprese  salad." <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> provide more examples of recognizing movie/cartoon characters and celebrities. Samples <ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref><ref type="bibr" target="#b24">(25)</ref> describe products based on the tag or packaging information.</p><p>VizWiz-Captions. SCST is performed except GIT2, and the full results are shown in <ref type="table" target="#tab_0">Table 14</ref>. <ref type="figure">Fig. 11</ref> visualizes the predictions on random test images. <ref type="figure" target="#fig_0">Fig. 12</ref> groups the results by different scenarios. The model can well recognize the banknotes, scene text on bottles/cans, menus, screens, etc., and can better help vision-impaired people in real use cases. The first row (1-5) of <ref type="figure" target="#fig_0">Fig. 12</ref> shows the generated captions on blurry images. The second row (6-10) shows images with low image quality or key information partially occluded. For example, GIT reads the scene text "metro," "diet coke," and "mortrin" in samples <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10)</ref>, and infers the object "toothpaste" and "hard drive" in samples <ref type="bibr" target="#b6">(7,</ref><ref type="bibr">8)</ref>. Samples <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> recognize banknotes in different currencies and denominations. <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> describe scene text on bottles and cans, thus providing more informative captions such as the "bacon bits" in (16) and the "nestle water" in <ref type="bibr" target="#b19">(20)</ref>. GIT also works well in summarizing menus, pages, and screens, as shown in the bottom row <ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref><ref type="bibr" target="#b24">(25)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Results on Visual Question Answering</head><p>Except on VizWiz-QA, the number of fine-tuning epochs is 20 and the learning rate is 1e ?5 . On VizWiz-QA, the number of epochs is 40 and the learning rate is 2e  <ref type="table" target="#tab_0">Table 15</ref>. <ref type="figure" target="#fig_2">Fig. 14 and Fig. 13</ref> show correct prediction on randomly selected images of VizWiz-VQA and ST-VQA, respectively. <ref type="figure" target="#fig_8">Fig. 16</ref> and <ref type="figure" target="#fig_6">Fig. 15</ref> show the randomly selected incorrect predictions.</p><p>Pred: a close up of a grey piece of fabric with a seam.</p><p>Pred: a close up of a yellow object on a white background.</p><p>Pred: the back of a package of food with the cooking instructions.</p><p>Pred: the front of a jar of chicken light salad dressing on a kitchen counter.</p><p>Pred: a hand holding a black calculator with a screen.</p><p>Pred: a container of old fashion hard candies on a table.</p><p>Pred: the top of a microwave with buttons on it.</p><p>Pred: a black bottle of moisture rich shampoo on a white blanket.</p><p>Pred: a grey and black cat with a pink collar laying on a couch.</p><p>Pred: a black television screen on a wooden table with a grey object.</p><p>Pred: the top of a box of frozen dinner on a wooden table.</p><p>Pred: the top of a box of pretzel bread on a counter.</p><p>Pred: the top of a box of healthy choice mediterranean balsamic garlic chicken frozen dinner.</p><p>Pred: a blank white piece of paper on a couch.</p><p>Pred: the top of a package of canadian bacon.</p><p>Pred: the top of a green bottle of liquor with a label.</p><p>Pred: the front cover of a catalog for 2012 catalog.</p><p>Pred: the top of a calculator with white buttons on a table.</p><p>Pred: a hand holding a piece of paper with a grocery list.</p><p>Pred: the front of a white box for a cell phone.</p><p>Pred: a blue sweater with a blue scarf hanging on a hanger.</p><p>Pred: the top of a box of fettuccine alfredo.</p><p>Pred: the top of a christmas tree with lights on it.</p><p>Pred: a bottle of organic apple cider tea sitting on top of a stove.</p><p>Pred: a bottle of 14 hands red wine on a table. <ref type="figure">Figure 11</ref>: Visualization of our model on random test images of VizWiz-Captions.</p><p>A black olympus device with a green screen and white text.</p><p>A red and yellow box of food with a recipe on the back.</p><p>A piece of paper with a questionnaire on it.</p><p>A white refrigerator with magnets on it.</p><p>A package of a microsoft computer with a red and blue cord.</p><formula xml:id="formula_5">(1) (2) (3) (4)<label>(5)</label></formula><p>A metro card is on a wooden table.</p><p>A box of toothpaste is on a red surface.</p><p>A hard drive with a white label and barcode on it.</p><p>A person is holding a can of diet coke.</p><p>A bottle of motrin is on a counter.</p><p>(6) (7) (8) (9)</p><p>A one dollar bill with the eye of providence on it.</p><p>A person holding a five dollar bill with a picture of abraham lincoln on the front.</p><p>A close up of a one dollar bill with george washington on the front.</p><p>A person is holding a one dollar bill in their hand.</p><p>A ten pound bill with a picture of queen elizabeth on it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ve, occluded):</head><p>A menu for a house favorite bbq on a table.</p><p>A menu for sushi appetizers on a computer screen.</p><p>A math problem with a parallel lines and a line of angles.</p><p>A page from a book about prokaryotic and eukaryotic cells.</p><p>A computer screen with a website for euphonious.   <ref type="figure" target="#fig_8">Figure 16</ref>: Visualization of incorrect predictions for the validation set on VizWiz-VQA.   <ref type="table" target="#tab_0">Table 18</ref> shows the fine-tuning hyperparameters on video tasks for GIT. <ref type="table" target="#tab_0">Table 16</ref> and <ref type="table" target="#tab_0">Table 17</ref> show the complete results on video captioning and video question answering, respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results on Image Classification</head><p>On ImageNet-1K <ref type="bibr" target="#b16">[17]</ref>, we map each label to a unique name. Each label belongs to an entry of WordNet hierarchy and is represented with a unique offset, e.g., 2012849. <ref type="figure" target="#fig_11">Fig. 17</ref> illustrates the python script to generate a readable unique name given the offset. The model is fine-tuned with 10 epochs and the learning rate is 1e ?5 . No beam search is performed during inference.</p><p>In the main paper, we demonstrated a decent accuracy of 88.79% top-1 on ImageNet-1k with our generative model. As no constraint is on the output, we find that only 13 or (or 0.026%) predictions are outside of the 1K category. <ref type="figure" target="#fig_12">Fig. 18</ref> illustrates 10 samples. Although deemed as incorrect, some predictions are reasonable. For example, the prediction of <ref type="figure" target="#fig_12">Fig. 18 (e)</ref> is ipad and is reasonable, although the ground-truth label is hand-held computer. These observations also imply that the generation model can quickly adapt to the classification task without pre-defining the vocabulary. <ref type="figure" target="#fig_13">Fig. 19</ref> and <ref type="figure" target="#fig_0">Fig. 20</ref> show the correct and incorrect predictions, respectively.  <ref type="table" target="#tab_3">Table 20</ref> shows the performance on six individual evaluation sets. <ref type="figure" target="#fig_0">Fig. 22</ref> shows the visualization samples with our TextCaps-fine-tuned GIT (denoted as GIT TextCaps ) and with the MJ+ST-fine-tuned GIT (denoted as GIT MJSJ ). For scene text recognition, we resize the image with the longer edge to 384 and pad the image to a square. Visually, GIT TextCaps can well recognize the scene text, almost as good as GIT MJSJ , but in the natural language form. GIT MJSJ can adapt well to the task and predict a clean result of the scene text. <ref type="figure" target="#fig_0">Fig. 22</ref> shows the visualizations on all six experimented benchmarks, i.e., IC13 <ref type="bibr" target="#b51">[52]</ref>, SVT <ref type="bibr" target="#b106">[107]</ref>, IIIT <ref type="bibr" target="#b79">[80]</ref>, IC15 <ref type="bibr" target="#b50">[51]</ref>, SVTP <ref type="bibr" target="#b85">[86]</ref>, CUTE <ref type="bibr" target="#b90">[91]</ref> from the top to the bottom row, respectively. GIT performs especially well on testing images visually similar to natural images, such as the CUTE dataset shown in the bottom row. Quantitatively, GIT achieves an even larger performance improvement of 3.9% absolute accuracy on Irregular-Text CUTE80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Results on Scene Text Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Model and data scaling</head><p>In the main paper, we present the impact of scaling on COCO, TextCaps and VizWiz-QA. <ref type="figure" target="#fig_0">Fig. 21</ref> shows results on other tasks. On scene-text-related QA tasks (a) and video captioning (d)/(e)/(f), both  <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b8">9]</ref> (b) MSRVTT-QA <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b113">114]</ref> (c) TGIF-Frame <ref type="bibr" target="#b46">[47]</ref>  larger model sizes and more pre-training data boost the performance significantly. For VQAv2 (b), the 0.8B data help little or even worsen the performance slightly. The task data <ref type="bibr" target="#b30">[31]</ref> are from COCO, and the first 20M image-text pairs are more similar to COCO images than the majority of the web crawled 0.8B data. This may indicate the first 20M image-text pairs are enough for VQAv2. For video QA (c), the improvement on more pre-training data is mild. The reason might be the domain gap between the image-text pairs and the video-question-answer triplets, which reduces the benefit of more image-text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Cross-attention-based decoder</head><p>We concatenate the representations of the image and the text as the input to the transformer. An alternative way is to use a cross-attention module to incorporate the image representations, as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b60">61]</ref>. The former allows the image tokens to attend each other, which may refine the representation for better performance; while the latter isolates each image token. However, the former uses a shared set of projections for both the image tokens and the text tokens, while the latter uses separate projection matrices. A shared projection may be hard to learn effectively. <ref type="table" target="#tab_0">Table 21</ref> shows the comparison with different sets of pre-training image-text pairs. With smaller dataset, the latter with cross-attention outperforms, while with large-scale data, the former wins. A plausible explanation is that with more pre-training data, the parameters are well optimized such that the shared projection can adapt to both the image and the text domains, which mitigates the drawback of the shared parameters. With the self-attention, the image token can be attended with each other for a better representation during decoding. In all experiments, we use the former architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Intermediate fine-tuning on VQA</head><p>For VQA, we conduct the intermediate fine-tuning by combining multiple VQA datasets. <ref type="table" target="#tab_3">Table 22</ref> shows the performance comparison with direct fine-tuning without the intermediate fine-tuning. From the results, we can see the intermediate fine-tuning improves the performance for all tasks, and the improvement is more if the target training data scale is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Scene text in pre-training data</head><p>We show in the main paper that a considerable amount of pre-training samples contain scene text descriptions. <ref type="figure" target="#fig_0">Fig. 23</ref> groups the pre-training samples with scene text from CC12M and the downloaded (a) <ref type="figure" target="#fig_12">Figure 18</ref>: Image samples on which the prediction of our GIT is out of the 1K categories with the whitespace removed on ImageNet-1K. by different scenarios. Samples <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref> show the associated text which contains the scene text in a natural language way. This is in line with the requirement of scene-text related tasks, TextCaps. <ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref> show examples of long pieces of texts. The pre-training samples also describe scene text in stylized fonts <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref>, leading to GIT's ability in robust scene text recognition. <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> contain pre-training examples with low-quality images and occluded/blurry/curved scene texts. In addition to the scene text pre-training samples, pre-training datasets also contain descriptions of diverse entities, e.g., the "apple logo" in <ref type="bibr" target="#b20">(21)</ref>, banknotes in <ref type="formula">(22)</ref>, celebrity "Biden" in (23), landmark "empire state building" in (24), and product "beats headphone" in <ref type="bibr" target="#b24">(25)</ref>. The pre-training data plays a critical role in GIT's capability in scene text description and informative caption generation.    (1) (2) (3) (4)</p><p>Mass &amp; flow production mass as well as flow production are characterized by the manufacturer of several number of a std product and stocked in the warehouses as finished goods awaiting sales.</p><p>Making the world a cleaner place one application at a time.</p><p>And as he sowed, some fell by the wayside; and it was trampled down, and the birds of the air devoured it.</p><p>How to measure a million end users for their IPv6 capability Be Google (or any other massively popular web service provider) or Get your code to run on a million users\' machines through another delivery channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-39</head><p>The Players \\u2013In practice, there are a lot of fails for venture capital investments, but a successful case can bring million profits \\u2013These investments in firms that do not trade on public stock exchanges are known as private equity investment.</p><p>(6) (7) (8) (9)</p><p>Art deco, considered by many to be the most glamorous decorative arts style.</p><p>By all counts, this is a luxury whisky. but compass box wants consumers to decide for themselves.</p><p>Feel like i shoulda been born a hermit crab. </p><p>The flowing translucent apple logo used on the press invitation for the september 10, 2019 event.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture of our GIT, composed of one image encoder and one text decoder. (a): The training task in both pre-training and captioning is the language modeling task to predict the associated description. (b): In VQA, the question is placed as the text prefix. (c): For video, multiple frames are sampled and encoded independently. The features are added with an extra learnable temporal embedding (initialized as 0) before concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>seq2seq attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Performance with different pre-training data scales and different model sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref>(8)<ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref> provide examples of reading scene text in Latin (Romance) languages such as French and Spanish.<ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> show GIT's ability in Pred: a bunch of green grapes hanging from a vine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Captioning results of our COCO-fine-tuned GIT on random samples from the nocaps validation set. Words not in COCO training captions are underlined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Pred: a couple of bees working on beehives in a field.Pred: a mitre saw sitting on top of a table in a workshop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Captioning results of our COCO-fine-tuned GIT on random samples whose prediction contains novel terms from the nocaps validation set. Novel terms, which are not in COCO training captions, are underlined.Pred: a bottle of dog house wine sits on a table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Pred: a clock with the name charvet on it Pred: soccer players with the number 13 on their jersey Pred: an airasia zest plane is parked on the tarmac.Pred: a can of sanpellegrino chino sits on a table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of our model on random validation images of TextCaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Grouped caption predictions from TextCaps. The scene text is underlined in descriptions. (1-5) Screen time. (6-10) Language-French/Spanish. (11-15) Language-Arabic/Japanese/Korean/Chinese. (16-20) Scene text in stylized fonts. (21-25) Coin/Curved text.A baseball game with a player wearing number 11 on his back.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Grouped caption predictions from TextCaps. (1-5) Numbers on jerseys. (6-10) Occluded scene text. (11-15) Hand-written scene text. (16-20) Long pieces of scene texts. (21-25) Bookpages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Grouped caption predictions on web images generated by TextCaps-fine-tuned GIT. (1-5) Logos. (6-10) Landmarks. (11-15) Foods. (16-20) Characters and celebrities. (21-25) Products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>? 5 .</head><label>5</label><figDesc>The input size is 384 and 576 for intermediate fine-tuning and the final fine-tuning, respectively. No intermediate fine-tuning is conducted for GIT B and GIT L . Full results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Grouped caption predictions from Vizwiz-Captions. (1-5) Blurry images. (6-10) Lowquality or occluded images. (11-15) Banknotes. (16-20) Bottles and cans. (21-25) Menus, pages, and screens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>from nltk . corpus import wordnet as wn def get_name ( offset ) : white_list = { 2012849 : ' crane bird ' , 3126707 : ' crane machine ' , 2113186 : ' cardigan dog ' , 2963159 : ' cardigan jacket ' , 3710637 : ' maillot tights ' , 3710721 : ' maillot bathing suit ' , } if offset in white_list : return white_list [ offset ] name = wn . s y n s e t _ f r o m _ p o s _ a n d _ o f f s e t ( 'n ' , offset ) . name () return name [ : -5 ] . replace ( '_ ' , ' ')</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 :</head><label>17</label><figDesc>Python script to generate a unique name for each offset in ImageNet-1K categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>Performance with different pre-training data scales and different model sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>Grouped representative pre-training samples from CC12M and the downloaded images. We highlight the informative descriptions in underline. (1-5) Pre-training samples with scene text descriptions. (6-10) Long pieces of scene texts. (11-15) Scene texts in stylized fonts. (16-20) Hard samples of blurry, occluded, or curved scene texts. (21-25) Pre-training data also contains diverse knowledge on logos, banknotes, celebrities, landmarks, products, etc..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with prior SOTA on image/video captioning and question answering (QA) tasks. *: evaluated on the public server. CIDEr scores are reported for Captioning tasks. Details of GIT2 are presented in supplementary materials. Prior SOTA 1 138.7 120.6 94.1 109.7 69.6 65.4 67.9 120.6 60 86.5 64.5 48.3 69.5 93.8 [127] [122] [30] [120] [5] [3] [5] [69] [92] [100] [100] [103] [125] [78] GIT (ours) 148.8 123.4 114.4 138.2 69.6 67.5 68.1 180.2 73.9 93.8 61.2 56.8 72.8 92.9 ? +10.1 +2.8 +20.3 +28.5 +0.0 +2.1 +0.2 +59.6 +13.9 +7.3 -3.3 +8.5 +3.3 -0.9 GIT2 (ours) 149.8 124.8 120.8 145.0 75.8 70.1 70.3 185.4 75.9 96.6 65.0 58.2 74.9 94.5 ? +11.1 + 4.2 +26.7 +35.3 +6.2 +4.7 +2.4 +64.8 +15.9 +10.1 +0.5 +9.9 +5.4 +0.7</figDesc><table><row><cell>Image captioning</cell><cell>Image QA</cell><cell></cell><cell cols="2">Video captioning</cell><cell></cell><cell cols="3">Video QA Text Rec.</cell></row><row><cell cols="2">COCO  OCR-VQA</cell><cell>MSVD</cell><cell>MSRVTT</cell><cell>VATEX  *</cell><cell>TVC  *</cell><cell>MSVD-QA</cell><cell>TGIF-Frame</cell><cell>Avg on 6</cell></row></table><note>* nocaps* VizWiz* TextCaps* ST-VQA* VizWiz*1 Prior SOTA: among all the numbers reported in publications before 8/2022, as far as we know. Preprint. Under review.arXiv:2205.14100v4 [cs.CV] 22 Aug 2022</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on image captioning. *: the nubmers are from<ref type="bibr" target="#b95">[96]</ref>; CE: cross-entropy optimization. All numbers are CIDEr scores, and other metrics are shown in supplementary materials.</figDesc><table><row><cell>Method</cell><cell>CE</cell><cell>Method</cell><cell>C</cell><cell cols="2">Method</cell><cell>Test</cell><cell>Method</cell><cell>Test</cell></row><row><cell cols="2">MiniVLM [106]119.8</cell><cell>BUTD [4]</cell><cell>120.5</cell><cell cols="2">OSCAR [66]</cell><cell>80.9</cell><cell>BUTD [4]*</cell><cell>33.8</cell></row><row><cell cols="2">DistillVLM [26]120.8</cell><cell cols="2">VinVL [127] 138.7</cell><cell cols="2">Human [2]</cell><cell>85.3</cell><cell cols="2">AoANet [42]* 34.6</cell></row><row><cell cols="2">ViTCap [25] OSCAR [66] 127.8 125.2</cell><cell>GIT</cell><cell>148.8</cell><cell cols="2">VIVO [41] VinVL [127]</cell><cell>86.6 92.5</cell><cell cols="2">M4C-Cap. [39]* 81.0 Anc.-Cap. [113] 87.4</cell></row><row><cell cols="2">VinVL [127] 130.8</cell><cell cols="2">(b) COCO test (c40)</cell><cell cols="2">UFO [105]</cell><cell>92.3</cell><cell>TAP [120]</cell><cell>103.2</cell></row><row><cell>UFO [105]</cell><cell>131.2</cell><cell></cell><cell></cell><cell cols="3">SimVLM [110] 115.2</cell><cell>TAP [120] #</cell><cell>109.7</cell></row><row><cell cols="2">Flamingo [3] 138.1</cell><cell>Method</cell><cell>test-std</cell><cell cols="2">LEMON [40]</cell><cell>114.3</cell><cell>Human [96]</cell><cell>125.5</cell></row><row><cell cols="2">LEMON [40] 139.1 SimVLM [110] 143.3</cell><cell>MTMA [30]</cell><cell>94.1</cell><cell cols="3">UniversalCap [16] 119.3 CoCa [122] 120.6</cell><cell>GIT</cell><cell>138.2</cell></row><row><cell>CoCa [122] OFA [108] GIT</cell><cell>143.6 144.8 145.3</cell><cell cols="2">(c) VizWiz-Captions GIT 114.4</cell><cell>GIT</cell><cell>(d) nocaps</cell><cell>123.4</cell><cell cols="2">(e) TextCaps</cell></row><row><cell cols="2">(a) COCO Karp. [53]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on visual question answering. (a): for VQAv2, approaches are divided according to whether the answer vocabulary is pre-defined (Closed) or not (Open) during inference. The model with closed vocabulary can be a classification model or generation model with constrained outputs, e.g.,<ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b60">61]</ref>. The two numbers in parenthesis are the number of parameters and the number of images (the images for pre-trained modules are not counted) in VL pretraining. (b): for TextVQA, Mia<ref type="bibr" target="#b86">[87]</ref> # is the winner entry of TextVQA Challenge 2021 with a fine-tuned T5-3B<ref type="bibr" target="#b88">[89]</ref> model. (c): ## : winner entry of 2021 VizWiz Grand Challenge Workshop.</figDesc><table><row><cell cols="2">VocabularyMethod</cell><cell>test-std</cell><cell>Method</cell><cell>test</cell><cell cols="2">Method</cell><cell>Test ANLS</cell></row><row><cell></cell><cell>OSCAR [66] UNITER [13] VILLA [28] UNIMO [65] ALBEF [62] VinVL [127] UFO [105]</cell><cell>73.82 74.02 74.87 75.27 76.04 76.60 76.76</cell><cell cols="2">M4C [39] LaAP-Net [36] 41.41 40.46 SA-M4C [50] 44.6 SMA [29] 45.51 TAP [120] 53.97 Flamingo [3] 54.1 Mia [87] # 73.67</cell><cell cols="3">M4C [39] SMA [29] CRN [71] LaAP-Net [36] SA-M4C [50] TAP [120] LaTr [5]</cell><cell>46.2 46.6 48.3 48.5 50.4 59.7 69.6</cell></row><row><cell>Closed</cell><cell>CLIP-ViL [95] METER [23] BLIP [61] SimVLM [110] (-, 1.8B)</cell><cell>76.70 77.64 78.32 80.34</cell><cell cols="2">GIT (b) TextVQA [97] 59.75</cell><cell cols="2">GIT Method</cell><cell>(d) ST-VQA [6]</cell><cell>69.6 test</cell></row><row><cell></cell><cell cols="2">Florence [123] (0.9B, 14M) 80.36 mPlug [60] (0.6B, 14M) 81.26 OFA [108] (0.9B, 54M) 82.0 CoCa [122] (2.1B, 4.8B) 82.3</cell><cell>Method [74] ## Flamingo [3]</cell><cell>test 60.6 65.4</cell><cell cols="3">BLOCK+CNN+W2V [81] 48.3 M4C [39] 63.9 LaAP-Net [36] 64.1 LaTr [5] 67.9</cell></row><row><cell>Open</cell><cell cols="2">Flamingo [3] (80B, 2.3B) 82.1 GIT (0.7B, 0.8B) 78.81</cell><cell cols="2">GIT (c) VizWiz-QA [34] 67.5</cell><cell>GIT</cell><cell cols="2">(e) OCR-VQA [81]</cell><cell>68.1</cell></row><row><cell></cell><cell>(a) VQAv2 [31]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on video captioning. E : model ensemble; T : with the subtitle as additional input.</figDesc><table><row><cell>Method</cell><cell></cell><cell>B@4</cell><cell>C</cell><cell>Method</cell><cell>B@4</cell><cell>C</cell><cell>Method</cell><cell>C</cell></row><row><cell cols="2">PickNet [12]</cell><cell>52.3</cell><cell>76.5</cell><cell>SAAT [130]</cell><cell>39.9</cell><cell>51.0</cell><cell>VaTeX [109]</cell><cell>45.1</cell></row><row><cell cols="2">GRU-EVE [1]</cell><cell>47.9</cell><cell>78.1</cell><cell>MGSA [11]</cell><cell>42.4</cell><cell>47.5</cell><cell cols="2">OpenBook [128] 57.5</cell></row><row><cell cols="2">SAAT [130]</cell><cell>46.5</cell><cell>81.0</cell><cell>POS+VCT [38]</cell><cell>42.3</cell><cell>49.1</cell><cell>VALUE [64] T</cell><cell>58.1</cell></row><row><cell cols="2">MGSA [11]</cell><cell>53.4</cell><cell>86.7</cell><cell>SibNet [72]</cell><cell>40.9</cell><cell>47.5</cell><cell cols="2">SwinBERT [69] 73.0</cell></row><row><cell cols="2">POS+VCT [38]</cell><cell>52.8</cell><cell>87.8</cell><cell>POS+CG [104]</cell><cell>42.0</cell><cell>48.7</cell><cell cols="2">C.4Cap. [100] ET 85.7</cell></row><row><cell cols="2">SibNet [72] POS+CG [104]</cell><cell>54.2 52.5</cell><cell>88.2 88.7</cell><cell>OA-BTG [126] STG-KD [83]</cell><cell>41.4 40.5</cell><cell>46.9 47.1</cell><cell>GIT</cell><cell>91.5</cell></row><row><cell cols="2">OA-BTG [126]</cell><cell>56.9</cell><cell>90.6</cell><cell>Support-set [85]</cell><cell>38.9</cell><cell>48.6</cell><cell cols="2">(a) VATEX [109] public test</cell></row><row><cell cols="2">STG-KD [83]</cell><cell>52.2</cell><cell>93.0</cell><cell>PMI-CAP [10]</cell><cell>42.1</cell><cell>49.4</cell><cell></cell></row><row><cell cols="2">PMI-CAP [10] ORG-TRL [129] SwinBERT [69]</cell><cell>54.6 54.3 58.2</cell><cell>95.1 95.2 120.6</cell><cell>ORG-TRL [129] OpenBook [128] SwinBERT [69]</cell><cell>43.6 33.9 41.9</cell><cell>50.9 52.9 53.8</cell><cell cols="2">Method X-L.+T. [133] E 81.4 C</cell></row><row><cell>GIT</cell><cell cols="2">79.5 (a) MSVD [9]</cell><cell>180.2</cell><cell cols="2">MV-GPT [92] T GIT (b) MSRVTT [114] 48.9 53.8</cell><cell>60 73.9</cell><cell cols="2">Flamingo [3] C.4Cap. [100] ET 86.5 84.2 GIT 93.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(e) VATEX [109] private test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results on video question answering. All are open-ended question answering tasks.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Method</cell><cell>Accuracy</cell><cell>Method</cell><cell>Accuracy</cell></row><row><cell>QueST [49]</cell><cell>34.6</cell><cell>JustAsk [117]</cell><cell>41.5</cell><cell>HCRN [57]</cell><cell>55.9</cell></row><row><cell>HCRN [57]</cell><cell>36.1</cell><cell>MV-GPT [92]</cell><cell>41.7</cell><cell>QueST [49]</cell><cell>59.7</cell></row><row><cell>CoMVT [93]</cell><cell>42.6</cell><cell>MERLOT[125]</cell><cell>43.1</cell><cell>ClipBERT[58]</cell><cell></cell></row><row><cell>JustAsk [117]</cell><cell>46.3</cell><cell>VIOLET [27]</cell><cell>43.9</cell><cell></cell><cell></cell></row><row><cell>VIOLET [27]</cell><cell>47.9</cell><cell>All-in-one [103]</cell><cell>46.8</cell><cell></cell><cell></cell></row><row><cell>All-in-one [103]</cell><cell>48.3</cell><cell>Flamingo [3]</cell><cell>47.4</cell><cell></cell><cell></cell></row><row><cell>GIT</cell><cell>56.8</cell><cell>GIT</cell><cell>43.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results on scene text recognition. MJ and ST indicate the MJSynth (MJ)<ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> and Synth-Text (ST)<ref type="bibr" target="#b32">[33]</ref> datasets used for training scene text recognition models.</figDesc><table><row><cell>Method</cell><cell cols="2">Fine-tuning data Average</cell></row><row><cell>SAM [67]</cell><cell>MJ+ST</cell><cell>87.8</cell></row><row><cell>Ro.Scanner [124]</cell><cell>MJ+ST</cell><cell>87.5</cell></row><row><cell>SRN [121]</cell><cell>MJ+ST</cell><cell>89.6</cell></row><row><cell>ABINet [24]</cell><cell>MJ+ST</cell><cell>91.9</cell></row><row><cell>S-GTR [37]</cell><cell>MJ+ST</cell><cell>91.9</cell></row><row><cell>MaskOCR [78]</cell><cell>MJ+ST</cell><cell>93.8</cell></row><row><cell>GIT</cell><cell>TextCaps MJ+ST</cell><cell>89.9 92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of larger text decoders. The models are pre-trained on a subset of 0.4B image-text pairs. No beam search and no SCST are performed.</figDesc><table><row><cell>Layers</cell><cell></cell><cell cols="2">COCO</cell><cell></cell><cell>nocaps</cell></row><row><cell></cell><cell>B@4</cell><cell>M</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell></row><row><cell>6</cell><cell>38.9</cell><cell cols="5">30.7 136.4 24.6 119.3 15.9</cell></row><row><cell>12</cell><cell>38.9</cell><cell cols="5">30.6 136.0 24.2 118.1 15.5</cell></row><row><cell>24</cell><cell>39.1</cell><cell cols="5">30.2 134.6 23.8 115.4 15.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Model configurations in pre-training. The decoder is a 6-layer transformer network. The hidden size is 768 with 12 attention heads except GIT2. Parameters of text token embeddings and the last projection weight before the softmax layer are shared and not counted in the model size.AppendixThe supplementary materials provide more details on the experiments, including results with different model variants, more visualizations, ablation analysis on decoder architectures, more results on data and model scaling, etc.</figDesc><table><row><cell cols="3">Name images image-text pairs</cell><cell>image encoder</cell><cell cols="3">epochs model size image size</cell></row><row><cell cols="2">GIT B 4M</cell><cell>10M</cell><cell>CLIP/ViT-B/16 [88]</cell><cell>30</cell><cell>129M</cell><cell>224</cell></row><row><cell cols="2">GIT L 14M</cell><cell>20M</cell><cell>CLIP/ViT-L/14 [88]</cell><cell>30</cell><cell>347M</cell><cell>224</cell></row><row><cell>GIT</cell><cell>0.8B</cell><cell>0.8B</cell><cell>Florence/CoSwin [123]</cell><cell>2</cell><cell>681M</cell><cell>384</cell></row><row><cell cols="2">GIT2 10.5B</cell><cell>12.9B</cell><cell>DaViT [21] (4.8B)</cell><cell>2</cell><cell>5.1B</cell><cell>384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Results on COCO captioning with Karpathy [53] split. SimVLM: C4 (800GB) dataset are used and not included in the table; Flamingo: 27M video-text pairs are not counted in the table. UniversalCaptioner: the extra 0.3B in parameters is CLIP/ViT-L, which is used as feature and keyword extractor. the data for pre-training CLIP/ViT-L are not counted . VinVL/LEMON/OSCAR/MiniVLM/DistillVLM: the extra parameters are for object detector; data for the object detectors are not counted.</figDesc><table><row><cell>Method</cell><cell cols="2">#Param. #Images</cell><cell></cell><cell cols="2">Cross-Entropy</cell><cell></cell><cell></cell><cell>SCST</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>B@4</cell><cell>M</cell><cell>C</cell><cell>S</cell><cell>B</cell><cell>M</cell><cell>C</cell><cell>S</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Tiny-sized models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MiniVLM [106]</cell><cell>46M+8M</cell><cell cols="2">11M 35.6</cell><cell cols="7">28.6 119.8 21.6 39.2 29.7 131.7 23.5</cell></row><row><cell>DistillVLM [26]</cell><cell>46M+8M</cell><cell>4M</cell><cell>35.6</cell><cell cols="3">28.7 120.8 22.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Base-sized models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViTCap [25]</cell><cell>0.2B</cell><cell>4M</cell><cell>36.3</cell><cell cols="7">29.3 125.2 22.6 41.2 30.1 138.1 24.1</cell></row><row><cell>OSCARB [66]</cell><cell>0.1B+64M</cell><cell>4M</cell><cell>36.5</cell><cell cols="7">30.3 123.7 23.1 40.5 29.7 137.6 22.8</cell></row><row><cell>VinVLB [127]</cell><cell cols="2">0.1B+0.2B 6M</cell><cell>38.2</cell><cell cols="7">30.3 129.3 23.6 40.9 30.9 140.4 25.1</cell></row><row><cell>UFOB [105]</cell><cell>0.1B</cell><cell>4M</cell><cell>36.0</cell><cell cols="3">28.9 122.8 22.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">UniversalCapB [16] 0.2B+0.3B 36M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">42.9 31.4 149.7 25.0</cell></row><row><cell>GITB</cell><cell>0.1B</cell><cell>4M</cell><cell>40.4</cell><cell cols="7">30.0 131.4 23.0 41.3 30.4 139.1 24.3</cell></row><row><cell></cell><cell></cell><cell cols="4">Large-sized models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSCARL [66]</cell><cell>0.3B+64M</cell><cell>4M</cell><cell>37.4</cell><cell cols="7">30.7 127.8 23.5 41.7 30.6 140.0 24.5</cell></row><row><cell>VinVLL [127]</cell><cell cols="2">0.3B+0.2B 6M</cell><cell>38.5</cell><cell cols="7">30.4 130.8 23.4 41.0 31.1 140.9 25.2</cell></row><row><cell>UFOL [105]</cell><cell>0.3B</cell><cell>4M</cell><cell>38.7</cell><cell cols="3">30.0 131.2 23.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BLIPViT-L [61]</cell><cell>-</cell><cell cols="2">129M 40.4</cell><cell>-</cell><cell>136.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">UniversalCapL [16] 0.5B+0.3B 36M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">42.9 31.5 150.2 25.2</cell></row><row><cell>mPLUG [60]</cell><cell>0.6B</cell><cell cols="2">14M 43.1</cell><cell cols="7">31.4 141.0 24.2 46.5 32.0 155.1 26.0</cell></row><row><cell>GITL</cell><cell>0.3B</cell><cell cols="2">14M 42.0</cell><cell cols="7">30.8 138.5 23.8 42.3 31.2 144.6 25.4</cell></row><row><cell></cell><cell></cell><cell cols="4">Huge/Giant-sized models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Flamingo [3]</cell><cell>80B</cell><cell>2.3B</cell><cell>-</cell><cell>-</cell><cell>138.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LEMONhuge [40]</cell><cell cols="3">0.7B+0.2B 0.2B 41.5</cell><cell cols="7">30.8 139.1 24.1 42.6 31.4 145.5 25.5</cell></row><row><cell>SimVLMHuge [110]</cell><cell>-</cell><cell cols="2">1.8B 40.6</cell><cell cols="3">33.7 143.3 25.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OFA [108]</cell><cell>0.9B</cell><cell cols="2">54M 43.9</cell><cell cols="7">31.8 145.3 24.8 44.9 32.5 154.9 26.6</cell></row><row><cell>CoCa [122]</cell><cell>2.1B</cell><cell cols="2">4.8B 40.9</cell><cell cols="3">33.9 143.6 24.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GIT</cell><cell>0.7B</cell><cell cols="2">0.8B 44.1</cell><cell cols="7">31.5 144.8 24.7 44.1 32.2 151.1 26.3</cell></row><row><cell>GIT2</cell><cell>5.1B</cell><cell cols="2">10.5B 44.1</cell><cell cols="7">31.4 145.0 24.8 44.0 32.2 152.7 26.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Results on COCO test set evaluated on the public server. c5/c40: Each image is paired with 5 or 40 reference ground-truth captions. B: BLEU [84]; M: METEOR [18]; R: ROUGE-L [68]; C: CIDEr-D [101]. 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5 VinVL [127] 81.9 96.9 66.9 92.4 52.6 84.7 40.4 74.9 30.6 40.8 60.4 76.8 134.7 138.7 GIT 84.0 97.9 69.8 94.4 55.6 87.6 43.2 78.3 31.9 42.0 62.0 78.4 145.5 148.8 GIT2 84.5 98.1 70.0 94.4 55.7 87.6 43.2 78.3 31.9 42.1 62.0 78.4 146.4 149.8</figDesc><table><row><cell>Method</cell><cell>B@1</cell><cell>B@2</cell><cell>B@3</cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell></row><row><cell></cell><cell cols="6">c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40</cell><cell>c5</cell><cell>c40</cell></row><row><cell>BUTD [4]</cell><cell>80.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Results on nocaps. in.: in-domain; near.: near domain; out.: out-of-domain; C: CIDEr. S: SPICE. 15.4 116.3 15.1 120.2 14.5 117.3 15.0 112.8 15.2 115.5 15.1 110.1 13.7 114.3 14.9 UniversalCap [16] 123.2 15.0 121.5 15.3 123.4 14.4 122.1 15.0 118.9 15.4 120.6 15.3 114.3 14.1 119.3 15.1 124.1 16.0 127.1 15.7 125.5 16.0 122.4 16.2 123.9 16.0 122.0 15.7 123.4 15.9 GIT2 126.9 16.1 125.8 16.2 130.6 15.8 126.9 16.1 124.2 16.4 125.5 16.1 122.3 15.6 124.8 16.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Validataion set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test set</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>in.</cell><cell></cell><cell cols="2">near.</cell><cell>out.</cell><cell></cell><cell cols="2">overall</cell><cell>in.</cell><cell></cell><cell cols="2">near.</cell><cell>out.</cell><cell></cell><cell cols="2">overall</cell></row><row><cell></cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell></row><row><cell>OSCAR [66]</cell><cell cols="16">85.4 11.9 84.0 11.7 80.3 10.0 83.4 11.4 84.8 12.1 82.1 11.5 73.8 9.7 80.9 11.3</cell></row><row><cell>Human [2]</cell><cell cols="16">84.4 14.3 85.0 14.3 95.7 14.0 87.1 14.2 80.6 15.0 84.6 14.7 91.6 14.2 85.3 14.6</cell></row><row><cell>VIVO [41]</cell><cell cols="16">92.2 12.9 87.8 12.6 87.5 11.5 88.3 12.4 89.0 12.9 87.8 12.6 80.1 11.1 86.6 12.4</cell></row><row><cell>VinVL [127]</cell><cell cols="16">103.7 13.7 95.6 13.4 83.8 11.9 94.3 13.1 98.0 13.6 95.2 13.4 78.0 11.5 92.5 13.1</cell></row><row><cell>UFO [105]</cell><cell cols="16">103.9 14.5 95.5 13.8 83.5 12.3 94.3 13.6 98.9 14.3 94.7 13.9 77.9 12.1 92.3 13.6</cell></row><row><cell>mPLUG [60]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-114.8 14.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="17">SimVLM [110] 113.7 -110.9 -115.2 -115.2 -113.7 -110.9 -115.2 -115.2 -</cell></row><row><cell cols="2">LEMON [40] 118.0 CoCa [122] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-122.4 15.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-120.6 15.5</cell></row><row><cell>GITB</cell><cell cols="8">100.7 13.8 97.7 13.5 89.6 12.5 96.6 13.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GITL</cell><cell cols="8">107.7 14.9 107.8 14.5 102.5 13.7 106.9 14.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GIT</cell><cell cols="2">129.8 16.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Results on TextCaps<ref type="bibr" target="#b95">[96]</ref>. Test set is evaluated by the server. *: the numbers are from<ref type="bibr" target="#b95">[96]</ref>. B: BLEU@4; M: METEOR; R: ROUGE-L; S: SPICE; C: CIDEr. # : winner entry of the CVPR 2021 workshop challenge<ref type="bibr" target="#b14">15</ref> .</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Validation set</cell><cell></cell><cell></cell><cell></cell><cell>Test set</cell><cell></cell></row><row><cell></cell><cell>B</cell><cell>M</cell><cell>R</cell><cell>S</cell><cell>C</cell><cell>B</cell><cell>M</cell><cell>R</cell><cell>S</cell><cell>C</cell></row><row><cell>BUTD [4]*</cell><cell cols="9">20.1 17.8 42.9 11.7 41.9 14.9 15.2 39.9 8.8</cell><cell>33.8</cell></row><row><cell>AoANet [42]*</cell><cell cols="10">20.4 18.9 42.9 13.2 42.7 15.9 16.6 40.4 10.5 34.6</cell></row><row><cell cols="11">M4C-Cap. [39]* 23.3 22.0 46.2 15.6 89.6 18.9 19.8 43.2 12.8 81.0</cell></row><row><cell cols="11">Anc.-Cap. [113] 24.7 22.5 47.1 15.9 95.5 20.7 20.7 44.6 13.4 87.4</cell></row><row><cell>TAP [120]</cell><cell cols="10">25.8 23.8 47.9 17.1 109.2 21.9 21.8 45.6 14.6 103.2</cell></row><row><cell>TAP [120] #</cell><cell cols="10">28.1 24.4 49.3 17.7 119.0 22.9 22.0 46.5 14.6 109.7</cell></row><row><cell>Human [96]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">24.4 26.1 47.0 18.8 125.5</cell></row><row><cell>GIT B</cell><cell cols="5">24.1 21.1 45.2 15.7 64.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GIT L</cell><cell cols="5">30.6 24.6 50.3 18.6 106.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GIT</cell><cell cols="10">37.0 27.6 54.1 21.1 143.7 33.1 26.2 52.2 19.6 138.2</cell></row><row><cell>GIT2</cell><cell cols="10">38.4 28.3 54.6 21.9 148.6 33.8 27.0 53.0 20.2 145.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Results on VizWiz-Captions. Both test-dev and test-std are evaluated on the server.</figDesc><table><row><cell>: winner entry of</cell></row></table><note>#</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Results on visual question answering. (a): for VQAv2, approaches are divided according to whether the answer vocabulary is pre-defined (Closed) or not (Open) during inference. The model with closed vocabulary can be a classification model or generation model with constrained outputs, e.g.,<ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b60">61]</ref>. (b): for TextVQA, Mia [87] # is the winner entry of TextVQA Challenge 2021 with a fine-tuned T5-3B [89] model. (c): ## : winner entry of 2021 VizWiz Grand Challenge Workshop.</figDesc><table><row><cell cols="2">Vocabulary Model</cell><cell></cell><cell></cell><cell cols="2">test-dev test-std</cell><cell>Model</cell><cell>validation</cell><cell>test</cell></row><row><cell></cell><cell cols="2">OSCAR [66]</cell><cell></cell><cell>73.61</cell><cell>73.82</cell><cell>M4C [39]</cell><cell>40.55</cell><cell>40.46</cell></row><row><cell></cell><cell cols="2">UNITER [13]</cell><cell></cell><cell>73.82</cell><cell>74.02</cell><cell>LaAP-Net [36]</cell><cell>41.02</cell><cell>41.41</cell></row><row><cell></cell><cell cols="3">Visual Parsing [116]</cell><cell>74.00</cell><cell>74.17</cell><cell>SA-M4C [50]</cell><cell>45.4</cell><cell>44.6</cell></row><row><cell></cell><cell cols="2">PixelBERT [43]</cell><cell></cell><cell>74.45</cell><cell>74.55</cell><cell>SMA [29]</cell><cell>44.58</cell><cell>45.51</cell></row><row><cell></cell><cell cols="2">VILLA [28]</cell><cell></cell><cell>74.69</cell><cell>74.87</cell><cell>TAP [120]</cell><cell>54.71</cell><cell>53.97</cell></row><row><cell></cell><cell cols="2">UNIMO [65]</cell><cell></cell><cell>75.06</cell><cell>75.27</cell><cell>Flamingo [3]</cell><cell>57.1</cell><cell>54.1</cell></row><row><cell>Closed</cell><cell cols="2">ALBEF [62]</cell><cell></cell><cell>75.84</cell><cell>76.04</cell><cell>LaTr [5]</cell><cell>61.05</cell><cell>61.60</cell></row><row><cell></cell><cell cols="2">VinVL [127]</cell><cell></cell><cell>76.52</cell><cell>76.60</cell><cell>Mia [87] #</cell><cell>-</cell><cell>73.67</cell></row><row><cell></cell><cell cols="2">UFO [105] CLIP-ViL [95] METER [23] BLIP [61]</cell><cell></cell><cell>76.64 76.48 77.68 78.25</cell><cell>76.76 76.70 77.64 78.32</cell><cell>GITB GITL GIT</cell><cell>18.81 37.47 59.93</cell><cell>--59.75</cell></row><row><cell></cell><cell cols="2">OFA [108]</cell><cell></cell><cell>79.87</cell><cell>80.02</cell><cell>GIT2</cell><cell>68.38</cell><cell>67.27</cell></row><row><cell></cell><cell cols="2">SimVLM [110] Florence [123]</cell><cell></cell><cell>80.03 80.16</cell><cell>80.34 80.36</cell><cell cols="2">(b) TextVQA [97]</cell></row><row><cell></cell><cell cols="2">mPlug [60]</cell><cell></cell><cell>81.27</cell><cell>81.26</cell><cell>Model</cell><cell>test-dev</cell><cell>test</cell></row><row><cell></cell><cell cols="2">CoCa [122]</cell><cell></cell><cell>82.3</cell><cell>82.3</cell><cell>[74] ##</cell><cell>61.8</cell><cell>60.6</cell></row><row><cell></cell><cell cols="3">Flamingo [3] (80B)</cell><cell>82.0</cell><cell>82.1</cell><cell>Flamingo [3]</cell><cell>65.7</cell><cell>65.4</cell></row><row><cell>Open</cell><cell cols="2">GITB (0.1B)</cell><cell></cell><cell>72.72</cell><cell>-</cell><cell>GITB</cell><cell>54.6</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GITL (0.3B)</cell><cell></cell><cell>75.51</cell><cell>-</cell><cell>GITL</cell><cell>62.5</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GIT (0.7B)</cell><cell></cell><cell>78.56</cell><cell>78.81</cell><cell>GIT</cell><cell>68.0</cell><cell>67.5</cell></row><row><cell></cell><cell cols="2">GIT2 (5.1B)</cell><cell></cell><cell>81.74</cell><cell>81.92</cell><cell>GIT2</cell><cell>70.97</cell><cell>70.1</cell></row><row><cell></cell><cell cols="3">(a) VQAv2 [31]</cell><cell></cell><cell></cell><cell cols="2">(c) VizWiz-QA [34]</cell></row><row><cell>Model</cell><cell cols="5">Val Acc. Val ANLS Test ANLS</cell><cell>model</cell><cell>val test</cell></row><row><cell cols="2">M4C [39] LaAP-Net [36] SA-M4C [50] TAP [120] LaTr [5]</cell><cell>38.1 39.7 42.2 50.8 61.64</cell><cell>47.2 49.7 51.2 59.8 70.2</cell><cell></cell><cell>46.2 48.5 50.4 59.7 69.6</cell><cell cols="2">BLOCK+CNN+W2V [81] -48.3 M4C [39] 63.5 63.9 LaAP-Net [36] 63.8 64.1 LaTr [5] 67.5 67.9</cell></row><row><cell>GITB GITL GIT</cell><cell></cell><cell>14.7 32.3 59.2</cell><cell>20.7 44.6 69.1</cell><cell></cell><cell>--69.6</cell><cell>GITB GITL GIT</cell><cell>57.3 57.5 62.4 62.9 67.8 68.1</cell></row><row><cell>GIT2</cell><cell></cell><cell>66.6</cell><cell>75.1</cell><cell></cell><cell>75.8</cell><cell>GIT2</cell><cell>69.9 70.3</cell></row><row><cell></cell><cell cols="3">(d) ST-VQA [6]</cell><cell></cell><cell></cell><cell cols="2">(e) OCR-VQA [81]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 16 :</head><label>16</label><figDesc>Results on video captioning. E : model ensemble; T : with the subtitle as additional input. YouCook2 and TVC are on the validation set.</figDesc><table><row><cell>Method</cell><cell>B@4 R M C</cell><cell>Method</cell><cell cols="2">B@4 R M C</cell></row><row><cell>VaTeX [109]</cell><cell>28.4 47.0 21.7 45.1</cell><cell>MMT [59] T</cell><cell cols="2">10.8 32.8 16.9 45.3</cell></row><row><cell cols="2">OpenBook [128] 33.9 50.2 23.7 57.5 VALUE [64] T ---58.1</cell><cell>HERO [63] T VALUE [64] T</cell><cell cols="2">12.3 34.1 17.6 49.9 11.6 33.9 17.6 50.5</cell></row><row><cell cols="2">SwinBERT [69] 38.7 53.2 26.2 73.0 C.4Cap. [100] ET 40.6 54.5 -85.7</cell><cell cols="3">SwinBERT [69] 14.5 36.1 18.5 55.4 C.4Cap. [100] ET 15.0 36.9 -66.0</cell></row><row><cell>GITB</cell><cell>37.9 51.9 24.4 60.0</cell><cell>GITB</cell><cell cols="2">13.0 33.2 16.6 47.3</cell></row><row><cell>GITL</cell><cell>41.6 54.3 26.2 72.5</cell><cell>GITL</cell><cell cols="2">14.9 35.4 18.0 55.7</cell></row><row><cell>GIT</cell><cell>41.6 55.4 28.1 91.5</cell><cell>GIT</cell><cell cols="2">16.2 36.7 18.9 63.0</cell></row><row><cell>GIT2</cell><cell>42.7 56.5 28.8 94.5</cell><cell>GIT2</cell><cell cols="2">16.9 37.2 19.4 66.1</cell></row><row><cell cols="2">(d) VATEX [109] public test</cell><cell cols="3">(f) TVC [59] validation</cell></row><row><cell>Method</cell><cell>C</cell><cell>Method</cell><cell></cell><cell>C</cell></row><row><cell cols="2">X-L.+T. [133] E 81.4</cell><cell cols="2">C.4Cap. [100] T</cell><cell>59.41</cell></row><row><cell cols="2">Flamingo [3] C.4Cap. [100] ET 86.5 84.2</cell><cell cols="3">C.4Cap. [100] ET 64.49 GIT 61.19</cell></row><row><cell>GIT</cell><cell>93.8</cell><cell>GIT2</cell><cell></cell><cell>65.02</cell></row><row><cell>GIT2</cell><cell>96.6</cell><cell cols="3">(g) TVC [59] private test</cell></row><row><cell cols="2">(e) VATEX [109] private test</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 :</head><label>17</label><figDesc>Results on video question answering. All are open-ended question answering tasks.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Method</cell><cell>Accuracy</cell><cell>Method</cell><cell>Accuracy</cell></row><row><cell>QueST [49]</cell><cell>34.6</cell><cell>JustAsk [117]</cell><cell>41.5</cell><cell>HCRN [57]</cell><cell>55.9</cell></row><row><cell>HCRN [57]</cell><cell>36.1</cell><cell>MV-GPT [92]</cell><cell>41.7</cell><cell>QueST [49]</cell><cell>59.7</cell></row><row><cell>CoMVT [93]</cell><cell>42.6</cell><cell>MERLOT[125]</cell><cell>43.1</cell><cell>ClipBERT[58]</cell><cell>60.3</cell></row><row><cell>JustAsk [117]</cell><cell>46.3</cell><cell>VIOLET [27]</cell><cell>43.9</cell><cell>All-in-one [103]</cell><cell>66.3</cell></row><row><cell>VIOLET [27]</cell><cell>47.9</cell><cell>All-in-one [103]</cell><cell>46.8</cell><cell>VIOLET [27]</cell><cell>68.9</cell></row><row><cell>All-in-one [103]</cell><cell>48.3</cell><cell>Flamingo [3]</cell><cell>47.4</cell><cell>MERLOT[125]</cell><cell>69.5</cell></row><row><cell>GITB</cell><cell>51.2</cell><cell>GITB</cell><cell>41.0</cell><cell>GITB</cell><cell>69.1</cell></row><row><cell>GITL</cell><cell>55.1</cell><cell>GITL</cell><cell>42.7</cell><cell>GITL</cell><cell>71.9</cell></row><row><cell>GIT</cell><cell>56.8</cell><cell>GIT</cell><cell>43.2</cell><cell>GIT</cell><cell>72.8</cell></row><row><cell>GIT2</cell><cell>58.2</cell><cell>GIT2</cell><cell>45.6</cell><cell>GIT2</cell><cell>74.9</cell></row><row><cell>(a) MSVD-QA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 18 :</head><label>18</label><figDesc>Fine-tuning epochs and learning rate of GIT on video tasks. SCST<ref type="bibr" target="#b89">[90]</ref> is performed for VATEX with the same hyperprameters.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Video Captioning</cell><cell></cell><cell cols="3">Video Question Answering</cell></row><row><cell></cell><cell cols="8">MSVD MSRVTT YouCook2 VATEX TVC MSVD MSRVTT TGIF-Frame</cell></row><row><cell>epochs</cell><cell>10</cell><cell>20</cell><cell>20</cell><cell>10</cell><cell>20</cell><cell>40</cell><cell>20</cell><cell>20</cell></row><row><cell cols="3">learning rate 1e ?6 2.5e ?6</cell><cell>1e ?5</cell><cell cols="3">2.5e ?6 5e ?6 1e ?5</cell><cell>1e ?5</cell><cell>1e ?5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 19 :</head><label>19</label><figDesc>Results on ImageNet-1k classification task. Our approach takes the class name as the caption and predict the label in an auto-regressive way without pre-defining the vocabulary.</figDesc><table><row><cell cols="2">Vocabulary Method</cell><cell>Top-1</cell></row><row><cell>Closed</cell><cell cols="2">ALIGN [48] Florence [123] 90.05 88.64</cell></row><row><cell></cell><cell>CoCa [122]</cell><cell>91.0</cell></row><row><cell></cell><cell>GIT B</cell><cell>78.86</cell></row><row><cell>Open</cell><cell>GIT L</cell><cell>84.05</cell></row><row><cell></cell><cell>GIT</cell><cell>88.79</cell></row><row><cell></cell><cell>GIT2</cell><cell>89.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 20 :</head><label>20</label><figDesc>Results on scene text recognition. MJ and ST indicate the MJSynth (MJ)<ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> and SynthText (ST)<ref type="bibr" target="#b32">[33]</ref> datasets used for training scene text recognition models.</figDesc><table><row><cell>Method</cell><cell>Fine-tuning</cell><cell></cell><cell>Regular Text</cell><cell></cell><cell></cell><cell>Irregular Text</cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>Data</cell><cell cols="6">IC13[52] SVT [107] IIIT [80] IC15[51] SVTP[86] CUTE [91]</cell><cell></cell></row><row><cell>SAM [67]</cell><cell>MJ+ST</cell><cell>95.3</cell><cell>90.6</cell><cell>93.9</cell><cell>77.3</cell><cell>82.2</cell><cell>87.8</cell><cell>87.8</cell></row><row><cell cols="2">Ro.Scanner [124] MJ+ST</cell><cell>94.8</cell><cell>88.1</cell><cell>95.3</cell><cell>77.1</cell><cell>79.5</cell><cell>90.3</cell><cell>87.5</cell></row><row><cell>SRN [121]</cell><cell>MJ+ST</cell><cell>95.5</cell><cell>91.5</cell><cell>94.8</cell><cell>82.7</cell><cell>85.1</cell><cell>87.8</cell><cell>89.6</cell></row><row><cell>ABINet [24]</cell><cell>MJ+ST</cell><cell>97.4</cell><cell>93.5</cell><cell>96.2</cell><cell>86.0</cell><cell>89.3</cell><cell>89.2</cell><cell>91.9</cell></row><row><cell>S-GTR [37]</cell><cell>MJ+ST</cell><cell>96.8</cell><cell>94.1</cell><cell>95.8</cell><cell>84.6</cell><cell>87.9</cell><cell>92.3</cell><cell>91.9</cell></row><row><cell>MaskOCR [78]</cell><cell>MJ+ST</cell><cell>97.8</cell><cell>94.1</cell><cell>96.5</cell><cell>88.7</cell><cell>90.2</cell><cell>92.7</cell><cell>93.8</cell></row><row><cell>GIT</cell><cell>TextCaps MJ+ST</cell><cell>94.2 97.3</cell><cell>91.5 95.2</cell><cell>92.9 95.3</cell><cell>78.2 83.7</cell><cell>87.1 89.9</cell><cell>95.5 96.2</cell><cell>89.9 92.9</cell></row><row><cell>GIT2</cell><cell>MJ+ST</cell><cell>97.8</cell><cell>95.5</cell><cell>97.6</cell><cell>85.6</cell><cell>91.3</cell><cell>99.0</cell><cell>94.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 21 :</head><label>21</label><figDesc>Comparison between pure self-attention-based decoder and the cross-attention-based decoder under different amounts of pre-training data. No SCST is applied on captioning. No intermediate fine-tuning is applied for VQA.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Captioning</cell><cell></cell><cell cols="3">Visual Question Answering</cell></row><row><cell cols="8">data Cross-Att. COCO nocaps TextCaps VizWiz ST-VQA TextVQA VizWiz</cell></row><row><cell>0.8B</cell><cell>w/o w/</cell><cell>144.2 120.3 143.2 118.2</cell><cell>143.7 139.3</cell><cell>107.2 103.0</cell><cell>65.3 63.1</cell><cell>58.5 55.6</cell><cell>59.1 58.9</cell></row><row><cell>10M</cell><cell>w/o w/</cell><cell>139.1 75.4 138.1 86.2</cell><cell>92.7 93.9</cell><cell>89.3 88.5</cell><cell>40.9 42.7</cell><cell>33.0 34.7</cell><cell>51.8 54.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 22 :</head><label>22</label><figDesc>Effectiveness of the intermediate fine-tuning on VQA tasks. The gain is large when the target training data scale is small.</figDesc><table><row><cell></cell><cell cols="5">OCR-VQA VQAv2 TextVQA VizWiz-QA ST-VQA</cell></row><row><cell>w/o inter. FT</cell><cell>67.6</cell><cell>78.0</cell><cell>59.0</cell><cell>66.6</cell><cell>66.9</cell></row><row><cell>w/ inter. FT</cell><cell>67.8</cell><cell>78.6</cell><cell>59.9</cell><cell>68.0</cell><cell>69.1</cell></row><row><cell>?</cell><cell>0.2</cell><cell>0.6</cell><cell>0.9</cell><cell>1.4</cell><cell>2.2</cell></row><row><cell>Train data</cell><cell>166K</cell><cell>122K</cell><cell>22K</cell><cell>20K</cell><cell>17K</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">pred.replace(' ', '') == gt.replace(' ', '')</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-recognizing-text</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://azure.microsoft.com/en-us/services/storage/blobs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">That is, the data preprocessing is faster than the training and is overlapped with the GPU training. 14 Disgusting images and images containing clear people identification information are excluded.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://vizwiz.org/workshops/2021-workshop/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Lin Liang for her help on scene text recognition experiments, Houdong Hu and Min Gao for their help on the pre-training data, and Lu Yuan and Bin Xiao for their help on the pre-trained image encoder. We would like to thank Nguyen Bach, Jiayuan Huang, Luis Vargas, Yumao Lu, Michael Zeng, and Xuedong Huang for their support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GITTextcaps: A sticker that says tiredness on it.</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GITMJST: tiredness</head><p>GITTextcaps: A sign that says royal on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2)</head><p>GITTextcaps: A green screen with the word betriebsbereit on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3)</head><p>GITTextcaps: A blue background with the word wolfgang written in white.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(4)</head><p>GITTextcaps: A blue background with the word distributed in yellow letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(5)</head><p>GITMJST: taqueria GITMJST: icebox GITMJST: colonial GITMJST: wyndham</p><p>GITTextcaps: A sign that says michoacana in red letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(6)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GITMJST: michoacana</head><p>GITTextcaps: A black sign that says taqueria on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>GITTextcaps: A sign that says the icebox on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(8)</head><p>GITTextcaps: A black background with the word colonial written on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(9)</head><p>GITTextcaps: A sign that says wyndham on it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GITMJST: kingfisher</head><p>GITTextcaps: A banner that says 'vijayawada' on it.</p><p>GITTextcaps: A black sign that says $ 6. 98 on it.</p><p>GITTextcaps: The station logo.</p><p>GITTextcaps: A sign with the numbers 3, 642, 039, 055 on it.</p><p>GITMJST: republic GITMJST: peperoni GITMJST: jewellery GITMJST: promod</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GITTextcaps:</head><p>The word republic is on a black background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GITMJST: republic</head><p>GITTextcaps: A sign that says republic on it.</p><p>GITTextcaps: A black sign that says 'pepperoni' on it.</p><p>GITTextcaps: The jewellery shop logo.</p><p>GITTextcaps: A sign that says " promod " on it.</p><p>GITMJST: cinerama GITMJST: neumos GITMJST: pioneer GITMJST: casbah GITTextcaps: A sign that says grandstand sports on it.</p><p>GITMJST: grandstand GITTextcaps: Cinerama logo.</p><p>GITTextcaps: A neon sign that says neumos on it.</p><p>GITTextcaps: The word pioneer is on a sign.</p><p>GITTextcaps: A sign that says the casbah on it.</p><p>GITMJST: imperial GITMJST: gembira GITMJST: celebrating GITMJST: bridgestone GITTextcaps: A red jersey with the name ronaldo on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GITMJST: ronaldo</head><p>GITTextcaps: A blue and white shirt with imperial college written on it.</p><p>GITTextcaps: A sign that says gembira in yellow letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(28)</head><p>GITTextcaps: A banner that says celebrating on it.</p><p>GITTextcaps: A tire with bridgestone written on it.</p><p>(30) <ref type="figure">Figure 22</ref>: Grouped scene text recognition predictions on all six experimented benchmarks. GIT Textcaps and GIT MJST are model variants finetuned with TextCaps caption data <ref type="bibr" target="#b95">[96]</ref>, and scene text recognition data MJ+ST <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33]</ref>, respectively. (1-5) IC13 <ref type="bibr" target="#b51">[52]</ref>. (6-10) SVT <ref type="bibr" target="#b106">[107]</ref>. <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> IIIT <ref type="bibr" target="#b79">[80]</ref>. <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> IC15 <ref type="bibr" target="#b50">[51]</ref>. (21-25) SVTP <ref type="bibr" target="#b85">[86]</ref>. (26-30) CUTE <ref type="bibr" target="#b90">[91]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Syed Zulqarnain Gilani, and Ajmal Mian. Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayyer</forename><surname>Aafaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">nocaps: novel object captioning at scale. In ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yana</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14198</idno>
		<title level="m">visual language model for few-shot learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latr: Layout-aware transformer for scene-text vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Ali Furkan Biten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikar</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appalaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scene text visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Ali Furkan Biten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Tito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Mafla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?al</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Rusinol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning modality interaction for temporal sentence localization and event captioning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion guided spatial attention for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Less is more: Picking informative frames for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">UNITER: universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Universal captioner: Long-tail vision-and-language model training through content-style separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Fiameni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12727</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT@ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Davit: Dual attention vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An empirical study of training end-to-end vision-and-language transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02387</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Read like humans: autonomous, bidirectional and iterative language modeling for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shancheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Injecting semantic concepts into end-to-end image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compressing visual-linguistic model via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">VIOLET : End-to-end video-language transformers with masked visual-token modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12681</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00753</idno>
		<title level="m">Anton van den Hengel, and Qi Wu. Structured multimodal attentions for textvqa</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiple transformer mining for vizwiz image caption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biaolong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 VizWiz Grand Challenge Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>In ICML</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Captioning images taken by people who are blind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilavra</forename><surname>Bhattacharya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08565</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Finding the evidence: Localization-aware answer prediction for text visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual semantics allow for textual reasoning better in scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint syntax representation learning and visual cue translation for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative answer prediction with pointer-augmented multimodal transformers for textvqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Scaling up vision-language pre-training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12233</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">VIVO: surpassing human performance in novel object captioning with visual vocabulary pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">TGIF-QA: toward spatiotemporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatially aware multimodal transformers for textvqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Icdar 2015 competition on robust reading. In ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Fernandez</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<meeting><address><addrLine>Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Hierarchical conditional relation networks for multimodal video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Thao Minh Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Tvr: A large-scale dataset for video-subtitle moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Effective and efficient vision-language learning by cross-modal skip-connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12005</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Value: A multi-task benchmark for video-and-language understanding evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><forename type="middle">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Swinbert: End-to-end transformers with sparse attention for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13196</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cascade reasoning network for text-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<editor>Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger Zimmermann</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sibnet: Sibling convolutional encoder for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Enhancing textual cues in multi-modal transformers for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 VizWiz Grand Challenge Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Liuyihang Song, Bin Wang, Yingya Zhang, and Pan Pan</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangliu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maskocr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00311</idno>
		<title level="m">Text recognition with masked encoder-decoder pretraining</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Ok-vqa: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Ocr-vqa: Visual question answering by reading text in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajeet</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph for video captioning with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Boxiao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahnakote</forename><surname>Trung Quy Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxuan</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Winner team mia at textvqa challenge 2021: Vision-and-language representation learning with pre-trained sequence-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15332</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhar</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahankote</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">End-to-end generative pretraining for multimodal video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08264</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Look before you speak: Visually contextualized utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">How much can CLIP benefit vision-and-language tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06383</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Textcaps: a dataset for image captioning with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">LXMERT: learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengyun</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05204</idno>
		<title level="m">Clip4caption ++: Multi-clip for video caption</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07303</idno>
		<title level="m">Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Controllable video captioning with pos sequence guidance based on gated fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">UFO: A unified transformer for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10023</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06946</idno>
		<title level="m">Minivlm: A smaller and faster vision-language model</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03052</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Towards accurate text-based image captioning with content diversity exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Probing inter-modality: Visual parsing with self-attention for vision-language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Probing inter-modality: Visual parsing with self-attention for vision-language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13488</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Just ask: Learning to answer questions from millions of narrated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Unified contrastive learning in image-text-label space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Crossing the format boundary of text and boxes: Towards unified vision-language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12085</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">TAP: text-aware pre-training for text-vqa and text-caption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Dinei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Flor?ncio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Towards accurate scene text recognition with semantic reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Legg</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<title level="m">Coca: Contrastive captioners are image-text foundation models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<meeting><address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Robustscanner: Dynamically enhancing positional clues for robust text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">MERLOT: multimodal neural script knowledge models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Object-aware aggregation with bidirectional temporal graph for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Vinvl: Making visual representations matter in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Open-book video captioning with retrieve-copy-generate network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Object relational graph with teacher-recommended learning for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaya</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Syntax-aware action targeting for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Vatex video captioning challenge 2020: Multi-view features and hybrid reward strategies for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longteng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11102</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Apache License 2</title>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark,MITlicense10https://github.com/openai/CLIP,MITlicense11https://github.com/microsoft/Oscar,MITlicense12https://github.com/kdexd/virtex,MITlicense" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
				<title level="m">spoon Pred: wall clock Pred: puck Pred: piggy bank Pred: steam locomotive Pred: minibus Pred: ice cream Pred: king penguin Pred: grey fox Pred: grasshopper Pred: pole Pred: bull mastiff Pred: lab coat Pred: puck Pred: photocopier Pred: gazelle Pred: gordon setter Pred: norfolk terrier Pred: plate Pred: sea cucumber Pred: cauliflower Pred: pirate Pred: fire screen Pred: black stork Pred: samoyed Pred: comic book Pred: traffic light Pred: dugong Pred: mountain bike Pred: hartebeest Pred: hyena Pred: rotisserie Pred: triumphal arch Pred: tree frog Pred: hand blower Figure 19: Visualization of correct predictions of our generative model on ImageNet-1K</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
