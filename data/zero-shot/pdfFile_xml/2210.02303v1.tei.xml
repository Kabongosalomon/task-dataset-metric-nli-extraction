<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMAGEN VIDEO: HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
							<email>jonathanho@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
							<email>williamchan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
							<email>sahariac@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
							<email>jwhang@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
							<email>ruiqig@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
							<email>agritsenko@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
							<email>pooleb@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<email>mnorouzi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
							<email>davidfleet@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
							<email>salimans@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IMAGEN VIDEO: HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial superresolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See imagen.research.google/video for samples. Figure 1: Imagen Video sample for the prompt: "A bunch of autumn leaves falling on a calm lake to form the text 'Imagen Video'. Smooth." The generated video is at 1280?768 resolution, 5.3 second duration and 24 frames per second. * Equal contribution. A colorful professional animated logo for 'Imagen Video' written using paint brush in cursive. Smooth animation. Blue flame transforming into the text "Imagen". Smooth animation Wooden figurine surfing on a surfboard in space.</p><p>Balloon full of water exploding in extreme slow motion.</p><p>Melting pistachio ice cream dripping down the cone.</p><p>A british shorthair jumping over a couch.</p><p>Coffee pouring into a cup.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative modeling has made tremendous progress with recent text-to-image systems like DALL-E 2 <ref type="bibr">(Ramesh et al., 2022)</ref>, Imagen <ref type="bibr" target="#b35">(Saharia et al., 2022b)</ref>, Parti <ref type="bibr" target="#b50">(Yu et al., 2022)</ref>, CogView <ref type="bibr" target="#b9">(Ding et al., 2021)</ref> and Latent Diffusion <ref type="bibr">(Rombach et al., 2022)</ref>. Diffusion models <ref type="bibr" target="#b41">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b16">Ho et al., 2020)</ref> in particular have found considerable success in multiple generative modeling tasks <ref type="bibr">Dhariwal &amp; Nichol, 2022)</ref> including density estimation <ref type="bibr" target="#b21">(Kingma et al., 2021)</ref>, text-to-speech <ref type="bibr" target="#b6">(Chen et al., 2021a;</ref><ref type="bibr">Kong et al., 2021;</ref><ref type="bibr" target="#b7">Chen et al., 2021b)</ref>, image-to-image <ref type="bibr" target="#b36">(Saharia et al., 2022c;</ref><ref type="bibr">a;</ref><ref type="bibr">Whang et al., 2022)</ref>, and text-to-image <ref type="bibr">(Rombach et al., 2022;</ref><ref type="bibr">Ramesh et al., 2022;</ref><ref type="bibr" target="#b35">Saharia et al., 2022b)</ref>.</p><p>Figure 2: Videos generated from various text prompts. Imagen Video produces diverse and temporally-coherent videos that are well-aligned with the given prompt.</p><p>A small hand-crafted wooden boat taking off to space.</p><p>A person riding a bike in the sunset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drone flythrough interior of Sagrada Familia cathedral</head><p>Wooden figurine walking on a treadmill made out of exercise mat.</p><p>Origami dancers in white paper, 3D render, ultra-detailed, on white background, studio shot, dancing modern dance.</p><p>Campfire at night in a snowy forest with starry sky in the background.</p><p>An astronaut riding a horse. <ref type="figure">Figure 3</ref>: Videos generated from various text prompts. Imagen Video produces diverse and temporally-coherent videos that are well-aligned with the given prompt.</p><p>A person riding a horse in the sunrise.</p><p>A happy elephant wearing a birthday hat walking under the sea.</p><p>Studio shot of minimal kinetic sculpture made from thin wire shaped like a bird on white background.</p><p>A bunch of colorful candies falling into a tray in the shape of text 'Imagen Video'. Smooth video.</p><p>A group of people hiking in a forest.</p><p>A goldendoodle playing in a park by a lake.</p><p>Incredibly detailed science fiction scene set on an alien planet, view of a marketplace. Pixel art. <ref type="figure">Figure 4</ref>: Videos generated from various text prompts. Imagen Video produces diverse and temporally-coherent videos that are well-aligned with the given prompt.</p><p>A bunch of autumn leaves falling on a calm lake to form the text 'Imagen Video'. Smooth.</p><p>Pouring latte art into a silver cup with a golden spoon next to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shoveling snow.</head><p>Drone flythrough of a tropical jungle covered in snow A beautiful sunrise on mars, Curiosity rover. High definition, timelapse, dramatic colors A shark swimming in clear Carribean ocean.</p><p>A hand lifts a cup. <ref type="figure">Figure 5</ref>: Videos generated from various text prompts. Imagen Video produces diverse and temporally-coherent videos that are well-aligned with the given prompt.</p><p>Our work aims to generate videos from text. Prior work on video generation has focused on more restricted datasets with autoregressive models <ref type="bibr" target="#b31">(Ranzato et al., 2014;</ref><ref type="bibr" target="#b39">Shi et al., 2015;</ref><ref type="bibr" target="#b10">Finn et al., 2016;</ref><ref type="bibr" target="#b19">Kalchbrenner et al., 2017;</ref><ref type="bibr" target="#b2">Babaeizadeh et al., 2021)</ref>, latent-variable models with autoregressive priors <ref type="bibr" target="#b24">(Mathieu et al., 2016;</ref><ref type="bibr" target="#b47">Vondrick et al., 2016;</ref><ref type="bibr" target="#b1">Babaeizadeh et al., 2018;</ref><ref type="bibr" target="#b23">Kumar et al., 2020)</ref>, and more recently non-autoregressive latent-variable approaches <ref type="bibr" target="#b40">(Gupta et al., 2022)</ref>. Diffusion models have also shown promise for video generation <ref type="bibr" target="#b18">(Ho et al., 2022b)</ref> at moderate resolution.  showed autoregressive generation with a RNN-based model with conditional diffusion observations. The concurrent work of <ref type="bibr" target="#b40">Singer et al. (2022)</ref> also applied text-to-video modelling with diffusion models, but built on a pretrained text-to-image model. <ref type="bibr" target="#b12">Harvey et al. (2022)</ref> generates videos up to 25 minutes in length with video diffusion models, however the domain is restricted.</p><p>In this work, we introduce Imagen Video, a text-to-video generation system based on video diffusion models <ref type="bibr" target="#b18">(Ho et al., 2022b)</ref> that is capable of generating high definition videos with high frame fidelity, strong temporal consistency, and deep language understanding. Imagen Video scales from prior work of 64-frame 128?128 videos at 24 frames per second to 128 frame 1280?768 high-definition video at 24 frames per second. Imagen Video has a simple architecture: The model consists of a frozen T5 text encoder <ref type="bibr">(Raffel et al., 2020)</ref>, a base video diffusion model, and interleaved spatial and temporal super-resolution diffusion models. Our key contributions are as follows:</p><p>1. We demonstrate the simplicity and effectiveness of cascaded diffusion video models for high definition video generation. 2. We confirm that recent findings in the text-to-image setting transfer to video generation, such as the effectiveness of frozen encoder text conditioning and classifier-free guidance. 3. We show new findings for video diffusion models that have implications for diffusion models in general, such as the effectiveness of the v-prediction parameterization for sample quality and the effectiveness of progressive distillation of guided diffusion models for the text-conditioned video generation setting. 4. We demonstrate qualitative controllability in Imagen Video, such as 3D object understanding, generation of text animations, and generation of videos in various artistic styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IMAGEN VIDEO</head><p>Our model, Imagen Video, is a cascade of video diffusion models <ref type="bibr">b)</ref>. It consists of 7 sub-models which perform text-conditional video generation, spatial super-resolution, and temporal super-resolution. With the entire cascade, Imagen Video generates high definition 1280?768 (width ? height) videos at 24 frames per second, for 128 frames (? 5.3 seconds)-approximately 126 million pixels. We describe the components and techniques that constitute Imagen Video in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DIFFUSION MODELS</head><p>Imagen Video is built from diffusion models <ref type="bibr" target="#b41">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b43">Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b16">Ho et al., 2020)</ref> specified in continuous time <ref type="bibr" target="#b45">(Tzen &amp; Raginsky, 2019;</ref><ref type="bibr">Song et al., 2021;</ref><ref type="bibr" target="#b21">Kingma et al., 2021)</ref>. We use the formulation of <ref type="bibr" target="#b21">Kingma et al. (2021)</ref>: the model is a latent variable model with latents z = {z t | t ? [0, 1]} following a forward process q(z|x) starting at data x ? p(x). The forward process is a Gaussian process that satisfies the Markovian structure:</p><formula xml:id="formula_0">q(z t |x) = N (z t ; ? t x, ? 2 t I), q(z t |z s ) = N (z t ; (? t /? s )z s , ? 2 t|s I)<label>(1)</label></formula><p>where 0 ? s &lt; t ? 1, ? 2 t|s = (1 ? e ?t??s )? 2 t , and ? t , ? t specify a noise schedule whose log signalto-noise-ratio ? t = log[? 2 t /? 2 t ] decreases monotonically with t until q(z 1 ) ? N (0, I). We use a continuous time version of the cosine noise schedule ). The generative model is a learned model that matches this forward process in the reverse time direction, generating z t starting from t = 1 and ending at t = 0.</p><p>Learning to reverse the forward process for generation can be reduced to learning to denoise z t ? q(z t |x) into an estimatex ? (z t , ? t ) ? x for all t. Like <ref type="bibr" target="#b43">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b16">Ho et al., 2020)</ref> and most follow-up work, we optimize the model by minimizing a simple noise-prediction loss:</p><formula xml:id="formula_1">L(x) = E ?N (0,I),t?U (0,1) ? ? (z t , ? t ) ? 2 2 (2) where z t = ? t x + ? t , and? ? (z t , ? t ) = ? ?1 t (z t ? ? tx? (z t , ? t ))</formula><p>. We will drop the dependence on ? t to simplify notation. In practice, we parameterize our models in terms of the v-parameterization <ref type="bibr">(Salimans &amp; Ho, 2022)</ref>, rather than predicting or x directly; see Section 2.4.</p><p>For conditional generative modeling, we provide the conditioning information c drawn jointly with x to the model asx ? (z t , c t ). We use these conditional diffusion models for spatial and temporal super-resolution in our pipeline of diffusion models: in these cases, c includes both the text and the previous stage low resolution video as well as a signal ? t that describes the strength of conditioning augmentation added to c. <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref> found it critical to condition all the super-resolution models with the text embedding, and we follow this approach.</p><p>We use the discrete time ancestral sampler <ref type="bibr" target="#b16">(Ho et al., 2020)</ref>, with sampling variances derived from lower and upper bounds on reverse process entropy <ref type="bibr" target="#b41">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b16">Ho et al., 2020;</ref>. This sampler can be formulated by using a reversed description of the forward process as q(z s |z t ,</p><formula xml:id="formula_2">x) = N (z s ;? s|t (z t , x),? 2 s|t I) (noting s &lt; t), wher? ? s|t (z t , x) = e ?t??s (? s /? t ) z t + (1 ? e ?t??s )? s x and? 2 s|t = (1 ? e ?t??s )? 2 s .<label>(3)</label></formula><p>Starting at z 1 ? N (0, I), the ancestral sampler follows the rule</p><formula xml:id="formula_3">z s =? s|t (z t ,x ? (z t )) + (? 2 s|t ) 1?? (? 2 t|s ) ?<label>(4)</label></formula><p>where is standard Gaussian noise, ? is a hyperparameter that controls the stochasticity of the sampler , and s, t follow a uniformly spaced sequence from 1 to 0. See Section 3 for sampler hyperparameter settings.</p><p>Alternatively, the deterministic DDIM sampler <ref type="bibr" target="#b42">(Song et al., 2020)</ref> can be used for sampling. This sampler is a numerical integration rule for the probability flow ODE <ref type="bibr">(Song et al., 2021;</ref><ref type="bibr">Salimans &amp; Ho, 2022)</ref>, which describes how a sample from a standard normal distribution can be deterministically transformed into a sample from the video data distribution using the denoising model. The DDIM sampler is useful for progressive distillation for fast sampling, as described in Section 2.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CASCADED DIFFUSION MODELS AND TEXT CONDITIONING</head><p>Cascaded Diffusion Models  are an effective method for scaling diffusion models to high resolution outputs, finding considerable success in both class-conditional ImageNet  and text-to-image generation <ref type="bibr">(Ramesh et al., 2022;</ref><ref type="bibr" target="#b35">Saharia et al., 2022b)</ref>. Cascaded diffusion models generate an image or video at a low resolution, then sequentially increase the resolution of the image or video through a series of super-resolution diffusion models. Cascaded Diffusion Models can model very high dimensional problems while still keeping each sub-model relatively simple. Imagen <ref type="bibr" target="#b35">(Saharia et al., 2022b)</ref> also showed that by conditioning on text embeddings from a large frozen language model in conjunction with cascaded diffusion models, one can generate high quality 1024 ? 1024 images from text descriptions. In this work we extend this approach to video generation. <ref type="figure">Figure 6</ref> summarizes the entire cascading pipeline of Imagen Video. In total, we have 1 frozen text encoder, 1 base video diffusion model, 3 SSR (spatial super-resolution), and 3 TSR (temporal superresolution) models -for a total of 7 video diffusion models, with a total of 11.6B diffusion model parameters. The data used to train these models is processed to the appropriate spatial and temporal resolutions by spatial resizing and frame skipping. At generation time, the SSR models increase spatial resolution for all input frames, whereas the TSR models increase temporal resolution by filling in intermediate frames between input frames. All models generate an entire block of frames simultaneously -so for instance, our SSR models do not suffer from obvious artifacts that would occur from naively running super-resolution on independent frames.</p><p>One benefit of cascaded models is that each diffusion model can be trained independently, allowing one to train all 7 models in parallel. Additionally, our super-resolution models are general purpose video super-resolution models, and they can be applied to real videos or samples from generative models other than the ones presented in this paper. This is similar to how Imagen's super-resolution models helped improve the fidelity of the images generated by Parti <ref type="bibr" target="#b50">(Yu et al., 2022)</ref>, which is an autoregressive text-to-image model. We intend to explore hybrid pipelines of multiple model classes further in future work. Input Text Prompt <ref type="figure">Figure 6</ref>: The cascaded sampling pipeline starting from a text prompt input to generating a 5.3second, 1280?768 video at 24fps. "SSR" and "TSR" denote spatial and temporal super-resolution respectively, and videos are labeled as frames?width?height. In practice, the text embeddings are injected into all models, not just the base model.  Our base model uses spatial convolutions, spatial self-attention and temporal self-attention. For memory efficiency, our spatial and temporal super-resolution models use temporal convolutions instead of attention, and our models at the highest spatial resolution do not have spatial attention.</p><p>Similar to <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref>, we utilize contextual embeddings from a frozen T5-XXL text encoder <ref type="bibr">(Raffel et al., 2020)</ref> for conditioning on the input text prompt. We find these embeddings to be critical for alignment between generated video and the text prompt. Similar to the findings of <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref>, we observe evidence of deeper language understanding, enabling us to generate the videos displayed in Figs. 2 to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">VIDEO DIFFUSION ARCHITECTURES</head><p>Diffusion models for image generation typically use a 2D U-Net architecture <ref type="bibr" target="#b33">(Ronneberger et al., 2015;</ref><ref type="bibr" target="#b38">Salimans et al., 2017;</ref><ref type="bibr" target="#b16">Ho et al., 2020)</ref> to represent the denoising modelx ? . This is a multiscale model consisting of multiple layers of spatial attention and convolution at each resolution, combined with shortcuts between layers at the same resolution. In earlier work on Video Diffusion Models, <ref type="bibr" target="#b18">Ho et al. (2022b)</ref> introduced the Video U-Net , which generalizes the 2D diffusion model architecture to 3D in a space-time separable fashion using temporal attention and convolution layers interleaved within spatial attention and convolution layers to capture dependencies between video frames. Our work builds on the Video U-Net architecture: see <ref type="figure" target="#fig_1">Figure 7</ref>. Following Video Diffusion Models, each of our denoising modelsx ? operate on multiple video frames simultaneously and thereby generate entire blocks of video frames at a time, which we find to be important to capture the temporal coherence of the generated video compared to frame-autoregressive approaches. Our spatial super-resolution (SSR) and temporal super-resolution (TSR) models condition on their input videos by concatenating an upsampled conditioning input channelwise to the noisy data z t , the same mechanism as SR3 <ref type="bibr" target="#b36">(Saharia et al., 2022c)</ref> and Palette : spatial upsampling before concatenation is performed using bilinear resizing, and temporal upsampling before concatenation is performed by repeating frames or by filling in blank frames.</p><p>Our base video model, which is the first model in the pipeline that generates data at the lowest frame count and spatial resolution, uses temporal attention to mix information across time. Our SSR and TSR models, on the other hand, use temporal convolutions instead of temporal attention. The temporal attention in the base model enables Imagen Video to model long term temporal dependencies, while the temporal convolutions in the SSR and TSR models allow Imagen Video to maintain local temporal consistency during upsampling. The use of temporal convolutions lowers memory and computation costs over temporal attention-this is crucial because the very purpose of the TSR and SSR models is to operate at high frame rates and spatial resolutions. In our initial experiments, we did not find any significant improvements when using temporal attention over temporal convolutions in our SSR and TSR models, which we hypothesize is due to the significant amount of temporal correlation already present in the conditioning input to these models.</p><p>Our models also use spatial attention and spatial convolutions. The base model and the first two spatial super-resolution models have spatial attention in addition to spatial convolutions. We found this to improve sample fidelity. However, as we move to higher resolutions, we switch to fully convolutional architectures, like <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref>, to minimize memory and compute costs in order to generate 1280?768 resolution data. The highest resolution SSR model in our pipeline is a fully convolutional model trained on random lower resolution spatial crops for training time memory efficiency, and we find that the model easily generalizes to the full resolution during sampling time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">v-PREDICTION</head><p>We follow <ref type="bibr">Salimans &amp; Ho (2022)</ref> and use v-prediction parameterization (v t ? ? t ?? t x) for all our models. The v-parameterization is particularly useful for numerical stability throughout the diffusion process to enable progressive distillation for our models. For models that operate at higher resolution in our pipeline, we also discovered that the v-parameterization avoids color shifting artifacts that are known to affect high resolution diffusion models, and in the video setting it avoids temporal color shifting that sometimes appears with -prediction models. Our use of v-parameterization also has the benefit of faster convergence of sample quality metrics: see Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">CONDITIONING AUGMENTATION</head><p>We use noise conditioning augmentation  for all our temporal and spatial superresolution models. Noise conditioning augmentation has been found to be critical for cascaded diffusion models for class-conditional generation  as well as text-to-image models <ref type="bibr" target="#b35">(Saharia et al., 2022b)</ref>. In particular, it facilitates parallel training of different models in the cascade, as it reduces the sensitivity to domain gaps between the output of one stage of the cascade and the inputs used in training the subsequent stage.</p><p>Following , we apply Gaussian noise augmentation with a random signal-to-noise ratio to the conditioning input video during training, and this sampled signal-to-noise ratio is provided to the model as well. At sampling time we use a fixed signal-to-noise ratio such as 3 or 5, representing a small amount of augmentation that aids in removing artifacts in the samples from the previous stage while preserving most of the structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">VIDEO-IMAGE JOINT TRAINING</head><p>We follow <ref type="bibr" target="#b18">Ho et al. (2022b)</ref> in jointly training all the models in the Imagen Video pipeline on images and videos. During training, individual images are treated as single frame videos. We achieve this by packing individual independent images into a sequence of the same length as a video, and bypass the temporal convolution residual blocks by masking out their computation path. Similarly, we disable cross-frame temporal attention by applying masking to the temporal attention maps. This strategy allows us to use to train our video models on image-text datasets that are significantly larger and more diverse than available video-text datasets. Consistent with <ref type="bibr" target="#b18">Ho et al. (2022b)</ref>, we observe that joint training with images significantly increases the overall quality of video samples. Another interesting artifact of joint training is the knowledge transfer from images to videos. For instance, while training on natural video data only enables the model to learn dynamics in natural settings, the model can learn about different image styles (such as sketch, painting, etc.) by training on images.</p><p>As a result, this joint training enables the model to generate interesting video dynamics in different styles. See <ref type="figure" target="#fig_2">Fig. 8</ref> for such examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">CLASSIFIER FREE GUIDANCE</head><p>We found classifier free guidance  to be critical for generating high fidelity samples which respect a given text prompt. This is consistent with earlier results on text-to-image models <ref type="bibr">Ramesh et al., 2022;</ref><ref type="bibr" target="#b35">Saharia et al., 2022b;</ref><ref type="bibr" target="#b50">Yu et al., 2022)</ref>.</p><p>In the conditional generation setting, the data x is generated conditional on a signal c, which here represents a contextualized embedding of the text prompt, and a conditional diffusion model can be trained by using this signal c as an additional input to the denoising modelx ? (z t , c). After training,  find that sample quality can be improved by adjusting the denoising predictionx ? (z t , c) usingx</p><formula xml:id="formula_4">? (z t , c) = (1 + w)x ? (z t , c) ? wx ? (z t ),<label>(5)</label></formula><p>where w is the guidance strength,</p><formula xml:id="formula_5">x ? (z t , c) is the conditional model, andx ? (z t ) =x ? (z t , c = ?)</formula><p>is an unconditional model. The unconditional model is jointly trained with the conditional model by dropping out the conditioning input c. The predictions of the adjusted denoising modelx ? (z t , c) are clipped to respect the range of possible pixel values, which we discuss in more detail in the next section. Note that the linear transformation in Equation 5 can equivalently be performed in</p><formula xml:id="formula_6">v-space (? ? (z t , c) = (1 + w)v ? (z t , c) ? wv ? (z t )) or -space (? ? (z t , c) = (1 + w)? ? (z t , c) ? w? ? (z t )).</formula><p>For w &gt; 0 this adjustment has the effect of over-emphasizing the effect of conditioning on the signal c, which tends to produce samples of lower diversity but higher quality compared to sampling from the regular conditional model . The method can be interpreted as a way to guide the samples towards areas where an implicit classifier p(c|z t ) has high likelihood; as such, it is an adaptation of the explicit classifier guidance method proposed by <ref type="bibr">Dhariwal &amp; Nichol (2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">LARGE GUIDANCE WEIGHTS</head><p>When using large guidance weights, the resultingx ? (z t , c) must be projected back to the possible range of pixel values at every sampling step to prevent train-test mismatch. When using large guidance weights, the standard approach, i.e., clipping the values to the right range (e.g., np.clip(x, -1, 1)), leads to significant saturation artifacts in the generated videos. A similar effect was observed in <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref> for text-to-image generation. <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref> use dynamic thresholding to alleviate this saturation issue. Specifically, dynamic clipping involves clipping the image to a dynamically chosen threshold s followed by scaling by s (i.e., np.clip(x, -s, s) / s) <ref type="bibr" target="#b35">(Saharia et al., 2022b)</ref>.</p><p>Although dynamic clipping can help with over-saturation, we did not find it sufficient in initial experiments. We therefore also experiment with letting w oscillate between a high and a low guidance weight at each alternating sampling step, which we find significantly helps with these saturation issues. We call this sampling technique oscillating guidance. Specifically, we use a constant high guidance weight for a certain number of initial sampling steps, followed by oscillation between high and low guidance weights: this oscillation is implemented simply by alternating between a large weight (such as 15) and a small weight (such as 1) over the course of sampling. We hypothesize that a constant high guidance weight at the start of sampling helps break modes with heavy emphasis on text, while oscillating between high and low guidance weights helps maintain a strong text alignment (via high guidance sampling step) while limiting saturation artifacts (via low guidance sampling step). We however observed no improvement in sample fidelity and more visual artifacts when applying oscillating guidance to models past the 80?48 spatial resolution. Thus we only apply oscillating guidance to the base and the first two SR models. <ref type="bibr">Salimans &amp; Ho (2022)</ref> proposed progressive distillation to enable fast sampling of diffusion models. This method distills a trained deterministic DDIM sampler <ref type="bibr" target="#b42">(Song et al., 2020</ref>) to a diffusion model that takes many fewer sampling steps, without losing much perceptual quality. At each iteration of the distillation process, an N -step DDIM sampler is distilled to a new model with N/2-steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">PROGRESSIVE DISTILLATION WITH GUIDANCE AND STOCHASTIC SAMPLERS</head><p>This procedure is repeated by halving the required sampling steps each iteration. <ref type="bibr">Meng et al. (2022)</ref> extend this approach to samplers with guidance, and propose a new stochastic sampler for use with distilled models. Here we show that this approach also works very well for video generation.</p><p>We use a two-stage distillation approach to distill a DDIM sampler <ref type="bibr" target="#b42">(Song et al., 2020)</ref> with classifierfree guidance. At the first stage, we learn a single diffusion model that matches the combined output from the jointly trained conditional and unconditional diffusion models, where the combination coefficients are determined by the guidance weight. Then we apply progressive distillation to that single model to produce models requiring fewer sampling steps at the second stage.</p><p>After distillation, we use a stochastic N -step sampler: At each step, we first apply one deterministic DDIM update with twice the original step size (i.e., the same step size as a N/2-step sampler), and then we perform one stochastic step backward (i.e., perturbed with noise following the forward diffusion process) with the original step size, inspired by <ref type="bibr" target="#b20">Karras et al. (2022)</ref>. See <ref type="bibr">Meng et al. (2022)</ref> for more details. Using this approach, we are able to distill all 7 video diffusion models down to just 8 sampling steps per model without any noticeable loss in perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We train our models on a combination of an internal dataset consisting of 14 million video-text pairs and 60 million image-text pairs, and the publicly available LAION-400M image-text dataset. To process the data into a form suitable for training our cascading pipeline, we spatially resize images and videos using antialiased bilinear resizing, and we temporally resize videos by skipping frames. Throughout our model development process, we evaluated Imagen Video on several different metrics, such as FID on individual frames <ref type="bibr" target="#b14">(Heusel et al., 2017)</ref>, FVD <ref type="bibr" target="#b46">(Unterthiner et al., 2019)</ref> for temporal consistency, and frame-wise CLIP scores <ref type="bibr" target="#b13">(Hessel et al., 2021;</ref><ref type="bibr">Park et al., 2021)</ref> for videotext alignment. Below, we explore the capabilities of our model and investigate its performance in regards to 1) scaling up the number of parameters in our model, 2) changing the parameterization of our model, and 3) distilling our models so that they are fast to sample from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UNIQUE VIDEO GENERATION CAPABILITIES</head><p>We find that Imagen Video is capable of generating high fidelity video, and that it possesses several unique capabilities that are not traditionally found in unstructured generative models learned purely from data. For example, <ref type="figure" target="#fig_2">Fig. 8</ref> shows that our model is capable of generating videos with artistic styles learned from image information, such as videos in the style of van Gogh paintings or watercolor paintings. <ref type="figure" target="#fig_3">Fig. 9</ref> shows that Imagen Video possesses an understanding of 3D structure, as it is capable of generating videos of objects rotating while roughly preserving structure. While the 3D consistency over the course of rotation is not exact, we believe Imagen Video shows that video models can serve as effective priors for methods that do force 3D consistency. <ref type="figure" target="#fig_4">Fig. 10</ref> shows that Imagen Video is also reliably capable of generating text in a wide variety of animation styles, some of which would be difficult to animate using traditional tools. We see results such as these as an exciting indication of how general purpose generative models such as Imagen Video can significantly decrease the difficulty of high quality content generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCALING</head><p>In <ref type="figure" target="#fig_5">Figure 11</ref> we show that our base video model strongly benefits from scaling up the parameter count of the video U-Net. We performed this scaling by increasing the base channel count and depth of the network. This result is contrary to the text-to-image U-Net scaling results by <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref>, which found limited benefit from diffusion model scaling when measured by image-text sample quality scores. We conclude that video modeling is a harder task for which performance is not yet saturated at current model sizes, implying future benefits to further model scaling for video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COMPARING PREDICTION PARAMETERIZATIONS</head><p>In early experiments we found that training -prediction models <ref type="bibr" target="#b16">(Ho et al., 2020)</ref> performed worse than v-prediction <ref type="bibr">(Salimans &amp; Ho, 2022)</ref> especially at high resolutions. Specifically, for high res-</p><p>A cat eating food out of a bowl, in style of Van Gogh.</p><p>A drone flythrough over a watercolor painting of a forest.</p><p>Drone flythrough of a pixel art of futuristic city.    olution SSR models, we observed that -prediction converges relatively slowly in terms of sample quality metrics and suffers from color shift and color inconsistency across frames in the generated videos. <ref type="figure">Fig. 12</ref> shows the comparison between -prediction and v-prediction on a 80?48 ? 320?192 video spatial super-resolution task. It is clear that -parameterization produces worse generations than v-parameterization. <ref type="figure">Fig. 13</ref> shows the quantitative comparison between the two parameterizations as a function of training steps. We observe that v parameterization converges much more faster than parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">PERCEPTUAL QUALITY AND DISTILLATION</head><p>In <ref type="table" target="#tab_1">Table 1</ref> we report perceptual quality metrics (CLIP score and CLIP R-Precision) for our model samples, as well as for their distilled version. Samples are generated and evaluated at 192?320 resolution for 128 frames at 24 frames per second. For CLIP score, we take the average score over all frames. For CLIP R-Precision (Park et al., 2021) we compute the top-1 accuracy (i.e. R = 1), treating the frames of a video sample as images sharing the same text label (the prompt). We repeat these over four different runs and report the mean and standard error.  <ref type="figure">Figure 13</ref>: Comparison between 80?48 ? 320?192 SSR models trained with -and v-prediction parameterizations. We report FID evaluated on the first upsampled frame; FVD score is excessively noisy for the -prediction model. We observe that the sample quality of the -prediction model converges much more slowly than that of the v-prediction model.</p><p>We find that distillation provides a very favorable trade-off between sampling time and perceptual quality: the distilled cascade is about 18? faster, while producing videos of similar quality to the samples from the original models. In terms of FLOPs, the distilled models are about 36? more efficient: The original cascade evaluates each model twice (in parallel) to apply classifier-free guidance, while our distilled models do not, since they distilled the effect of guidance into a single model. We provide samples from our original and distilled cascade in <ref type="figure">Figure 14</ref> for illustration. Sampling from the original pipeline takes 618 seconds for one batch of samples, while sampling from the distilled pipeline takes 35 seconds, making the distilled pipeline about 18? faster. We also explored two different classifier-free guidance settings for the base models: constant guidance with w = 6 and oscillating guidance which alternates between w = 15 and w = 1, following <ref type="bibr" target="#b35">Saharia et al. (2022b)</ref>. When using oscillating guidance, the fully distilled pipeline performs the same as the original model, or even slightly better. When using fixed guidance, our fully distilled pipeline scores slightly lower than the original model, though the difference is minor. Combining the original base model with fixed guidance and distilled super-resolution models produced the highest CLIP score. For all models, generated samples obtain better perceptual quality metrics than the original ground truth data: By using classifier-free guidance our models sample from a distribution tilted towards these quality metrics, rather than from an accurate approximation of the original data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LIMITATIONS AND SOCIETAL IMPACT</head><p>Generative modeling has made tremendous progress, especially in recent text-to-image models <ref type="bibr" target="#b35">(Saharia et al., 2022b;</ref><ref type="bibr">Ramesh et al., 2022;</ref><ref type="bibr">Rombach et al., 2022)</ref>. Imagen Video is another step forward in generative modelling capabilities, advancing text-to-video AI systems. Video generative models can be used to positively impact society, for example by amplifying and augmenting human creativity. However, these generative models may also be misused, for example to generate fake, hateful, explicit or harmful content. We have taken multiple steps to minimize these concerns, for <ref type="figure">Figure 14</ref>: Frames from videos generated by Imagen Video for the text prompt "A teddy bear wearing sunglasses playing guitar next to a cactus." The samples on the left are produced by our original model cascade, while the samples on the right are from our distilled cascade with 8 sampling steps per stage. Both used constant guidance with w = 6 and static clipping.</p><p>example in internal trials, we apply input text prompt filtering, and output video content filtering. However, there are several important safety and ethical challenges remaining. Imagen Video and its frozen T5-XXL text encoder were trained on problematic data <ref type="bibr" target="#b5">(Bordia &amp; Bowman, 2017;</ref><ref type="bibr" target="#b4">Birhane et al., 2021;</ref><ref type="bibr" target="#b3">Bender et al., 2021)</ref>. While our internal testing suggests much of explicit and violent content can be filtered out, there still exists social biases and stereotypes which are challenging to detect and filter. We have decided not to release the Imagen Video model or its source code until these concerns are mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We presented Imagen Video: a text-conditional video generation system based on a cascade of video diffusion models. By extending the text-to-image diffusion models of Imagen <ref type="bibr" target="#b35">(Saharia et al., 2022b)</ref> to the time domain, and training jointly on video and images, we obtained a model capable of generating high fidelity videos with good temporal consistency while maintaining the strong features of the original image system, such as the ability to accurately spell text. We transferred multiple methods from the image domain to video, such as v-parameterization <ref type="bibr">(Salimans &amp; Ho, 2022)</ref>, conditioning augmentation , and classifier-free guidance , and found that these are also useful in the video setting. Video modeling is computationally demanding, and we found that progressive distillation <ref type="bibr">(Salimans &amp; Ho, 2022;</ref><ref type="bibr">Meng et al., 2022</ref>) is a valuable technique for speeding up video diffusion models at sampling time. Given the tremendous recent progress in generative modeling, we believe there is ample scope for further improvements in video generation capabilities in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Video U-Net space-time separable block. Spatial operations are performed independently over frames with shared parameters, whereas the temporal operation mixes activations over frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Snapshots of frames from videos generated by Imagen Video demonstrating the ability of the model to generate dynamics in different artistic styles. A 3D model of a 1800s victorian house. Studio lighting. A 3D model of a car made out of sushi. Studio lighting. A 3D model of an elephant origami. Studio lighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Snapshots of frames from videos generated by Imagen Video demonstrating the model's understanding of 3D structures. A colorful professional animated logo for 'Diffusion' written using paint brush in cursive. Smooth animation. Sprouts in the shape of text 'Imagen' coming out of a fairytale book. Thousands of fast brush strokes slowly forming the text 'Imagen Video' on a light beige canvas. Smooth animation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Snapshots of frames from videos generated by Imagen Video demonstrating the ability of the model to render a variety of text with different style and dynamics. Comparison on CLIP scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Scaling Comparison for the base 16?40?24 video model on FVD and CLIP scores (on 0-100 scale). Both FVD and CLIP scores are computed on 4096 video samples. We see clear signs of improvement on both metrics when scaling from 500M to 1.6B to 5.6B parameters.80?48 input video frames SSR to 320?192 with -prediction SSR to 320?192 with v-predictionFigure 12: Comparison between -prediction (middle row) and v-prediction (bottom row) for a 8?80?48?8?320?192 spatial super-resolution architecture at 200k training steps. The frames from the -prediction model are generally worse, suffering from unnatural global color shifts across frames. The frames from the v-prediction model do not and are more consistent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>CLIP scores and CLIP R-Precision(Park et al., 2021)  values for generated samples and ground truth videos on prompts from our test set. Cells highlighted in green represent distilled models. We compare three different combinations: original pipeline, distilled SR models on top of original base model, and fully distilled pipeline. The original base models use 256 sampling steps, and original SR models use 128 steps. All distilled models use 8 sampling steps per stage.</figDesc><table><row><cell>Guidance w</cell><cell cols="5">Base Steps SR Steps CLIP Score CLIP R-Precision Sampling Time</cell></row><row><cell>constant=6</cell><cell>256</cell><cell>128</cell><cell>25.19?.03</cell><cell>92.12?.53</cell><cell>618 sec</cell></row><row><cell>oscillate(15,1)</cell><cell>256</cell><cell>128</cell><cell>25.02?.08</cell><cell>89.91?.96</cell><cell>618 sec</cell></row><row><cell>constant=6</cell><cell>256</cell><cell>8</cell><cell>25.29?.05</cell><cell>90.88?.50</cell><cell>135 sec</cell></row><row><cell>oscillate(15,1)</cell><cell>256</cell><cell>8</cell><cell>25.15?.09</cell><cell>88.78?.69</cell><cell>135 sec</cell></row><row><cell>constant=6</cell><cell>8</cell><cell>8</cell><cell>25.03?.05</cell><cell>89.68?.38</cell><cell>35 sec</cell></row><row><cell>oscillate(15,1)</cell><cell>8</cell><cell>8</cell><cell>25.12?.07</cell><cell>90.97?.46</cell><cell>35 sec</cell></row><row><cell>ground truth</cell><cell></cell><cell></cell><cell>24.27</cell><cell>86.18</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>We give special thanks to Jordi Pont-Tuset and Shai Noy for engineering support. We also give thanks to our artist friends, Alexander Chen, Irina Blok, Ian Muldoon, Daniel Smith, and Pedro Vergani for helping us test Imagen Video and lending us their amazing creativity. We are extremely grateful for the support from Erica Moreira for compute resources. Finally, we give thanks to Elizabeth Adkison, James Bradbury, Nicole Brichtova, Tom Duerig, Douglas Eck, Dumitru Erhan, Zoubin Ghahramani, Kamyar Ghasemipour, Victor Gomes, Blake Hechtman, Jonathan Heek, Yash Katariya, Sarah Laszlo, Sara Mahdavi, Anusha Ramesh, Tom Small, and Tris Warkentin for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laion-400m</surname></persName>
		</author>
		<ptr target="https://laion.ai/blog/laion-400-open-dataset/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1710.11252</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fitvid: Overfitting in pixel-level video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taghi</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.13195" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FAccT 2021</title>
		<meeting>FAccT 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multimodal datasets: misogyny, pornography, and malignant stereotypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vinay Uday Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kahembwe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01963</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identifying and Reducing Gender Bias in Word-Level Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wave-Grad: Estimating Gradients for Waveform Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cogview: Mastering text-to-image generation via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2105.13290" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1605.07157</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart&amp;apos;in-Mart&amp;apos;in</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Maskvit: Masked visual pre-training for video prediction. ArXiv, abs/2206.11894, 2022</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flexible diffusion modeling of long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Naderiparizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Weilbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<idno>abs/2205.11495</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Clipscore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">and Tim Salimans. Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<title level="m">Video Diffusion Models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1610.00527</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Elucidating the design space of diffusionbased generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00364</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Variational diffusion models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Videoflow: A conditional flow-based model for stochastic video generation. arXiv: Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On distillation of guided diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew Pamela Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmark for compositional text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=bKBhQhPeKaF" />
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">D</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<idno>abs/1412.6604</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis with Latent Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Palette: Image-to-Image Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Seyed Kamyar Seyed Ghasemipour</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Progressive Distillation for Fast Sampling of Diffusion Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Make-A-Video: Text-to-Video Generation without Text-Video Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Denoising diffusion implicit models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Generative Modeling by Estimating Gradients of the Data Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09883</idno>
		<title level="m">Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">FVD: A new Metric for Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop: Deep Generative Models for Highly Structured Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1609.02612</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Chitwan Saharia, Alexandros G. Dimakis, and Peyman Milanfar. Deblurring via Stochastic Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Diffusion Probabilistic Modeling for Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09481</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Scaling Autoregressive Models for Content-Rich Text-to-Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Burcu Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui Wu Jason</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10789</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
