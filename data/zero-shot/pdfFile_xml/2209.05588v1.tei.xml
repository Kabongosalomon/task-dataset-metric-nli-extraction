<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CenterFormer: Center-based Transformer for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
							<email>zhouzixiang@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Imaging Lab</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computational Imaging Lab</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computational Imaging Lab</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
							<email>panqu.wang@tusimple.aihassan.foroosh@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Imaging Lab</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computational Imaging Lab</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CenterFormer: Center-based Transformer for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LiDAR point cloud</term>
					<term>3D Object detection</term>
					<term>Transformer</term>
					<term>Multi- frame fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Query-based transformer has shown great potential in constructing long-range attention in many image-domain tasks, but has rarely been considered in LiDAR-based 3D object detection due to the overwhelming size of the point cloud data. In this paper, we propose CenterFormer, a center-based transformer network for 3D object detection. CenterFormer first uses a center heatmap to select center candidates on top of a standard voxel-based point cloud encoder. It then uses the feature of the center candidate as the query embedding in the transformer. To further aggregate features from multiple frames, we design an approach to fuse features through cross-attention. Lastly, regression heads are added to predict the bounding box on the output center feature representation. Our design reduces the convergence difficulty and computational complexity of the transformer structure. The results show significant improvements over the strong baseline of anchor-free object detection networks. CenterFormer achieves state-of-the-art performance for a single model on the Waymo Open Dataset, with 73.7% mAPH on the validation set and 75.6% mAPH on the test set, significantly outperforming all previously published CNN and transformer-based methods. Our code is publicly available at https://github.com/TuSimple/centerformer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>LiDAR is an important sensing and perception tool in autonomous driving due to its ability to provide highly accurate 3D point cloud data of the scanned environment. LiDAR-based 3D object detection aims to detect the bounding boxes of the objects in the LiDAR point cloud. Compared to image-domain object detection, the scanned points in LiDAR data may be sparse and irregularly spaced depending on the distance from the sensor. Most recent methods rely on discretizing the point clouds into voxels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b48">49]</ref> or projected bird's eye view (BEV) feature maps <ref type="bibr" target="#b17">[18]</ref> to use 2D or 3D convolution networks. Sometimes, it requires a second stage RCNN <ref type="bibr" target="#b10">[11]</ref>-style refinement network to compensate for the information loss in the voxelization. However, current two-stage networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b52">53]</ref> lack contextual and global information learning. They only use the local features of the proposal (RoI) to refine the results. The features in other boxes or neighboring positions that could also be beneficial to the refinement are neglected. Moreover, the environment of the autonomous driving scene is not stationary. The local feature learning has more limitations when using a sequence of scans.</p><p>In the image domain, transformer encoder-decoder structure has become a competitive method for the detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b63">64]</ref> and segmentation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b1">2]</ref> tasks. The transformer is able to capture long-range contextual information in the whole feature map and different feature domains. One of the most representative methods is DETR <ref type="bibr" target="#b3">[4]</ref>, which uses the parametric query to directly learn object information from an encoder-decoder transformer. DETR is trained end-to-end as a set matching problem to avoid any handcrafting processes like non-maximum suppression (NMS). However, there are two major problems in the DETRstyle encoder-decoder transformer network: First, the computational complexity grows quadratically as the input size increases. This limits the transformer to take only low-dimensional features as input which leads to low performance on small objects. Second, the query embedding is learned through the network so that the training is hard to converge.</p><p>Can we design a transformer encoder-decoder network for the LiDAR point cloud in order to better perceive the global connection of point cloud data? Considering the sheer size of LiDAR point cloud data, and the relatively small sizes of objects to be detected, voxel or BEV feature map representations need to be large enough to keep the features for such objects to be separable. As a result, it is impractical to use the transformer encoder structure on the feature map due to the large input size. In addition, if we use a large feature map for the transformer decoder, the query embedding is also difficult to focus on meaningful attention during training. To mitigate these converging problems, one solution is to provide the transformer with a good initial query embedding and confine the attention learning region to a smaller range. In the center-based 3D object detection network <ref type="bibr" target="#b52">[53]</ref>, the feature at the center of an object is used to capture all object information, hence the center feature is a good substitute for the object feature embedding. Multi-scale image pyramid and deformable convolution <ref type="bibr" target="#b15">[16]</ref> are two common methods to increase the receptive field of the feature learning without significantly increasing the complexity. Some recent works <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b14">15]</ref> apply these two methods in the transformer networks.</p><p>Taking the aforementioned aspects into consideration, we propose a centerbased transformer network, named Center Transformer (CenterFormer), for 3D object detection. Specifically, we first use a standard voxel-based backbone network to encode the point cloud into a BEV feature representation. Next, we employ a multi-scale center proposal network to convert the feature into different scales and predict the initial center locations. The feature at the proposed center is fed into a transformer decoder as the query embedding. In each transformer module, we use a deformable cross attention layer to efficiently aggregate the features from the multi-scale feature map. The output object representation then regresses to other object properties to create the final object prediction. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our method can model object-level connection and longrange feature attention. To further explore the ability of the transformer, we also propose a multi-frame design to fuse features from different frames through crossattention. We test CenterFormer on the large scale Waymo Open Dataset <ref type="bibr" target="#b37">[38]</ref> and the nuScenes dataset <ref type="bibr" target="#b2">[3]</ref>. Our method outperforms the popular center-based 3D object detection networks which are dominant on public benchmarks by a large margin, achieving state-of-the-art performance, with 73.7% and 75.6% mAPH on the waymo validation and test sets, respectively. The contributions of our method can be summarised as follows:</p><p>-We introduce a center-based transformer network for 3D object detection.</p><p>-We use the center feature as the initial query embedding to facilitate learning of the transformer. -We propose a multi-scale cross-attention layer to efficiently aggregate neighboring features without significantly increasing the computational complexity. -We propose using the cross-attention transformer to fuse object features from different frames. -Our method outperforms all previously published methods by a large margin, setting a new state-of-the-art performance on the Waymo Open Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LiDAR-based 3D Object Detection</head><p>Compared to the well-established point cloud processing networks like Point-Net <ref type="bibr" target="#b30">[31]</ref> and PointNet++ <ref type="bibr" target="#b31">[32]</ref>, most recent LiDAR detection and segmentation methods <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63]</ref> voxelize the point cloud in a fixed 3D space into a BEV/voxel representation and use conventional 2D/3D convolutional networks to predict the 3D bounding boxes. Other methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9]</ref> detect the objects on a projected range image. There are also some methods that use hybrid features along with the voxel network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51]</ref>, and combine multi-view features in the voxel feature representation <ref type="bibr" target="#b33">[34]</ref>. VoxelNet <ref type="bibr" target="#b58">[59]</ref> uses a PointNet inside each voxel to encode all points into a voxel feature. This feature encoder later became an essential method in voxel-based point cloud networks. PointPillar <ref type="bibr" target="#b17">[18]</ref> proposes the pillar feature encoder to directly encode the point cloud into the BEV feature map so that only 2D convolution is needed in the network. Similar to image object detection, 3D object detection methods can be divided into anchor-based <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b48">49]</ref> and anchor-free <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b9">10]</ref> methods. Anchorbased methods detect objects through a classification of all predefined object anchors, while anchor-free methods generally consider objects as keypoints and find those keypoints at the local heatmap maxima. Even though anchor-based methods can achieve a good performance, they rely heavily on hyper-parameter tuning. On the other hand, as the anchor-free methods become more prevailing in image-domain tasks, many 3D and LiDAR works have adopted the same design and show a more efficient and competitive performance. Many works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref> also require an RCNN-style second stage refinement. Feature maps for each bounding box proposal are aggregated through RoIAlign or RoIPool. Center-Point <ref type="bibr" target="#b52">[53]</ref> detects objects using center heatmap and regresses other bounding box information using center feature representation.</p><p>Most methods directly concatenate points from different frames based on the ego-motion estimation to use the multi-frame information. This assumes the model can align object features from different frames. However, independently moving objects cause misalignment of features across frames. Recent multi-frame methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b51">52]</ref> use an LSTM or a GNN module to fuse the previous state feature with the current feature map. 3D-MAN <ref type="bibr" target="#b49">[50]</ref> uses a multi-frame alignment and aggregation module to learn the temporal attention of predictions from multiple frames. The feature of each box is generated from the RoI pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformer</head><p>Originally proposed in the Natural Language Processing (NLP) community, transformer <ref type="bibr" target="#b39">[40]</ref> is becoming a competitive feature learning module in computer vision. Compared to traditional CNN, the transformer has a bigger receptive field, and feature aggregation is based on the response learned directly from pairwise features. A transformer encoder <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b14">15]</ref> usually serves as a replacement for the convolution layer in the backbone network. Meanwhile, the transformer decoder uses high-level query feature embedding as the input and extracts features from feature encoding through cross-attention, which is more common in detection and segmentation tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b42">43]</ref>. DETR <ref type="bibr" target="#b3">[4]</ref> uses a transformer encoderdecoder structure to predict objects from learned query embedding. Deformable DETR <ref type="bibr" target="#b63">[64]</ref> improves the DETR training through a deformable attention layer. Some recent methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b54">55]</ref> show that DETR is easier to converge using guidance like anchor boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">3D Transformer</head><p>An important design in transformer structure is the position embedding due to permutation invariance of the transformer input. However, 3D point clouds already have position information in them, which leads to deviance in the design of 3D transformers. Point transformer <ref type="bibr" target="#b56">[57]</ref> proposes a point transformer layer in the PointNet structure, where the position embedding in the transformer is the pairwise point distances. 3DETR <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b22">[23]</ref> use a DETR-style transformer decoder in the point cloud except that the query embedding in the decoder is sampled from Farthest Point Sampling (FPS) and learned through classification. Voxel Transformer <ref type="bibr" target="#b24">[25]</ref> introduces a voxel transformer layer to replace the sparse convolution layer in the voxel-based point cloud backbone network. SST <ref type="bibr" target="#b7">[8]</ref> uses a Single-stride Sparse Transformer as the backbone network to prevent information loss in downsampling of the previous 3D object detector. CT3D <ref type="bibr" target="#b32">[33]</ref> uses a transformer to learn a refinement of the initial prediction from local points. In contrast to the above methods, our CenterFormer tailors the DETR to work on LiDAR point clouds with lower memory usage and faster convergence. Moreover, CenterFormer can learn both object-level self-attentions and local cross-attentions without requiring a first-stage bounding box prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Center-based 3D Object Detection is motivated by the recent anchor-free image-domain object detection methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>. It detects each object as a center keypoint by predicting a heatmap on the BEV feature map. Given the output of a common voxel point cloud feature encoder M ? R h * w * c , where h and w are the BEV map size and c is the feature dimension, center-based LiDAR object detection predicts both a center heatmap H ? R h * w * l and the box regression B ? R h * w * 8 through two separated heads. Center heatmap H has l channels, one for each object class. In training, the ground truth is generated from the Gaussian heatmap of the annotated box center. Box regression B contains 8 object properties: the grid offset from the predicted grid center to the real box center, the height of the object, the 3D size, and the yaw rotation angle. During the evaluation, it takes the class and regression predictions at the top N highest heatmap scores and uses NMS to predict the final bounding box. Transformer Decoder aggregates features from the source representation to each query based on the query-key pairwise attention. Each transformer module consists of three layers: A multi-head self-attention layer, a multi-head crossattention layer, and a feed-forward layer. In each layer, there is also a skip connection that connects the input and the output features and layer normalization. Let f q and f k be the query feature and key feature. The multi-head attention can be formulated as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Cross-Attention</head><formula xml:id="formula_0">f out i = M m=1 W m [ j??j ?( Q i K j ? d ) ? V j ]<label>(1)</label></formula><formula xml:id="formula_1">Q i = f q i W q + E pos i , K j = f k j W k + E pos j , V j = f k j W v<label>(2)</label></formula><p>where i and j are the indices of query feature and source feature respectively, m is the head index, ? j is the set of attending key features, ? is the softmax function, d is the feature dimension, E pos is the position embedding and W is the learnable weight. In the self-attention layer, the query feature and the key feature come from the same set of query feature embedding, while in the cross-attention layer, the set of key features is the source feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Center Transformer</head><p>The architecture of our model is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We use a standard sparse voxel-based backbone network <ref type="bibr" target="#b52">[53]</ref> to process each point cloud into a BEV feature representation. We then encode the BEV feature into a multi-scale feature map and predict the center proposals. The proposed centers are then used as the query feature embedding in a transformer decoder to aggregate features from other centers and from multi-scale feature maps. Finally, we use a regression head to predict the bounding box at each enhanced center feature. In our multi-frame CenterFormer, the last BEV features of frames are fused together in both the center prediction stage and the cross-attention transformer.</p><p>Multi-scale Center Proposal Network A DETR-style transformer encoder requires the feature map to be compressed into a small size so that the computation cost is acceptable. This makes the network lose fine-grained features that are crucial for the detection of small objects, which typically occupy &lt; 1% of the space in the BEV map. Therefore, we propose a multi-scale center proposal network (CPN) to replace the transformer encoder for the BEV feature. In order to prepare a multi-scale feature map, we use a feature pyramid network to process the BEV feature representation into three different scales. At the end of each scale, we add a convolutional block attention module (CBAM) <ref type="bibr" target="#b44">[45]</ref> to enhance the feature via channel-wise and spatial attention.</p><p>We use a center head on the highest scale feature map C to predict an lchannel heatmap of object centers. Each channel contains the heatmap score of one class. The location of the top N heatmap scores will be taken out as the center proposals. We used N = 500 in our experiments empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale Center Transformer Decoder</head><p>We extract the features at the proposed center locations as the query embedding for the transformer decoder. We use a linear layer to encode the location of the centers into a position embedding. Traditional DETR decoder initializes the query with a learnable parameter.</p><p>Consequently, the attention weights acquired in the decoder are almost the same among all features. By using the center feature as the initial query embedding, we can guide the training to focus on the feature that contains meaningful object information. We use the same self-attention layer in the vanilla Transformer decoder to learn contextual attention between objects. The complexity of computing the cross-attention of a center query to all multi-scale BEV features is O( S s=1 h s w s N ). Since the BEV map resolution needs to be relatively large to maintain the fine-grained features for small objects, it is impractical to use all BEV features as the attending keypoints. Alternatively, we confine the attending keypoints to a small 3 ? 3 window near the center location at each scale, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. The complexity of this cross-attention is O(9SN ), which is more efficient than the normal implementation. Because of multi-scale features, we are able to capture a wide range of features around proposed centers. The Multi-scale cross-attention can be formulated as:</p><formula xml:id="formula_2">MSCA(p) = M m=1 W m [ S s=1 j??j ?( Q i K s j ? d ) ? V s j ],<label>(3)</label></formula><p>where p denotes the center proposal, ? j here is the window around the center, and s is the index of the scale. The feed-forward layer is also kept unchanged.</p><p>Multi-scale Deformable Cross-attention layer Inspired by <ref type="bibr" target="#b63">[64]</ref>, we also used a deformable cross-attention layer to sample the attending keypoints automatically. <ref type="figure" target="#fig_2">Figure 3</ref> shows the structure of the deformable cross-attention layer.</p><p>Compared to the normal multi-head cross-attention layer, deformable crossattention uses a linear layer to learn 2D offsets ?p of the reference center location p at all heads and scales. The feature at p + ?p will be extracted as the crossattention attending feature through bilinear sampling. We use a linear layer to directly learn the attention scores from the query embedding. Features from multiple scales are aggregated together to form the cross-attention layer output:</p><formula xml:id="formula_3">MSDCA(p) = M m=1 W m [ S s=1 K k=1 ?(W msk C(p))x s (p + ?p msk )],<label>(4)</label></formula><p>where x s is the multi-scale BEV feature, C(p) is the center feature, and ?(W msk C(p)) is the attention weight. We used K = 15 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-frame CenterFormer</head><p>Multi-frame is commonly used in 3D detection to improve performance. Current CNN-based detectors cannot effectively fuse features from a fast-moving object, while the transformer structure is more suitable for the fusion due to the attention mechanism. To further explore the potential of CenterFormer, we propose a multi-frame feature fusing method using the cross-attention transformer. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we process each frame individually using the same backbone  <ref type="figure">Fig. 4</ref>: The network structure of spatial-aware fusion. To focus on the current centers, we use current BEV feature as the reference to learn attention.</p><p>network. The last BEV feature of the previous frames is transformed to current coordinates and fused with the current BEV feature in both the center head and cross-attention layer.</p><p>Due to object movements, the center of an object may shift in different frames. Since we only need to predict the center in the current frame, we use a spatial-aware fusion in the center head to alleviate the misalignment error. As shown in <ref type="figure">Figure 4</ref>, the spatial-aware module uses a similar spatial attention layer as CBAM <ref type="bibr" target="#b44">[45]</ref> to calculate pixel-wise attention based on the current BEV feature. We concatenate the current BEV feature and weighted previous BEV feature and use an additional convolution layer to fuse them together. We also add the time embedding to the BEV features based on their relative time. Finally, we feed the output fused features to the center head to predict the center candidates.</p><p>In the cross-attention layer, we use the location of the center proposal to find the corresponding features in the aligned previous frames. The extracted features will be added to the attending keys. Since our normal cross-attention design uses features in a small window close to the center location, it has limited learnability if the object was out of the window area due to fast movement. Meanwhile, our deformable cross-attention is able to model any level of movement, and is more suitable for the long time-range case. Because our multi-frame model only needs the final BEV feature of the previous frame, it is easy to be deployed to the online prediction by saving the BEV feature in a memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Functions</head><p>Besides the general classification and regression loss functions, we add two additional loss functions to better account for the center-based object detection. First, Inspired by the design in CIA-SSD <ref type="bibr" target="#b46">[47]</ref>, we move the IoU-aware confidence rectification module from the second stage of other methods to the regression head. More specifically, we predict an IoU score iou for each bounding box proposal, which is supervised with the highest IoU between the prediction and all ground truth annotations in a smooth L1 loss. During the evaluation, we rectify the confidence score with the predicted IoU score using ? ? = ? * iou ? , where ? is the confidence score and ? is a hyperparameter controlling the degree of rectification. Second, similar to <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b12">13]</ref>, we also added a corner heatmap head alongside the center heatmap head as auxiliary supervision. For each box, we generate the corner heatmap of four bounding box edge centers and the object center using the same methods to draw the center heatmap except that the Gaussian radius is half size. During training, we supervise the corner prediction with an MSE loss on the region where the ground truth heatmap score is above 0.</p><p>The final loss used in our model is the weighted combination of the following four parts: L = w hm L hm + w reg L reg + w iou L iou + w cor L cor . We use focal loss <ref type="bibr" target="#b20">[21]</ref> and L1 loss for the heatmap classification and box regression. The weights of heatmap classification loss, box regression loss, IoU rectification loss, and corner classification loss are [1, 2, 1, 1], respectively, in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present our experimental results on two large-scale LiDAR object detection benchmarks: Waymo Open Dataset <ref type="bibr" target="#b37">[38]</ref> and nuScenes dataset <ref type="bibr" target="#b2">[3]</ref>. Due to page limitation, the experimental results on the nuScenes dataset, more analysis as well as the details on the choice of network parameters, and additional visualizations are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Waymo Open Dataset (WOD) is a large-scale LiDAR point cloud dataset for the autonomous driving environment. It contains 798, 202, and 150 sequences for training, validation, and testing, respectively. Each sequence is 20-second long, captured by a 10 FPS LiDAR sensor with 64 lines in 360 ? . WOD provides bounding box annotations for three classes: vehicles, pedestrians, and cyclists. The evaluation metrics used in WOD are mean average precision (mAP) and mAP weighted by heading accuracy (mAPH). Each object is categorized into two levels of difficulty, where LEVEL 1 (L1) denotes that there are more than 5 points on the object and LEVEL 2 (L2) denotes that there are 1 ? 5 points on the object or the object is manually labeled as L2. The evaluation of the primary metric mAPH L2 includes both L1 and L2 objects. We set the range of the 3D voxel space as [?75.2m, 75.2m] for the X and Y axes, and [?2m, 4m] for the Z axis. The size of each voxel is set to (0.1m, 0.1m, 0.15m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We follow the same VoxelNet backbone network design as <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b33">34]</ref>. In our center proposal network, we process the output of the BEV feature into three scales through one upsample layer and one downsample layer. We set the transformer layer/head numbers to 3/4 when using the normal cross-attention, and to 2/6 when using the deformable cross-attention. During evaluation, we use the NMS For our 8 frames model, we use ? = [1, 1, 1] to get a better result for the cyclist. We also increase the center proposal number N to 1000 in evaluation. We trained our model using AdamW optimizer with the one-cycle policy. We trained the single-frame and multi-frame models on 8 Nvidia A100 GPUs with batch sizes 32 and 16. Due to the memory limitation, 4 frames and 8 frames are first split into two 2 frames and 4 frames mini-batch. The points from frames in each mini-batch will be first concatenated together. Hence our multi-frame model only needs to fuse two BEV features together. We apply the object copy &amp; paste augmentation during training with the same fade strategy in <ref type="bibr" target="#b40">[41]</ref>. More details are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object Detection Results</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we show the results on the validation set. Anchor-free center-based method CenterPoint achieves better performance than the anchor-based methods. PVRCNN++ is the best anchor-based method so far, yet shows weak performance on small objects. This demonstrates the limitation of using manually designed anchors for the detection of objects that have large size variations. Our single frame model outperforms CenterPoint by 1.7%. Using a multi-frame model can significantly increase the mAPH performance. Our 2/4/8 frames model reaches the mAPH of 72.8%, 73.2% and 73.7%, respectively, becoming the new state-of-the-art. The pedestrian class benefits most from the multi-frame since the pedestrian point cloud suffers most from the occlusion and noise, as well as its small size. The overall better performance verifies the effectiveness of our proposed transformer model. In <ref type="table" target="#tab_1">Table 2</ref>, we show the single model results on the test set. The prediction result is submitted to the online server for evaluation. Our method outperforms all the previous methods by a large margin. The result on vehicle and pedestrian classes have significant improvements (+3.8% and +3.1% on L2 mAPH) as a result of the long-range contextual information learning of the transformer.  To fairly compare with more recent methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33</ref>] that train the model and report the result on only the vehicle class, we train the single frame Cen-terFormer only on Vehicle class too. We show the results in <ref type="table" target="#tab_2">Table 3</ref>. As shown in the table, even with the simplest design, CenterFormer outperforms both the recent transformer-based methods and CNN-based baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In <ref type="table" target="#tab_3">Table 4</ref>, we investigate the effect of each added component in our method on a single frame. We use the previous center-based method as the baseline. After changing the RPN to multi-scale CPN and separating the detection into the center proposal and box regression, our method reaches a similar performance despite we flatten the regression head to 1D. The transformer self-attention layer and cross-attention layer both can improve the results, and when used together, the result reaches 67.0%. This indicates that the self-attention layer and the cross-attention layer learn features separately. Corner auxiliary supervision can additionally improve the result by 0.1%. On the other hand, deformable crossattention achieves a better result of 67.3%. When trained with IoU rectification, the results get a significant boost to 68.7% and 68.3% for the models using cross- attention and deformable cross-attention. The fade augmentation strategy can further improve the result by 0.4% and 0.7%. This is because the model can adjust to the real data distribution at the end of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>Comparison with deformable DETR Deformable DETR <ref type="bibr" target="#b63">[64]</ref> aims to speed up the learning speed and reduce the computation cost in the DETR structure. Compared to Deformable DETR, our method has three major differences. First, we completely remove the transformer encoder to enable a larger encoded feature map. Second, we use the center feature rather than the learnable parameter as the query embedding for the transformer decoder. Experiment shows that using the center feature as the initial query embedding outperforms the parametric embedding by 1.5% mAPH. This is because the center feature already contains object-level information, which makes it easier to learn pairwise attentions. Third, we use a similar training strategy as <ref type="bibr" target="#b52">[53]</ref> rather than the endto-end set matching training strategy. The set matching training is known for being hard to converge. Since we already have an initial center proposal, we can limit the network to learn only when the proposal is close to the ground truth annotation to speed up the training. Experiment shows that if we use the same set matching training strategy in DETR, the mAPH result is 46.3%, which is more than 20% lower than our current training method.</p><p>Visualization of the learned attention The visualizations of the learned self-and cross-attention are illustrated in <ref type="figure">Figure 5</ref>. The self-attention learning is mainly focusing on the feature of the same class or nearby objects that have the same attributes. For instance, the vehicles on the same line or same parking area will have higher attention weight. The offsets learned by the deformable attention layer vary among different scales. The offsets in two lower scales mostly lead to Ground Truth Bounding Box Self-Attention Deformable Cross-Attention (Three Scales) <ref type="figure">Fig. 5</ref>: The visualization of the learned self-and cross-attention weight.</p><p>The lightness or darkness of the color represents the value of attention weight. The red box denotes the ground truth box and blue box denotes the predicted box. In cross-attention, the sampled keypoints are drawn with red, green and blue for the scale from low to high. Best viewed in color.</p><p>the keypoints inside or around the object, whereas the offsets in the higher scale can sample far-range features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel center-based transformer for 3D object detection. Our method provides a solution to improve the anchor-free 3D object detection network through object-level attention learning. Compared to the DETR-style transformer networks, we use the center feature as the initial query embedding in the transformer decoder to speed up the convergence. We also avoid high computational complexity by focusing the cross-attention learning of each query in a small multi-scale window or a deformable region. Results show that the proposed method outperforms the strong baseline in the Waymo Open Dataset, and reaches state-of-the-art performance when extended to multi-frame. We hope our design will inspire more future work in query-based transformers for LiDAR point cloud analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>VoxelNet backbone network We adopt the same VoxelNet backbone network design in CenterPoint <ref type="bibr" target="#b52">[53]</ref>. In specific, we first use an average pooling in each voxel to encode the point cloud into a voxel feature map. Then, a VoxelNet <ref type="bibr" target="#b58">[59]</ref> with sparse convolution is used to extract features in the voxel map. Except for the down-sample layer, all residual blocks use the submanifold sparse convolution layer to minimize the computation cost. The VoxelNet backbone network downsamples the dimensions of the x-axis and y-axis with a factor of 8 and the z-axis with a factor of 16. Finally, the output voxel feature map is reshaped to the BEV for the following process.</p><p>Multi-scale CPN We design the Multi-scale CPN to achieve two goals: First, we want to encode the BEV feature into different scale levels for the transformer decoder. Second, the BEV feature map should be large enough to separate small objects like the pedestrian. Since in our experiment, the size of each BEV grid in the VoxelNet output feature is [0.8m ? 0.8m], which is similar to the average pedestrian object size, we need to up-sample the feature map to avoid the voxelization error. We also use a down-sample layer to extract larger-scale features. The overall network structure is shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Spatial-aware heatmap fusion To focus the heatmap fusion on the current object center location, we use the current BEV feature as the reference to learn spatial attention. We concatenate the current feature and the weighted previous features together and use another 3 ? 3 convolution layer to compress the BEV feature into the same size as the single frame input in the heatmap head.</p><p>Training details Generally, the transformer decoder requires a matching process, e.g. Hungarian matching, to find the closest ground truth bounding box to the prediction in training. The computation cost of this process becomes unacceptable when we match two 3D bounding boxes with orientation. Hence, we use the same training strategy in the center-based object detection network, i.e. only training the network when the proposed center is at the same position as the ground truth center. To utilize all annotation information in training, we manually select the center positions of all ground truth bounding boxes as the initial center proposals in training. And the position with the highest heatmap response other than those positions are selected as the remaining proposals. This allows the network to have a meaningful training objective from the beginning of the training, and thus converges faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B NuScenes Dataset Result</head><p>NuScenes Dataset (ND) <ref type="bibr" target="#b2">[3]</ref> is a large-scale dataset created by Motional. It contains 1000 scenes of 20s each, which are split into 700,150,150 sequences for the training, validation and testing. ND uses a 32 lines LiDAR with the frequency of 20 FPS. In each keyframe that is sampled every 0.5s, ND provides bounding box annotations for 10 different classes. The evaluation metrics used by ND are mean average precision (mAP) and nuScenes detection score (NDS). In contrast to WOD, the mAP used by NP only matches objects according to the 2D center distance in BEV rather than IoU. NDS is computed based on a weighted sum of Average Translation/ Scale/ Orientation/ Velocity and Attribute Errors (ATE/ ASE/ AOE/ AVE/ AAE) on the set of true positives. Followed by <ref type="bibr" target="#b52">[53]</ref>, we set the range of the 3D voxel space as [?54m, 54m] for the X and Y axes, and [?5m, 3m] for the Z axis. The size of each voxel is set to (0.075m, 0.075m, 0.2m). We show the comparison of the results on the nuScenes validation set in <ref type="table" target="#tab_4">Table 5</ref>. We compare our base CenterFormer model with the CenterPoint baseline using the same training configuration. Due to the time limitation, we did not include further experiments on ND using some of our more complex structures, like deformable cross-attention and multi-frame fusion. Nevertheless, our base CenterFormer already outperforms CenterPoint as shown in the table. The improvement comes mainly from the bounding box regression since these two methods share a similar center-based classification design. This result validates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Analysis</head><p>The effect of our multi-frame design In <ref type="table" target="#tab_5">Table 6</ref>, we show the improvement of our multi-frame CenterFormer compared to the point concatenation method used by most LiDAR object detectors. The point concatenation method has significant improvement (+2.6%) on two frames. But it has less effect when using more frames. In contrast, our multi-frame CenterFormer has constant improvement when using more frames. In 2, 4 and 8 frames, multi-frame CenterFormer achieves 1.0%, 0.5% and 1.1% higher mAPH than the point concatenation method. Our deformable CenterFormer achieves better performance than the base model on multi-frame due to the ability to model cross-attention in a larger range. We also compared the performance on different speeds in <ref type="figure">Figure 7</ref>. The speed of a object is categorized into stationary (&lt; 0.2m/s), slow (0.2 ? 1m/s), medium (1 ? 3m/s), fast (3 ? 10m/s) or very fast (&gt; 10m/s). We can see that the main improvement in the point concatenation method comes from the stationary objects, and the slow-speed objects even have worse performance. On the contrary, our multi-frame CenterFormer achieves better performance throughout all categories. Transformer layer and head number We show the comparison of results using different transformer layers and head numbers in <ref type="table" target="#tab_6">Table 7</ref>. The results indicate that more transformer layers and heads do not assure better performance. The base transformer model with 3 layers and 4 heads and the deformable transformer model with 2 layers and 6 heads has the best performance. Cross-attention field We show the comparison of results using different crossattention window sizes and offset numbers in <ref type="table">Table 8</ref>. Increasing the window size does not have any performance gain. This is because the sizes of each grid in three scales are [0.4m, 0.8m, 1.6m] in our setting. The 3?3 attention window can encompass the region of almost all pedestrian and cyclist objects. If we increase it to 5 ? 5 or 7 ? 7, although it can include more features for the vehicle, the added feature for the pedestrian and cyclist is almost unrelated. On the other hand, the number of offsets used in the deformable cross-attention also does not increase the performance monotonically. We find the offset number of 15 has the best performance. Position embedding Position embedding is important in the transformer model to capture the spatial relationship between input features. Standard position embedding is either crafted manually using sine and cosine distances or learned through a linear layer. However, 3D point clouds, as a specific type of data, contain the position information in the raw feature. They do not necessarily need the position embedding since the spatial information is already in the encoded feature. We test the performance of different position embedding methods using 20% of training data. The LEVEL 2 mAPH result is shown in <ref type="table" target="#tab_7">Table 9</ref>. We can see without the position embedding, the result drops significantly to 59.5%. This indicates the absolute position information is still an important feature to guide the attention learning of the transformer model. The learnable encoding method also outperforms the sinusoidal encoding method by a large margin.</p><p>Converging difficulty compared with DETR In <ref type="figure">Figure 8</ref>, We show the LEVEL 2 mAPH result comparison of CenterFormer and DETR in a 20 epochs  training cycle. We implement the DETR-style set matching training strategy using the same backbone network in CenterFormer. We can see that not only CenterFormer can reach a much higher result than DETR, but also converge much faster than DETR. Comparison with two-stage LiDAR detection Most two-stage LiDAR detection methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b19">20]</ref> apply the RCNN-style refinement network in the 3D domain. The second stage aggregates RoI features in each first-stage pro-posal to refine both the classification and regression prediction. There are two drawbacks in this design. First, the second stage only utilizes local RoI features. It cannot retrieve global information and depends heavily on the quality of the first feature and proposal. Second, it has a large computation overhead, especially when used in LiDAR point clouds with a large size of points or voxel features. The network will predict the same object information twice, which results in a cumbersome structure and prohibitive run-time. In contrast, our method works between one-stage and two-stage. We use a center proposal network to generate initial center queries. The self-attention layer allows the network to directly learn object-level contextual information. The cross-attention layer can also capture long-range information in the multi-scale BEV feature. The classification and regression are done only once in our method. <ref type="figure">Figure 9</ref> shows the qualitative result of our proposed method. Our method can make accurate predictions with a high confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The comparison of CenterFormer with RCNN-style detector. RCNN aggregates point or grid features in RoI, while CenterFormer can learn object-level contextual information and long range features through an attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The overall architecture of CenterFormer. The network consists of four parts: a voxel feature encoder that encodes the raw point cloud into a BEV feature representation, a multi-scale center proposal network (CPN), the centerbased transformer decoder, and a regression head to predict the bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the cross-attention layer. (Left) Multi-scale crossattention. (Right) Multi-scale deformable cross-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The network structure of multi-scale CPN. Each Conv block contains a convolution layer with kernel size 3 ? 3, a batchnorm layer and a relu activation layer. We use convolution layer with stride and transpose convolution layer as the down-sample and up-sample layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>The LEVEL 2 mAPH results comparison of CenterFormer and DETR in different epochs. Visualization of CenterFormer predictions. The red box denotes the ground truth bounding box. The blue box denotes the predictions with confidence score &gt; 0.4. Truncated objects whose center is outside of the 50m range are not visualized. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The detection results on WOD validation set. ?: Deformable Center-Former. ? Reported by PVRCNN++.</figDesc><table><row><cell>Method</cell><cell>Frame</cell><cell>Vehicle L1 (mAP/APH)</cell><cell>Vehicle L2 (mAP/APH)</cell><cell>Pedestrain L1 (mAP/APH)</cell><cell>Pedestrain L2 (mAP/APH)</cell><cell>Cyclist L1 (mAP/APH)</cell><cell>Cyclist L2 (mAP/APH)</cell><cell>Mean L2 mAPH</cell></row><row><cell>StarNet [28]</cell><cell>1</cell><cell>55.1/54.6</cell><cell>48.7/48.3</cell><cell>68.3/60.9</cell><cell>59.3/52.8</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-</cell></row><row><cell>SECOND  ? [49]</cell><cell>1</cell><cell>72.3/71.7</cell><cell>63.9/63.3</cell><cell>68.7/58.2</cell><cell>60.7/51.3</cell><cell>60.6/59.3</cell><cell>58.3/57.1</cell><cell>57.2</cell></row><row><cell>LiDAR R-CNN [20]</cell><cell>1</cell><cell>73.5/73.0</cell><cell>64.7/64.2</cell><cell>71.2/58.7</cell><cell>63.1/51.7</cell><cell>68.6/66.9</cell><cell>66.1/64.4</cell><cell>60.1</cell></row><row><cell>Part-A2  ? [37]</cell><cell>1</cell><cell>77.1/76.5</cell><cell>68.5/68.0</cell><cell>75.2/66.9</cell><cell>66.2/58.6</cell><cell>68.6/67.4</cell><cell>66.1/64.9</cell><cell>63.8</cell></row><row><cell>3D-MAN [50]</cell><cell>16</cell><cell>74.5/74.0</cell><cell>67.6/67.1</cell><cell>71.7/67.7</cell><cell>62.6/59.0</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-</cell></row><row><cell>PV-RCNN++ [35]</cell><cell>1</cell><cell>79.1/78.6</cell><cell>70.3/69.9</cell><cell>80.6/74.6</cell><cell>71.9/66.3</cell><cell>73.5/72.4</cell><cell>70.7/69.6</cell><cell>68.6</cell></row><row><cell>CenterPoint [53]</cell><cell>1</cell><cell>-/ -</cell><cell>-/67.9</cell><cell>-/ -</cell><cell>-/65.6</cell><cell>-/ -</cell><cell>-/68.6</cell><cell>67.4</cell></row><row><cell>CenterPoint [53]</cell><cell>2</cell><cell>-/ -</cell><cell>-/69.7</cell><cell>-/ -</cell><cell>-/70.3</cell><cell>-/ -</cell><cell>-/70.9</cell><cell>70.3</cell></row><row><cell>CenterFormer</cell><cell>1</cell><cell>75.0/74.4</cell><cell>69.9/69.4</cell><cell>78.0/72.4</cell><cell>73.1/67.7</cell><cell>73.8/72.7</cell><cell>71.3/70.2</cell><cell>69.1</cell></row><row><cell>CenterFormer  ?</cell><cell>1</cell><cell>75.2/74.7</cell><cell>70.2/69.7</cell><cell>78.6/73.0</cell><cell>73.6/68.3</cell><cell>72.3/71.3</cell><cell>69.8/68.8</cell><cell>69.0</cell></row><row><cell>CenterFormer</cell><cell>2</cell><cell>77.1/76.6</cell><cell>72.2/71.7</cell><cell>80.9/77.6</cell><cell>76.2/73.0</cell><cell>76.0/75.1</cell><cell>73.6/72.7</cell><cell>72.5</cell></row><row><cell>CenterFormer  ?</cell><cell>2</cell><cell>77.0/76.5</cell><cell>72.1/71.6</cell><cell>81.4/78.0</cell><cell>76.7/73.4</cell><cell cols="2">76.6/75.7 74.2/73.3</cell><cell>72.8</cell></row><row><cell>CenterFormer  ?</cell><cell>4</cell><cell>78.1/77.6</cell><cell>73.4/72.9</cell><cell>81.7/78.6</cell><cell>77.2/74.2</cell><cell>75.6/74.8</cell><cell>73.4/72.6</cell><cell>73.2</cell></row><row><cell>CenterFormer  ?</cell><cell>8</cell><cell cols="2">78.8/78.3 74.3/73.8</cell><cell>82.1/79.3</cell><cell>77.8/75.0</cell><cell>75.2/74.4</cell><cell>73.2/72.3</cell><cell>73.7</cell></row><row><cell cols="9">IoU threshold of [0.8, 0.55, 0.55] and ? = [1, 1, 4] for vehicle, pedestrian and cy-</cell></row><row><cell>clist.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The single-model detection result on WOD testing set.</figDesc><table><row><cell>Method</cell><cell>Vehicle L1 (mAP/APH)</cell><cell>Vehicle L2 (mAP/APH)</cell><cell>Pedestrain L1 (mAP/APH)</cell><cell>Pedestrain L2 (mAP/APH)</cell><cell>Cyclist L1 (mAP/APH)</cell><cell>Cyclist L2 (mAP/APH)</cell><cell>Mean L2 mAPH</cell></row><row><cell>StarNet [28]</cell><cell>61.5/61.0</cell><cell>54.9/54.5</cell><cell>67.8/59.9</cell><cell>61.1/54.0</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-</cell></row><row><cell>PPBA [5]</cell><cell>64.6/64.1</cell><cell>56.2/55.8</cell><cell>69.7/61.7</cell><cell>63.0/55.8</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-</cell></row><row><cell>M3DETR [12]</cell><cell>77.7/77.1</cell><cell>70.5/70.0</cell><cell>68.2/58.5</cell><cell>60.6/52.0</cell><cell>67.3/65.7</cell><cell>65.3/63.8</cell><cell>61.9</cell></row><row><cell>3D-MAN [50]</cell><cell>78.7/78.3</cell><cell>70.4/70.0</cell><cell>70.0/66.0</cell><cell>64.0/60.3</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-</cell></row><row><cell>RSN [1]</cell><cell>80.7/80.3</cell><cell>71.9/71.6</cell><cell>78.9/75.6</cell><cell>70.7/67.8</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-</cell></row><row><cell cols="2">PV-RCNN++ [35] 81.6/81.2</cell><cell>73.9/73.5</cell><cell>80.4/75.0</cell><cell>74.1/69.0</cell><cell>71.9/70.8</cell><cell>69.3/68.2</cell><cell>70.2</cell></row><row><cell>CenterPoint [53]</cell><cell>81.1/80.6</cell><cell>73.4/73.0</cell><cell>80.5/77.3</cell><cell>74.6/71.5</cell><cell>74.6/73.7</cell><cell>72.2/71.3</cell><cell>71.9</cell></row><row><cell>SST 3f [8]</cell><cell>81.0/80.6</cell><cell>73.1/72.7</cell><cell>83.3/79.7</cell><cell>76.9/73.5</cell><cell>75.7/74.6</cell><cell>73.2/72.2</cell><cell>72.8</cell></row><row><cell>AFDetV2 [13]</cell><cell>81.7/81.2</cell><cell>74.3/73.9</cell><cell>81.3/78.1</cell><cell>75.5/72.4</cell><cell cols="2">76.4/75.4 74.1/73.0</cell><cell>73.1</cell></row><row><cell>CenterFormer</cell><cell cols="2">84.7/84.4 78.1/77.7</cell><cell>84.6/81.8</cell><cell>79.4/76.6</cell><cell>75.5/74.5</cell><cell>73.3/72.4</cell><cell>75.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The comparison of single-frame CenterFormer (trained on vehicle only) with other methods on WOD validation set for vehicle class.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Ref</cell><cell cols="2">L2 mAP L2 mAPH</cell></row><row><cell></cell><cell>RSN [1]</cell><cell cols="2">CVPR 2021 66.0</cell><cell>65.5</cell></row><row><cell>CNN</cell><cell cols="3">Voxel R-CNN [25] Pyramid R-CNN [24] ICCV 2021 AAAI 2021 LiDAR R-CNN [20] CVPR 2021 68.3 66.6 67.2</cell><cell>-66.7 67.9</cell></row><row><cell></cell><cell>BtcDet [48]</cell><cell>AAAI 2022</cell><cell>70.1</cell><cell>69.6</cell></row><row><cell></cell><cell>BoxeR-3D [29]</cell><cell cols="2">CVPR 2022 63.9</cell><cell>63.7</cell></row><row><cell>Transformer</cell><cell cols="3">Voxel transformer [25] ICCV 2021 M3DETR [12] WACV 2022 66.6 65.9 3D-MAN [50] CVPR 2021 67.6 SST 1f [8] CVPR 2022 68.0 CT3D [33] ICCV 2021 69.0</cell><cell>65.3 66.0 67.1 67.6 -</cell></row><row><cell></cell><cell>CenterFormer</cell><cell cols="2">ECCV 2022 70.7</cell><cell>70.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The ablation of the LEVEL 2 mAPH result improvement of each component on the validation set using single frame. SA, CA and DCA denote the self-attention layer, cross-attention layer and deformable cross-attention layer. IoU and Corner denote IoU rectification and Corner auxiliary supervision. Fade denotes the fade augmentation strategy.</figDesc><table><row><cell cols="7">CPN SA CA Corner DCA IoU Fade Vehicle Pedstrain Cyclist Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.4</cell><cell>63.4</cell><cell>67.8 65.9</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>68.5</cell><cell>62.7</cell><cell>67.0 66.1</cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell>68.7</cell><cell>64.5</cell><cell>67.3 66.8</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>68.7</cell><cell>64.2</cell><cell>66.7 66.5</cell></row><row><cell cols="2">? ? ?</cell><cell></cell><cell></cell><cell>69.3</cell><cell>64.8</cell><cell>66.8 67.0</cell></row><row><cell cols="2">? ? ?</cell><cell>?</cell><cell></cell><cell>69.5</cell><cell>65.3</cell><cell>66.4 67.1</cell></row><row><cell cols="2">? ? ?</cell><cell>?</cell><cell>?</cell><cell>69.5</cell><cell>66.9</cell><cell>69.7 68.7</cell></row><row><cell cols="2">? ? ?</cell><cell>?</cell><cell>? ?</cell><cell>69.4</cell><cell>67.7</cell><cell>70.2 69.1</cell></row><row><cell>? ?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>69.3</cell><cell>65.1</cell><cell>67.5 67.3</cell></row><row><cell>? ?</cell><cell></cell><cell>?</cell><cell>? ?</cell><cell>69.2</cell><cell>66.7</cell><cell>69.1 68.3</cell></row><row><cell>? ?</cell><cell></cell><cell>?</cell><cell>? ? ?</cell><cell>69.7</cell><cell>68.3</cell><cell>68.8 69.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The detection result on the ND validation set. ?: Base CenterFormer model without IoU rectification and multi-frame fusion. * : Our implementation uses the same backbone network and training configuration. NDS ? ATE ? ASE ? AOE ? AVE ? AAE ?</figDesc><table><row><cell cols="2">Method mAP ? PointPillars [18] 39.3</cell><cell>53.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pillar-OD [44]</cell><cell>44.4</cell><cell>56.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSN [62]</cell><cell>45.3</cell><cell>57.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CBGS [61]</cell><cell>51.4</cell><cell>62.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CenterPoint  *  [53] 55.2</cell><cell cols="4">64.4 29.3 25.7 29.6</cell><cell cols="2">27.2 19.5</cell></row><row><cell>CenterFormer  ?</cell><cell cols="7">55.4 65.2 27.5 25.2 27.5 24.3 20.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The LEVEL 2 mAPH results comparison of the multi-frame Center-Former on WOD validation set. All models are trained without the IoU rectification. ?: CenterFormer using point concatenation.?: Deformable CenterFormer.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Fig. 7: The LEVEL 2 mAPH re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">sults comparison breakdown by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">speed.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.5 80.0</cell><cell cols="7">One Frame Two Frames Concatenation Two Frames Multi-frame Fusion</cell><cell>75.9</cell><cell>76.2</cell><cell>77.4</cell><cell>79</cell><cell>79.4</cell><cell>80.3</cell></row><row><cell>Method CenterFormer CenterFormer ? CenterFormer ? CenterFormer ? CenterFormer</cell><cell cols="4">Frame Vehicle Pedestrian Cyclist Mean 1 69.0 66.8 68.0 67.9 2 70.6 70.2 70.7 70.5 4 71.7 70.8 71.6 71.4 8 72.0 71.6 2 70.9 70.4 71.8 71.0 71.6 71.7</cell><cell>Vehicle LEVEL_2 mAPH</cell><cell>65.0 67.5 70.0 72.5 75.0</cell><cell>66.4</cell><cell>68.2</cell><cell>68.2</cell><cell>67.2</cell><cell>65.9</cell><cell>67.8</cell><cell>67.7</cell><cell>67.7</cell><cell>68.8</cell></row><row><cell>CenterFormer  ?</cell><cell>2</cell><cell>70.7</cell><cell>71.1</cell><cell>72.6 71.5</cell><cell></cell><cell>62.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CenterFormer  ? CenterFormer  ?</cell><cell>4 8</cell><cell>71.9 73.4</cell><cell>72.2 73.4</cell><cell>71.5 71.9 71.7 72.8</cell><cell></cell><cell>60.0</cell><cell cols="6">Stationary Slow</cell><cell>Medium</cell><cell>Fast</cell><cell>Very_Fast</cell></row><row><cell cols="14">the superiority of our proposed CenterFormer over the traditional center-based</cell></row><row><cell cols="5">detector in different point cloud structures.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>The LEVEL 2 mAPH result comparison of using different layer and head configurations on WOD validation set. (Left) Base CenterFormer. (Right) Deformable CenterFormer. All experiments use only 20% of uniformly sampled training data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>The LEVEL 2 mAPH result comparison of the position encoding methods on WOD validation set. All experiments use only 20% of uniformly sampled training data.</figDesc><table><row><cell cols="4">Encoder Vehicle Pedestrian Cyclist Mean</cell></row><row><cell>None</cell><cell>60.3</cell><cell>56.3</cell><cell>61.3 59.5</cell></row><row><cell cols="2">Sinusoidal 62.7</cell><cell>58.6</cell><cell>63.5 61.7</cell></row><row><cell>Linear</cell><cell>65.2</cell><cell>60.9</cell><cell>66.0 64.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Yufei Xie for his help with refactoring the code for release.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Range conditioned dilated convolutions for scale invariant 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alexander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving 3d object detection through progressive population based augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Voxel r-cnn: Towards high performance voxel-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Embracing single stride 3d object detector with sparse transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rangedet: In defense of range view for lidar-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Afdet: Anchor free one stage 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">M3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Afdetv2: Rethinking the necessity of the second stage for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An lstm approach to temporal 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jianwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chunyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pengchuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jianfeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Haozhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yichen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaiwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lingxi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Honggang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qingming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lidar r-cnn: An efficient and universal 3d object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dab-detr: Dynamic anchor boxes are better queries for detr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Group-free 3d object detection via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pyramid r-cnn: Towards better performance and adaptability for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Voxel transformer for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An end-to-end transformer model for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Starnet: Targeted computation for object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>arXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Boxer: Box-attention for 2d and 3d transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hvpr: Hybrid voxel-point representation for single-stage 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving 3d object detection with channel-wise transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>arXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object progposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<title level="m">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rsn: Range sparse net for efficient, accurate lidar 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pointaugmenting: Cross-modal augmentation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Centernet3d: An anchor free object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">arXiv</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pillar-based object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cia-ssd: Confident iou-aware single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weiliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sijin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chi-Wing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Behind the curtain: Learning occluded shapes for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">3d-man: 3d multi-frame attention network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hvnet: Hybrid voxel network for lidar based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yutong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yixuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephen Ching-Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baining</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>arXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-tosequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Panoptic-polarnet: Proposal-free lidar point cloud panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Class-balanced grouping and sampling for point cloud 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">arXiv</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Ssn: Shape signature networks for multi-class object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The LEVEL 2 mAPH result comparison of using different window sizes in our base CenterFormer and different offset numbers in deformable Center-Former on WOD validation set. (Left) Base CenterFormer</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>Right) Deformable</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
