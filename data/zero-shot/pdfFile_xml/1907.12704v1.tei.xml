<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Wang</surname></persName>
							<email>wangxiya16@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
							<email>liuyushen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
							<email>zwicker@cs.umd.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised feature learning for point clouds has been vital for large-scale point cloud understanding. Recent deep learning based methods depend on learning global geometry from self-reconstruction. However, these methods are still suffering from ineffective learning of local geometry, which significantly limits the discriminability of learned features. To resolve this issue, we propose MAP-VAE to enable the learning of global and local geometry by jointly leveraging global and local self-supervision. To enable effective local self-supervision, we introduce multi-angle analysis for point clouds. In a multi-angle scenario, we first split a point cloud into a front half and a back half from each angle, and then, train MAP-VAE to learn to predict a back half sequence from the corresponding front half sequence. MAP-VAE performs this half-to-half prediction using RNN to simultaneously learn each local geometry and the spatial relationship among them. In addition, MAP-VAE also learns global geometry via self-reconstruction, where we employ a variational constraint to facilitate novel shape generation. The outperforming results in four shape analysis tasks show that MAP-VAE can learn more discriminative global or local features than the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point clouds have become a popular 3D representation in machine vision, autonomous driving, and augmented reality, because they are easy to acquire and manipulate. Therefore, point cloud analysis has emerged as a crucial problem in the area of 3D shape understanding. With the help of extensive supervised information, recent deep learning based feature learning techniques have achieved unprecedented results in classification, detection and segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>. However, supervised learning requires intense manual labeling effort to obtain supervised information. Therefore, unsupervised feature learning is an attractive alternative and a promising research challenge.</p><p>Several studies have tried to adress this challenge <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref>. To learn the structure of a point cloud without additional supervision, these generative models are trained by self-supervision, such as self-reconstruction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32]</ref> or distribution approximation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34]</ref>, which is implemented by auto-encoder or generative adversarial networks <ref type="bibr" target="#b8">[9]</ref> respectively. To capture finer global structure, some methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref> first learn local structure information in point cloud patches based on which the global point cloud is then reconstructed. Because of lacking effective and semantic local structure supervision, however, error may accumulate in the local structure learning process, which limits the network's ability in 3D point cloud understanding.</p><p>To resolve this issue, we propose a novel deep learning model for unsupervised point cloud feature learning by simultaneously employing effective local and global selfsupervision. We introduce multi-angle analysis for point clouds to mine effective local self-supervision, and combine it with global self-supervision under a variational constraint. Hence we call our model Multi-Angle Point Cloud Variational Auto-Encoder (MAP-VAE). Specifically, to employ local self-supervision, MAP-VAE first splits a point cloud into a front half and a back half under each of several incrementally varying angles. Then, MAP-VAE performs half-to-half prediction to infer a sequence of several back halves from the corresponding sequence of the complementary front halves. Half-to-half prediction aims to capture the geometric and structural information of local regions on the point cloud through varying angles. More-over, by leveraging global self-supervision, MAP-VAE conducts self-reconstruction in company with each half-to-half prediction to capture the geometric and structural information of the whole point cloud. Self-reconstruction is started from a variational feature space, which enables MAP-VAE to generate new shapes by capturing the distribution information over training point clouds in the feature space. In summary, our contributions are as follows: i) We propose MAP-VAE to perform unsupervised feature learning for point clouds. It can jointly leverage effective local and global self-supervision to learn finegrained geometry and structure of point clouds.</p><p>ii) We introduce multi-angle analysis for point cloud understanding, which provides semantic local selfsupervision to learn local geometry and structure.</p><p>iii) We provide a novel way to consistently split point clouds into semantic regions according to view angles, which enables the exploration of the fine-grained discriminative information of point cloud regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Deep learning models have led to significant progress in feature learning for 3D shapes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11]</ref>. Here, we focus on reviewing studies on point clouds. For supervised methods, supervised information, such as shape class labels or segmentation labels, are required to train deep learning models in the feature learning process. In contrast, unsupervised methods are designed to mine self-supervision information from point clouds for training, which eliminates the need for supervised information that can be tedious to obtain. We briefly review the state-of-the-art methods in these two categories as follows. Supervised feature learning. As a pioneering work, Point-Net <ref type="bibr" target="#b25">[26]</ref> was proposed to directly learn features from point clouds by deep learning models. However, PointNet is limited in capturing contextual information among points. To resolve this issue, various techniques were proposed to establish graph in a local region to capture the relationship among points in the region <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>. Furthermore, multi-scale analysis <ref type="bibr" target="#b26">[27]</ref> was introduced to extract more semantic features from the local region by separating points into scales or bins, and then, aggregating these features by concatenation <ref type="bibr" target="#b37">[38]</ref> or RNN <ref type="bibr" target="#b24">[25]</ref>. These methods require supervised information in the feature learning process, which is different from unsupervised approach in MAP-VAE. Unsupervised feature learning. An intuitive approach to mine self-supervised information is to perform selfreconstruction which first encodes a point cloud into a feature and then decodes the feature back to a point cloud. Such global self-supervision is usually implemented by an autoencoder network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32]</ref>. With the help of adversarial training, a different kind of global self-supervision is employed to train the network to generate plausible point clouds by learning a mapping from a known distribution to the unknown distribution that the point clouds are sampled from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34]</ref>. For finer global structure information, some methods take a step further to jointly employ local structure information captured in local regions <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref>. These methods first learn local structure information in point cloud patches by clustering <ref type="bibr" target="#b41">[42]</ref>, conditional point distribution <ref type="bibr" target="#b32">[33]</ref>, graph convolution <ref type="bibr" target="#b33">[34]</ref>, or fully connected layers <ref type="bibr" target="#b31">[32]</ref>, based on which the whole point cloud is then reconstructed. However, because of lacking effective and semantic local structure supervision, this process is prone to error accumulation in the local structure learning process, which limits the network's ability in point cloud understanding. To resolve this issue, MAP-VAE introduces multiangle analysis for point clouds, which provides effective and semantic local self-supervision. MAP-VAE can also simultaneously employ local and global self-supervision, which further differentiates it from other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>To jointly leverage local and global self-supervision to learn features from point clouds, MAP-VAE simultaneously conducts half-to-half prediction and self-reconstruction by three branches, i.e., which we call aggregation branch A, reconstruction branch R, and prediction branch P, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, branch A and branch P together perform the half-to-half prediction while branch A and branch R together perform the self-reconstruction.</p><p>A training sample T i provided to MAP-VAE to learn is formed by a front half sequence S F , a back half sequence S B , and an original point cloud M . The corresponding elements in sequences S F and S B are a front half m F v (in red) and its complementary back half m B v (in green) which are obtained by splitting the original point cloud M (in blue) from a specific angle v.</p><p>The aggregation branch A encodes the geometry of local point clouds and their spatial relationship by aggregating At the same time, the prediction branch P performs halfto-half prediction by decoding the learned angle-specific feature h i into a back half sequence S B which is paired with the corresponding front half sequence S F . This prediction is conducted by a prediction RNN U P which tries to predict the sequence S B as similar as possible to S B .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-angle splitting</head><p>Multi-angle splitting. A key idea in MAP-VAE is a novel multi-angle analysis for point coulds to mine effective local self-supervision. Intuitively, observing a point cloud from different angles, explicitly presents the correspondences and relationships among different shape regions, given as the correspondence between the front and back halves of the shape in each view. Our multi-angle analysis provides multiple regions (front halves) of the point cloud as input, from which the corresponding missing regions (back halves) need to be inferred. This encourages MAP-VAE to learn a detailed shape representation that facilitates high quality classification, segmentation, shape synthesis, and point cloud completion.</p><p>We achieve this by splitting a point cloud into a front and a back half from different angles, where the front half is the half nearer to the viewpoint than the back half. This enables MAP-VAE to observe different semantic parts of a point cloud, and it also preserves the spatial relationship among the parts by incrementally varying angles.</p><p>For a point cloud M , we split M from V different angles into front halves (in red) and their complementary back halves (in green), as shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>, where the viewpoints are located around M on a circle. From the v-th viewpoint, M is split into a front half m F v and a back half m B v in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, where m F v is formed by the N nearest points (in red) of M to the viewpoint while m B v is formed by the N farthest points (in green) to the viewpoint. Geodesic splitting. A naive way of finding the N nearest points to define a front half m F v is to sort all points on M by the Euclidean distance between each point and the viewpoint. However, on some point clouds, this method may not produce semantic front halves, since the regions in a front half are not continuous, as demonstrated in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>. It is important to encode semantic front halves, since this would help MAP-VAE to seamlessly understand the entire surface from a viewpoint under a multi-angle scenario.</p><p>To resolve this issue, we leverage the geodesic distance on the point cloud <ref type="bibr" target="#b2">[3]</ref> to sort the points. Specifically, we first find a nearest point u to the v-th viewpoint on M by Euclidean distance. Then, we sort the rest of points on M in terms of their geodesic distances to the found nearest point u, as shown by the geodesic map in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>. Finally, u and its nearest N ? 1 points form the front half m F v , while the farthest N points form the back half m B v , as illustrated by the red part and green part in <ref type="figure" target="#fig_2">Fig. 3</ref> (c), respectively. Half-to-half sequence pairs. To leverage the correspondence between front half and back half and their spatial relationship under different angles, we establish a half-to-half sequence pair starting from each one of the V angles.</p><p>Along the circle direction of varying angles, as illustrated by the clockwise green arrow in <ref type="figure" target="#fig_1">Fig. 2 (b</ref> </p><formula xml:id="formula_0">, where S F = [m F v |v ? [1, V ], |v| = W ], S B = [m B v |v ? [1, V ], |v| = W ]</formula><p>and each element in S F corresponds to its complementary element in S B . Thus, a half-to-half sequence pair (S F , S B ) consists of S F and S B .</p><p>To comprehensively observe the point cloud, we select W angles which uniformly cover the whole shape in each half-to-half sequence pair (S F , S B ). As demonstrated by the dotted lines in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>, we select W = 3 angles in order to form the first (S F , S B ), and then, along the green arrow, we form the last (S F , S B ) by angles indicated by the solid lines. Each (S F , S B ) forms a training sample T i in company with the point cloud M . Finally, we obtain all <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>.</p><formula xml:id="formula_1">V training samples {T i |i ? [1, V ]} from M in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MAP-VAE</head><p>Aggregation branch A. For a training sample T i containing a half-to-half sequence pair (S F , S B ) and the point cloud M , aggregation branch A encodes the global geometry of M , local geometry of each one of W m F v in S F , and the spatial relationship among m F v . Aggregation branch A first extracts the geometry of each involved point cloud into a low-level feature by encoder, and then, aggregates all the low-level features with their spatiality by aggregation RNN U A . Specifically, we extract the low-level feature f of 2N points on M by a global encoder, and the low-level feature f F v of N points on m F v by a local encoder. Both the global and local encoders employ the same architecture as the encoder in PointNet++ <ref type="bibr" target="#b26">[27]</ref>, the only difference is the input number of points. Subsequently, aggregation RNN U A aggregates f and all f F v in W + 1 steps, where we employ GRU cell with 512 hidden state. Finally, we use the hidden state as the angle-specific feature h i of M since the first front half in S F is observed starting from the i-th angle. Reconstruction branch R. By decoding the learned anglespecific feature h i , reconstruction branch R tries to generate a point cloud M as similar as possible to the original point cloud M by a global decoder D. D is formed by 3 fully connected layers (1024-2048-6114) and 2 convolutional layers (with 256 and 3 1?1 kernels each), where batch normalization is used between every two layers. Here, we prefer Earth Movers distance (EMD) <ref type="bibr" target="#b28">[29]</ref> to Chamfer Distance (CD) <ref type="bibr" target="#b5">[6]</ref> to evaluate the distance between the reconstructed M and the original M , since EMD is more faithful than CD to the visual quality of point clouds <ref type="bibr" target="#b0">[1]</ref>. The EMD distance between M and M is regarded as the cost of reconstruction to optimize, as defined below, where ? is a bijection from a point x on M to its corresponding point ?(x) on M ,</p><formula xml:id="formula_2">C D = min ?:M ?M x?M x ? ?(x) 2 .<label>(1)</label></formula><p>In addition, we employ a variational constraint <ref type="bibr" target="#b20">[21]</ref> in reconstruction branch R to facilitate novel shape generation. This is implemented by a variational reparameterization process, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The variational reparameterization transforms the angle-specific feature h i into another latent vector z that roughly follows a unit multidimensional Gaussian distribution. After training, branch R can generate a novel shape by sampling a latent vector from the unit Gaussian to the global decoder D.</p><p>Specifically, the variational reparameterization first employs fully connected layers to respectively estimate the mean ? and variance ? for the distribution of h i . Then, a noise vector ? is sampled from a unit multi-dimensional Gaussian distribution N (0, 1), as ? ? N (0, 1) and ? ? R 1?Z . Finally, we scale the noise ? by ? and further shift it by ?, such that the latent vector z = ? + ? ?. The variational reparameterization enables reconstruction branch R to push the distribution q(z|h i ) to follow the unit multidimensional Gaussian distribution by minimizing the KL divergence between the distribution q(z|h i ) and N (0, 1).</p><p>Thus, the cost of reconstruction branch R is defined based on Eq. (1) below, where ? is a balance parameter.</p><formula xml:id="formula_3">C R = C D + ? ? KL(q(z|h i )||N (0, 1)).<label>(2)</label></formula><p>Prediction branch P. Similar to reconstruction branch R, prediction branch P decodes the learned angle-specific feature h i to predict the back half sequence S B corresponding to S F . Branch P tries to predict a back half sequence S B as similar as possible to S B by a prediction RNN U P . At each of W steps, U P predicts one back half m B v in the same order of elements in S B . This enables U P to learn the halfto-half correspondence and the spatial relationship among the halves of M . To further push MAP-VAE to comprehensively understand the point cloud, U P predicts the lowlevel feature f B v of each one of W back half m B v rather than the spatial point coordinates of m B v , which is complementary to reconstruction branch R. The ground truth low-level feature f B v of m B v is also extracted by the local encoder in branch A. Thus, the cost of branch P is defined as follows,</p><formula xml:id="formula_4">C P = 1 W ? v?[1,V ],|v|=W f B v ? f B v 2 2 .<label>(3)</label></formula><p>Objective function. For a sample T i , MAP-VAE is trained by minimizing all the aforementioned costs of each branch, as defined below, where ? is a balance parameter.</p><formula xml:id="formula_5">min C R + ? ? C P .<label>(4)</label></formula><p>After training, MAP-VAE represents the point cloud M as a global feature H by aggregating the angle-specific feature h i learned from each sample T i of M using max pooling, such that H = Pool i? <ref type="bibr">[1,V ]</ref> {h i }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results and analysis</head><p>In this section, we first explore how the parameters involved in MAP-VAE affect the discriminability of learned global features in shape classification. Then, MAP-VAE is evaluated in shape classification, segmentation, novel shape generation, and point cloud completion by comparing with state-of-the-art methods.  <ref type="table">Table.</ref> 2.  <ref type="table">Table.</ref> 3.</p><p>Training. We pre-train the global and local encoders in MAP-VAE respectively under the dataset involved in experiments in a self-reconstruction task, where the decoder in PointNet++ <ref type="bibr" target="#b26">[27]</ref> for segmentation is modified to work with our encoders to produce three dimensional point coordinates in the last layer. After each PointNet++-based autoencoder is trained, the pre-trained global and local encoders are fixed for more efficient training of MAP-VAE.</p><p>In all experiments, we choose a more challenging way to train MAP-VAE by all point clouds in multiple shape classes of a benchmark rather than a single shape class, where each point cloud has 2048 points and each half has N = 1024 points. In shape classification experiments, we train a linear SVM to evaluate the raw discriminability of the learned global feature H.</p><p>Initially, we employ V = 12 angles to analyze a point cloud and form a training sample by W = 6 angles uniformly covering the point cloud. We set balance parameter ? = 0.01 and ? = 1000 to make each cost in the same order of magnitude. We use a Z = 128 dimensional unit Gaussian for the variational constraint. Parameter setting. All experiments on parameter effect exploration are conducted under ModelNet10 <ref type="bibr" target="#b36">[37]</ref>.</p><p>We first evaluate how ? affects MAP-VAE by comparing the results of different ? candidates including {10, 100, 1000, 10000}. As shown in <ref type="table">Table 1</ref>, the results get better with increasing ? until ? = 1000 and degenerate when ? is too big. This observation demonstrates a proper range of ?. We use ? = 1000 in the following experiments.</p><p>Then, we evaluate how ? affects MAP-VAE by comparing the results of different ? candidates including {0.1, 0.05, 0.01, 0.005, 0.001}. As shown in <ref type="table">Table 2</ref>, the results get better with decreasing ? until ? = 0.01 and degenerate when ? is too small. This observation demonstrates how enforcing a unit Gaussian distribution on the latent vector z too loosely or strictly affects the discriminability of learned global features. We also visualize the point clouds reconstructed by branch R under different ?, as demonstrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. We find ? affects the reconstructed point clouds in a similar way to how it affects the discriminability of learned global features. In the following experiments, we set ? to 0.01.</p><p>Subsequently, we explore how the number of angles W of in a training sample affects the performance of MAP-VAE, as shown in <ref type="table">Table.</ref> 3, where several candidate W including {1, 3, 6, 12} are employed. We find W = 6 achieves the best result, where fewer angles provide less local information while more angles increase redundancy. We also observe a similar phenomenon in the reconstructed point clouds shown in <ref type="figure" target="#fig_4">Fig.5</ref>. In addition, we also explore other ways of distributing the W = 6 angles, such as continuously ("S-6") or randomly ("R-6"), respectively. We find our employed uniform placement is the best, since each training sample could cover the whole point cloud. In the following experiments, we use W = 6.</p><p>Finally, we explore the effect of the Z-dimensional unit Gaussian distribution. In <ref type="table">Table.</ref> 4, we compare the results obtained with different Z, including {32, 64, 128, 256}. The results get better with increasing Z until Z = 128 while degenerating when Z is too big. We believe Z depends on the number of training samples, and both 64 and 128 are good for Z under ModelNet10. Z is set to 128 below. Ablation study. We further explore how each module in MAP-VAE contributes to the performance. As shown in <ref type="table">Table.</ref> 5, we remove a loss each time to highlight the corresponding module. The degenerated results indicate that all elements contribute to the discriminability of learned features, and self-reconstruction ("No R") contributes more than the half-to-half prediction ("No P").  In addition, we highlight our half-to-half prediction by showing the results obtained only by the pre-trained global encoder in <ref type="figure" target="#fig_0">Fig. 1</ref> and this global encoder with a variational constraint (using the same balance weights as MAP-VAE), as shown by "AE" and "VAE". These results show that half-to-half prediction can help MAP-VAE understand point clouds better by leveraging effective local selfsupervision. Moreover, the lower result of "Euclid" also indicates geodesic distance is superior to Euclidean distance to obtain semantic front half in the splitting of a point cloud. Shape classification. We evaluate MAP-VAE in shape classification by comparing it with state-of-the-art methods under ModelNet40 <ref type="bibr" target="#b36">[37]</ref> and ModelNet10 <ref type="bibr" target="#b36">[37]</ref>. All the compared methods perform unsupervised 3D feature learning while using various 3D representations, including voxels, views and point clouds. As shown in <ref type="table" target="#tab_3">Table 6</ref>, MAP-VAE obtains the best performance among these methods under ModelNet10. We employ the same parameters involved in <ref type="table" target="#tab_2">Table 5</ref> to produce our result under ModelNet40, where MAP-VAE also outperforms all point cloud based methods. Although view-based VIPGAN is a little better than ours, it cannot generate 3D shapes. These results indicate that MAP-VAE learns more discriminative global features for point clouds with the ability of leveraging more effective local self-supervision. Note that the results of LGAN, FNet and NSampler are trained under a version of ShapeNet55 that contains more than 57,000 3D shapes. However, there are only 51,679 3D shapes from ShapeNet55 that are available for public download. Therefore, MAP-VAE cannot be trained under the same number of shapes. To perform fair comparison, we use the codes of LGAN and FNet to produce their results under the same shapes in ModelNet, as shown by "LGAN(MN)" and "FNet(MN)".  <ref type="bibr" target="#b25">[26]</ref> is employed in this experiment, where point clouds in 16 shape classes are involved to train MAP-VAE with the same parameters in <ref type="table" target="#tab_3">Table 6</ref>. We first extracted the learned feature of each point from the second-last layer in the global decoder D. Extracting the feature of each single point in the decoding procedure represents the ability of MAP-VAE to understand shapes locally at each point. Second, we map the ground truth label of each point to the reconstructed point cloud by voting 5 nearest labels for each reconstructed point. Third, we train a per-point softmax classifier under the training set, and test the classifier under the test set.</p><p>We use the same approach to obtain the results of the autoencoder in LGAN <ref type="bibr" target="#b0">[1]</ref>, and this autoencoder with a variational constraint. As the comparison shown in <ref type="table" target="#tab_4">Table 7</ref>, our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation comparison between "</head><p>LGAN" (a), "LGAN1" (b) in <ref type="table" target="#tab_4">Table 7</ref> and MAP-VAE (c). The ground truth is shown in (d). A color in the same row represents a part class. results significantly outperform "LGAN" and "LGAN1" in terms of both mIoU and classification accuracy. We further visualize the segmentation comparison on four cases in <ref type="figure" target="#fig_6">Fig. 7</ref>. We find that the captured local geometry information helps MAP-VAE not only to reconstruct point clouds better, but also to learn more discriminative features for each point for better segmentation. Finally, we show 100 segmentation results in two challenging shape classes, respectively, i.e., airplane and chair, in <ref type="figure" target="#fig_5">Fig. 6</ref>. The consistent segmentation results also justify the good performance of MAP-VAE. Shape generation. Next we demonstrate how to generate novel shapes using the trained MAP-VAE. Here, we first sample a noise vector from the employed Z-dimensional unit Gaussian distribution, and then, convey the noise to the global decoder D in branch R in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Using MAP-VAE trained under ModelNet10 in <ref type="table" target="#tab_3">Table 6</ref>, we generate some novel shapes in each of 10 shape classes in <ref type="figure" target="#fig_0">Fig. 10 (a)</ref>, where we sample 4 noise vectors around the feature center of each shape class to generate 4 shape class specific shapes. The generated point clouds are sharp and with high fidelity, where more local geometry details are learned. Moreover, we also observe high quality point clouds in the same shape class from MAP-VAE trained under ShapeNet part dataset in <ref type="table" target="#tab_4">Table 7</ref>. We generate 100 airplanes using 100 noise vectors sampled around the feature center of the airplane class, as demonstrated in <ref type="figure" target="#fig_0">Fig. 10 (b)</ref>.</p><p>We further show the point clouds generated by the interpolated features between two feature centers of two shape classes, where the MAP-VAE trained under ModelNet10 in <ref type="table" target="#tab_3">Table 6</ref> is used. The interpolated point clouds are plausible and meaningful, and smoothly changed from one center to another center in <ref type="figure" target="#fig_0">Fig. 11 (a)</ref> and (b). Similar results can be observed by interpolations between two shapes in the same class in <ref type="figure" target="#fig_0">Fig. 11 (c) and (d)</ref>, where the MAP-VAE trained under ShapeNet part dataset in <ref type="table" target="#tab_4">Table 7</ref> is used. <ref type="figure">Figure 8</ref>. Compared to the three generated class centers under ModelNet10, MAP-VAE (in (c)) learns more local geometry details than "LGAN" (in (a)) and "LGAN1" (in (b)) in <ref type="table" target="#tab_4">Table 7</ref>, due to the effective local self-supervision in half-to-half prediction. <ref type="figure">Figure 9</ref>. Visual comparison with "LGAN" (in (b)) in <ref type="table" target="#tab_4">Table 7</ref> and "PCN-EMD" (in (c)) in <ref type="table" target="#tab_5">Table 8</ref> for the completion of partial point clouds (a). MAP-VAE (in (d)) completes more geometry details.</p><p>Finally, we visually highlight the advantage of MAP-VAE by the point cloud generated at the feature center of a shape class. As demonstrated in <ref type="figure">Fig. 8</ref>, we compare MAP-VAE with the autoencoder in LGAN <ref type="bibr" target="#b0">[1]</ref>, and this autoencoder with a variational constraint. Using the trained decoder of each method, we generate a point cloud from the feature center at each of the three shape classes. Compared to the three class centers in <ref type="figure">Fig. 8 (a)</ref> and <ref type="figure">Fig. 8 (b)</ref>, MAP-VAE in <ref type="figure">Fig. 8 (c)</ref> can generate point clouds with more local geometry details, such as sharp edges of parts.  Point cloud completion. MAP-VAE can also be used in point cloud completion, where we set W = 12 and remove the KL loss for fair comparison. We evaluate our performance under the training and test sets of partial point clouds in two challenging shape classes in <ref type="bibr" target="#b3">[4]</ref>, i.e., airplane and chair, where we employ the complete point clouds in <ref type="bibr" target="#b25">[26]</ref> as ground truth. Since each partial point cloud has different number of points, we resample 2048 points to obtain the front and back halves. We compare with the state-of-the-art methods in <ref type="table" target="#tab_5">Table 8</ref>. The lowest EMD distance shows that MAP-VAE outperforms all competitors. In addition, we also visually compare the completed point clouds in <ref type="figure">Fig. 9</ref>, where MAP-VAE completes more local geometry details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We propose MAP-VAE for unsupervised 3D point cloud feature learning by jointly leveraging local and global selfsupervision. MAP-VAE effectively learns local geometry and structure on point clouds from semantic local selfsupervision provided by our novel multi-angle analysis. The outperforming results in various applications show that MAP-VAE successfully learns more discriminative global or local features for point clouds than state-of-the-art. <ref type="figure" target="#fig_0">Figure 11</ref>. We show shape interpolation results between two different shape classes under ModelNet10 in (a) and (b), and the shape interpolation results between two shapes in the same class under the ShapeNet part dataset in (c) and (d). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The framework of MAP-VAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Geodesic splitting M into a front half m F v (in red) and a back half m B v (in green) from the v-th angle. (b) M is further split from all V angles located around M in clockwise order, where subset with W = 3 out of V angles (indicated by dotted or solid line) are selected to establish a half-to-half sequence pair. (c) The training sample Ti from each one of V angles. all front halves in sequence S F in order. It first extracts the low-level feature f of the original point cloud M and the low-level feature f F v of each front half m F v by a global encoder and a local encoder, respectively. Then, it learns the angle-specific feature h i of M by aggregating all low-level features f F v using an aggregation RNN U A . The reconstruction branch R performs selfreconstruction by decoding the learned angle-specific feature h i into a point cloud M . This reconstruction is conducted by a global decoder D which tries to generate M as similar as possible to M . In addition, R employs a variational constraint to facilitate novel shape generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The comparison of front halves (red parts in (a) and (c)) split by Euclidean distance and geodesic distance (map in (b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The point clouds are reconstructed in (b)-(f) under different ? compared in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The original point clouds in (a) are reconstructed in (b)-(e) under different W compared in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>We show segmentation results from the airplane class in (a) and the chair class in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>High fidelity novel shape generation by MAP-VAE trained under ModelNet10 in (a) and ShapeNet part dataset in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>), we select front halves m F v and their complementary back halves m B v from W out of the V angles. The selected m F v form a front half sequence S F while the complementary m B v form a back half sequence S B</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .Table 3 .Table 4 .</head><label>1234</label><figDesc>The effect of ?, ? = 0.01, W = 6, Z = 128. ACC% 93.72 93.94 94.82 93.72 The effect of ?, ? = 1000, W = 6, Z = 128. The effect of W , ? = 0.01, ? = 1000, Z = 128. ACC% 92.95 93.39 94.82 93.17 93.39 92.95 The effect of Z, ? = 0.01, ? = 1000, W = 6.</figDesc><table><row><cell>?</cell><cell></cell><cell>10</cell><cell>100</cell><cell cols="2">1000 10000</cell><cell></cell></row><row><cell>?</cell><cell>0.1</cell><cell></cell><cell>0.05</cell><cell cols="2">0.01 0.005 0.001</cell><cell></cell></row><row><cell cols="6">ACC% 92.62 92.84 94.82 93.39 93.17</cell><cell></cell></row><row><cell>W</cell><cell>1</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell>S-6</cell><cell>R-6</cell></row><row><cell>Z</cell><cell></cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell></cell></row><row><cell cols="6">ACC% 93.28 94.16 94.82 93.94</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Ablation study,? = 0.01,? = 1000,W = 6,Z = 128.</cell></row><row><cell>No R No P No KL</cell><cell>All</cell><cell>AE</cell><cell>VAE Eucli</cell></row><row><cell cols="4">% 91.63 92.40 93.17 94.82 92.29 93.28 93.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">The classification accuracy (%) comparison among un-</cell></row><row><cell cols="4">supervised 3D feature learning methods under ModelNet40 and</cell></row><row><cell cols="3">ModelNet10. ? = 0.01,? = 1000,W = 6,Z = 128.</cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">Modality MN40% MN10%</cell></row><row><cell>T-L Network[8]</cell><cell>Voxel</cell><cell>74.40</cell><cell>-</cell></row><row><cell>Vconv-DAE[30]</cell><cell>Voxel</cell><cell>75.50</cell><cell>80.50</cell></row><row><cell>3DGAN[36]</cell><cell>Voxel</cell><cell>83.30</cell><cell>91.00</cell></row><row><cell>VSL[24]</cell><cell>Voxel</cell><cell>84.50</cell><cell>91.00</cell></row><row><cell>VIPGAN[17]</cell><cell>View</cell><cell>91.98</cell><cell>94.05</cell></row><row><cell>LGAN[1]</cell><cell>Points</cell><cell>85.70</cell><cell>95.30</cell></row><row><cell>LGAN[1](MN)</cell><cell>Points</cell><cell>87.27</cell><cell>92.18</cell></row><row><cell>NSampler[28]</cell><cell>Points</cell><cell>88.70</cell><cell>95.30</cell></row><row><cell>FNet[40]</cell><cell>Points</cell><cell>88.40</cell><cell>94.40</cell></row><row><cell>FNet[40](MN)</cell><cell>Points</cell><cell>84.36</cell><cell>91.85</cell></row><row><cell>MRTNet[7]</cell><cell>Points</cell><cell>86.40</cell><cell>-</cell></row><row><cell>3DCapsule[42]</cell><cell>Points</cell><cell>88.90</cell><cell>-</cell></row><row><cell>PointGrow[33]</cell><cell>Points</cell><cell>85.80</cell><cell>-</cell></row><row><cell>PCGAN[2]</cell><cell>Points</cell><cell>87.80</cell><cell>-</cell></row><row><cell>Our</cell><cell>Points</cell><cell>90.15</cell><cell>94.82</cell></row><row><cell cols="4">Shape segmentation. We evaluate the local features</cell></row><row><cell cols="4">learned by MAP-VAE for each point in shape segmentation.</cell></row><row><cell cols="2">The ShapeNet part dataset</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>The segmentation comparison among unsupervised 3D feature learning methods under ShapeNet part dataset. The metric is mIoU(%) and per-point classification accuracy(%) on points. ? = 0.01,? = 1000,W = 6,Z = 128. Lamp Laptop Motor Mug Pistol Rocket SkateTable mIoULGAN<ref type="bibr" target="#b0">[1]</ref> 57.04 54.13 48.67 62.55 43.24 68.37 58.34 74.27 68.38 53.35 82.62 18.60 75.08 54.70 37.17 46.71 66.39 LGAN1 [1] 56.28 52.16 57.85 62.66 42.01 67.66 52.25 75.37 68.63 49.07 81.52 19.20 75.43 54.34 35.09 41.48 65.73 Ours 67.95 62.73 67.08 72.95 58.45 77.09 67.34 84.83 77.07 60.89 90.84 35.82 87.73 64.24 44.97 60.36 74.75 ACC LGAN [1] 78.24 74.93 84.36 77.02 71.10 78.23 78.34 84.41 78.29 69.05 86.86 67.93 90.42 81.95 68.44 82.27 78.25 LGAN1 [1] 77.35 73.64 84.05 75.93 69.82 77.35 77.45 83.72 78.10 68.45 85.85 66.06 89.69 81.43 67.59 81.10 77.33 Ours 87.45 83.50 93.79 86.12 83.28 87.03 88.08 93.15 86.66 79.31 94.89 77.37 98.86 90.51 77.14 93.21 86.25</figDesc><table><row><cell>Methods</cell><cell>Mean Aero</cell><cell>Bag</cell><cell>Cap</cell><cell>Car</cell><cell>Chair</cell><cell>Ear</cell><cell>Guitar Knife</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>The completion comparison under airplane and chair classes in terms of EMD/point, ? = 0,? = 1000,W = 12,Z = 0.</figDesc><table><row><cell>Class</cell><cell>EPN[4]</cell><cell cols="4">Folding[40] PCN-CD[41] PCN-EMD[41] LGAN[1]</cell><cell>Our</cell></row><row><cell cols="2">Airplane 0.061960</cell><cell>0.156438</cell><cell>0.046637</cell><cell>0.038752</cell><cell cols="2">0.033218 0.032328</cell></row><row><cell>Chair</cell><cell>0.076802</cell><cell>0.297427</cell><cell>0.086787</cell><cell>0.068074</cell><cell cols="2">0.055908 0.055696</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z B P R S</forename><surname>Chun-Liang Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<idno>abs/1810.05795</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The heat method for distance computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wardetzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="90" to="99" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3D local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11209</biblScope>
			<biblScope unit="page" from="620" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parts4feature: Learning 3d global features from generally semantic parts in multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3D local features from raw voxels based on a novel permutation voxelization strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="481" to="494" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mesh convolutional restricted boltzmann machines for unsupervised learning of features with structure preservation on 3D meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Network and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2268" to="2281" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised 3D local feature learning by circle convolutional restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5331" to="5344" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep spatiality: Unsupervised learning of spatiallyenhanced global and local 3D features by deep neural network with coupled softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3049" to="3063" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BoSCC: Bag of spatial context correlations for spatially enhanced 3D shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3707" to="3720" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregating sequential views for 3d global feature learning by cnn with hierarchical attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3986" to="3999" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">View interprediction gan: Unsupervised representation learning for 3D shapes by learning global shape memories to support local view predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seqviews2seqlabels: Learning 3D global features via aggregating sequential views by rnn with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1941" to="1983" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Y2seq2seq: Cross-modal representation learning for 3D shape and text by joint reconstruction and prediction of view and word sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3Dviewgraph: Learning global features for 3d shapes from a graph of unordered views with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a hierarchical latent-variable model of 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G O</forename><surname>Ii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Point2sequence: Learning the shape representation of 3D point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neuralsampler: Euclidean point cloud auto-encoder and sampler. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">VConv-DAE: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pointwise:an unsupervised point-wise feature learning network. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pointgrow: Autoregressively learned point cloud generation with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<idno>abs/1810.05591</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning localized generative models for 3D point clouds via graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno>abs/1801.07829</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11212</biblScope>
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 International Conference on 3D Vision</title>
		<meeting>2018 International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<idno>abs/1812.10775</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
