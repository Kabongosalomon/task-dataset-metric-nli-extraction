<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Extreme Inception Network for Edge Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Soria</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Autonomous University of Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMAYUK Research Group</orgName>
								<orgName type="institution">National University of Chimborazo</orgName>
								<address>
									<settlement>Riobamba</settlement>
									<country key="EC">Ecuador</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Sappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Autonomous University of Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">FIEC, CIDIS</orgName>
								<orgName type="institution">ESPOL Polytechnic University</orgName>
								<address>
									<settlement>Guayaquil</settlement>
									<country key="EC">Ecuador</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricio</forename><surname>Humanante</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">UMAYUK Research Group</orgName>
								<orgName type="institution">National University of Chimborazo</orgName>
								<address>
									<settlement>Riobamba</settlement>
									<country key="EC">Ecuador</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Arbarinia</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Justus-Liebig University</orgName>
								<address>
									<settlement>Giessen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Extreme Inception Network for Edge Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Edge Detection Deep Learning CNN Contour Detection Boundary Detection Segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Edge detection is the basis of many computer vision applications. State of the art predominantly relies on deep learning with two decisive factors: dataset content and network's architecture. Most of the publicly available datasets are not curated for edge detection tasks. Here, we offer a solution to this constraint. First, we argue that edges, contours and boundaries, despite their overlaps, are three distinct visual features requiring separate benchmark datasets. To this end, we present a new dataset of edges. Second, we propose a novel architecture, termed Dense Extreme Inception Network for Edge Detection (DexiNed), that can be trained from scratch without any pretrained weights. DexiNed outperforms other algorithms in the presented dataset. It also generalizes well to other datasets without any fine-tuning. The higher quality of DexiNed is also perceptually evident thanks to the sharper and finer edges it outputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Edges provide important clue in visual information processing, from classical computer vision algorithms in image recognition <ref type="bibr" target="#b1">[2]</ref> to modern techniques in generative adversarial networks (GAN) <ref type="bibr" target="#b2">[3]</ref>. Despite their importance, robust edge detection remains an open problem. To demonstrate this, let's examine four example images in <ref type="figure" target="#fig_1">Fig. 1</ref>. The third to fifth rows illustrate the results of three state-of-the-art models (RCF <ref type="bibr" target="#b3">[4]</ref>, BDCN <ref type="bibr" target="#b4">[5]</ref>, and CATS <ref type="bibr" target="#b5">[6]</ref>) that are trained on the BSDS dataset <ref type="bibr" target="#b0">[1]</ref>. It is qualitatively visible that neither of these models faithfully detects the edges of tiny details. For instance, while the internal edges of the bell sculpture or the helicopter are fully annotated in the ground truth, the edge output of those models do not capture these details. We can observe a similar phenomenon in the stripes of the building and the zebra. In this case, although these details are also excluded in the ground truth, the edges are clearly visible in the image. Grounded on this, we argue that current models of edge detection require further improvement to robustly detect edges and generalize well to new scenes independent of their training set.   <ref type="bibr" target="#b0">[1]</ref>. The suffix in model names (i.e., BSDS and BIPEDv2) indicates the training set of that model.</p><p>Following this, we argue that one of the main limitations of current deep learning (DL) approaches is the annotated ground truth in the training set. BSDS is widely accepted within the community as the benchmark dataset to train and evaluate edge-detection algorithms. However, this dataset was originally designed and annotated for scene segmentation. Therefore the annotated ground truth correspond mainly to high-level object boundaries rather than low-level edges. Given the DL based models are strongly shaped by their training data, we need to acquire independent datasets for three tasks of edge-, contour, and boundary-detection <ref type="bibr" target="#b6">[7]</ref>. This is of great importance for both training and evaluation. It is challenging for a network to learn these three concepts from a dataset that blends edges, contours and boundaries. It is also difficult to judge the fitness of a network, whether it is performing better or worse in one of those tasks.</p><p>To showcase this importance at a glance, we can look at the bottom two rows of <ref type="figure" target="#fig_1">Fig. 1</ref>. The only difference between those two rows is the training set. When our network-Dense Extreme Inception Network for Edge Detection (DexiNed)-is trained on BSDS, it suffers from the same set of problems as other models. However, when we train DexiNed on our dataset (BIPEDv2), it generalizes well to BSDS images in fine-scale edges. The image content of these two datasets is rather dissimilar. The BIPEDv2 mainly contains images of the urban settings. The accurate detection of edges in the zebra and bell sculpture images suggests that the DexiNed-BIPEDv2 is robust to novel scenes. This robustness is thanks to the careful annotation of edges in the proposed dataset. We ensured all the edges, within or across objects, are reflected in the ground truth data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Edges, contours and boundaries</head><p>The following three terms: edge-, contour-and boundary-detection are often used interchangeably despite their differences. This is a potential cause of confusion in the interpretation and evaluation of models. Thus, we start by reviewing the origin of each term. In the 1980s, edge detection was defined as the intensity changes in a vicinity originated by discontinuities along the surface, reflectance, or illumination <ref type="bibr" target="#b7">[8]</ref>. This was revisited emphasizing the properties of objects, such as their photometrical, geometrical and physical characteristics <ref type="bibr" target="#b8">[9]</ref>. Accordingly, contours and boundaries are a subset of edges and are associated with semantically meaningful entities <ref type="bibr" target="#b9">[10]</ref>, for example, the silhouette and outline of an object <ref type="bibr" target="#b10">[11]</ref>. Boundary refers to the object borders in the image plane, corresponding to pixel ownership within a scene <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. On the contrary, contour refers to the borders of a region within a given object <ref type="bibr" target="#b6">[7]</ref>.</p><p>To summarize, the goal of an edge detection algorithm is to capture all the meaningful intensity discontinuities in an image. It does not concern itself with pixel ownership or open and closed shapes. These are the domain of contour and boundary detection to eliminate edges that do not correspond to salient features of objects and shapes. In this article, we focus on edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Structure of data matters</head><p>Currently, deep learning is the primary approach in the task of edge detection. It is well established that the quality of the training dataset is a critical factor within this framework <ref type="bibr" target="#b13">[14]</ref>. To this end, state-of-the-art heavily relies on the Berkeley segmentation dataset (i.e., BSDS300 <ref type="bibr" target="#b12">[13]</ref> and BSDS500 <ref type="bibr" target="#b0">[1]</ref>) even though image segmentation was the original objective of this dataset and its refined version turned into a benchmark dataset of boundary detection. Most studies train their models on BSDS300 and validate on BSDS500.</p><p>Each image contains boundary annotations drawn by at least five different participants. In many cases, there is a large discrepancy among different human-drawn boundaries (see <ref type="figure">Fig. 2</ref>). This issue is partially overcome by computing the loss function over a consensus ground truth, e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. In the consensus stage, pixels that are annotated by more than two participants are considered as true edges, otherwise discarded. This facilitates the convergence of the training process <ref type="bibr" target="#b14">[15]</ref>. Nevertheless, this consensus stage also filters out a large portion of true edges. This is visually evident in <ref type="figure">Fig. 2</ref>. For instance, all the zebra stripes vanish even with a moderate consensus </p><formula xml:id="formula_0">(b) BSDS500 [1]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>A careful examination of the the BSDS benchmark datasets for two sample images and their corresponding ground truths. There is a large discrepancy among human-drawn "edges". Enforcing consensus among two/three annotations results in the elimination of true edges. This issue demonstrates that BSDS is a suitable dataset to evaluate boundaries but not edges.</p><p>of two participants. A similar observation can be made in the case of the red car. This problem exists in many images of the BSDS. Hence, the networks trained on the BSDS learn to become a boundary detector rather than an edge detector. The distinction between the two can be of great importance, for example, when edges are the building blocks of another computer vision application, as in <ref type="bibr" target="#b15">[16]</ref>. This issue is addressed in this study. We present a benchmark dataset of edges that allows for an accurate evaluation of edge detection algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Contributions</head><p>In this paper, we address the aforementioned issues by proposing an end-to-end deep learning approach for the task of edge detection. Our primary objective is to identify all true edges in any given image. To demonstrate this, we train a network only on one dataset and evaluate it on several publicly available datasets. We show that our model obtains a higher degree of generalization in comparison to the other algorithms of state-of-the-art. We conduct an exhaustive quantitative evaluation on two datasets with annotated edges (i.e., MDBD <ref type="bibr" target="#b6">[7]</ref> and BIPED). On other datasets with contour/boundary annotation, we qualitatively show the robustness of our model. In the current work, we extend our previous conference paper <ref type="bibr" target="#b16">[17]</ref> in the following aspects:</p><p>? Presenting a detailed description of the proposed architecture, DexiNed (Dense eXtreme Inception Network for Edge Detection), and thoroughly analysing the impact of its different components.</p><p>? Boosting the degree of annotation, specifically for fine scale edges, in the proposed benchmark dataset of edge detection, referred to as BIPED (Barcelona Images for Perceptual Edge Detection) 1 .</p><p>? Establishing a common evaluation benchmark among four state-of-the-art networks by interchanging the training and validation set between two datasets of edge detection (BIPED and MDBD).</p><p>? Improving the loss functions from BDCN <ref type="bibr" target="#b4">[5]</ref> by modifying the values to better balance the portion of positive and negative samples in each DexiNed output and incorporating averaging at the level of pixels.</p><p>The rest of the paper is organized as follow. Section 2 summarizes the most relevant works on edge detection. Then, the proposed approach is described in Section 3. Acquired dataset and ground truth generated for training and testing, as well as the datasets used for validating the proposed DexiNed architecture, are presented in Section 4. In Section 5, quantitative and qualitative details are summarized. Finally, conclusions are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a large body of literature on edge detection, for a detailed description see reviews on <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>. In this section, a set of representative algorithms are detailed. They can be broadly categorized into four groups: ) driven by low-level features; ) brain-inspired; ) classical learning-based; and ) deep learning. Algorithms driven by low-level features: The early edge detection algorithms such as Sobel <ref type="bibr" target="#b17">[18]</ref>, Robert and Prewitt <ref type="bibr" target="#b18">[19]</ref> are based on the first-order derivative. The input image is smoothed by the linear local filters, normally, a set of two orientations (horizontal and vertical). In the end, edges are detected by thresholding. These operators advanced by considering the second-order derivative, where edges are detected by the extraction of zero-crossing points. In the mid-eighties Canny <ref type="bibr" target="#b7">[8]</ref> proposed an edge detector by grouping three different key processes. Despite being antiquated, these approaches are still used in some modern computer vision applications. Many variants of the above-mentioned algorithms have been present in the literature.</p><p>Brain-inspired algorithms: This set of edge detection algorithms rely on known mechanisms of the biological visual system. Since the 1960s, experiments on monkeys and cats started resulted in important advances on the understanding of Primary Visual Cortex (V1). For instance, it was discovered that a group of simple cells are responsive to edges <ref type="bibr" target="#b19">[20]</ref>. The authors in <ref type="bibr" target="#b19">[20]</ref> develop a mathematical model to simulate the human retinal vision by Gaussian derivatives; since then, many algorithms have been implemented resembling the edge detection of the biological neurons in real-world images. For example, in <ref type="bibr" target="#b20">[21]</ref> a special line weight function is introduced for edge enhancement, which consists of a combination of zero and second-order Hermite functions. Later on, in <ref type="bibr" target="#b11">[12]</ref> Gabor energy maps have been proposed to recreate the non-classical receptive field of V1 for contour detection. This proposal has been evaluated with 40 images-this dataset could be considered as the first annotated ground truth proposed in the literature to validate contour detection algorithms. This approach has been later on improved by <ref type="bibr" target="#b21">[22]</ref> by modeling spatial facilitation and surround inhibition with local grouping functions and Gabor filters respectively. Recently, in <ref type="bibr" target="#b22">[23]</ref>, a modulation of double opponent cells is performed to extract more complex edge properties from color and texture. This was complemented by the introduction of the second visual area and accounting for feedback connections <ref type="bibr" target="#b23">[24]</ref>. Subsequent experiments demonstrated that modeling various surround modulations, typical in neurons of the visual cortex, boost the performance of edge detection <ref type="bibr" target="#b23">[24]</ref>. A combination of three Gabor filters at different scales was proposed in <ref type="bibr" target="#b6">[7]</ref> followed by a PCA and a machine-learning classifier. Lading us to the next group that are learning-based.</p><p>Classical learning-based algorithms: The challenge of edge detection in natural scenes has motivated the development of the learning-based algorithm. One of the first learning approaches for boundary detection has been presented in <ref type="bibr" target="#b12">[13]</ref>, which uses different filters to extract gradients from brightness, color and texture. The authors then train a logistic regression classifier to generate a Probabilistic boundary (Pb). Later on, different variants of Pb detectors have been proposed, like the one from <ref type="bibr" target="#b24">[25]</ref>. These new proposals focus on global information besides using local information like the original approach. They are referred to as globalized Probability of boundary (gPb). In these approaches, the gradients are computed by three scales for each image channel individually. In <ref type="bibr" target="#b25">[26]</ref>, a conditional random field-based approach has been proposed. Its maximum likelihood parameters are trained given the image edge annotations. This model captures the continuity and frequency of different junctions by a continuity structure. On the contrary, the usage of a sparse code gradient has been proposed in <ref type="bibr" target="#b26">[27]</ref>. In this case, the patches are classified by a support vector machine. Lastly, the usage of a Bayesian model has been considered in <ref type="bibr" target="#b27">[28]</ref>. Edgemaps have been predicted by a Sequential Monte Carlo based approach using different gradient distributions. The random forest framework is also considered for edge detection, where each tree is trained independently to capture complex local edge structures <ref type="bibr" target="#b28">[29]</ref>.</p><p>Deep learning algorithms: During the last decade, CNNs have become the standard model in computer vision <ref type="bibr" target="#b29">[30]</ref>. It has been shown that the internal representation of object classification networks relies on edges <ref type="bibr" target="#b30">[31]</ref>. The first CNN based model for edge detection has been proposed in <ref type="bibr" target="#b31">[32]</ref>. In this approach, like in most DL models, given an annotated edge-map dataset, the network learns to predict edges from an RGB image. Since this original work, several methods have been proposed <ref type="bibr" target="#b9">[10]</ref>; the backbone of most architectures is VGG16 <ref type="bibr" target="#b32">[33]</ref>. From this architecture, just the convolutional layers are used (13 convolutional layers). The parameters of this architecture are obtained by training the network in the ImageNet dataset <ref type="bibr" target="#b29">[30]</ref> as a classification problem. Then, the network is fine-tuned by training it on the datasets conceived for boundary detection. This is the case of HED <ref type="bibr" target="#b14">[15]</ref>, RCF <ref type="bibr" target="#b3">[4]</ref>, CED <ref type="bibr" target="#b33">[34]</ref>, BDCN <ref type="bibr" target="#b4">[5]</ref>, CATS <ref type="bibr" target="#b5">[6]</ref> and many others. In HED <ref type="bibr" target="#b14">[15]</ref> a multi-scale network together with a deep supervision technique is configured. Hence, each VGG block has as an output an edge prediction, which can be considered as a space scale representation. RCF <ref type="bibr" target="#b3">[4]</ref> uses the same configuration as HED, but instead of getting edges from each VGG block, it extracts edges from each layer on the blocks. On the contrary to previous approaches, in CED refinement blocks are used to merge outputs from every single block and predicts boundaries sharper than the ones from HED. Even though the quantitative performance of the aforementioned methods overcomes the state-of-the-art of classical learning algorithms, the predicted edge-maps are not as sharp as expected, somehow detected edges are coarse. Hence, Generative Adversarial Networks (GAN) <ref type="bibr" target="#b34">[35]</ref> have been considered to sharpen those coarse edges (e.g., <ref type="bibr" target="#b35">[36]</ref>); lastly, Context-Aware Tracing Strategy (CATS) is proposed to improve the edge-map crispness.</p><p>Although most of the DL based models for edge detection have been based on the usage of VGG16, there are a few approaches that are based on other models, like ResNet50 <ref type="bibr" target="#b36">[37]</ref> that is used in <ref type="bibr" target="#b3">[4]</ref>. With these variants, tinny improvements have been obtained, about 1%. Despite that, in the current work VGG16 is going to be described as the standard architecture used on the edge detection approaches. Most of the models based on VGG16 outperform traditional edge detection methods in the standard edge detection datasets such as BSDS <ref type="bibr" target="#b0">[1]</ref>, NYUD <ref type="bibr" target="#b37">[38]</ref>, PASCAL-CONTEXT <ref type="bibr" target="#b38">[39]</ref> and MDBD-Multicue Dataset for Boundary Detection <ref type="bibr" target="#b6">[7]</ref>. Despite this, even after obtaining high ranked performance, some of these methods still present drawbacks or lack of generalization or coarser predicted edge-maps, see results in <ref type="figure" target="#fig_1">Fig. 1</ref>. For instance, it can be observed that in some cases the generated edge-maps do not predic the human perceptual edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>This section presents the proposed architecture and the loss function used to train the model proposed in the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dense Extreme Inception Network for Edge Detection</head><p>DexiNed is designed to allow an end-to-end training without the need to weight initialization from pre-trained object detection models, like in most of DL based edge detectors. In our previous work <ref type="bibr" target="#b39">[40]</ref> we observed that the edge-features computed in shallow layers are often lost in the deeper layers. This inspired us to design an architecture similar to Xception <ref type="bibr" target="#b40">[41]</ref> but with two parallel skip-connections that materialize on all the edge information computed across different layers. DexiNed can be interpreted as a collection of two sub-networks (see <ref type="figure" target="#fig_3">Fig. 3</ref>): the dense extreme inception network (Dexi) and the upsampling network (USNet). Dexi receives an RGB image as input processing it in different blocks, whose feature-maps are fed to USNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dexi</head><p>The Dexi architecture contains six blocks acting similar to an encoder. Each block is a collection of smaller sub-blocks with a group of convolutional layers. Skip-connections couple the blocks as well as their sub-blocks (depicted in light gray and blue rectangular shapes in <ref type="figure" target="#fig_3">Fig. 3</ref>). The feature-maps generated at each of the blocks are fed to a separate USNet to create intermediate edge-maps. These intermediate edge-maps are concatenated to form a stack of learned filters. At the very end of the network, these features are fused to generate a single edge-map.</p><p>Each sub-block (blue rectangles in <ref type="figure" target="#fig_3">Fig. 3</ref>) constitutes two convolutional layers (the number of kernels is specified at the right side of blue rectangles). All kernels are of size 3 ? 3. In the very first block, convolutions are with a stride of 2, hence 2 in its name. Each convolutional layer is followed by batch normalization and a rectified linear unit (ReLU). From the Block 3 (light grey rectangles) the last convolutional, of the last sub-block, does not contain ReLU function. Rectangles in red are max-polling operators with a 3 ? 3 kernel size and stride of 2.</p><p>The multitude of convolutional operations over the depth of processing causes important edge features to vanish <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b39">40]</ref>. To avoid this issue we introduce parallel skip-connections, inspired by   [37] and <ref type="bibr" target="#b42">[43]</ref>. From the third block (Block-3) forward, the output of each sub-block is averaged with another skip-connection termed second skip-connections-SSC (green rectangles on the right side of <ref type="figure" target="#fig_3">Fig. 3</ref>). After the max-pooling operation, these SSC average the output of connected subblocks prior to summation with the first skip-connection-FSC,green rectangles on the left side of <ref type="figure" target="#fig_3">Fig. 3</ref>. In parallel to this, the output of max-pooling layers is directly fed to subsequent subblocks. For instance, the sub-blocks of block-3 receive input from the first max-pooling; and the sub-blocks of block-4 receive input with a summation of the first and second max-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">USNet</head><p>The upsampling network (USNet) is a conditional stack of two blocks. Each one of then consists of a sequence of one convolutional and deconvolutional layer that up-sample features on each pass. The block-2 gets activated only to scale the input feature-maps from the Dexi network. This block is iterated until the feature-map reaches a scale twice the size of the GT. Once this condition is met, the feature-map is fed to the block-1. The block-1 processes the input with a kernel of size 1 ? 1, followed by a ReLU activation function. Next, it performs a transpose convolution (deconvolution) with a kernel of size ? , where is the input feature-map scale level. With the last deconvolution of block-1, the feature-map reaches the same size as the GT. The last convolutional layer does not have an activation function.</p><p>We considered three strategies for upsampling blocks: bi-linear interpolation, sub-pixel convolution and transpose convolution. This is an influential factor in generating thin edges, a desired feature that, for example, enhances the visualization of edge-maps. We considered this point in the design of DexiNed and a detailed evaluation of it is presented in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head><p>The DexiNed can be formalized as a regression mapping function ?(?); in other words,? = ?( , ), where the input image is ? ? ? ? and ? {0, 1} ? ?1 is the corresponding ground truth edge-map, , , and are image sizes-is set to 3 due to the RGB channels used in the model's training. The output? corresponds to a set of predicted edge-maps,? = [? 1 ,? 2 , ...,? ], is the edge-map predicted from the DexiNed predictions, is the number of outputs from DexiNed (rectangles in gray, <ref type="figure" target="#fig_3">Fig. 3</ref>) and the last fused edge-map.? corresponds to the result of the last convolutional layer, which fuse the concatenation of all? (rectangles in gray), when the weight is initialized by 1?( ? 1). Note that? has the same size as . The loss function in our preliminary work <ref type="bibr" target="#b16">[17]</ref> was considered from HED <ref type="bibr" target="#b14">[15]</ref> (weighted cross-entropy). In the current work loss functions from HED, RCF and BDCN have been evaluated; after such evaluation the BDCN loss function, ?, has been selected with a little modification as depicted below: </p><p>then,</p><formula xml:id="formula_2">? = ? =1 *<label>(2)</label></formula><p>where is the weight of the cross-entropy loss, ? and + denote negative and positive edge samples in the given GT, respectively. is the sigmoid function. The is a set of hyper-parameters used to balance the number of positive and negative samples. Those values will be given in the implementation details section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>This section details the datasets used for training and evaluation the proposed approach as well as to compare it with state-of-the-art approaches. The second contribution of the current work is the update of the original benchmark dataset, referred to as Barcelona Images for Perceptual Edge Detection (BIPED), which has been initially presented in our previous work <ref type="bibr" target="#b16">[17]</ref>. The update presented in the current work is detailed below. All the images contained in the seminal work have been processed again. The BIPED dataset contains 250 real-world images, of 1280 ? 720 pixels, of urban environments. Ground truth (GT) edge-maps have been generated using the crowdsourcing tool Labelbox 2 . <ref type="figure">Figure 4</ref> presents an illustration from this dataset along with its annotated edges. The pink lines in <ref type="figure">Fig. 4( )</ref>, which are polylines, correspond to edges used to draw open contours in the scene; on the contrary, blue lines, which are polygonal lines, correspond to closed contours in scene. <ref type="figure">Figure 4</ref>( ) shows the edge-map corresponding to the <ref type="figure">Fig. 4( )</ref>. The annotation process has been applied over the whole dataset. To generate high precision edge-maps, each image was processed following four steps:</p><p>1. Computer vision experts annotate all the edges on each image, just one annotation per image. 2. The administrator (also a computer vision expert) reviews the output of the previous step. 3. The obtained GTs, are used to train the HED model <ref type="bibr" target="#b14">[15]</ref> and validate the sanity of predicted edges. 4. Considering the results of HED, the administrator crosscheck the entire dataset correcting mistakes and adding new annotations. This version was presented in <ref type="bibr" target="#b16">[17]</ref>. 5. With the updated version of LabelBox, improvements in Zoom tool make us to see additional details that were not considered in the previous version, due to the lack of deep appreciation on BIPED images. This make us to add more annotation in almost the whole images. The differences between initial annotations (BIPED-GTv1) and current one (BIPED-GTv2) can be appreciated in <ref type="figure" target="#fig_6">Fig. 5</ref>; new annotation tools allow to draw very tinny edges from the given images.</p><p>The administrator remains the same person throughout the whole process to ensure the same set of criteria is applied to all annotated images. The BIPED is a suitable dataset to benchmark results of edge detection algorithms. To this end, we have released it publicly for the benefit of the research community. The contribution of BIPED and DexiNed can be appreciated on recent works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, just to mention a few. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BIPED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BIPED-GTv1 BIPED-GTv2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Testing with Benchmark Datasets</head><p>A comprehensive qualitative evaluation of the proposed model has been conducted on the datasets most widely used in the literature for edge, contour, and boundary detection, namely MDBD <ref type="bibr" target="#b6">[7]</ref>, BSDS500 <ref type="bibr" target="#b0">[1]</ref>, BSDS300 <ref type="bibr" target="#b12">[13]</ref>, NYUD <ref type="bibr" target="#b37">[38]</ref>, PASCAL-CONTEXT <ref type="bibr" target="#b38">[39]</ref> (refereed hereinafter to as PASCAL), CID <ref type="bibr" target="#b11">[12]</ref>, DCD <ref type="bibr" target="#b45">[46]</ref> and our proposed BIPED. Within this list, MDBD is the most relevant dataset to the presented model, given its annotations correspond to true edges. The other datasets are more appropriate for the task of boundary and contour detections. In spite of that, results on all these datasets are presented to show the effectiveness of our approach and have a better comparison to state of the art.</p><p>MDBD: The Multicue Dataset for Boundary Detection was collected for psychophysical studies of object boundary perception in complex images. Multiple cues such as luminance, color, motion and the binocular disparity have been considered in its design <ref type="bibr" target="#b6">[7]</ref>. The MDBD is composed of short binocular video sequences of natural scenes (containing 10 frames per scene). Hundred high definition images of size 1280 ? 720 were extracted from these videos. Each image has been annotated by several participants (six times for edge detection and five times for boundary detection). The proposed DexiNed architecture has been evaluated with the edge annotations of this dataset. Usually, in this dataset, 80% of images are used for training, while the remaining are used to evaluate the learning-based algorithm. Following this, we randomly selected 20% of images to evaluate the performance of DexiNed. The models considered for quantitative evaluations have been trained again for a fair comparison with the 80% selected in the current work.</p><p>CID: The Contour Image Database is a set of 40 gray-scale images with their corresponding ground truth contours <ref type="bibr" target="#b11">[12]</ref>. The size of the images in this dataset is 512 ? 512. The main limitation of this dataset is related to the small number of annotated images. The proposed DexiNed architecture has been evaluated on the entire dataset, similarly to previous works in the literature (e.g., <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref>). This dataset is difficult for DL-based approaches due to their gray-scale nature and missed annotations in some of the provided images. DCD: Dataset of Contour Drawings is collected with the aim to generate contour drawings (boundary-like drawings that capture the outline of a given visual scene) <ref type="bibr" target="#b45">[46]</ref>. The DCD contains 1000 images, from that set 100 images are selected for testing. Ground truth annotations were collected by using a game application. For each image several annotations were collected; then, after an exhaustive evaluation, just five contours per each image were kept as ground truths. In the current work, after checking the provided ground truth, several wrong annotations have been found; <ref type="figure" target="#fig_7">Figure 6</ref>( ) shows an illustration where it can be easily appreciated the wrong contours provided by <ref type="bibr" target="#b45">[46]</ref>. Hence, in order to perform a fair evaluation of trained architectures the annotations of the 100 testing images have been carefully checked by removing wrong GTs. Hence, in the resulting set, which still has 100 images, each image contains between two and five annotations. <ref type="figure" target="#fig_7">Figure 6</ref>( ? ) shows the results after detecting and removing wrong contours provided by <ref type="bibr" target="#b45">[46]</ref>. BSDS500: Berkeley Segmentation DataSet (BSDS), the first version has been published in 2001 <ref type="bibr" target="#b12">[13]</ref> and consists of 300 images split up into 200 for training and 100 for validation, termed BSDS300; the last version <ref type="bibr" target="#b0">[1]</ref>, adds 200 new images for the testing. Every image in BSDS is annotated at least by 5 subjects, this dataset contains images of 481 ? 321. This dataset is mainly intended for image segmentation and boundary detection, therefore, as it will be illustrated in the next sections, for the edge detection purpose some images are not well annotated. Generally, to evaluate the performance of a DL model in BSDS500, the new 200 images are used for testing while the BSDS300 is used for the network training purpose. As DexiNed is trained only on BIPED dataset, the qualitative evaluation depicted in Section 5 split up the results in two parts: BSDS300 and BSDS500. The images considered for qualitative comparison are taken from the test part of BSDS500 and validation part from BSDS300.</p><p>NYUD: New York University Dataset is a set of 1449 RGBD images from 464 indoor scenarios, intended for segmentation purposes. This dataset is split up into three subsets-i.e., training, validation and testing sets. The testing set contains 654 images, while the remaining images are used for training and validation purposes. In the current work, just the testing set has been selected for evaluating the proposed model, since DexiNed has been trained just with BIPED. Although most of the images in NYUD are fully annotated for segmentation, there are a few of them with poor annotations. This fact (missing edges in some images) affects the quality of DL based edge detection approaches.</p><p>PASCAL: The PASCAL <ref type="bibr" target="#b38">[39]</ref> is a popular dataset used for segmentation with a wide variety of object categories. Currently, most of the major DL methods for edge detection use PASCAL for training and testing (e.g., <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b3">[4]</ref>), due to its ground-truths correspond to scenes different to the ones depicted in BSDS. This dataset contains 11530 annotated images, however, just around 5% of randomly selected images (505) have been considered for testing DexiNed. Although the images in PASCAL have more diverse labeled data, most of the images are annotated only for a couple of objects even though the scene has a vast number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>This section firstly describes the metrics used for the evaluations; then, details about the implementation and settings of the proposed approach are provided. Finally, a large set of experimental results is presented together with comparisons with state-of-the-art approaches. Again, most of the non-edge detection datasets on the literature are used just for qualitative evaluation and, for the edge detection, BIPED and MDBD approaches, are used for quantitative evaluations and comparisons with DexiNed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metrics</head><p>Edge detection algorithms can be evaluated following two approaches. Indirectly through their impact on other computer vision tasks <ref type="bibr" target="#b8">[9]</ref>. Directly in comparison to human drawn edges. In this work, we opted for the latter, which is a common practice in the evaluation of benchmark datasets. These evaluation metrics are as follow:</p><p>1. Optimal Dataset Scale (ODS) computed by using a global threshold for the entire dataset; 2. Optimal Image Scale (OIS) computed by using a different threshold on every image; 3. Average Precision (AP). The F-measure-= 2? ? + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Notes</head><p>The proposed DexiNed architecture has been trained from scratch without relying on pretrained weights. This is a unique feature of the proposed model. Most state-of-the-art networks depend on pre-trained weights of the ImageNet dataset. On average, DexiNed converges after 9 epochs (15 epochs in <ref type="bibr" target="#b16">[17]</ref>) with a batch size of 8 using Adam optimizer and learning rate of 10 ?4 -decreasing at 10 and 15 epochs by a factor of 0.1, the weight decay considered for the training was 10 ?8 . The training procedure takes around 1 day in a TITAN X GPU input with color images of size 352?352, 480?480 for MDBD. The weights for fusion layer are initialized as: 1 ?1 (see Section 3.4 for details on ). After a hyperparameter search to optimize DexiNed, the best performance was obtained using kernels of size 3?3, 1?1 and s?s on the different conv and deconv layers, with Xavier initializer <ref type="bibr" target="#b46">[47]</ref> in Dexi and normal distribution in the last conv and deconv layers of USNet. The of the loss function is set tp [0.7, 0.7, 1.1, 1.1, 0.3, 0.3, 1.3].</p><p>We randomly selected 200 images of BIPED to train and validate DexiNed. The remaining 50 images were used for testing. In order to increase the number of training images, a data augmentation process has been performed as follow: i) given the high resolution nature of BIPED images, each image is split in half along its width; ii) similarly to HED, each of the resulting images is rotated by 15 different angles and crop by the inner axis oriented rectangle; iii) images are horizontally flipped; and finally iv) two gamma corrections have been applied (0.3030, 0.6060). This augmentation process resulted in 288 images per each of the given images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Architecture Setup</head><p>This section presents evaluations on different DexiNed configurations. The first sub-section presents details on the upsampling methods and the merging process selected for estimating the edge-maps. In the second sub-section, evaluations on the advantage of using skip connections are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Upsampling Methods and DexiNed Predictions</head><p>Regarding the selection of the best upsampling method, in our preliminary work <ref type="bibr" target="#b16">[17]</ref> three approaches were evaluated. In that evaluation it is shown that the upsampling performed by the transpose convolution with trainable kernels gives the best results. Hence, in the current work it is also selected as upsampling method. On the other hand, regarding the strategy used to merge the different outputs-Output1, Output2, Output3, Output4, Output5, Output6, DexiNed-f, DexiNeda-the DexiNed architecture ( <ref type="figure" target="#fig_3">Fig. 3</ref>) has been empirically evaluated as performed in <ref type="bibr" target="#b16">[17]</ref> showing that the best results are obtained from DexiNed-f and DexiNed-a. DexiNed-f corresponds to the result obtained by the fusion process at the end of the DexiNed architecture (see <ref type="figure" target="#fig_3">Fig. 3</ref>), while DexiNed-a corresponds to the edge-maps obtained from the average of all DexiNed predictions (DexiNed-f included). It is shown in <ref type="bibr" target="#b16">[17]</ref> that both merging strategies reach similar quantitative results; hence hereinafter results from both merging strategies are presented; in some cases, due to space limitations, just results from DexiNed-f are depicted, in those cases results are named as DexiNed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Ablation Study</head><p>This subsection presents a quantitative study of the critical DexiNed parts and settings. As our model is composed with two types of skip connections (rectangles in green in the left and right sides in <ref type="figure" target="#fig_3">Fig. 3</ref>) a study of different number of skip-connections is performed. It is presented in <ref type="figure" target="#fig_8">Fig. 7(</ref> )-0 does not use skip-connections, 1 uses just one skip connection (left side in <ref type="figure" target="#fig_3">Fig. 3)</ref>;</p><p>2 uses the two connections. It is clear that the usage of two skip-connections (DexiNed2C) improves the performance of the proposed architecture. The proposed architecture has been also trained with different loss functions to evaluate its performance. Loss functions from the following approaches have been considered: HED <ref type="bibr" target="#b14">[15]</ref>, RCF <ref type="bibr" target="#b3">[4]</ref>, BDCN <ref type="bibr" target="#b4">[5]</ref>, and BDCNloss2 (it is a slightly modified function from BDCN <ref type="bibr" target="#b4">[5]</ref>). As illustrates in the <ref type="figure" target="#fig_8">Fig. 7</ref>( ? ) the modified BDCN loss function outperforms its counterparts with almost 1%. Finally, to choose the best DexiNed performance, we have evaluated its prediction in different epochs, till 25 epochs. As it can be appreciated in <ref type="figure" target="#fig_8">Fig. 7(</ref> ) the best performance is reached when 11 epochs are considered. In the following sections of this manuscript, the comparisons of DexiNed performance on the different edge detection datasets correspond to the architecture DexiNEd2C, with 11 epochs, and using BDCNloss2 function, termed just as DexiNed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Quantitative Comparison</head><p>This section presents comparisons of DexiNed with the state-of-the-art approaches on edge detection datasets. Additionally, a comparison of the generalization capability in two datasets (i.e., MDBD and BIPED) is studied; in other words, a study that shows results of generalization from one dataset to another dataset is presented. As introduced in Sec. 1, BSDS <ref type="bibr" target="#b0">[1]</ref> is not considered since this dataset cannot generalize results on edge domain. The state-of-the-art approaches for edge, contour, and boundary detection <ref type="bibr" target="#b9">[10]</ref> have been selected for comparison with DexiNed; these approaches are the following: RCF <ref type="bibr" target="#b3">[4]</ref>, BDCN <ref type="bibr" target="#b4">[5]</ref>, and CATS <ref type="bibr" target="#b5">[6]</ref>. In order to perform a fair comparison, these approaches have been trained on two datasets intended for edge detection-the MDBD and our BIPED datasets. It should be noticed that the same augmentation processes has been applied in all the cases; additionally, in the case of MDBD, all models have been trained with the same training set, instead of randomly selecting images from the MDBD dataset, as performed in most of the publications. <ref type="table" target="#tab_2">Table 1</ref> shows different evaluations for DexiNed (DexiNed-f and DexiNed-a) and the approaches from the state of the art mentioned above. Results from both scenarios are presentedtrained and tested on the same dataset and cross-evaluations (i.e., trained in a dataset and evaluated in the other dataset). It can be appreciated that DexiNed reaches the best performance (in all three metrics ODS, OIS, and AP) when trained and tested in the same dataset. Furthermore, DexiNed reaches also the best result if trained in BIPED but evaluated in the MDBD dataset. On the other hand, regarding the dataset generalization capability, considering ODS evaluation metric, we know that the best performance is reached when trained and evaluated in the same dataset. Hence, we propose to analyze the loss in performance when evaluated in another dataset different to the one used for training. In other words, what is the performance with respect to the values reached when trained and evaluated in the same dataset. This loss in performance is smaller if the different approaches are trained in BIPED, on average just 3,74% of decrease in performance is appreciated when evaluated in MDBD; on the contrary, this loss in performance reaches on average 7,14% if trained in MDBD and evaluated in BIPED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Comparison</head><p>This section presents just some illustrations as qualitative comparisons of the edge-maps predicted from all the models considered during the quantitative comparison in the previous section. Note that all these models have been trained on BIPED; <ref type="figure" target="#fig_9">Figure 8</ref> shows edge-maps of the evaluated architectures (trained on BIPED) when used in the datasets detailed in Section 4.2. Qualitative results, similar to those presented in <ref type="figure" target="#fig_9">Fig. 8</ref>, but trained on MDBD <ref type="bibr" target="#b6">[7]</ref>, are presented as supplementary material. Some comments and conclusions from the analysis of the obtained results are presented below:</p><p>? BIPED: For this dataset two illustrations are presented since this dataset is used for training the selected models considered in the comparisons. These two illustrations correspond to the best and the worst ODS results from DexiNed. As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, edge maps predicted by BDCN, CATS, and DexiNed are clean and accurate representations. Perceptually, we can say that edge maps predicted by BDCN and DexiNed contains more correctly detected edges. On the contrary, edge maps predicted by CATS are cleaner but containing less predictions. As a conclusion, while searching for a model that predicts more edges but also less artifacts, DexiNed provide us those requirements without compromise them.</p><p>? Other datasets: MDBD, CID, DCD, BSDS300, BSDS500, NUYD, and PASCAL. As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, results in these datasets are also similar to predictions from the test set on BIPED. The CATS model shows cleaner edge-maps but with less edges than DexiNed. A more perceptual difference of this claim can be observe in the predictions of MDBD, CID, NYUD, and PASCAL. Looking at the last row in <ref type="figure" target="#fig_9">Fig. 8</ref>, which corresponds to PASCAL dataset, several edges on the floor of the scene have been detected by DexiNed, but none of the other models are able to detect them.</p><p>To finish, the second version of BIPED gives the generalization robustness to the models considered for comparison. Overall, in all the datasets presented in <ref type="figure" target="#fig_9">Fig. 8</ref>, thanks to the unique characteristics on BIPED, there are most perceptual edges predicted in the images. Concerning to the DexiNed architecture, the predictions given by this model are cleaner than from its counterparts and without compromising true edge lost, the training procedure is simpler that in any other DL based models considered for comparison. DexiNed does not need pre-training weights, it can converge in less time than other models but still reach the state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a robust edge detection model that exhibits a great degree of generalization to new scenes. To this end, first, we presented a benchmark dataset carefully designed for the task of edge detection. Second, we designed a network with parallel skipconnections that learn edges without the need for ImageNet pre-trained weights. We demonstrated the generalization power of our approach by training a network on a single dataset and evaluating it on other benchmark datasets. Overall, our results show the possibility of training a deep learning model of edge detection from scratch in an end-to-end fashion. These findings open the opportunity to explore smaller networks for the task of edge detection by reducing the number of hyperparameters settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The results of several algorithms on the BSDS dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fused</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Flowchart of proposed architecture (DexiNed). It consists of two building blocks a Dense Extreme Inception Network and an upsampling Net (USNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, ( , ) = [ 1 , ..., ], = ? [ * log (? ) + (1 ? ) * log(1 ? (? ))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 1 .Figure 4 :</head><label>14</label><figDesc>Training with BIPED Dataset ( ) Original RGB image ( ) Super imposed edge-map ( ) Annotated edge-map A sample image from the BIPED dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Difference of BIPED GT previous annotation and the proposed version, BIPEd-GTv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>( ) A DCD test image with the provided annotations<ref type="bibr" target="#b45">[46]</ref>. ( ? ) Contours after removing wrong annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>( ) Evaluation of DexiNed architecture with (DexiNed1C: 1 connection; DexiNed2C: 2 connections) and without (DexiNed0C) skip-connections. ( ) DexiNed performance evolution during training. ( ? ) Evaluation with different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>The results of a few state-of-the-art architectures trained on the proposed BIPED dataset. Note that the ground truth of these datasets, except for BIPED and MDBD, correspond to boundary and segmentation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Quantitative results: The performance of DexiNed model and BIPED dataset are compared to the last DL based models and the edge detection based dataset, MDBD<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="3">Trained on Tested on ODS</cell><cell>OIS</cell><cell>AP</cell><cell>Tested on</cell><cell>ODS</cell><cell>OIS</cell><cell>AP</cell></row><row><cell>RCF[4] (2019) BDCN[5] (2020) CATS[6] (2021) DexiNed-f (Ours) DexiNed-a (Ours)</cell><cell>MDBD [7]</cell><cell>MDBD [7]</cell><cell>.879 .887 .891 .891 .894</cell><cell cols="2">.888 .926 .891 .793 .899 .809 .896 .930 .902 .951</cell><cell>BIPED</cell><cell cols="2">.8140 .828 .871 .854 .863 .768 .813 .831 .791 .787 .807 .844 .789 .813 .875</cell></row><row><cell>RCF[4] (2019) BDCN[5] (2020) CATS[6] (2021) DexiNed-f (Ours) DexiNed-a (Ours)</cell><cell>BIPED</cell><cell>BIPED</cell><cell>.849 .890 .887 .895 .893</cell><cell cols="2">.861 .906 .899 .934 .892 .817 .900 .927 .897 .940</cell><cell>MDBD [7]</cell><cell>.839 .855 .837 .863 .862</cell><cell>.854 .865 .864 .692 .840 .496 .871 .867 .874 .919</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Dataset and code: https://github.com/xavysp/DexiNed</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://labelbox.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.161</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An effective and fast iris recognition system based on a combined multiscale feature extraction technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouridane</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2007.06.030</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2007.06.030" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="868" to="879" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2878849</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1939" to="1946" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BDCN: Bi-directional cascade network for perceptual edge detection, Transactions on Pattern Analysis and Machine Intelligence (2020) 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3007074</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unmixing convolutional features for crisp edge detection, Transactions on Pattern Analysis and Machine Intelligence (2021) 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3084197</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A systematic comparison between visual cues for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>M?ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in computer vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">dge detection techniques-an overview, Pattern Recognition and Image Analysis C/C of Raspoznavaniye Obrazov I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analiz Izobrazhenii</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An overview of contour detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11633-018-1117-z</idno>
		<ptr target="https://doi.org/10.1007/s11633-018-1117-z" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contour completion without region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2016.2564646</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3597" to="3611" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contour detection based on nonclassical receptive field inhibition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Westenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.814250</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="729" to="739" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2004.1273918</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-017-1004-z</idno>
		<ptr target="https://doi.org/10.1007/s11263-017-1004-z" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation in mri scans using deeply-supervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention-Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dense extreme inception network: Towards a robust cnn model for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Poma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Winter Conference on Applications of Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Camera models and machine perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Department</title>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
	<note type="report_type">Technion</note>
	<note>Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<title level="m">Digital Picture Processing</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1982" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The gaussian derivative model for spatial vision. i-retinal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian derivative model for edge enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Basu</surname></persName>
		</author>
		<idno type="DOI">10.1016/0031-3203(94)90124-4</idno>
		<idno>1016/0031-3203(94)90124-4</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1451" to="1461" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extraction of salient contours from cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2007.02.009</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2007.02.009" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3100" to="3109" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Boundary detection using double-opponency and spatial sparseness constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2015.2425538</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2565" to="2578" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feedback and surround modulated boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Parraga</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-017-1035-5</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1367" to="1380" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using contours to detect and localize junctions in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scale-invariant contour completion using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiaofeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multiscale particle filter framework for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Widynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mignotte</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2014.2307856</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1922" to="1935" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast edge eetection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2014.2377715</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1558" to="1570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deciphering image contrast in object classification deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gil-Rodr?guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="61" to="76" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fields</surname></persName>
		</author>
		<title level="m">Neural network nearest neighbor fields for image transforms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Asian Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Unpublished results 2014</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep crisp boundaries: From boundaries to higher-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2874279</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Contourgan: Image contour detection with generative adversarial network, Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.09.033</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2018.09.033" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving edge detection in rgb images by adding nir channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Soria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal-Image Technology and Internet-Based Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Refined plane segmentation for cuboid-shaped objects by leveraging edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>D?rr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Ole</forename><surname>Salscheider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Furmans</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA51294.2020.00096</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="432" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dexined-based aluminum alloy grain boundary detection algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Chinese Control And Decision Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5647" to="5652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Photo-sketching: Inferring contour drawings from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Winter Conference on Applications of Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>Y. W. Teh, M. Titterington</editor>
		<meeting><address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Chia Laguna Resort</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ecuador, and the Ph.D. degree in Computer Science from Autonomous University of Barcelona in 2019, Spain. Currently, He is an Adjunct Professor at National University of Chimborazo</title>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Ecuador</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xavier Soria: received his B.S. degree in Educational Informatics from National University of Chimborazo</orgName>
		</respStmt>
	</monogr>
	<note>His research interest includes computer vision and its applications</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Argentina, in 1995, and the Ph.D. degree from the Polytechnic University of Catalonia, Spain</title>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Ecuador</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Angel Sappa: received the Electromechanical Engineering degree from National University of La Pampa ; ESPOL Polytechnic University</orgName>
		</respStmt>
	</monogr>
	<note>Currently he is a Senior Researcher at the Computer Vision Center</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Humanante: received the Systems Engineer degree from the Polytechnic School of Chimborazo, Ecuador, in 1998, and the Ph.D. degree in Training in the Knowledge Society from the University of Salamanca</title>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Spain; Ecuador</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National University of Chimborazo</orgName>
		</respStmt>
	</monogr>
	<note>Since 1999, he is a Full Professor and Researcher at the</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">He is currently a research scientist at Justus-Liebig-Universit?t Giessen. His research interests include artificial and biological vision</title>
		<imprint/>
		<respStmt>
			<orgName>Universitet and MSc. in Computer Vision jointly from Universit? de Bourgogne, Universitat de Girona, and Heriot-Watt University. He obtained his Ph.D. from Universitat Aut?noma de Barcelona</orgName>
		</respStmt>
	</monogr>
	<note>Arash Akbarinia: received BSc. in Software Engineering from G?teborgs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
