<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FINE-TUNING WAV2VEC2 FOR SPEAKER RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nik</forename><surname>Vaessen</surname></persName>
							<email>nvaessen@science.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computing and Information Sciences</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
							<email>dvanleeuwen@science.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computing and Information Sciences</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FINE-TUNING WAV2VEC2 FOR SPEAKER RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speaker recognition</term>
					<term>wav2vec2</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores applying the wav2vec2 framework to speaker recognition instead of speech recognition. We study the effectiveness of the pre-trained weights on the speaker recognition task, and how to pool the wav2vec2 output sequence into a fixed-length speaker embedding. To adapt the framework to speaker recognition, we propose a singleutterance classification variant with cross-entropy or additive angular softmax loss, and an utterance-pair classification variant with BCE loss. Our best performing variant achieves a 1.88% EER on the extended voxceleb1 test set compared to 1.69% EER with an ECAPA-TDNN baseline. Code is available at github.com/nikvaessen/w2v2-speaker.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the field of natural language processing (NLP) it has become standard to fine-tune self-supervised pre-trained models, such as BERT <ref type="bibr" target="#b0">[1]</ref>, XLNet <ref type="bibr" target="#b1">[2]</ref>, and T5 <ref type="bibr" target="#b2">[3]</ref>, on a wide variety of NLP tasks. Recently, this framework of pre-training and fine-tuning has also been successfully used in automatic speech recognition with wav2vec2 <ref type="bibr" target="#b3">[4]</ref>. The aim of this study is to explore the feasibility of fine-tuning the wav2vec2 pretrained network on a different task than speech recognition, namely speaker recognition.</p><p>The BERT and wav2vec2 network have commonalities in their design. Both have stacks of transformer layers and they use self-supervised, contrastive pre-training with masked input. However, they differ in three major aspects: 1) the input tokens to the encoder in wav2vec2 are raw audio processed by a CNN instead of WordPiece embeddings, 2) wav2vec2 uses relative positional embeddings computed by a CNN instead of sinusoidal positional embeddings, 3) there is no class token and equivalent next-sentence prediction task in the pretraining procedure of wav2vec2.</p><p>The class token in BERT is used when fine-tuning on sentence-pair classification tasks, e.g., entailment, and for single-sentence classification tasks, e.g., sentiment analysis. It is not used for single sentence tagging tasks, e.g., named entity recognition. In this respect, speech recognition is analogous to sentence tagging where each output token can represent a phone or letter. Hence, speech recognition does not require a class token. In contrast, speaker recognition corresponds to either sentence-pair classification, or singlesentence classification, and might therefore benefit from a class token which summarizes the whole input sequence.</p><p>Our work focuses on the following questions. First and foremost, we want to find out whether the pre-trained wav2vec2 weights are an effective initialization for speaker recognition. We further want to explore if wav2vec2 can be adapted to speaker recognition without a class token and next-sentence prediction task. One solution is to pool the variable-length sequence of wav2vec2 embeddings into a fixed-size speaker embedding. This raises the questions if pooling is an effective replacement for a class token, and if so, which pooling method is most suitable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Wav2vec2 has already been applied to a variety of speechrelated tasks. In <ref type="bibr" target="#b4">[5]</ref> the network is used for speaker recognition and language identification in both a single and multitask learning setting. The authors of <ref type="bibr" target="#b5">[6]</ref> show good performance for language identification with 25 languages but modify wav2vec2 to use log-mel spectogram input instead of raw waveforms. In <ref type="bibr" target="#b6">[7]</ref> the (frozen) wav2vec2 embeddings are input to a learnable downstream model for carrying out emotion recognition. Meanwhile <ref type="bibr" target="#b7">[8]</ref> manages to fine-tune the wav2vec2 model itself on emotion recognition with CTC loss by using emotion-labeled phonetic units. The LeBenchmark <ref type="bibr" target="#b8">[9]</ref> uses the wav2vec2 models as a baseline and encapsulates speech recognition, spoken language understanding, emotion recognition and speech translation in a single benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The wav2vec2 architecture</head><p>The wav2vec2 framework <ref type="bibr" target="#b3">[4]</ref> applies the concept of selfsupervised pre-training with transformers to automatic speech recognition. In <ref type="figure">Fig. 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Feature extraction</head><p>The first step is to encode a raw audio waveform (normalized to zero mean and unit variance) into learned representations with a discrete time unit. The feature extractor consists of 7 consecutive 1-dimensional convolutions with 512 channels and respective kernel sizes of (10, 3, 3, 3, 3, 2, 2) and stride (5, 2, 2, 2, 2, 2, 2). The output of the first convolutional layer is group normalized <ref type="bibr" target="#b9">[10]</ref> such that each of the 512 channel sequences has zero mean and unit variance before GELU activation <ref type="bibr" target="#b10">[11]</ref> is applied. The other convolutional layers do not have any normalization layers and their output is directly activated with GELU. The output of the feature extractor is an encoded vector sequence with dimensionality 512. Each vector has a receptive field of 20 ms which is similar to the window sizes in spectral-based representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Projection, SpecAugment &amp; positional embedding</head><p>After the feature extraction each encoded vector representation in the sequence is independently normalized to zero mean and unit variance and projected into 768 dimensions by a single, shared fully-connected layer called the feature projector. On all projections dropout is applied (but no activation). Then, masking is applied over the whole sequence analogous to SpecAugment <ref type="bibr" target="#b11">[12]</ref>; 0 or more random sets of consecutive vectors (masking in time domain) as well as 0 or more random sets of consecutive channels (masking in "frequency" domain) have their values blanked to 0. This masked projected sequence is then convolved by a single layer with a kernel size of 128, a stride of 1, padding of 64 and 16 groups followed by GELU activation in order to create a relative po-sitional embedding for each projected representation. This relative positional embedding is summed with the original input of the convolution, which changes the receptive field from 20 ms to 2.5 s. As a final step each vector is independently normalized with LayerNorm and dropout is applied again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Transformer</head><p>The masked and projected sequence with both local and positional information is fed through an encoder with 12 consecutive transformer layers. Each transformer layer consists of a residual 12-headed self-attention module and a residual 2-layer feed forward network with respectively 3072 and 768 units. LayerDrop <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> is applied such that each transformer layer is potentially skipped. The final output sequence, with each representation potentially having both local and global information due to self-attention, is used in a downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Three wav2vec2 variants for speaker recognition</head><p>The original wav2vec2 framework fine-tunes on speech recognition by independently labeling each wav2vec2 output embedding with a shared fully-connected layer, and optimizes with CTC loss <ref type="bibr" target="#b14">[15]</ref>. We propose two adaptions to this design for the speaker recognition task, inspired by BERTs <ref type="bibr" target="#b0">[1]</ref> single-sentence and sentence-pair classification setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Speaker recognition as single-utterance classification</head><p>The current paradigm in speaker recognition with deep neural networks is to train models with a classification-based approach <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. To mimic this architectural paradigm two modifications to wav2vec2 are made. First, the sequence of wav2vec2 embeddings is reduced to a single embedding during training (see subsection 3.3). Secondly, we add a fully connected layer which uses the pooled embedding to classify each speaker in the training data with cross-entropy (CE) or angular additive softmax (AAM) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> loss. A trial is evaluated with the cosine similarity between two pooled embeddings. We refer to these variants as w2v2-ce and w2v2-aam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Speaker recognition as utterance-pair classification</head><p>The second approach directly computes a similarity score. The two audio segments of a speaker recognition trial are first processed independently up to the encoder part of the network. Then, the two sequences of input tokens are concatenated, accompanied by special tokens at the embedding level: A start (all +1), separator (all ?1), and end (all ?1) token. The first wav2vec2 embedding in the output sequence (corresponding to the start token) is used as input to a logistic regression with one dense layer. The singular output is the logit for the BCE loss and the score for evaluating a trial.</p><p>During training a batch consists of 8 speakers, 4 utterances per speaker, and 16 same and different speaker-pairs. We refer to this third variant as w2v2-bce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pooling methods</head><p>We propose several pooling methods to reduce the variablelength sequence of wav2vec2 embeddings to a fixed-size speaker embedding. We first consider the standard statistical pooling methods mean, max, mean&amp;std and quantile. They aggregate each dimension over the time axis. The mean&amp;std variant doubles the embedding dimensionality while quantile pooling expands each dimension five-fold with quantiles (0, 0.25, 0.5, 0.75, 1). We also assess taking the first, middle or last embedding of the sequence as a "pooling" strategy as well as randomly selecting the index. Lastly we consider inserting a "start" token (all values +1) before the input sequence of the encoder and then selecting the first output token as the speaker embedding. This is termed first&amp;cls. Unlike BERT our "start" token token does not have a meaningful prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>All experiments are conducted on the VoxCeleb datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which consist of interviews of celebrities extracted from YouTube. The VoxCeleb2 dev set, which contains 1.1 M audio files over 6 k speakers, is used for training. A validation set is created based on 2 % of the dev set data which includes all speakers but does not overlap in recordings. For this validation set a random trial set of 5 k same and 5 k different pairs is generated, from which we compute the validation EER. This is used to select the best checkpoint during a training run as well as to tune hyperparameters. For evaluation we use the cleaned original (vox1-o, 40 speakers, 37 k trials), extended (vox1-e, 1251 speakers, 580 k trials) and hard (vox1-h, 1190 speakers, 550 k trials, equal nationality and sex) test sets from voxceleb 1 <ref type="bibr" target="#b20">[21]</ref>. There is no speaker overlap between the voxceleb1 test sets and the voxceleb2 dev set. The experiments with the wav2vec2 network use the pretrained weights 1 on Librispeech <ref type="bibr" target="#b21">[22]</ref> released on Hugging-Face <ref type="bibr" target="#b22">[23]</ref> by Fairseq <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Computational budget and fair comparison</head><p>We compare the performances of models under similar computational budgets. Each network is trained with a batch size of 3.2M audio samples <ref type="bibr" target="#b3">[4]</ref> by randomly sampling 3 seconds from 66 different audio files. No data augmentation techniques are used. We train for 100k iterations, approximately 6 epochs, with Adam <ref type="bibr" target="#b24">[25]</ref> and a OneCycle learning rate schedule <ref type="bibr" target="#b25">[26]</ref>. The maximum learning rate (LR) of the cycle is  <ref type="bibr" target="#b26">[27]</ref> with 5k iterations and 2) tuning on a 7-sized grid centered around the LR with the steepest slope and bounded by the LRs where the loss respectively started and stopped decreasing. Models are evaluated with a cosine score between speaker embeddings lacking any further post-processing, except for the wav2vec2-bce variant which computes scores directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baseline systems</head><p>We train two popular baselines models for speaker recognition, x-vector <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> and ECAPA-TDNN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>, and compare them to the three wav2vec2 adaptions w2v2-ce, w2v2-aam and w2v2-bce. All five models have the computation budget described in subsection 4.2. The X-vector and ECAPA-TDNN networks use 40-dimensional filterbanks as input. The w2v2-ce and w2v2-aam variants use mean&amp;std pooling which was chosen as it is also used in the x-vector network architecture. The w2v2-aam variant and ECAPA-TDNN use the AAM softmax loss with a scale of 30 and a margin of 0.2 in this and further experiments. The feature encoder part of the wav2vec2 architecture is frozen for the whole training procedure which corresponds to <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Variation in pooling</head><p>Next we explore the different poolings methods proposed in Section 3.3. This was carried out for the single-utterance classification variants: w2v2-ce and w2v2-aam. We use the same training settings as in the baseline comparison and only vary the pooling method. Note therefore that the learning rate was tuned to the "mean&amp;std" setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>We perform several ablations on the best-performing wav2vec2 variant for speaker recognition. These ablations are trained with 100k iterations except in the experiments for the batch size. The first set of ablations study the effect of not freezing the feature extractor in the fine-tuning procedure as well as randomly initialising the whole network and thus not using any pre-trained weights. The second set of ablations explore the relative importance of the regularisation techniques in the network architecture. We first disable only LayerDrop, then sequentially disable dropout and the masking of certain frames as well. The third set simply studies the effect of increasing (with 50k iterations) or decreasing (with 200k iterations) the chosen batch size by a factor of 2. The last ablations involve the learning rate schedule. We first test two constant learning rates: 3 ? 10 ?6 is the LR where the loss started increasing in the LR range test and 10 ?5 is the LR with the steepest decrease. The second schedule exponentially decays the LR from 10 ?5 to 3 ? 10 ?6 . The final schedule is the tri-stage learning rate schedule similar to <ref type="bibr" target="#b3">[4]</ref> which includes a warm-up phase linearly increasing the LR from 10 ?7 to 10 ?5 in 10k iterations, a constant phase of 40k iterations, and an exponentially decreasing stage from 10 ?5 to 10 ?7 for the remaining iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline comparison</head><p>The LR range test and grid-tuning approach found the following learning rates: 10 ?4 for x-vector, 10 ?3 for ECAPA-TDNN, 9 ? 10 ?5 for w2v2-ce, 5 ? 10 ?5 for w2v2-aam, and 3 ? 10 ?5 for w2v2-bce. <ref type="table" target="#tab_1">Table 1</ref> shows four runs with these learning rates. We see that ECAPA-TDNN performs best on all test sets. The w2v2-aam network is the best performing wav2vec2 variant. Both w2v2-ce and w2v2-aam manage to improve on the x-vector architecture. Modeling speaker recognition as utterance-pair classification (w2v2-bce) performed worst. <ref type="table" target="#tab_2">Table 2</ref> compares the 9 different pooling strategies with the w2v2-ce and w2v2-aam networks. We observe that first&amp;cls pooling performs best for both networks. The difference between first&amp;cls pooling and the other pooling methods is more pronounced for w2v2-aam than for w2v2-ce. The low inter-model variance of random pooling shows that each wav2vec2 embedding is a stand-alone speaker embedding. Not shown, using random pooling in the evaluation for networks which were trained with first&amp;cls pooling degrades the EER with 0.2% points. Moreover, creating ensembles out of different embeddings in the output sequence did not improve performance. This suggests that the transformer layer processes each embedding in the sequence similarly. <ref type="table" target="#tab_3">Table 3</ref> shows the results of the ablation study. We see that freezing the feature extractor leads to worse performance but is more stable across runs with different seeds. We also note a large degradation in performance when initializing with random weights instead of the pre-trained weights. The regularisation settings for fine-tuning on speech recognition are also beneficial for fine-tuning on speaker recognition. Increasing the batch size beyond 3.2M audio samples does not increase performance, although it does decrease the variance slightly. Using a learning rate schedule with a warm-up phase, such as the tri-stage or OneCycle schedule, is critical for stable and good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Pooling methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>We have shown that the wav2vec2 framework can be successfully adapted to the speaker recognition task and that the pretrained weights used for fine-tuning on speech recognition are also useful for fine-tuning on speaker recognition. Good results with first&amp;cls pooling indicate that including a class token in the pre-training procedure is promising future work. Modeling speaker recognition as a paired-utterance classification problem did not perform well. Future work in that direction might involve limiting the attention mechanism to the opposite utterance to simplify the learning task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>EER performance of standard filterbank-based speaker recognition networks as well as the fine-tuned wav2vec2 variations. We ran N = 4 runs to compute the standard deviations. Bold indicates best performance.</figDesc><table><row><cell></cell><cell cols="3">EER performance (mean, std in %)</cell><cell></cell></row><row><cell>NETWORK</cell><cell>vox1-o</cell><cell>vox1-e</cell><cell>vox1-h</cell><cell></cell></row><row><cell>x-vector [16]</cell><cell>5.22 0.12</cell><cell>5.60 0.05</cell><cell>8.75</cell><cell>0.05</cell></row><row><cell>ECAPA-TDNN [17]</cell><cell>1.61 0.03</cell><cell>1.69 0.03</cell><cell>3.10</cell><cell>0.05</cell></row><row><cell>w2v2-ce</cell><cell>2.25 0.20</cell><cell>2.58 0.10</cell><cell>4.91</cell><cell>0.13</cell></row><row><cell>w2v2-aam</cell><cell>1.91 0.12</cell><cell>2.22 0.04</cell><cell>4.33</cell><cell>0.08</cell></row><row><cell>w2v2-bce</cell><cell>7.28 0.22</cell><cell>7.19 0.22</cell><cell cols="2">11.34 0.83</cell></row><row><cell cols="3">found by 1) performing an LR range test</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The performance of different pooling strategies for the single utterance classification architectures on the extended voxceleb1 test set. Each method was trained with N = 3 random seeds. The evaluation of random pooling was repeated four times for each run.</figDesc><table><row><cell></cell><cell cols="2">EER on vox1-e (mean, std in %)</cell></row><row><cell>pooling</cell><cell>w2v2-ce</cell><cell>w2v2-aam</cell></row><row><cell>max</cell><cell>4.79 0.55</cell><cell>2.27 0.04</cell></row><row><cell>quantile</cell><cell>2.75 0.17</cell><cell>2.21 0.03</cell></row><row><cell>mean</cell><cell>2.69 0.06</cell><cell>2.11 0.05</cell></row><row><cell>mean&amp;std</cell><cell>2.60 0.08</cell><cell>2.18 0.03</cell></row><row><cell>first</cell><cell>2.61 0.10</cell><cell>2.15 0.05</cell></row><row><cell>first&amp;cls</cell><cell>2.52 0.11</cell><cell>2.06 0.03</cell></row><row><cell>middle</cell><cell>2.55 0.07</cell><cell>2.18 0.03</cell></row><row><cell>last</cell><cell>2.58 0.07</cell><cell>2.18 0.03</cell></row><row><cell>random (N = 1)</cell><cell>2.56 0.002</cell><cell>2.40 0.002</cell></row><row><cell>random (N = 3)</cell><cell>2.70 0.13</cell><cell>2.37 0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>EER performance under varying ablated configurations for the w2v2-aam network variant. Each configuration was run with N = 3 random seeds. One run with exponential decay diverged and we therefore show N = 2 results for that row.</figDesc><table><row><cell>EER on vox-e</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See https://huggingface.co/facebook/wav2vec2-base</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring wav2vec 2.0 on Speaker Verification and Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1509" to="1513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved language identification through cross-lingual self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diptanu Gon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kritika</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04082</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotion Recognition from Speech Using wav2vec 2.0 Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Pepino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Riera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciana</forename><surname>Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3400" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The role of phonetic units in speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01132</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sol?ne</forename><surname>Evain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1439" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11556</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brecht</forename><surname>Desplanques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenthe</forename><surname>Thienpondt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Demuynck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3830" to="3834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large Margin Softmax Loss for Speaker Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2873" to="2877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VoxCeleb: A Large-Scale Speaker Identification Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2616" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Speechbrain: A general-purpose speech toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
