<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask R-CNN with Pyramid Attention Network for Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software &amp; Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyao</forename><surname>Zhong</surname></persName>
							<email>zhuoyao.zhong@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huo</surname></persName>
							<email>qianghuo@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mask R-CNN with Pyramid Attention Network for Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new Mask R-CNN based text detection approach which can robustly detect multioriented and curved text from natural scene images in a unified manner. To enhance the feature representation ability of Mask R-CNN for text detection tasks, we propose to use the Pyramid Attention Network (PAN) as a new backbone network of Mask R-CNN. Experiments demonstrate that PAN can suppress false alarms caused by text-like backgrounds more effectively. Our proposed approach has achieved superior performance on both multi-oriented (ICDAR-2015, ICDAR-2017 MLT) and curved (SCUT-CTW1500) text detection benchmark tasks by only using single-scale and single-model testing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text detection has drawn increasing attentions from the computer vision community <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> since it has a wide range of applications in document analysis, robot navigation, OCR translation, image retrieval and augmented reality. However, because of diverse text variabilities in colors, fonts, orientations, languages and scales, extremely complex and text-like backgrounds, as well as some distortions and artifacts caused by image capturing like non-uniform illumination, low contrast, low resolution and occlusion, text detection in natural scene images is still an unsolved problem.</p><p>Nowadays, with the astonishing development of deep learning, great progress has been made in this field. Lots of state-of-the-art convolutional neural network (CNN) based object detection and segmentation frameworks, such as Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>, SSD <ref type="bibr" target="#b22">[23]</ref> and FCN <ref type="bibr" target="#b26">[27]</ref>, have been borrowed to solve the text detection problem and substan-* Equal contribution. ? This work was done when Zhida Huang and Zhuoyao Zhong were interns in Speech Group, Microsoft Research Asia, Beijing, China.</p><p>tially outperform traditional MSER <ref type="bibr" target="#b28">[29]</ref> or SWT <ref type="bibr" target="#b6">[7]</ref> based bottom-up text detection approaches. For example, some approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref> formulate text detection as a semantic segmentation problem and employ an FCN to make a pixel-level text/non-text prediction, based on which a text saliency map can be generated. As only coarse text-blocks can be detected from the saliency map, complex postprocessing steps are needed to extract accurate bounding boxes of text-lines. Unlike FCN-based methods, another category of methods treats text as a specific object and leverages effective object detection frameworks like R-CNN <ref type="bibr" target="#b8">[9]</ref>, Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>, SSD <ref type="bibr" target="#b22">[23]</ref>, YOLO <ref type="bibr" target="#b30">[31]</ref> and DenseBox <ref type="bibr" target="#b13">[14]</ref> to detect words or text-lines from images directly. Although these approaches are composed of simpler pipelines, they still struggle with curved text detection. To solve this problem, some recent approaches like PixelLink <ref type="bibr" target="#b5">[6]</ref>, FTSN <ref type="bibr" target="#b4">[5]</ref>, and IncepText <ref type="bibr" target="#b36">[37]</ref>, propose to formulate text detection as an instance segmentation problem so that both straight text and curved text can be detected in a unified manner. Specifically, PixelLink proposes to detect text by linking pixels within the same text instances together, while FTSN and IncepText borrow the FCIS framework <ref type="bibr" target="#b18">[19]</ref> to solve the text detection problem. Although promising results have been achieved, the used instance segmentation approaches have now been surpassed by the latest state-of-the-art Mask R-CNN approach on general instance segmentation tasks <ref type="bibr" target="#b10">[11]</ref>. Therefore, it is straightforward to use Mask R-CNN to further improve the text detection performance.</p><p>In this paper, we present an effective Mask R-CNN based text detection approach which can detect multi-oriented and curved text from natural scene images in a unified manner. To enhance the feature representation ability of Mask R-CNN, we propose to use the Pyramid Attention Network (PAN) <ref type="bibr" target="#b17">[18]</ref> as a new backbone network of Mask R-CNN. Experiments demonstrate that PAN can suppress false alarms caused by text-like backgrounds more effectively. Our proposed approach has achieved superior performance on both multi-oriented (ICDAR-2015 <ref type="bibr" target="#b16">[17]</ref>, ICDAR-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we focus on reviewing recently proposed CNN based text detection approaches and recent developments in instance segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Text Detection</head><p>State-of-the-art CNN based object detection and segmentation frameworks have been widely used to solve the text detection problem recently. Some of these methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref> borrow the idea of semantic segmentation and employ an FCN to make a pixel-level text/non-text prediction, which produces a text saliency map for text detection. However, only coarse text-blocks can be detected from this saliency map, so complex post-processing steps are needed to extract accurate bounding boxes of text-lines. Another category of methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13]</ref> treats text as a specific object and leverages state-of-the-art object detection frameworks to detect word or text-lines from images directly. Jaderberg et al. <ref type="bibr" target="#b14">[15]</ref> adapted R-CNN for text detection, while its performance was limited by the traditional region proposal generation methods. Gupta et al. <ref type="bibr" target="#b9">[10]</ref> borrowed the YOLO framework and employed a fullyconvolutional regression network to perform text detection and bounding box regression at all locations and multiple scales of an image. Zhong et al. <ref type="bibr" target="#b39">[40]</ref> and Liao et al. <ref type="bibr" target="#b19">[20]</ref> employed the Faster R-CNN and SSD frameworks to solve the word-level horizontal text detection problem, respectively. In order to extend Faster R-CNN and SSD to multi-oriented text detection, Ma et al. <ref type="bibr" target="#b27">[28]</ref> and Liu et al. <ref type="bibr" target="#b24">[25]</ref> proposed quadrilateral anchors to hunt for inclined text proposals which could better fit the multi-oriented text instances. To overcome the inefficiency of anchor mechanism <ref type="bibr" target="#b12">[13]</ref>, Zhou et al. <ref type="bibr" target="#b40">[41]</ref> and He et al. <ref type="bibr" target="#b12">[13]</ref> borrowed the idea of DenseBox and used a one-stage FCN to output pixel-wise textness scores as well as the quadrilateral bounding boxes through all locations and scales of an image. Although these approaches are composed of simpler pipelines, they still struggle with curved text detection. Recently, instead of detecting the whole words or text-lines directly, Tian et al. <ref type="bibr" target="#b33">[34]</ref> and Shi et al. <ref type="bibr" target="#b32">[33]</ref> adopted object detection methods to detect text segments firstly, then grouped these text segments into words or lines with some simple text-line grouping algorithms or the learned linkage information, respectively. Intuitively, these methods can be applied for curved text detection, but they make the total text detection pipeline more sophisticated. Moreover, the segment grouping problem itself is a nontrivial problem, especially when the layout is complex, e.g., text with large character spacing, which will affect the text detection performance too. To overcome the above problems, some recent approaches propose to for-mulate text detection as an instance segmentation problem so that both straight text and curved text can be detected in a unified manner. Deng et al. <ref type="bibr" target="#b5">[6]</ref> proposed to detect text by linking pixels within the same text instances together. Dai et al. <ref type="bibr" target="#b4">[5]</ref> and Yang et al. <ref type="bibr" target="#b36">[37]</ref> adopted the FCIS framework <ref type="bibr" target="#b18">[19]</ref> to solve the text detection problem. In this paper, we borrowed Mask R-CNN, which is the latest state-of-the-art instance segmentation approach, to further enhance the text detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Instance Segmentation</head><p>Instance segmentation is a challenging task because it requires the correct detection of all objects in an image while also precisely segmenting each instance. Dai et al. <ref type="bibr" target="#b2">[3]</ref> proposed a complex multiple-stage cascade that predicts segment proposals from bounding-box proposals, followed by classification. Later, Li et al. <ref type="bibr" target="#b18">[19]</ref> combined the segment proposal system in <ref type="bibr" target="#b1">[2]</ref> and R-FCN <ref type="bibr" target="#b3">[4]</ref> for fully convolutional instance segmentation (FCIS). Although fast, FCIS exhibits systematic errors on overlapping instances and creates spurious edges <ref type="bibr" target="#b10">[11]</ref>. More recently, Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> extended Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. It introduced RoIAlign [11] to replace RoIPool <ref type="bibr" target="#b7">[8]</ref> to fix the pixel misalignment and used ResneXt <ref type="bibr" target="#b35">[36]</ref> as the base network. Moreover, it took advantage of Feature Pyramid Network (FPN <ref type="bibr" target="#b20">[21]</ref>) to strengthen feature representation ability and partially eased the problem of small object detection. In this paper, to further enhance the feature representation ability of Mask R-CNN, we propose to incorporate the Pyramid Attention Network (PAN) <ref type="bibr" target="#b17">[18]</ref> into the Mask R-CNN framework. Experiments demonstrate that PAN can suppress false alarms caused by text-like backgrounds more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>Our Mask R-CNN based text detection network is composed of four modules: 1) A PAN backbone network that is responsible for computing a multi-scale convolutional feature pyramid over a full image; 2) A region proposal network (RPN) that generates rectangular text proposals; 3) A Fast R-CNN detector that classifies extracted proposals and outputs the corresponding quadrilateral bounding boxes; 4) A mask prediction network that predicts text masks for input proposals. A schematic view of our text detection network is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref> and details are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pyramid Attention Network</head><p>Recently, Li et al. <ref type="bibr" target="#b17">[18]</ref> proposed a Pyramid Attention Network (PAN) that combines the attention mechanism and spatial pyramid to extract precise dense features for semantic segmentation tasks. It mainly consists of two mod- ules, i.e., a Feature Pyramid Attention (FPA) module and a Global Attention Up-sample (GAU) module. The FPA module performs spatial pyramid attention on high-level features and combines global pooling to learn better highlevel feature representations. The GAU module is attached on each decoder layer to provide global context as a guidance of low-level features to select category localization details. Owing to these tactful designs, PAN achieves stateof-the-art segmentation performance on the VOC2012 and Cityscapes benchmark tasks. Inspired by this, we propose to use PAN as a new backbone network to improve the feature representation learning for our Mask R-CNN based text detection model.</p><p>We build PAN on top of ResNet50 <ref type="bibr" target="#b11">[12]</ref> and ResNeXt50 <ref type="bibr" target="#b35">[36]</ref>. The implementations of PAN generally follow <ref type="bibr" target="#b17">[18]</ref> with just some modest modifications. As shown in <ref type="figure" target="#fig_1">Fig.  2</ref>, our FPA module takes the output features of the Res-4 layers in ResNet50 or ResNeXt50 as input, on which it performs 3?3 dilated convolution with sampling rates 3, 6, 12 respectively to better extract context information. These three feature maps are then concatenated and dimension reduced by a 1?1 convolution layer. After that, FPA performs a 1?1 convolution on the input Res-4 features further, whose output is multiplied with the above context features in a pixel-wise manner. The extracted features are added with the output features of the global pooling branch to get the final pyramid attention features. The GAU module, as is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, performs 3?3 convolution on the low-level features to reduce channels of feature maps from CNNs. The global context generated from high-level features is through a 1?1 convolution with instance nor- malization <ref type="bibr" target="#b34">[35]</ref> and ReLU nonlinearity, then multiplied by the low-level features. Finally, the high-level features after up-sampling are added with the weighted low-level features to generate the GAU features. With the above FPA and GAU modules, we construct a powerful feature pyramid with three levels, i.e., P 2 , P 3 and P 4 , whose strides are 4, 8 and 16, respectively. The overall PAN architecture is depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. We refer readers to <ref type="bibr" target="#b17">[18]</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region Proposal Network</head><p>Three RPNs are attached to P 2 , P 3 and P 4 respectively, each of which slides a small network densely on the corresponding pyramid level to perform text/non-text classification and bounding box regression. The small network is implemented as a 3?3 convolutional layer followed by two sibling 1?1 convolutional layers, which are used for predicting textness score and rectangular bounding box locations respectively. As the size and aspect ratio variabilities  of scene text instances are wider than general objects, we design a complicated set of anchors following <ref type="bibr" target="#b19">[20]</ref>. Specifically, we design 6 anchors at each sliding position on each pyramid level in {P 2 , P 3 , P 4 } by using 6 aspect ratios {0.2, 0.5, 1.0, 2.0, 4.0, 8.0} and one scale in {32, 64, 128}. The detection results of all three RPNs are aggregated together to construct a proposal set {D}. Then, we use the standard non-maximum suppression (NMS) algorithm with an IoU threshold of 0.7 to remove redundant proposals in {D}, and select the top-N scoring proposals for the succeeding Fast R-CNN and mask prediction network. N is set to 2000 in both the training and testing stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fast R-CNN &amp; Mask Prediction Network</head><p>After the region proposal generation step, extracting effective features for each proposal is critical to the performance of the following Fast R-CNN and mask prediction network. In the original Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>, the features of all proposals are extracted from the last convolution layer of the backbone network, which would lead to insufficient features for small proposals. In the recent FPN <ref type="bibr" target="#b20">[21]</ref>, the features of proposals are extracted from different pyramid levels according to their sizes, i.e., the features of small proposals are extracted from low-level pyramid levels, while large proposals from high-level pyramid levels. Although more effective, there still exists room for further improvement Then, for Fast R-CNN, the output features are globally average pooled before the final text/non-text classification and quadrilateral bounding box regression layers, while for the mask prediction network, the output features are followed by four consecutive 3?3 convolutional layers and then upsampled before the final mask prediction layers. Numbers denote spatial resolution and channels. <ref type="bibr" target="#b21">[22]</ref>. As the features from the P 2 and P 3 levels have higher resolution and contain more detailed information, which are complementary to more abstract but low-resolution features from the P 4 level, it is straightforward to combine these three pyramid levels together to improve the feature representation ability. To achieve this, we borrow the idea of ION <ref type="bibr" target="#b0">[1]</ref> and propose a Skip-RoIAlign method to fuse the P 2 , P 3 and P 4 levels. Concretely, for each proposal, we apply ROIAlign over P 2 , P 3 and P 4 pyramid levels respectively and extract three feature descriptors with a fixed spatial size of 7?7, which are concatenated and dimension reduced with a 1?1 convolutional layer to obtain the final ROI features. These ROI features are then fed into the network head for text/non-text classification, quadrilateral bounding box regression and mask prediction. Details of the network head are depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>. The head includes the 5-th stage of ResNet50 or ResNeXt50, which is shared by the Fast R-CNN and mask prediction network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Loss Functions</head><p>Multi-task loss for RPN. There are two sibling output layers for each individual RPN, i.e., a text/non-text classification layer and a rectangular bounding box regression layer. The multi-task loss function can be denoted as follows:</p><formula xml:id="formula_0">L RP Np i = L R cls (c, c * ) + ? loc L R loc (r, r * ),<label>(1)</label></formula><p>where c and c * are predicted and ground-truth labels respectively, L R cls (c, c * ) is a softmax loss for classification tasks; r and r * represent the predicted and ground-truth 4-dimensional parameterized regression targets as stated in <ref type="bibr" target="#b31">[32]</ref>, L R loc (r, r * ) is a smooth-L 1 loss <ref type="bibr" target="#b7">[8]</ref> for regression tasks. ? loc is a loss-balancing parameter, and we set ? loc = 3.</p><p>The total loss of RPN L RP N is the sum of the losses of the three RPNs. Multi-task loss for Fast R-CNN. Fast R-CNN also has two sibling output layers: 1) A text/non-text classification layer, which is the same as the above-mentioned RPN; 2) A quadrilateral bounding box regression layer. The multi-task loss function for Fast R-CNN is defined as follows: </p><formula xml:id="formula_1">L F RCN = L F cls (c, c * ) + ? loc L F loc (t, t * ),<label>(2)</label></formula><formula xml:id="formula_2">* x1 = (x g 1 ? x p 1 )/P w , * y1 = (y g 1 ? y p 1 )/P h , * x2 = (x g 2 ? x p 2 )/P w , * y2 = (y g 2 ? y p 1 )/P h , * x3 = (x g 3 ? x p 2 )/P w , * y3 = (y g 3 ? y p 2 )/P h , * x4 = (x g 4 ? x p 1 )/P w , * y4 = (y g 4 ? y p 2 )/P h . (3) L F loc (t, t * )</formula><p>is also a smooth-L 1 loss and we set ? loc = 1. Loss for mask prediction network. Let m and m * be the predicted and ground-truth mask targets respectively and L mask (m, m * ) be a standard binary cross-entropy loss for mask prediction tasks. Based on these definitions, the loss function can be defined as follows:</p><formula xml:id="formula_3">L M ASK = L mask (m, m * ).<label>(4)</label></formula><p>The overall loss function for training the proposed Mask R-CNN based text detection model can be denoted as:</p><formula xml:id="formula_4">L = L RP N + L F RCN + ? mask L M ASK ,<label>(5)</label></formula><p>where ? mask is a loss-balancing parameter for L M ASK , and we set ? mask = 0.03125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Training Details</head><p>In each training iteration of RPN, we sample a mini-batch of 128 positive and 128 negative anchors for each RPN. An anchor is assigned a positive label if it has the highest IoU for a given ground-truth bounding box or has an IoU over 0.7 with any ground-truth bounding box, and a negative label if its IoU overlap is less than 0.3 for all ground-truth bounding boxes. For Fast R-CNN, we sample a mini-batch of 64 positive and 192 negative text proposals in each iteration. A proposal is assigned a positive label if it has an IoU over 0.5 with any ground-truth bounding box, otherwise assigned a negative label. For the sake of efficiency, the IoU overlaps between proposals and ground-truth boxes are calculated using their axis-aligned rectangular bounding boxes. Only the positive text proposals are used for training the mask prediction network. The mask target is the intersection between a proposal and its associated ground-truth mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed method on several standard benchmark tasks including ICDAR-2015 <ref type="bibr" target="#b16">[17]</ref> and ICDAR-2017 MLT 2017 <ref type="bibr" target="#b29">[30]</ref> for multi-oriented text detection, and SCUT-CTW 1500 <ref type="bibr" target="#b25">[26]</ref> for curved text detection. Text instances are labeled in word-level with quadrilateral bounding boxes in the former two datasets and in text-line level with 14 coordinate points in SCUT-CTW 1500. ICDAR-2017 MLT is built for the multi-lingual scene text detection and script identification challenge in the ICDAR-2017 Robust Reading Competition, which includes 9 languages: Chinese, Japanese, Korean, English, French, Arabic, Italian, German and Indian. It contains 7,200, 1,800 and 9,000 images for training, validation and testing, respectively. ICDAR-2015 is built for the Incidental Scene Text challenge in the ICDAR-2015 Robust Reading Competition, which contains 1,000 and 500 images for training and testing. SCUT-CTW 1500 is a curved text detection dataset, including 1,000 training images and 500 testing images.</p><p>To make our results comparable to others, we use the online official evaluation tools to evaluate the performance of our approach on ICDAR-2017 MLT and ICDAR-2015, and use the evaluation tool provided by the authors of <ref type="bibr" target="#b25">[26]</ref> on SCUT-CTW 1500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>The weights of ResNet50 or ResNeXt50 related layers in the PAN backbone network are initialized by using the corresponding pre-trained models from the ImageNet classification task <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref>. The weights of the new layers for PAN, RPN, Fast R-CNN and mask prediction network are initialized by using random weights with a Gaussian distribution of mean 0 and standard deviation 0.01. Our Mask R-CNN based text detection model is trained in an endto-end manner and optimized by the standard SGD algorithm with a momentum of 0.9 and weight decay of 0.0005. For ICDAR-2017 MLT, we use the training and validation data, i.e., a total of 9,000 images for training, while for both ICDAR-2015 and SCUT-CTW 1500, we only use the provided training images for training.</p><p>We implement our approach based on MXNet and experiments are conducted on a workstation with 4 Nvidia P100 GPUs. We adopt a multi-scale training strategy. The scale S is defined as the length of the shorter side of an image. In In the testing phase, we keep the top-2000 scoring text proposals generated by RPN for the succeeding Fast R-CNN. After the Fast R-CNN step, quadrilateral bounding boxes of detected text instances are predicted and suppressed by the Skewed NMS <ref type="bibr" target="#b27">[28]</ref> algorithm with an IoU threshold of 0.3. Finally, ROI features in the axisaligned rectangular bounding box of each remaining text instance are fed into the mask prediction network to get the text mask. For the ICDAR-2017 MLT and ICDAR-2015 datasets, we directly use the quadrilateral bounding boxes predicted by the Fast R-CNN module as the final detection results, while for the curved text detection dataset SCUT-CTW 1500, we use the text masks predicted by the mask prediction network as the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Component evaluation</head><p>In this section, we conduct a series of ablation experiments to evaluate the effectiveness of the base convolutional network and PAN on ICDAR-2017 MLT, ICDAR-2015 and SCUT-CTW 1500 text detection benchmark datasets. All the experiments are based on single-model and single-scale testing. The scales of testing images are set as 1440, 1024 and 512 for ICDAR-2017 MLT, ICDAR-2015 and SCUT-CTW 1500, respectively. ResNeXt50 is better than ResNet50. As an important part of a backbone network (e.g., ResNet50-FPN), the base convolutional network (e.g., ResNet50) affects the text detec-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Prior Arts</head><p>We compare the performance of our approach with other most competitive results on the ICDAR-2017 MLT, ICDAR-2015 and SCUT-CTW 1500 text detection benchmark datasets. For fair comparisons, we report all results without using recognition information. As shown in Tables 4-6, our approach achieves the best performance on these three datasets by only using single-scale and single-model testing. Specifically, as shown in <ref type="table" target="#tab_2">Table 4</ref>, our approach outperforms the closest method <ref type="bibr" target="#b23">[24]</ref> significantly by improving the F-measure from 0.707 to 0.743 on the challenging ICDAR-2017 MLT dataset, even though <ref type="bibr" target="#b23">[24]</ref> applies multiscale testing to achieve the best possible performance. On the ICDAR-2015 dataset, as shown in <ref type="table">Table 5</ref>, even though some other approaches have used extra training data, our approach still achieves the best result of 0.815, 0.908 and 0.859 in recall, precision and F-measure respectively. On the SCUT-CTW 1500 dataset, our approach has achieved a new state-of-the-art result, i.e., 0.832, 0.868 and 0.850 in recall, precision and F-measure respectively as shown in Table 6, outperforming other methods by a large margin. The superior performance achieved by our proposed approach on these three challenging text detection benchmarks can demonstrate the advantage of our approach. Some qualitative detection results are depicted in Figs. 6-8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ExtraData R P F Proposed ? 0.815 0.908 0.859 IncepText <ref type="bibr" target="#b36">[37]</ref> ? 0.806 0.905 0.853 FTSN <ref type="bibr" target="#b4">[5]</ref> D 0.800 0.886 0.841 R2CNN <ref type="bibr" target="#b15">[16]</ref> D 0.797 0.856 0.825 DDR <ref type="bibr" target="#b12">[13]</ref> D 0.800 0.820 0.810 EAST <ref type="bibr" target="#b40">[41]</ref> -0.783 0.832 0.807 RRPN <ref type="bibr" target="#b27">[28]</ref> D 0.732 0.822 0.774 SegLink <ref type="bibr" target="#b32">[33]</ref> D 0.731 0.768 0.749 <ref type="table">Table 5</ref>: Comparison with prior arts on ICDAR-2015. R, P and F stand for recall, precision and F-measure respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>R P F Proposed 0.832 0.868 0.850 CTD+TLOC <ref type="bibr" target="#b25">[26]</ref> 0.698 0.774 0.734</p><p>DMPNet <ref type="bibr" target="#b24">[25]</ref> 0.560 0.699 0.622 EAST <ref type="bibr" target="#b40">[41]</ref> 0.491 0.787 0.604 CTPN <ref type="bibr" target="#b33">[34]</ref> 0.538 0.604 0.569 <ref type="table">Table 6</ref>: Comparison with prior arts on SCUT-CTW 1500. R, P and F stand for recall, precision and F-measure respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>A new Mask R-CNN based text detection approach has been proposed in this paper. Thanks to the flexibility of Mask R-CNN, the proposed approach can detect multioriented and curved text from natural scene images robustly in a unified manner. Moreover, we demonstrate that using the Pyramid Attention Network (PAN) as a new backbone network of Mask R-CNN enhances the feature representation ability of Mask R-CNN significantly, so that false alarms caused by text-like backgrounds are suppressed more effectively. Our proposed approach has achieved superior performance on both multi-oriented (ICDAR-2015, ICDAR-2017 MLT) and curved (SCUT-CTW1500) text detection benchmark tasks by only using single-scale and single-model testing. However, our approach still has some limitations. First, the running speed of our approach is not fast enough due to the computation intensive PAN backbone network and Mask R-CNN framework. Moreover, our approach struggles with skewed nearby long text-lines owing to the limitation of rectangular proposals generated by RPN. More researches are needed to address these challenging problems.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of our Mask R-CNN based text detector, which consists of a PAN backbone network, a region proposal network, a Fast R-CNN detector and a mask prediction network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>FPA of our PAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>GAU of our PAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Architecture of our PAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of Fast R-CNN and mask prediction network. The ROI features are first fed into the the 5-th stage of ResNet50 or ResneXt50, whose output features are shared by both Fast R-CNN and mask prediction network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where t = {( xi , yi )|i ? {1, 2, 3, 4}} and t * = {( * xi , * yi )|i ? {1, 2, 3, 4}} represent the predicted and ground-truth 8-dimensional parameterized coordinate offsets. Let {(x g i , y g i )|i ? {1, 2, 3, 4}} denote the four vertices of G and (x p 1 , y p 1 , x p 2 , y p 2 , P w , P h ) be the top-left and bottom-right coordinates, width and height of an input proposal P . The parameterizations of t * are denoted as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>PAN can effectively suppress some false alarms caused by text-like backgrounds. The first row: detection results of Mask R-CNN with FPN. The second row: detection results of Mask R-CNN with PAN. each training iteration, a selected training image is individually rescaled by randomly sampling a scale S from the set {480, 576, 720, 928, 1088}, {480, 576, 688, 720, 928}, and {300, 400, 500, 600, 704} for ICDAR-2017 MLT, ICDAR-2015 and SCUT-CTW 1500, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>BaseTable 1 :</head><label>1</label><figDesc>Component evaluation on ICDAR-2017 MLT. R, P and F stand for recall, precision and F-measure respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Detection results of our proposed Mask R-CNN based text detector on ICDAR-2017 MLT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Detection results of our proposed Mask R-CNN based text detector on SCUT-CTW1500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Component evaluation on ICDAR-2015. R, P and F stand for recall, precision and F-measure respectively.</figDesc><table><row><cell>Base Network FPN PAN ResneXt50 D ResneXt50 D 0.832 0.868 0.850 R P F 0.826 0.839 0.833</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Component evaluation on SCUT-CTW 1500. R, P and F stand for recall, precision and F-measure respectively. tion performance a lot. Here we compare the performance of two different base convolutional networks, i.e., Resnet50 and ResneXt50, on ICDAR-2017 MLT and ICDAR-2015. As shown inTable 1andTable 2, ResneXt50 can consistently outperform ResNet50. In the following experiments,</figDesc><table><row><cell>Method</cell><cell>R</cell><cell>P</cell><cell>F</cell></row><row><cell>Proposed</cell><cell cols="3">0.698 0.800 0.743</cell></row><row><cell>FOTS MS [24]</cell><cell cols="3">0.623 0.818 0.707</cell></row><row><cell>SCUT DLVClab1 [30]</cell><cell cols="3">0.545 0.802 0.649</cell></row><row><cell cols="4">SARI FDU RRPN v1 [28] 0.555 71.17 0.623</cell></row><row><cell>TDN SJTU2017 [30]</cell><cell cols="3">0.471 0.642 0.543</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Comparison with prior arts on ICDAR-2017 MLT. R, P and F stand for recall, precision and F-measure respectively. MS indicates using multi-scale testing.we will use ResneXt50 as our base convolutional network.PAN is more powerful than FPN. We compare PAN with FPN [21] on all three datasets. As shown in Tables 1-3, no matter which base network is used, PAN consistently outperforms FPN on all datasets, which can demonstrate the effectiveness of PAN. The major improvement of PAN comes from the higher precision especially when the base network is relatively weaker, e.g., when ResNet50 is used as the base network on ICDAR-2017 MLT, PAN can increase the precision by 4.3% absolutely(Table 1). Some qualitative comparison examples on ICDAR-2015 are shownFig. 6, from which we can find that some false alarms caused by text-like backgrounds could be suppressed by PAN.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2017">MLT<ref type="bibr" target="#b29">[30]</ref>) and curved (SCUT-CTW1500<ref type="bibr" target="#b25">[26]</ref>) text detection benchmark tasks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fused text segmentation networks for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03272</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6773" to="6780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="745" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07709</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Q</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3454" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1712.02170</idno>
		<title level="m">Detecting curve text in the wild: New dataset and new solution. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Robust widebaseline stereo from maximally stable extremal regions. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification -rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3482" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Inceptext: A new inception-text module with deformable psroi pooling for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01167</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeptext: A new approach for text proposal generation and text detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
