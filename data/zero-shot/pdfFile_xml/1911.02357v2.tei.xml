<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
							<email>paul.bergmann@mvtec.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MVTec Software GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
							<email>fauser@mvtec.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MVTec Software GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
							<email>sattlegger@mvtec.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MVTec Software GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
							<email>steger@mvtec.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MVTec Software GmbH</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in highresolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomalyfree training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of realworld datasets, including the recently introduced MVTec Anomaly Detection dataset that was specifically designed to benchmark anomaly segmentation algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised pixel-precise segmentation of regions that appear anomalous or novel to a machine learning model is an important and challenging task in many domains of computer vision. In automated industrial inspection scenarios, it is often desirable to train models solely on a single class of anomaly-free images to segment defective regions during inference. In an active learning setting, regions that are detected as previously unknown by the current model can be included in the training set to improve the model's performance.</p><p>Recently, efforts have been made to improve anomaly detection for one-class or multi-class classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. However, these algorithms assume that anomalies manifest themselves in the form of images of an entirely different class and a simple binary imagelevel decision whether an image is anomalous or not must be made. Little work has been directed towards the development of methods that can segment anomalous regions that only differ in a very subtle way from the training data. Bergmann et al. <ref type="bibr" target="#b6">[7]</ref> provide benchmarks for several state-of-the-art algorithms and identify a large room for improvement.</p><p>Existing work predominantly focuses on generative algorithms such as Generative Adversarial Networks (GANs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> or Variational Autoencoders (VAEs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>. These detect anomalies using per-pixel reconstruction errors or by evaluating the density obtained from the model's probability distribution. This has been shown to be problematic due to inaccurate reconstructions or poorly calibrated likelihoods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The performance of many supervised computer vision algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref> is improved by transfer learning, i.e. by using discriminative embeddings from pretrained networks. For unsupervised anomaly detection, such approaches have not been thoroughly explored so far. Recent work suggests that these feature spaces generalize well for anomaly detection and even simple baselines outperform generative deep learning approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. However, the performance of existing methods on large high-resolution image datasets is hampered by the use of shallow machine learning pipelines that require a dimensionality reduction of the used feature space. Moreover, they rely on heavy training data subsampling since their capacity does not suffice to model highly complex data distributions with a large number of training samples.</p><p>We propose to circumvent these limitations of shallow models by implicitly modeling the distribution of training features with a student-teacher approach. This leverages the high capacity of deep neural networks and frames anomaly detection as a feature regression problem. Given a descriptive feature extractor pretrained on a large dataset of patches from natural images (the teacher), we train an ensemble of student networks on anomaly-free training <ref type="figure">Figure 2</ref>: Schematic overview of our approach. Input images are fed through a teacher network that densely extracts features for local image regions. An ensemble of M student networks is trained to regress the output of the teacher on anomaly-free data. During inference, the students will yield increased regression errors e and predictive uncertainties v in pixels for which the receptive field covers anomalous regions. Anomaly maps generated with different receptive fields can be combined for anomaly segmentation at multiple scales. data to mimic the teacher's output. During inference, the students' predictive uncertainty together with their regression error with respect to the teacher are combined to yield dense anomaly scores for each input pixel. Our intuition is that students will generalize poorly outside the manifold of anomaly-free training data and start to make wrong predictions. <ref type="figure" target="#fig_0">Figure 1</ref> shows qualitative results of our method when applied to images selected from the MVTec Anomaly Detection dataset <ref type="bibr" target="#b6">[7]</ref>. A schematic overview of the entire anomaly detection process is given in <ref type="figure">Figure 2</ref>. Our main contributions are:</p><p>? We propose a novel framework for unsupervised anomaly detection based on student-teacher learning. Local descriptors from a pretrained teacher network serve as surrogate labels for an ensemble of students. Our models can be trained end-to-end on large unlabeled image datasets and make use of all available training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We introduce scoring functions based on the students' predictive variance and regression error to obtain dense anomaly maps for the segmentation of anomalous regions in natural images. We describe how to extend our approach to segment anomalies at multiple scales by adapting the students' and teacher's receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We demonstrate state-of-the-art performance on three real-world computer vision datasets. We compare our method to a number of shallow machine learning classifiers and deep generative models that are fitted directly to the teacher's feature distribution. We also compare it to recently introduced deep learning based methods for unsupervised anomaly segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There exists an abundance of literature on anomaly detection <ref type="bibr" target="#b26">[27]</ref>. Deep learning based methods for the segmentation of anomalies strongly focus on generative models such as autoencoders <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> or GANs <ref type="bibr" target="#b30">[31]</ref>. These attempt to learn representations from scratch, leveraging no prior knowledge about the nature of natural images, and segment anomalies by comparing the input image to a reconstruction in pixel space. This can result in poor anomaly detection performance due to simple per-pixel comparisons or imperfect reconstructions <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anomaly Detection with Pretrained Networks</head><p>Promising results have been achieved by transferring discriminative embedding vectors of pretrained networks to the task of anomaly detection by fitting shallow machine learning models to the features of anomaly-free training data. Andrews et al. <ref type="bibr" target="#b2">[3]</ref> use activations from different layers of a pretrained VGG network and model the anomaly-free training distribution with a ?-SVM. However, they only apply their method to image classification and do not consider the segmentation of anomalous regions. Similar experiments have been performed by Burlina et al. <ref type="bibr" target="#b9">[10]</ref>. They report superior performance of discriminative embeddings compared to feature spaces obtained from generative models.</p><p>Nazare et al. <ref type="bibr" target="#b23">[24]</ref> investigate the performance of different off-the-shelf feature extractors pretrained on an image classification task for the segmentation of anomalies in surveillance videos. Their approach trains a 1-Nearest-Neighbor (1-NN) classifier on embedding vectors extracted from a large number of anomaly-free training patches. Prior to the training of the shallow classifier, the dimensionality of the network's activations is reduced using Principal Component Analysis (PCA). To obtain a spatial anomaly map during inference, the classifier must be evaluated for a large number of overlapping patches, which quickly becomes a performance bottleneck and results in rather coarse anomaly maps. Similarly, Napoletano et al. <ref type="bibr" target="#b22">[23]</ref> extract activations from a pretrained ResNet-18 for a large number of cropped training patches and model their distribution using K-Means clustering after prior dimensionality reduction with PCA. They also perform strided evaluation of test images during inference. Both approaches sample training patches from the input images and therefore do not make use of all possible training features. This is necessary since, in their frame- <ref type="figure">Figure 3</ref>: Pretraining of the teacher networkT to output descriptive embedding vectors for patch-sized inputs. The knowledge of a powerful but computationally inefficient network P is distilled intoT by decoding the latent vectors to match the descriptors of P . We also experiment with embeddings obtained using selfsupervised metric learning techniques based on triplet learning. Information within each feature dimension is maximized by decorrelating the feature dimensions within a minibatch. work, feature extraction is computationally expensive due to the use of very deep networks that output only a single descriptor per patch. Furthermore, since shallow models are employed for learning the feature distribution of anomaly-free patches, the available training information must be strongly reduced.</p><p>To circumvent the need for cropping patches and to speed up feature extraction, Sabokrou et al. <ref type="bibr" target="#b29">[30]</ref> extract descriptors from early feature maps of a pretrained AlexNet in a fully convolutional fashion and fit a unimodal Gaussian distribution to all available training vectors of anomaly-free images. Even though feature extraction is achieved more efficiently in their framework, pooling layers lead to a downsampling of the input image. This strongly decreases the resolution of the final anomaly map, especially when using descriptive features of deeper network layers with larger receptive fields. In addition, unimodal Gaussian distributions will fail to model the training feature distribution as soon as the problem complexity rises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Open-Set Recognition with Uncertainty Estimates</head><p>Our work draws some inspiration from the recent success of open-set recognition in supervised settings such as image classification or semantic segmentation, where uncertainty estimates of deep neural networks have been exploited to detect out-of-distribution inputs using MC Dropout <ref type="bibr" target="#b13">[14]</ref> or deep ensembles <ref type="bibr" target="#b18">[19]</ref>. Seeboeck et al. <ref type="bibr" target="#b32">[33]</ref> demonstrate that uncertainties from segmentation networks trained with MC Dropout can be used to detect anomalies in retinal OCT images. Beluch et al. <ref type="bibr" target="#b5">[6]</ref> show that the variance of network ensembles trained on an image classification task serves as an effective acquisition function for active learning. Inputs that appear anomalous to the current model are added to the training set to quickly enhance its performance.</p><p>Such algorithms, however, demand prior labeling of images by domain experts for a supervised task, which is not always possible or desirable. In our work, we utilize feature vectors of pretrained networks as surrogate labels for the training of an ensemble of student networks. The predictive variance together with the regression error of the ensemble's output mixture distribution is then used as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Student-Teacher Anomaly Detection</head><p>This section describes the core principles of our proposed method. Given a training dataset D = {I 1 , I 2 , . . . , I N } of anomaly-free images, our goal is to create an ensemble of student networks S i that can later detect anomalies in test images J. This means that they can assign a score to each pixel indicating how much it deviates from the training data manifold. For this, the student models are trained against regression targets obtained from a descriptive teacher network T pretrained on a large dataset of natural images. After the training, anomaly scores can be derived for each image pixel from the students' regression error and predictive variance. Given an input image I ? R w?h?C of width w, height h, and number of channels C, each student S i in the ensemble outputs a feature map S i (I) ? R w?h?d . It contains descriptors y (r,c) ? R d of dimension d for each input image pixel at row r and column c. By design, we limit the students' receptive field, such that y (r,c) describes a square local image region p (r,c) of I centered at (r, c) of side length p. The teacher T has the same network architecture as the student networks. However, it remains constant and extracts descriptive embedding vectors for each pixel of the input image I that serve as deterministic regression targets during student training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Local Patch Descriptors</head><p>We begin by describing how to efficiently construct a descriptive teacher network T using metric learning and knowledge distillation techniques. In existing work for anomaly detection with pretrained networks, feature extractors only output single feature vectors for patchsized inputs or spatially heavily downsampled feature maps <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>. In contrast, our teacher network T efficiently outputs descriptors for every possible square of side length p within the input image. T is obtained by first training a networkT to embed patch-sized images p ? R p?p?C into a metric space of dimension d using only convolution and max-pooling layers. Fast dense local feature extraction for an entire input image can then be achieved by a deterministic network transformation ofT to T as described in <ref type="bibr" target="#b3">[4]</ref>. This yields significant speedups compared to previously introduced methods that perform patch-based strided evaluations. To letT output semantically strong descriptors, we investigate both selfsupervised metric learning techniques as well as distilling knowledge from a descriptive but computationally inefficient pretrained network. A large number of training patches p can be obtained by random crops from any image database. Here, we use ImageNet <ref type="bibr" target="#b17">[18]</ref>.</p><p>Knowledge Distillation. Patch descriptors obtained from deep layers of CNNs trained on image classification tasks perform well for anomaly detection when modeling their distribution with shallow machine learning models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. However, the architectures of such CNNs are usually highly complex and computationally inefficient for the extraction of local patch descriptors. Therefore, we distill the knowledge of a powerful pretrained network P intoT by matching the output of P with a decoded version of the descriptor obtained fromT :</p><formula xml:id="formula_0">L k (T ) = ||D(T (p)) ? P (p)|| 2 .</formula><p>(1) D denotes a fully connected network that decodes the ddimensional output ofT to the output dimension of the pretrained network's descriptor.</p><p>Metric Learning. If for some reason pretrained networks are unavailable, one can also learn local image descriptors in a fully self-supervised way <ref type="bibr" target="#b11">[12]</ref>. Here, we investigate the performance of discriminative embeddings obtained using triplet learning. For every randomly cropped patch p, a triplet of patches (p, p + , p ? ) is augmented. Positive patches p + are obtained by small random translations around p, changes in image luminance, and the addition of Gaussian noise. The negative patch p ? is created by a random crop from a randomly chosen different image. Intriplet hard negative mining with anchor swap <ref type="bibr" target="#b36">[37]</ref> is used as a loss function for learning an embedding sensitive to the 2 metric</p><formula xml:id="formula_1">L m (T ) = max{0, ? + ? + ? ? ? },<label>(2)</label></formula><p>where ? &gt; 0 denotes the margin parameter and in-triplet distances ? + and ? ? are defined as:</p><formula xml:id="formula_2">? + = ||T (p) ?T (p + )|| 2 (3) ? ? = min{||T (p) ?T (p ? )|| 2 , ||T (p + ) ?T (p ? )|| 2 }.<label>(4)</label></formula><p>Descriptor Compactness. As proposed by Vassileios et al. <ref type="bibr" target="#b34">[35]</ref>, we minimize the correlation between descriptors within one minibatch of inputs p to increase the descriptors' compactness and remove unnecessary redundancy:</p><formula xml:id="formula_3">L c (T ) = i =j c ij ,<label>(5)</label></formula><p>where c ij denotes the entries of the correlation matrix computed over all descriptorsT <ref type="bibr">(p)</ref> in the current minibatch.</p><p>The final training loss forT is then given as</p><formula xml:id="formula_4">L(T ) = ? k L k (T ) + ? m L m (T ) + ? c L c (T ),<label>(6)</label></formula><p>where ? k , ? m , ? c ? 0 are weighting factors for the individual loss terms. <ref type="figure">Figure 3</ref> summarizes the entire learning process for the teacher's discriminative embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ensemble of Student Networks for Deep Anomaly Detection</head><p>Next, we describe how to train student networks S i to predict the teacher's output on anomaly-free training data. We then derive anomaly scores from the students' predictive uncertainty and regression error during inference. First, the vector of component-wise means ? ? R d and standard deviations ? ? R d over all training descriptors is computed for data normalization. Descriptors are extracted by applying T to each image in the dataset D. We then train an ensemble of M ? 1 randomly initialized student networks S i , i ? {1, . . . , M } that possess the identical network architecture as the teacher T . For an input image I, each student outputs its predictive distribution over the space of possible regression targets for each local image region p (r,c) centered at row r and column c. Note that the students' architecture with limited receptive field of size p allows us to obtain dense predictions for each image pixel with only a single forward pass, without having to actually crop the patches p (r,c) . The students' output vectors are modeled as a Gaussian distribution P r(y|p (r,c) ) = N (y|? Si (r,c) , s) with constant covariance s ? R, where ? Si (r,c) denotes the prediction made by S i for the pixel at (r, c). Let y T (r,c) denote the teacher's respective descriptor that is to be predicted by the students. The loglikelihood training criterion L(S i ) for each student network then simplifies to the squared 2 -distance in feature space:</p><formula xml:id="formula_5">L(S i ) = 1 wh (r,c) ||? Si (r,c) ? (y T (r,c) ? ?)diag(?) ?1 || 2 2 ,<label>(7)</label></formula><p>where diag(?) ?1 denotes the inverse of the diagonal matrix filled with the values in ?.</p><p>Scoring Functions for Anomaly Detection. Having trained each student to convergence, a mixture of Gaussians can be obtained at each image pixel by equally weighting the ensemble's predictive distributions. From it, measures of anomaly can be obtained in two ways: First, we propose to compute the regression error of the mixture's mean ? (r,c) with respect to the teacher's surrogate label:</p><formula xml:id="formula_6">e (r,c) = ||? (r,c) ? (y T (r,c) ? ?)diag(?) ?1 || 2 2 (8) = 1 M M i=1 ? Si (r,c) ? (y T (r,c) ? ?)diag(?) ?1 2 2<label>(9)</label></formula><p>The intuition behind this score is that the student networks will fail to regress the teacher's output within anomalous  regions during inference since the corresponding descriptors have not been observed during training. Note that e (r,c) is non-constant even for M = 1, where only a single student is trained and anomaly scores can be efficiently obtained with only a single forward pass through the student and teacher network, respectively. As a second measure of anomaly, we compute for each pixel the predictive uncertainty of the Gaussian mixture as defined by Kendall et al. <ref type="bibr" target="#b13">[14]</ref>, assuming that the student networks generalize similarly for anomaly-free regions and differently in regions that contain novel information unseen during training:</p><formula xml:id="formula_7">v (r,c) = 1 M M i=1 ||? Si (r,c) || 2 2 ? ||? (r,c) || 2 2 .<label>(10)</label></formula><p>To combine the two scores, we compute the means e ? , v ? and standard deviations e ? , v ? of all e (r,c) and v (r,c) , respectively, over a validation set of anomaly-free images. Summation of the normalized scores then yields the final anomaly score: <ref type="figure" target="#fig_1">Figure 4</ref> illustrates the basic principles of our anomaly detection method on the MNIST dataset, where images with label 0 were treated as the normal class and all other classes were treated as anomalous. Since the images of this dataset are very small, we extracted a single feature vector for each image usingT and trained an ensemble of M = 5 patch-sized students to regress the teacher's output. This results in a single anomaly score for each input image. Feature descriptors were embedded into 2D using multidimensional scaling <ref type="bibr" target="#b8">[9]</ref> to preserve their relative distances.</p><formula xml:id="formula_8">e (r,c) +? (r,c) = e (r,c) ? e ? e ? + v (r,c) ? v ? v ? .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Scale Anomaly Segmentation</head><p>If an anomaly only covers a small part of the teacher's receptive field of size p, the extracted feature vector predominantly describes anomaly-free traits of the local image region. Consequently, the descriptor can be predicted well by the students and anomaly detection performance will decrease. One could tackle this problem by downsampling the input image. This would, however, lead to an undesirable loss in resolution of the output anomaly map.</p><p>Our framework allows for explicit control over the size of the students' and teacher's receptive field p. Therefore, we can detect anomalies at various scales by training multiple student-teacher ensemble pairs with varying values of p. At each scale, an anomaly map with the same size as the input image is computed. Given L studentteacher ensemble pairs with different receptive fields, the normalized anomaly scores? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To demonstrate the effectiveness of our approach, an extensive evaluation on a number of datasets is performed. We measure the performance of our studentteacher framework against existing pipelines that use shallow machine learning algorithms to model the feature distribution of pretrained networks. To do so, we compare to a K-Means classifier, a One-Class SVM (OC-SVM), and a 1-NN classifier. They are fitted to the distribution of the teacher's descriptors after prior dimensionality reduction using PCA. We also experiment with deterministic and variational autoencoders as deep distribution models over the teacher's discriminative embedding. The 2reconstruction error <ref type="bibr" target="#b12">[13]</ref> and reconstruction probability <ref type="bibr" target="#b1">[2]</ref> are used as the anomaly score, respectively. We further compare our method to recently introduced generative and discriminative deep learning based anomaly detection models and report improved performance over the state of the art. We want to stress that the teacher has not observed images of the evaluated datasets during pretraining to avoid an unfair bias. <ref type="figure">Figure 5</ref>: Anomaly detection at multiple scales: Architectures with receptive field of size p = 17 manage to accurately segment the small scratch on the capsule (top row). However, defects at a larger scale such as the missing imprint (bottom row) become problematic. For increasingly larger receptive fields, the segmentation performance for the larger anomaly increases while it decreases for the smaller one. Our multiscale architecture mitigates this problem by combining multiple receptive fields.</p><p>As a first experiment, we perform an ablation study to find suitable hyperparameters. Our algorithm is applied to a one-class classification setting on the MNIST <ref type="bibr" target="#b19">[20]</ref> and CIFAR-10 <ref type="bibr" target="#b16">[17]</ref> datasets. We then evaluate on the much more challenging MVTec Anomaly Detection (MVTec AD) dataset, which was specifically designed to benchmark algorithms for the segmentation of anomalous regions. It provides over 5000 high-resolution images divided into ten object and five texture categories. To highlight the benefit of our multi-scale approach, an additional ablation study is performed on MVTec AD, which investigates the impact of different receptive fields on the anomaly detection performance.</p><p>For our experiments, we use identical network architectures for the student and teacher networks, with receptive field sizes p ? {17, 33, 65}. All architectures are simple CNNs with only convolutional and max-pooling layers, using leaky rectified linear units with slope 5?10 ?3 as activation function. <ref type="table">Table 4</ref> shows the specific architecture used for p = 65. For p = 17 and p = 33, similar architectures are given in in Appendix A.</p><p>For the pretraining of the teacher networksT , triplets augmented from the ImageNet dataset are used. Images are zoomed to equal width and height sampled from {4p, 4p + 1, . . . , 16p} and a patch of side length p is cropped at a random location. A positive patch p + for each triplet is then constructed by randomly translating  the crop location within the interval {? p?1 4 , . . . , p?1 4 }. Gaussian noise with standard deviation 0.1 is added to p + . All images within a triplet are randomly converted to grayscale with a probability of 0.1. For knowledge distillation, we extract 512-dimensional feature vectors from the fully connected layer of a ResNet-18 that was pretrained for classification on the ImageNet dataset. For network optimization, we use the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with an initial learning rate of 2 ? 10 ?4 , a weight decay of 10 ?5 , and a batch size of 64. Each teacher network outputs descriptors of dimension d = 128 and is trained for 5 ? 10 4 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MNIST and CIFAR-10</head><p>Before considering the problem of anomaly segmentation, we evaluate our method on the MNIST and CIFAR-10 datasets, adapted for one-class classification. Five students are trained on only a single class of the dataset, while during inference images of the other classes must be detected as anomalous. Each image is zoomed to the students' and teacher's input size p and a single feature vector is extracted by passing it through the patch-sized networksT and? i . We examine different teacher networks by varying the weights ? k , ? m , ? c in the teacher's loss function L(T ). The patch size for the experiments in this subsection is set to p = 33. As a measure of anomaly detection performance, the area under the ROC curve is evaluated. Shallow and deep distributions models are trained on the teacher's descriptors of all available in-distribution samples. We additionally report numbers for OCGAN <ref type="bibr" target="#b25">[26]</ref>, a recently proposed generative model directly trained on the input images. Detailed information on training parameters for all methods on this dataset is found in Appendix B. <ref type="table" target="#tab_3">Table 2</ref> shows our results. Our approach outperforms the other methods for a variety of hyperparameter settings. Distilling the knowledge of the pretrained ResNet-18 into the teacher's descriptor yields slightly better performance than training the teacher in a fully self-supervised way using triplet learning. Reducing descriptor redundancy by minimizing the correlation matrix yields improved results. On average, shallow models and autoencoders fitted to our teacher's feature distribution outperform OCGAN but do not reach the performance of our approach. Since for 1-NN, every single training vector can be stored, Category p = 17 p = 33 p = 65 Multiscale  <ref type="table">Table 3</ref>: Performance of our algorithm on the MVTec AD dataset for different receptive field sizes p. Combining anomaly scores across multiple receptive fields shows increased performance for many of the dataset's categories. We report the normalized area under the PRO curve up to an average falsepositive rate of 30%.</p><p>it performs exceptionally well on these small datasets.</p><p>On average, however, our method still outperforms all evaluated approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MVTec Anomaly Detection Dataset</head><p>For all our experiments on MVTec AD, input images are zoomed to w = h = 256 pixels. We train on anomalyfree images for 100 epochs with batch size 1. This is equivalent to training on a large number of patches per batch due to the limited size of the networks' receptive field. We use Adam with initial learning rate 10 ?4 and weight decay 10 ?5 . Teacher networks were trained with ? k = ? c = 1 and ? m = 0, as this configuration performed best on MNIST and CIFAR-10. Ensembles contain M = 3 students.</p><p>To train shallow classifiers on the teacher's output descriptors, a subset of vectors is randomly sampled from the teacher's feature maps. Their dimension is then reduced by PCA, retaining 95% of the variance. The variational and deterministic autoencoders are implemented using a simple fully connected architecture and are trained on all available descriptors. In addition to fitting the models directly to the teacher's feature distribution, we benchmark our approach against the best performing deep learning based methods presented by Bergmann et al. <ref type="bibr" target="#b6">[7]</ref> on this dataset. These methods include the CNN-Feature Dictionary <ref type="bibr" target="#b22">[23]</ref>, the SSIM-Autoencoder <ref type="bibr" target="#b7">[8]</ref>, and AnoGAN <ref type="bibr" target="#b30">[31]</ref>. All hyperparameters are listed in detail in Appendix C.</p><p>We compute a threshold-independent evaluation metric based on the per-region-overlap (PRO), which weights ground-truth regions of different size equally. This is in contrast to simple per-pixel measures, such as ROC, for which a single large region that is segmented correctly can make up for many incorrectly segmented small ones. It was also used by Bergmann et al. in <ref type="bibr" target="#b6">[7]</ref>. For computing the PRO metric, anomaly scores are first thresholded to make a binary decision for each pixel whether an anomaly Layer</p><p>Output <ref type="table" target="#tab_1">Size  Parameters  Kernel Stride  Input  65?65?3  Conv1  61?61?128  5?5  1  MaxPool 30?30?128  2?2  2  Conv2  26?26?128  5?5  1  MaxPool 13?13?128  2?2  2  Conv3  9?9?128  5?5  1  MaxPool  4?4?256  2?2  2  Conv4  1?1?256  4?4  1  Conv5  1?1?128  3?3  1  Decode  1?1?512  1?1  1   Table 4</ref>: General outline of our network architecture for training teachersT with receptive field size p = 65. Leaky rectified linear units with slope 5?10 ?3 are applied as activation functions after each convolution layer. Architectures for p = 17 and p = 33 are given in Appendix A.</p><p>is present or not. For each connected component within the ground truth, the relative overlap with the thresholded anomaly region is computed. We evaluate the PRO value for a large number of increasing thresholds until an average per-pixel false-positive rate of 30% for the entire dataset is reached and use the area under the PRO curve as a measure of anomaly detection performance. Note that for high false-positive rates, large parts of the input images would be wrongly labeled as anomalous and even perfect PRO values would no longer be meaningful. We normalize the integrated area to a maximum achievable value of 1. <ref type="table" target="#tab_1">Table 1</ref> shows our results training each algorithm with a receptive field of p = 65 for comparability. Our method consistently outperforms all other evaluated algorithms for almost every dataset category. The shallow machine learning algorithms fitted directly to the teacher's descriptors after applying PCA do not manage to perform satisfactorily for most of the dataset categories. This shows that their capacity does not suffice to accurately model the large number of available training samples. The same can be observed for the CNN-Feature Dictionary. As it was the case in our previous experiment on MNIST and CIFAR-10, 1-NN yields the best results amongst the shallow models. Utilizing a large number of training features together with deterministic autoencoders increases the performance, but still does not match the performance of our approach. Current generative methods for anomaly segmentation such as Ano-GAN and the SSIM-autoencoder perform similar to the shallow methods fitted to the discriminative embedding of the teacher. This indicates that there is indeed a gap between methods that learn representations for anomaly detection from scratch and methods that leverage discriminative embeddings as prior knowledge. <ref type="table">Table 3</ref> shows the performance of our algorithm for different receptive field sizes p ? {17, 33, 65} and when combining multiple scales. For some objects, such as bottle and cable, larger receptive fields yield better results. For others, such as wood and toothbrush, the inverse behavior can be observed. Combining multiple scales enhances the performance for many of the dataset categories. A qualitative example highlighting the benefit of our multi-scale anomaly segmentation is visualized in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Size</head><p>Parameters <ref type="table" target="#tab_1">Kernel Stride  Input  33?33?3  Conv1  29?29?128  3?3  1  MaxPool 14?14?128  2?2  2  Conv2  10?10?256  5?5  1  MaxPool  5?5?256  2?2  2  Conv3  4?4?256  2?2  1  Conv4  1?1?128  4?4  1  Decode  1?1?512  1?1  1</ref> (a) Architecture for p = 33. <ref type="table" target="#tab_1">Input  17?17?3  Conv1  12?12?128  5?5  1  Conv2  8?8?256  5?5  1  Conv3  4?4?256  5?5  1  Conv4  1?1?128  4?4  1  Decode  1?1?512  1?1  1</ref> (b) Architecture for p = 17. <ref type="table">Table 5</ref>: Network architectures for teacher networksT with different receptive field sizes p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Output Size Parameters Kernel Stride</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel framework for the challenging problem of unsupervised anomaly segmentation in natural images. Anomaly scores are derived from the predictive variance and regression error of an ensemble of student networks, trained against embedding vectors from a descriptive teacher network. Ensemble training can be performed end-to-end and purely on anomaly-free training data without requiring prior data annotation. Our approach can be easily extended to detect anomalies at multiple scales. We demonstrate improvements over current stateof-the-art methods on a number of real-world computer vision datasets for one-class classification and anomaly segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Network Architectures</head><p>A description of the network architecture for a patchsized teacher networkT with receptive field of size p = 65 can be found in our main paper <ref type="table">(Table 4</ref>). Architectures for teachers with receptive field sizes p = 33 and p = 17 are depicted in Tables 5a and 5b, respectively. Leaky rectified linear units with slope 5 ? 10 ?3 are used as activation function after each convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Experiments on MNIST and CIFAR-10</head><p>Here, we give details about additional hyperparameters for our experiments on the MNIST and CIFAR-10 datasets. We additionally provide the per-class ROC-AUC values for the two datasets in Tables 6 and 7, respectively.</p><p>Hyperparameter Settings. For the deterministic 2autoencoder ( 2 -AE) and the variational autoencoder (VAE), we use a fully connected encoder architecture of shape 128-64-32-10 with leaky rectified linear units of slope 5 ? 10 ?3 . The decoder is constructed in a manner symmetric to the encoder. Both autoencoders are trained for 100 epochs at an initial learning rate of 10 ?2 using the Adam optimizer and a batch size of 64. A weight decay rate of 10 ?5 is applied for regularization. To evaluate the reconstruction probability of the VAE, five independent forward passes are performed for each feature vector. For the One-Class SVM (OC-SVM), a radial basis function kernel is used. K-Means is trained with 10 cluster centers and the distance to the single closest cluster center is evaluated as the anomaly score for each input sample. For 1-NN, the feature vectors of all available training samples are stored and tested during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Experiments on MVTec AD</head><p>We give additional information on the hyperparameters used in our experiments on MVTec AD for both shallow machine learning models as well as deep learning methods.</p><p>Shallow Machine Learning Models. For the 1-NN classifier, we construct a dictionary of 5000 feature vectors and take the distance to the closest training sample as anomaly score. For the other shallow classifiers, we fit their parameters on 50 000 training samples, randomly chosen from the teacher's feature maps. The K-Means algorithm is run with 10 cluster centers and measures the distance to the nearest cluster center in the feature space during inference. The OC-SVM employs a radial basis function kernel.</p><p>Deep-Learning Based Models. For evaluation on MVTec AD, the architecture of the 2 -AE and VAE are identical to the ones used on the MNIST and CIFAR-10 dataset. Each fully connected autoencoder is trained for 100 epochs. We use Adam with initial learning rate 10 ?4 and weight decay 10 ?5 . Batches are constructed from 512 randomly sampled vectors of the teacher's feature maps. The reconstruction probability of the VAE is computed by five individual forward passes through the network. For the evaluation of AnoGAN, the SSIM-Autoencoder, and the CNN-Feature Dictionary, we use the same hyperparameters as Bergmann et al. in the MVTec AD dataset paper <ref type="bibr" target="#b6">[7]</ref>. Only a slight adaption is applied to the CNN-Feature Dictionary by cropping patches of size p = 65 and performing the evaluation by computing anomaly scores for overlapping patches with a stride of 4 pixels.</p><p>Qualitative Results. We provide additional qualitative results of our method on MVTec AD for three objects and three textures in <ref type="figure" target="#fig_3">Figure 6</ref>. For each category, anomaly maps for multiple defect classes are provided. Our method performs well across different defect types and sizes. The results are shown for an ensemble of 3 students and a multi-scale architecture of receptive field sizes in {17, 33, 65} pixels.   <ref type="table">Table 6</ref>: Results on the MNIST dataset. For each method and digit, the area under the ROC curve is given. For our algorithm, we evaluate teacher networks trained with different loss functions. corresponds to setting the respective loss weight to 1, otherwise it is set to 0.  <ref type="table">Table 7</ref>: Results on the CIFAR-10 dataset. For each method and class, the area under the ROC curve is given. For our algorithm, we evaluate teacher networks trained with different loss functions. corresponds to setting the respective loss weight to 1, otherwise it is set to 0.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative results of our anomaly detection method on the MVTec Anomaly Detection dataset. Top row: Input images containing defects. Center row: Ground truth regions of defects in red. Bottom row: Anomaly scores for each image pixel predicted by our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Embedding vectors visualized for ten samples of the MNIST dataset. Larger circles around the students' mean predictions indicate increased predictive variance. Being only trained on a single class of training images, the students manage to accurately regress the features solely for this class (green). They yield large regression errors and predictive uncertainties for images of other classes (red). Anomaly scores for the entire dataset are displayed in the bottom histogram. a scoring function to segment anomalous regions in test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>c) of each scale l can be combined by simple averaging:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results of our method on selected textures (left) and objects (right) of the MVTec Anomaly Detection dataset. Our algorithm performs robustly across various defect categories, such as color defects, contaminations, and structural anomalies. Top row: Input images containing defects. Center row: Ground truth regions of defects in red. Bottom row: Anomaly scores for each image pixel predicted by our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>0.700 0.734 0.731 0.797 0.751 0.801 0.859 0.7502 L k Lm Lc Ours 0.789 0.849 0.734 0.748 0.851 0.793 0.892 0.830 0.862 0.848 0.8196 Ours 0.784 0.836 0.706 0.742 0.826 0.768 0.870 0.815 0.857 0.831 0.8035 Ours 0.804 0.855 0.706 0.709 0.798 0.738 0.860 0.797 0.849 0.824 0.7940 Ours 0.766 0.817 0.715 0.736 0.855 0.763 0.885 0.819 0.838 0.827 0.8021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on the MVTec Anomaly Detection dataset. For each dataset category, the normalized area under the PRO-curve up to an average false positive rate per-pixel of 30% is given. It measures the average overlap of each ground-truth region with the predicted anomaly regions for multiple thresholds. The best-performing method for each dataset category is highlighted in boldface.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on MNIST and CIFAR-10. For each method, the average area under the ROC curve is given, computed across each dataset category. For our algorithm, we evaluate teacher networks trained with different loss functions. corresponds to setting the respective loss weight to 1, otherwise it is set to 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>0.999 0.942 0.963 0.975 0.980 0.991 0.981 0.939 0.981 0.9750 1-NN 0.989 0.998 0.962 0.970 0.980 0.955 0.979 0.981 0.968 0.971 0.9753 KMeans 0.973 0.995 0.898 0.948 0.960 0.920 0.948 0.948 0.940 0.927 0.9457 OC-SVM 0.980 0.998 0.887 0.944 0.964 0.909 0.949 0.957 0.935 0.940 0.9463 2 -AE 0.992 0.999 0.967 0.980 0.988 0.970 0.988 0.987 0.978 0.983 0.9832 VAE 0.983 0.998 0.915 0.941 0.969 0.925 0.964 0.940 0.955 0.945 0.9535 L k L m L c Ours 0.999 0.999 0.990 0.993 0.992 0.993 0.997 0.995 0.986 0.991 0.9935 Ours 0.999 0.999 0.988 0.992 0.988 0.993 0.997 0.995 0.984 0.991 0.9926 Ours 0.999 0.999 0.992 0.992 0.988 0.993 0.997 0.995 0.988 0.992 0.9935 Ours 0.999 0.999 0.989 0.990 0.990 0.990 0.997 0.993 0.981 0.989 0.9917</figDesc><table><row><cell>Method</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Mean</cell></row><row><cell>OCGAN</cell><cell>0.998</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00057</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variational Autoencoder based Anomaly Detection using Reconstruction Probability. SNU Data Mining Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transfer Representation-Learning for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Jerone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis D</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anomaly Detection Workshop at ICML2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast Dense Feature Extraction with CNNs that have Pooling or Striding Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewodros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Habtegebrial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04488</idno>
		<title level="m">Shadi Albarqouni, and Nassir Navab. Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">M</forename><surname>N?rnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?hler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MVTec AD -A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving Unsupervised Defect Segmentation by Applying Structural Similarity to Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting>the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modern multidimensional scaling: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingwer</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Groenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="280" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Where&apos;s Wally Now? Deep Generative and Discriminative Embeddings for Novelty Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Jeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Anomaly detection using one-class neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06360</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised natural image patch learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dov</forename><surname>Danon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="237" />
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization. 3rd International Conference for Learning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet Classification With Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Metric Learning for Novelty and Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idoia</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Do Deep Generative Models Know What They Don</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09136</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">t Know? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Nanofibrous Materials by CNN-Based Self-Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Are pre-trained cnns good feature extractors for anomaly detection in surveillance videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">F</forename><surname>Tiago S Nazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moacir A</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08495</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep transfer learning for multiple class novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OCGAN: One-class novelty detection using GANs with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A review of novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="215" to="249" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Informed democracy: Voting-based novelty detection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1810.12819.pdf" />
	</analytic>
	<monogr>
		<title level="m">29th British Machine Vision Conference: BMVC 2018</title>
		<meeting><address><addrLine>Newcastle, UK; Durham</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="3" to="6" />
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepanomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth. F-Anogan</surname></persName>
		</author>
		<title level="m">Fast unsupervised anomaly detection with generative adversarial networks. Medical image analysis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting Epistemic Uncertainty of Anatomy Segmentation for Anomaly Detection in Retinal OCT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seebock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Ignacio</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrvoje</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Bogunovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Klimscha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Not all areas are equal: Transfer learning for semantic segmentation via hierarchical region selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6128" to="6136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meissner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sgarlata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tomassini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<title level="m">Space Novelty Detection with Variational Autoencoders. MICCAI 2019 International Workshop on Computational Diffusion MRI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Ponsa Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="119" to="120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
