<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Munchausen Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Vieillard</surname></persName>
							<email>vieillard@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team Universit? de Lorraine</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<region>Inria, IECL</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
							<email>pietquin@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
							<email>mfgeist@google.com</email>
							<affiliation key="aff2">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Munchausen Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most algorithms, based on temporal differences, replace the true value of a transiting state by their current estimate of this value. Yet, another estimate could be leveraged to bootstrap RL: the current policy. Our core contribution stands in a very simple idea: adding the scaled log-policy to the immediate reward. We show that slightly modifying Deep Q-Network (DQN) in that way provides an agent that is competitive with distributional methods on Atari games, without making use of distributional RL, n-step returns or prioritized replay. To demonstrate the versatility of this idea, we also use it together with an Implicit Quantile Network (IQN). The resulting agent outperforms Rainbow on Atari, installing a new State of the Art with very little modifications to the original algorithm. To add to this empirical study, we provide strong theoretical insights on what happens under the hood -implicit Kullback-Leibler regularization and increase of the action-gap.</p><p>margin, but it also overtakes C51 [8], the first agent based on distributional RL (distRL). As far as we know, M-DQN is the first agent not using distRL that outperforms a distRL agent 1 . The current state of the art for single agent algorithms is considered to be Rainbow <ref type="bibr" target="#b17">[18]</ref>, that combines C51 with other enhacements to DQN, and does not rely on massivly distributed computation (unlike  R2D2 [19], SEED [12]  or Agent57 [4]). To demonstrate the versatility of the M-RL idea, we apply the same recipe to modify Implicit Quantile Network (IQN) <ref type="bibr" target="#b10">[11]</ref>, a recent distRL agent. The resulting Munchausen-IQN (M-IQN) surpasses Rainbow, installing a new state of the art.</p><p>To support these empirical results, we provide strong theoretical insights about what happens under the hood. We rewrite M-DQN under an abstract dynamic programming scheme and show that it implicitly performs Kullback-Leibler (KL) regularization between consecutive policies. M-RL is not the first approach to take advantage of KL regularization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref>, but we show that, because this regularization is implicit, it comes with stronger theoretical guarantees. From this, we link M-RL to Conservative Value Iteration (CVI) <ref type="bibr" target="#b19">[20]</ref> and Dynamic Policy Programming (DPP) [3] that were not introduced with deep RL implementations. We also draw connections with Advantage Learning (AL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> and study the effect of M-RL on the action-gap <ref type="bibr" target="#b12">[13]</ref>. While M-RL is not the first scheme to induce an increase of the action-gap [7], it is the first one that allows quantifying this increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Munchausen Reinforcement Learning</head><p>RL is usually formalized within the Markov Decision Processes (MDP) framework. An MDP models the environment and is a tuple {S, A, P, r, ?}, with S and A the state and action spaces, P the Markovian transition kernel, r the bounded reward function and ? the discount factor. The RL agent interacts with the MDP using a policy ?, that associates to every state either an action (deterministic policy) or a distribution over actions (stochastic policy). The quality of this interaction is quantified by the expected discounted cumulative return, formalized as the state-action value function, q ? (s, a) = E ? [ ? t=0 ? t r(s t , a t )|s 0 = s, a 0 = a], the expectation being over trajectories induced by the policy ? and the dynamics P . An optimal policy satisfies ? * ? argmax ? q ? . The associated optimal value function q * = q ? * satisfies the Bellman equation q * (s, a) = r(s, a) + ?E s |s,a [max a q * (s , a )]. A deterministic greedy policy satisfies ?(a|s) = 1 for a ? argmax a q(s, a ) and will be written ? ? G(q). We also use softmax policies, ? = sm(q) ? ?(a|s) = exp q(s,a) a exp q(s,a ) . A standard RL agent maintains both a q-function and a policy (that can be implicit, for example ? ? G(q)), and it aims at learning an optimal policy. To do so, it often relies on Temporal Difference (TD) updates. To recall the principle of TD learning, we quickly revisit the classical Q-learning algorithm <ref type="bibr" target="#b33">[34]</ref>. When interacting with the environment the agent observes transitions (s t , a t , r t , s t+1 ). Would the optimal q-function q * be known in the state s t+1 , the agent could use it as a learning target and build successive estimates as q(s t , a t ) ? q(s t , a t ) + ?(r t + ? max a q * (s t+1 , a ) ? q(s t , a t )), using the Bellman equation, ? being a learning rate. Yet, q * is unknown, and the agent actually uses its current estimate q instead, which is known as bootstrapping.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most Reinforcement Learning (RL) algorithms make use of Temporal Difference (TD) learning <ref type="bibr" target="#b28">[29]</ref> in some ways. It is a well-known bootstrapping mechanism that consists in replacing the unknown true value of a transiting state by its current estimate and use it as a target for learning. Yet, agents compute another estimate while learning that could be leveraged to bootstrap RL: their current policy. Indeed, it reflects the agent's hunch about which actions should be executed next and thus, which actions are good. Building upon this observation, our core contribution stands in a very simple idea: optimizing for the immediate reward augmented by the scaled log-policy of the agent when using any TD scheme. We insist right away that this is different from maximum entropy RL <ref type="bibr" target="#b35">[36]</ref>, that subtracts the scaled log-policy to all rewards, and aims at maximizing both the expected return and the expected entropy of the resulting policy. We call this general approach "Munchausen Reinforcement Learning" (M-RL), as a reference to a famous passage of The Surprising Adventures of Baron Munchausen by Raspe <ref type="bibr" target="#b23">[24]</ref>, where the Baron pulls himself out of a swamp by pulling on his own hair.</p><p>To demonstrate the genericity and the strength of this idea, we introduce it into the most popular RL agent: the seminal Deep Q-Network (DQN) <ref type="bibr" target="#b22">[23]</ref>. Yet, DQN does not compute stochastic policies, which prevents using log-policies. So, we first introduce a straightforward generalization of DQN to maximum entropy RL <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17]</ref>, and then modify the resulting TD update by adding the scaled logpolicy to the immediate reward. The resulting algorithm, referred to as Munchausen-DQN (M-DQN), is thus genuinely a slight modification of DQN. Yet, it comes with strong empirical performances. On the Arcade Learning Environment (ALE) <ref type="bibr" target="#b5">[6]</ref>, not only it surpasses the original DQN by a large</p><p>We argue that the q-function is not the sole quantity that could be used to bootstrap RL. Let's assume that an optimal deterministic policy ? * is known. The log-policy is therefore 0 for optimal actions, and ?? for sub-optimal ones. This is a very strong learning signal, that we could add to the reward to ease learning, without changing the optimal control. The optimal policy ? * being obviously unknown, we replace it by the agent's current estimate ?, and we assume stochastic policies for numerical stability. To sum up, M-RL is a very simple idea, that consists in replacing r t by r t + ? ln ?(a t |s t ) in any TD scheme, assuming that the current agent's policy ? is stochastic, so as to bootstrap the current agent's guess about what actions are good.</p><p>To demonstrate the generality of this approach, we use it to enhance the seminal DQN <ref type="bibr" target="#b22">[23]</ref> deep RL algorithm. In DQN, the q-values are estimated by an online Q-network q ? , with weights copied regularly to a target network q?. The agent behaves following a policy ? ? ? G(q ? ) (with ?-greedy exploration), and stores transitions (s t , a t , r t , s t+1 ) in a FIFO replay buffer B. DQN performs stochastic gradient descent on the loss? B [(q ? (s t , a t ) ?q dqn (r t , s t+1 )) 2 ], regressing the targetq dqn : q dqn (r t , s t+1 ) = r t + ? a ?A ??(a |s t+1 )q?(s t+1 , a ) with ?? ? G(q?).</p><p>To derive Munchausen-DQN (M-DQN), we simply modify the regression target. M-RL assumes stochastic policies while DQN computes deterministic policies. A simple way to address this is to not only maximize the return, but also the entropy of the resulting policy, that is adopting the viewpoint of maximum entropy RL <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17]</ref>. It is straightforward to extend DQN to this setting, see Appx. A.1 for a detailed derivation. We call the resulting agent Soft-DQN (S-DQN). Let ? be the temperature parameter scaling the entropy, it just amounts to replace the original regression target b? q s-dqn (r t , s t+1 ) = r t +? a ?A ??(a |s t+1 ) q?(s t+1 , a )?? ln ??(a |s t+1 ) with ?? = sm( q? ? ), <ref type="bibr" target="#b0">(1)</ref> where we highlighted the differences with DQN in blue. Notice that this is nothing more than the most straightforward discrete-actions version of Soft Actor-Critic (SAC) <ref type="bibr" target="#b16">[17]</ref>. Notice also that in the limit ? ? 0 we retrieve DQN. The last step to obtain M-DQN is to add the scaled log-policy to the reward. Let ? ? [0, 1] be a scaling factor, the regression target of M-DQN is thu? q m-dqn (r t , s t+1 ) = r t +?? ln ??(a t |s t ) + ? a ?A ??(a |s t+1 ) q?(s t+1 , a )?? ln ??(a |s t+1 ) , <ref type="bibr" target="#b1">(2)</ref> still with ?? = sm( q? ? ), where we highlighted the difference with Soft-DQN in red (retrieved by setting ? = 0). Hence, M-DQN is genuinely obtained by replacingq dqn byq m-dqn as the regression target of DQN. All details of the resulting algorithm are provide in Appx. B.1. Despite being an extremely simple modification of DQN, M-DQN is very efficient. We show in <ref type="figure" target="#fig_0">Fig. 10</ref> the Human-normalized mean and median scores for various agents on the full set of 60 Atari games of ALE (more details in Sec. 4). We observe that M-DQN significantly outperforms DQN, but also C51 <ref type="bibr" target="#b7">[8]</ref>. As far we know, M-DQN is the first method that is not based on distRL which overtakes C51. These are quite encouraging empirical results.</p><p>To demonstrate the versatility of the M-RL principle, we also combine it with IQN <ref type="bibr" target="#b10">[11]</ref>, a recent and efficient distRL agent (note that IQN has had recent successors, such as Fully Parameterized Quantile Function (FQF) <ref type="bibr" target="#b34">[35]</ref>, to which in principle, we could also apply M-RL). We denote the resulting algorithm M-IQN. In a nutshell, IQN does not estimate the q-function, but the distribution of which the q-function is the mean, using a distributional Bellman operator. The (implicit) policy is still greedy according to the q-function, computed as the (empirical) mean of the estimated distribution. We apply the exact same recipe: derive soft-IQN using the principle of maximum entropy RL (which is as easy as for DQN), and add the scaled log-policy to the reward. For the sake of showing the generality of our method, we combine M-RL with a version of IQN that uses 3-steps returns (and we compare to IQN and Rainbow, that both use the same). We can observe on <ref type="figure" target="#fig_0">Fig. 10</ref> that M-IQN outperforms Rainbow, both in terms of mean and median scores, and thus defines the new state of the art. In addition, even when using only 1-step returns, M-IQN still outperforms Rainbow. This result and the details of M-IQN can be found respectively in Appx. B.3 and B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">What happens under the hood?</head><p>The impressive empirical results of M-RL (see Sec. 4 for more) call for some theoretical insights. To provide them, we frame M-DQN in an abstract Approximate Dynamic Programming (ADP) framework and analyze it. We mainly provide two strong results: (1) M-DQN implicitly performs KL regularization between successive policies, which translates in an averaging effect of approximation errors (instead of accumulation in general ADP frameworks); (2) it increases the action-gap by a quantifiable amount which also helps dealing with approximation errors. We also use this section to draw connections with the existing literature in ADP. Let's first introduce some additional notations.</p><p>We write ? X the simplex over the finite set X and Y X the set of applications from X to the set Y . With this, an MDP is {S, A, P ? ? S?A S , r ? R S?A , ? ? (0, 1)}, the state and action spaces being assumed finite. For f, g ? R S?A , we define a component-wise dot product f, g = ( a f (s, a)g(s, a)) s ? R S . This will be used with q-functions and (log-) policies, e.g. for expectations: E a??(?|s) [q(s, a)] = ?, q (s). For v ? R S , we have P v = (E s |s,a [v(s )]) s,a = ( s P (s |s, a)v(s )) s,a ? R S?A . We also defined a policy-induced transition kernel P ? as P ? q = P ?, q . With these notations, the Bellman evaluation operator is T ? q = r + ?P ? q and its unique fixed point is q ? . An optimal policy still satisfies ? * ? argmax ??? S A q ? . The set of greedy policies can be written as G(q) = argmax ??? S A ?, q . We'll also make use of the entropy of a policy, H(?) = ? ?, ln ? , and of the KL between two policies, KL(? 1 ||? 2 ) = ? 1 , ln ? 1 ? ln ? 2 .</p><p>A softmax is the maximizer of the Legendre-Fenchel transform of the entropy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>, sm(q) = argmax ? ?, q + H(?). Using this and the introduced notations, we can write M-DQN in the following abstract form (each iteration consists of a greedy step and an evaluation step):</p><formula xml:id="formula_0">? k+1 = argmax ??? S A ?, q k + ? H(?) q k+1 = r+?? ln ? k+1 + ?P ? k+1 , q k ?? ln ? k+1 + k+1 . M-VI(?, ? )<label>(3)</label></formula><p>We call the resulting scheme Munchausen Value Iteration, or M-VI(?,? ). The term k+1 stands for the error between the actual and the ideal update (sampling instead of expectation, approximation of q k by a neural network, fitting of the neural network). Removing the red term, we retrieve approximate VI (AVI) regularized by a scaled entropy, as introduced by Geist et al. <ref type="bibr" target="#b14">[15]</ref>, of which Soft-DQN is an instantiation (as well as SAC, with additional error in the greedy step). Removing also the blue term, we retrieve the classic AVI <ref type="bibr" target="#b25">[26]</ref>, of which DQN is an instantiation.</p><p>To get some insights, we rewrite the evaluation step, setting ? = 1 and with q k q k ? ? ln ? k :</p><formula xml:id="formula_1">q k+1 = r + ? ln ? k+1 + ?P ? k+1 , q k ? ? ln ? k+1 + k+1 ? q k+1 ? ? ln ? k+1 = r + ?P ? k+1 , q k ? ? ln ? k ? ? ln ? k+1 ? k + k+1 ? q k+1 = r + ?P ( ? k+1 , q k ? ? KL(? k+1 ||? k )) + k+1 .</formula><p>Then, the greedy step can be rewritten as (looking at what ? k+1 maximizes) ?, q k + ? H(?) = ?, q k + ? ln ? k ? ? ?, ln ? = ?, q k ? ? KL(?||? k ).</p><p>We have just shown that M-VI(1,? ) implicitly performs KL regularization between successive policies. This is a very insightful result as KL regularization is the core component of recent efficient RL agents such as TRPO <ref type="bibr" target="#b26">[27]</ref> or MPO <ref type="bibr" target="#b1">[2]</ref>. It is extensively discussed by Vieillard et al. <ref type="bibr" target="#b31">[32]</ref>. Interestingly, we can show that the sequence of policies produced by M-VI(?,? ) is the same as the one of their Mirror Descent VI (MD-VI), with KL scaled by ?? and entropy scaled by (1 ? ?)? . Thus, M-VI(?,? ) is equivalent to MD-VI(?? , (1 ? ?)? ), as formalized below (proof in Appx. A.2). Theorem 1. For any k ? 0, define q k = q k ? ?? ln ? k , we have</p><formula xml:id="formula_3">(3) ? ? k+1 = argmax ??? S A ?, q k ? ?? KL(?||? k ) + (1 ? ?)? H(?) q k+1 = r + ?P ( ? k+1 , q k ? ?? KL(? k+1 ||? k ) + (1 ? ?)? H(? k+1 )) + k+1 .</formula><p>Moreover, [32, Thm. 1] applies to M-VI(1,? ) and <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Thm. 2]</ref> applies to M-VI(? &lt; 1,? ).</p><p>In their work, Vieillard et al. <ref type="bibr" target="#b31">[32]</ref> show that using regularization can reduce the dependency to the horizon (1 ? ?) ?1 and that using a KL divergence allows for a compensation of the errors k over iterations, which is not true for classical ADP. We refer to them for a detailed discussion on this topic. However, we would like to highlight that they acknowledge that their theoretical analysis does not apply to the deep RL setting. The reason being that their analysis does not hold when the greedy step is approximated, and they deem as impossible to do the greedy step exactly when using neural network. Indeed, computing ? k+1 by maximizing eq. (4) yields an analytical solution proportional to ? k exp( q k ? ), and that thus depends on the previous policy ? k . Consequently, the solution to this equation cannot be computed exactly when using deep function approximation (unless one would be willing to remember every computed policy). On the contrary, their analysis applies in our deep RL setting. In M-VI, the KL regularization is implicit, so we do not introduce errors in the greedy step. To be precise, the greedy step of M-VI is only a softmax of the q-function, which can be computed exactly in a discrete actions setting, even when using deep networks. Their strong bounds for MD-VI therefore hold for M-VI, as formalized in Thm. 1, and in particular for M-DQN.</p><p>Indeed, let q? k be the k th update of the target network, write q k = q? k , ? k+1 = sm( q k ? ), and define k+1 = q k+1 ? (r + ? ln ? k+1 ? ?P ? k+1 , q k ? ? ln ? k+1 ), the difference between the actual update and the ideal one. As a direct corollary of Thm. 1 and [32, Thm. 1], we have that, for ? = 1,</p><formula xml:id="formula_4">q * ? q ? k ? ? 2 1 ? ? 1 k k j=1 j ? + 4 (1 ? ?) 2 r max + ? ln |A| k ,</formula><p>with r max the maximum reward (in absolute value), and with q ? k the true value function of the policy of M-DQN. This is a very strong bound. The error term is 1 k k j=1 j ? , to be compared to the one of AVI <ref type="bibr" target="#b25">[26]</ref>, (1??) k j=1 ? k?j j ? . Instead of having a discounted sum of the norms of the errors, we have the norm of the average of the errors. This is very interesting, as it allows for a compensation of errors between iterations instead of an accumulation (sum and norm do not commute). The error term is scaled by (1 ? ?) ?1 (the average horizon of the MDP), while the one of AVI would be scaled by (1 ? ?) ?2 . This is also quite interesting, a ? close to 1 impacts less negatively the bound. We refer to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Sec. 4.1]</ref> for further discussions about the advantage of this kind of bounds. Similarly, we could derive a bound for the case ? &lt; 1, and even more general and meaningful component-wise bounds. We defer the statement of these bounds and their proofs to Appx. A.3.</p><p>From Eq. (3), we can also relate the proposed approach to another part of the literature. Still from basic properties of the Legendre-Fenchel transform, we have that max ? q, ? + ? H(?) = ? k+1 , q k +? H(? k+1 ) = ln 1, exp q . In other words, if the maximizer is the softmax, the maximum is the log-sum-exp. Using this, Eq. (3) can be rewritten as (see Appx. A.4 for a detailed derivation)</p><formula xml:id="formula_5">q k+1 = r + ?P (? ln 1, exp q k ? ) + ?(q k ? ? ln 1, exp q k ? ) + k+1 .<label>(5)</label></formula><p>This is very close to Conservative Value Iteration 2 (CVI) <ref type="bibr" target="#b19">[20]</ref>, a purely theoretical algorithm, as far as we know. With ? = 0 (without Munchausen), we get Soft Q-learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. Notice that with this, CVI can be seen as soft Q-learning plus a scaled and smooth advantage (the term ?(q k ? ? ln 1, exp q k ? )). With ? = 1, we retrieve a variation of Dynamic Policy Programming (DPP) [3, Appx. A]. DPP has been extended to a deep learning setting <ref type="bibr" target="#b29">[30]</ref>, but it is less efficient than DQN 3 <ref type="bibr" target="#b31">[32]</ref>. Taking the limit ? ? 0, we retrieve Advantage Learning (AL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> (see Appx. A.4):</p><formula xml:id="formula_6">q k+1 = r + ?P ? k+1 , q k + ?(q k ? ? k+1 , q k ) + k+1 with ? k+1 ? G(q k ).<label>(6)</label></formula><p>AL aims at increasing the action-gap <ref type="bibr" target="#b12">[13]</ref> defined as the difference, for a given state, between the (optimal) value of the optimal action and that of the suboptimal ones. The intuitive reason to want a large action-gap is that it can mitigate the undesirable effects of approximation and estimation errors made on q on the induced greedy policies. Bellemare et al. <ref type="bibr" target="#b6">[7]</ref> have introduced a family of Bellman-like operators that are gap-increasing. Not only we show that M-VI is gap-increasing but we also quantify the increase. To do so, we introduce some last notations. As we explained before, with ? = 0, M-VI(0, ? ) reduces to AVI regularized by an entropy (that is, maximum entropy RL). Without error, it is known that the resulting regularized MDP has a unique optimal policy ? ? * and a unique optimal q-function 4 q ? * <ref type="bibr" target="#b14">[15]</ref>. This being defined, we can state our result (proven in Appx. A.5). <ref type="bibr" target="#b1">2</ref> In CVI, 1, exp q k ? is replaced by 1 |A| , exp q k ? . <ref type="bibr" target="#b2">3</ref> In fact, Tsurumine et al. <ref type="bibr" target="#b29">[30]</ref> show better performance for deep DPP than for DQN in their setting. Yet, their experiment involves a small number of interactions, while the function estimated by DPP is naturally diverging. See <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">Sec. 6]</ref> for further discussion about this. <ref type="bibr" target="#b3">4</ref> It can be related to the unregularized optimal q-function, q ? * ? q * ? ? ? ln |A| 1?? <ref type="bibr" target="#b14">[15]</ref>.</p><p>Theorem 2. For any state s ? S, define the action-gap of an MPD regularized by an entropy scaled by ? as ? ? * (s) = max a q ? * (s, a) ? q ? * (s, ?) ? R A + . Define also ? ?,? k (s) as the action-gap for the k th iteration of M-VI(?,? ), without error ( k = 0): ? ?,? k (s) = max a q k (s, a) ? q k (s, ?) ? R A + . Then, for any s ? S, for any 0 ? ? ? 1 and for any ? &gt; 0, we have</p><formula xml:id="formula_7">lim k?? ? ?,? k (s) = 1 + ? 1 ? ? ? (1??)? * (s),</formula><p>with the convention that ? ? 0 = 0 for ? = 1.</p><p>Thus, the original action-gap is multiplied by 1+? 1?? with M-VI. In the limit ? = 1, it is even infinite (and zero for the optimal actions). This suggests choosing a large value of ?, but not too close to 1 (for numerical stability: if having a large action-gap is desirable, having an infinite one is not).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Munchausen agents. We implement M-DQN and M-IQN as variations of respectively DQN and IQN from Dopamine <ref type="bibr" target="#b9">[10]</ref>. We use the same hyperparameters for IQN <ref type="bibr" target="#b4">5</ref> , and we only change the optimizer from RMSProp to Adam for DQN. This is actually not anodyne, and we study its impact in an ablation study. We also consider a Munchausen-specific modification, log-policy clipping. Indeed, the log-policy term is not bounded, and can cause numerical issues if the policy becomes too close to deterministic. Thus, with a hyperparameter l 0 &lt; 0, we replace ? ln ?(a|s) by [? ln ?(a|s)] 0 l0 , where [?] y x is the clipping function. For numerical stability, we use a specific log-sum-exp trick to compute the log-policy (see App. B.1). Hence, we add three parameters to the modified agent: ?, ? and l 0 . After some tuning on a few Atari games, we found a working zone for these parameters to be ? = 0.9, ? = 0.03 and l 0 = ?1, used for all experiments, in M-DQN and M-IQN. All details about the rest of the parameters can be found in Appx. B.1. DQN and IQN use ?-greedy policies to interact with the environment. Although M-DQN and M-IQN produce naturally stochastic policies, we use the same ?-greedy policies. We discuss this further in Appx. B.2, where we also compare to stochastic policies.</p><p>Baselines. First, we consider both DQN and IQN, as these are the algorithms we modify. Second, we compare to C51 because, as far as we know, it has never been outperformed by a non-distRL agent before. We also consider Rainbow, as it stands for being the state-of-the-art non-distributed agent on ALE. All our baselines are taken from Dopamine. For Rainbow, this version doesn't contain all the original improvements, but only the ones deemed as the more important and efficient by Hessel et al. <ref type="bibr" target="#b17">[18]</ref>: n-steps returns and Prioritized Experience Replay (PER) <ref type="bibr" target="#b24">[25]</ref>, on top of C51.</p><p>Task. We evaluate our methods and the baselines in the ALE environment, i.e. on the full set of 60 Atari games. Notice that it is not a "canonical" environment. For example, choosing to end an episode when an agent loses a life or after game-over can dramatically change the score an agent can reach (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">Fig. 4]</ref>). The same holds for using sticky actions, introducing stochasticity in the dynamics (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">Fig. 6]</ref>). Even the ROMs could be different, with unpredictable consequences (e.g. different video encoding). Here, we follow the methodological best practices proposed by Machado et al. <ref type="bibr" target="#b21">[22]</ref> and instantiated in Dopamine <ref type="bibr" target="#b9">[10]</ref>, that also makes the ALE more challenging. Notably, the results we present are hardly comparable to the ones presented in the original publications of DQN <ref type="bibr" target="#b22">[23]</ref>, C51 <ref type="bibr" target="#b7">[8]</ref>, Rainbow <ref type="bibr" target="#b17">[18]</ref> or IQN <ref type="bibr" target="#b10">[11]</ref>, that use a different, easier, setting. Yet, for completeness, we report results on one game (Asterix) using an ALE setting as close as possible to the original papers, in Appx. B.4: the baseline results match the previously published ones, and M-RL still raises improvement. We also highlight that we stick to a single-agent version of the environment: we do not claim that our method can be compared to highly distributed agents, such as R2D2 <ref type="bibr" target="#b18">[19]</ref> or Agent57 <ref type="bibr" target="#b3">[4]</ref>, that use several versions of the environment in parallel, and train on a much higher number of frames (around 10G frames vs 200M here). Yet, we are confident that our approach could easily apply to such agents.</p><p>Metrics. All algorithms are evaluated on the same training regime (details in Appx.B.1), during 200M frames, and results are averaged over 3 seeds. As a metric for any games, we compute the "baseline-normalized" score, for each iteration (here, 1M frames), normalized so that 0% corresponds to a random score, and 100% to the final performance of the baseline. At each iteration, the score is the undiscounted sum of rewards, averaged over the last 100 learning episodes. The normalized score is then a?r |b?r| , with a the score of the agent, b the score of the baseline, and r the score of a random policy. For a human baseline, the scores are those provided in <ref type="table">Table 3</ref> (Appx. B.6), for an agent baseline the score is the one after 200M frames. With this, we provide aggregated results, showing the mean and the median over games, as learning proceeds when the baseline is the human score (e.g., <ref type="figure" target="#fig_0">Fig. 1</ref>), or after 200M steps with human and Rainbow baselines in Tab. 3 (more results in Appx. B.6, as learning proceeds). We also compute a "baseline-improvement" score as a?b |b?r| , and use it to report a per-game improvement after 200M frames ( <ref type="figure" target="#fig_3">Fig. 4</ref>, M-Agent versus Agent, or Appx. B.6). Action-gap. We start by illustrating the action-gap phenomenon suggested by Thm. 2. To do so, let q ? be the qfunction of a given agent after training for 200M steps. At any time-step t, write? t ? argmax a?A q ? (s t , a) the current greedy action, we compute the empirical action-gap as the difference of estimated values between the best and second best actions, q ? (s t ,? t ) ? max a?A\{?t} q ? (s t , a). We do so for M-DQN, for AL (that was introduced specifically to increase the action-gap) and for DQN with Adam optimizer (Adam DQN), as both build on top of it (only changing the regression targets, see Appx. B.1 for details). We consider the game Asterix, for which the final average performance of the agents are (roughly) 15k for Adam DQN, 13k for AL and 20k for M-DQN. We report the results on <ref type="figure" target="#fig_1">Fig. 2</ref>: we run each agent for 10 trajectories, and average the resulting action-gaps (the length of the resulting trajectory is the one of the shorter trajectory, we also apply an exponential smoothing of 0.99). Both M-DQN and AL increase the action-gaps compared to Adam DQN. If AL increases it more, it seems also to be less stable, and less proportional to the original action-gap. Despite this increase, it performs worse than Adam DQN (13k vs 15k), while M-DQN increases it and performs better (20k vs 15k). An explanation to this phenomenon could the one of Van Seijen et al. <ref type="bibr" target="#b30">[31]</ref>, who suggest that what is important is not the value of the action gap itself, but its uniformity over the state-action space: here, M-DQN seems to benefit from a more stable action-gap than AL. This figure is for an illustrative purpose, one game is not enough to draw conclusions. Yet, the following ablation shows that globally M-DQN performs better than AL. Also, it benefits from more theoretical justifications (not only quantified action-gap increase, but also implicit KL-regularization and resulting performance bounds).</p><p>Ablation study. We've build M-DQN from DQN by adding the Adam optimizer (Adam DQN), extending it to maximum entropy RL (Soft-DQN, Eq. (1)), and then adding the Munchausen term (M-DQN, Eq. (2)). A natural ablation is to remove the Munchausen term, and use only maximum entropy RL, by considering M-DQN with ? = 0 (instead of 0.9 for M-DQN), and the same ? (here, 3e ? 2), which would give Soft-DQN(? ). However, Thm. 1 states that M-DQN performs entropy regularization with an implicit coefficient of (1 ? ?)? , so to compare M-DQN and Soft-DQN fairly, one should evaluate Soft-DQN with such a temperature, that is 3e ? 3 in this case. We denote this ablation as Soft-DQN((1 ? ?)? ). As sketched in Sec. 3, AL can also be seen as a limit case (on an abstract way, as ? ? 0, see also Appx. B.1 for details on the algorithm). We provide an ablation study of all these variations, all using Adam (except DQN), in <ref type="figure" target="#fig_2">Fig. 3</ref>. All methods perform better than DQN. Adam DQN performs very well and is even competitive with C51. This is an interesting insight, as changing the optimizer compared to the published parameters dramatically improves the performance, and Adam DQN could be considered as a better baseline <ref type="bibr" target="#b5">6</ref> . Surprisingly, if better than DQN, Soft-DQN does not perform better than Adam DQN. This suggests that maximum entropy RL alone might not be sufficient. We kept the temperature ? = 0.03, and one could argue that it was not tuned for Soft DQN, but it is on par with the temperature of similar algorithms <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>. We observe that AL performs better than Adam DQN. Again, we kept ? = 0.9, but this is consistent with the best performing parameter of Bellemare et al. <ref type="bibr">[7,</ref> e.g., <ref type="figure" target="#fig_9">Fig. 7</ref>]. The proposed M-DQN outperforms all other methods, both in mean and median, and especially Soft-DQN by a significant margin (the sole difference being the Munchausen term).</p><p>Comparison to the baselines. We report aggregated results as Human-normalized mean and median scores on <ref type="figure" target="#fig_0">Figure 1</ref>, that compares the Munchausen agents to the baselines. M-DQN is largely  over DQN, and outperforms C51 both in mean and median. It is remarkable that M-DQN, justified by theoretically sound RL principles and without using common deep RL tricks like n-steps returns, PER or distRL, is competitive with distRL methods. It is even close to IQN (in median), considered as the best distRL-based agent. We observe that M-IQN, that combines IQN with Munchausen principle, is better than all other baselines, by a significant margin in mean. We also report the final Human-normalized and Rainbow-normalized scores of all the algorithms in <ref type="table" target="#tab_0">Table 1</ref>. These results are on par with the Human-normalized scores of <ref type="figure" target="#fig_0">Fig. 1</ref> (see Appx. B.6 for results over frames). M-DQN is still close to IQN i median, is better than DQN, and C51, while M-IQN is the best agent w.r.t. all metrics.</p><p>Per-game improvements. In <ref type="figure" target="#fig_3">Figure 4</ref>, we report the improvement for each game of the Munchausen agents over the algorithms they modify. The "Munchausened" versions show significant improvements, on a large majority of Atari games (53/60 for M-DQN vs DQN, 40/60 for M-IQN vs IQN). This result also explains the sometime large difference between the mean and median metrics, as some games benefit from a particularly large improvement. All learning curves are in Appx B.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we presented a simple extension to RL algorithms: Munchausen RL.  action-gap, and we have quantified this increase, that can be infinite in the limit. In the end, this work highlights that a thoughtful revisiting of the core components of reinforcement learning can lead to new and efficient deep RL algorithms.</p><p>Content. These appendices provide the following additional material:</p><p>? Appx. A details the derivations made in the paper and proves the stated results.</p><p>? Appx. B provides additional experimental details, such as a detailed description of the Munchausen agents, and additional results and visualisations.</p><p>Code. All the code used for the experiments is available online at https://github.com/ google-research/google-research/tree/master/munchausen_rl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed derivation and proofs</head><p>This appendix provides additional details regarding the derivation sketched in the main paper as well as the proofs of the stated results:</p><p>? Appx. A.1 details the derivation of Soft-DQN.</p><p>? Appx. A.2 proves the result that relates Munchausen VI to Mirror Descent VI.</p><p>? Appx. A.3 provides and proves component-wise bounds for Munchausen VI, that also apply to Munchausen-DQN.</p><p>? Appx. A.4 details the derivation that allows linking the proposed Munchausen approach to the literature.</p><p>? Appx. A.5 proves the result quantifying the increase of the action-gap.</p><p>First, we recall the notations introduced in the main paper as well as some useful facts about (regularized) MDPs.</p><p>We write ? X the simplex over the finite set X and Y X the set of applications from X to the set Y . An MDP is a tuple {S, A, P, r, ?}, with S and A the state and action spaces (here assumed finite), P ? ? S?A S the Markovian transition kernel, r ? R S?A the reward function, uniformly bounded by r max , and ? ? (0, 1) the discount factor. A policy ? ? ? S A associates to each state a distribution over actions (a deterministic policy being a special case), and the quality of a policy is quantified by the expected discounted cumulative return, formalized as the state-action value function, q ? (s, a) = E ? [ ? t=0 ? t r(s t , a t )|s 0 = s, a 0 = a], the expectation being over trajectories induced by the policy ? and the dynamics.</p><p>For f, g ? R S?A , we define a component-wise dot product f, g = ( a f (s, a)g(s, a)) s ? R S . This will be used with q-functions and (log-) policies. For v ? R S , we have P v = (E s |s,a [v(s )]) s,a ? R S?A . We also defined a policy-induced transition kernel P ? as P ? q = P ?, q . With this, the Bellman evaluation operator is T ? q = r + ?P ? q and its unique fixed point is q ? .</p><p>An optimal policy satisfies ? * ? argmax ? q ? , component-wise, and the associated (unique) optimal value function q * = q ? * satisfies the Bellman equation q * (s, a) = r(s, a)+?E s |s,a [max a q * (s , a )]. We write the set of greedy policies as G(q) = argmax ??? S A ?, q . We'll also use softmax policies, ? = sm(q) ? ?(a|s) = exp q(s,a) a exp q(s,a ) . We'll also make use of the entropy of a policy, H(?) = ? ?, ln ? , and of the KL between two policies, KL(? 1 ||? 2 ) = ? 1 , ln ? 1 ? ln ? 2 . An MDP regularized by a scaled entropy ? H(?), also known as maximum entropy RL, optimizes for the reward r ? ? ln ?. It has a unique optimal qfunction q ? * and a unique optimal policy ? ? * , related by ? ? * = sm(q ? * ); it is related to the solution of the unregularized MDP by q ? * ? q * ? ? ? ln |A| 1?? <ref type="bibr" target="#b14">[15]</ref>. We also write q ? ? the value function of the policy ? in this regularized MDP.</p><p>Lastly, by classic properties of the Legendre-Fenchel transform <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>, we have ?q ? R S?A : max ??? S A q, ? + ? H(?) = ? ln 1, exp q ? = ? , q + ? H(? ) with ? = sm(q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Derivation of Soft-DQN</head><p>Soft-DQN can be derived from the maximum entropy RL framework. To do so, it is sufficient to follows the derivation that Haarnoja et al. <ref type="bibr" target="#b16">[17]</ref> made for SAC. In our case, the actions being discrete, no approximation is necessary for computing the policy (there is no actor), which gives Soft-DQN.</p><p>Alternatively, and equivalently, one can derive Soft-DQN as an approximate VI scheme for an MDP regularized by a scaled entropy. The regularized VI scheme is <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>:</p><formula xml:id="formula_8">? k+1 = argmax ? ?, q k + ? H(?) q k+1 = r + ?P ( ? k+1 , q k + ? H(? k+1 )) + k+1 .</formula><p>From Legendre-Fenchel, ? k+1 = sm(q k ). Using basic calculus, we have ? k+1 , q k + ? H(? k+1 ) = ? k+1 , q k ? ? ? k+1 , ln ? k+1 = ? k+1 , q k ? ? ln ? k+1 .</p><p>Thus, we can write equivalently the regularized VI scheme as q k+1 = r + ?P ? k+1 , q k ? ? ln ? k+1 + k+1 , with ? k+1 = sm(q k ), which is basically the Soft-DQN target depicted in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Thm. 1</head><p>The proof is similar to the one done in the main paper for the case ? = 1. Recall Eq. <ref type="formula" target="#formula_0">(3)</ref>, that gives an iteration of M-VI(?,? ):</p><formula xml:id="formula_9">? k+1 = argmax ??? S A ?, q k + ? H(?) q k+1 = r + ?? ln ? k+1 + ?P ? k+1 , q k ? ? ln ? k+1 + k+1 .</formula><p>Define for any k ? 0 the term q k as q k = q k ? ?? ln ? k .</p><p>By basic calculus, we can rewrite the evaluation step as follows: q k+1 = r + ?? ln ? k+1 + ?P ? k+1 , q k ? ? ln ? k+1 + k+1 = r + ?? ln ? k+1 + ?P ? k+1 , q k ? ?? ln ? k + ?? ln ? k ? ? ln ? k+1 + k+1</p><formula xml:id="formula_10">? q k+1 = r + ?P ? k+1 , q k ? ?? ln ? k+1 ? k ? (1 ? ?)? ln ? k+1 + k+1 = r + ?P ( ? k+1 , q k ? ?? KL(? k+1 ||? k ) + (1 ? ?)? H(? k+1 )) + k+1 .</formula><p>For the greedy step, we have: ?, q k + ? H(?) = ?, q k ? ? ln ? = ?, q k + ?? ln ? k ? ? ln ? = ?, q k ? ?? ln</p><formula xml:id="formula_11">? ? k ? (1 ? ?)? ln ? = ?, q k ? ?? KL(?||? k ) + (1 ? ?)? H(?).</formula><p>Therefore, we have shown that</p><formula xml:id="formula_12">? k+1 = argmax ??? S A ?, q k + ? H(?) q k+1 = r + ?? ln ? k+1 + ?P ? k+1 , q k ? ? ln ? k+1 + k+1 ? k+1 = argmax ??? S A ?, q k ? ?? KL(?||? k ) + (1 ? ?)? H(?) q k+1 = r + ?P ( ? k+1 , q k ? ?? KL(? k+1 ||? k ) ? (1 ? ?)? H(? k+1 )) + k+1 .</formula><p>This is exactly the update rule of MD-VI(?? , (1 ? ?)? ) by Vieillard et al. <ref type="bibr" target="#b31">[32]</ref>. Initialized with the same policy ? 0 and such that q 0 = q 0 ? ? ln ? 0 , both algorithms will produce the same sequence of policies (for the same sequence of errors). This is enough for [32, Thm. 1] to apply to M-VI(1,? ), producing the same sequence of policies that MD-VI(? ,0), the result bounding component-wise q * ? q ? k (it only involves the computed policy). This is also enough for [32, Thm. 2] to apply to M-VI(?,? ), producing the same sequence of policies that MD-VI(?? , (1 ? ?)? ), the result bounding component-wise q</p><formula xml:id="formula_13">(1??)? * ? q ? k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Component-wise bounds for Munchausen VI</head><p>We state the component-wise bounds for M-VI, announced in Sec. 3. We recall that they apply to M-DQN, as explained in Sec. 3 (by defining to what corresponds q k and k for M-DQN). First, we provide a bound for the case ? = 1. Corollary 1. Let (q k , ? k ) k?0 be the sequence of q-functions and policies produced by M-VI(1,? ), with ? 0 the uniform policy and q 0 such that q 0 ? ? ln ? 0 ? ? rmax 1?? . Define</p><formula xml:id="formula_14">E k = ? k j=1 j , and A 1 k = (I ? ?P ? * ) ?1 ? (I ? ?P ? k ) ?1 . Assume that q k ? ? ln ? k ? ? rmax 1?? .</formula><p>We have that:</p><formula xml:id="formula_15">0 ? q * ? q ? k ? A 1 k E k k + 4 (1 ? ?) 2 r max + ? ln |A| k 1,</formula><p>with 1 ? R S?A the vector whose all components are equal to 1.</p><p>Proof. Thanks to Thm. 1, M-VI(1,? ) produces the same sequence of policies that MD-VI(? ,? ) with ? = ? and ? = 0, and a sequence of q-functions related by q k = q k ? ? ln ? k (q k being the q-functions computed by MD-VI(? ,? )). Thm. 1 of Vieillard et al. <ref type="bibr" target="#b31">[32]</ref> thus readily applies, the assumption q</p><formula xml:id="formula_16">k ? ? rmax 1?? translating into q k ? ? ln ? k ? ? rmax 1?? .</formula><p>Notice that that the assumption that q k ? ? ln ? k ? ? rmax 1?? is not strong, it can be ensured by clipping the q k -values (see also <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Rk. 1]</ref>). Without this, a similar bound would still hold, but with a quadratic dependency of the error term to the horizon, instead of a linear one. Notice that the bound in supremum norm provided in Sec. 3 is a direct corollary of Cor. 1.</p><p>Next, we provide a bound for the case ? &lt; 1. Corollary 2. Let (q k , ? k ) k?0 be the sequence of q-functions and policies produced by M-VI(?,? ), with ? 0 the uniform policy, and with 0 ? ? &lt; 1. For the sequence of policies ? 0 , . . . , ? k , we define P k:j = P ? k P ? k?1 . . . P ?j if j ? k, I else, with I ? R (S?A)?(S?A) the identity matrix. We also define</p><formula xml:id="formula_17">A 2 k:j = P k?j ? (1??)? * + (I ? ?P ? k+1 ) ?1 P k:j+1 (I ? ?P ?j ), and E ? k = (1 ? ?) k j=1 ? k?j j .</formula><p>With these notations, we have</p><formula xml:id="formula_18">0 ? q (1??)? * ?q (1??)? ? k+1 ? k j=1 ? k?j A 2 k:j E ? j +? k (1+ 1 ? ? 1 ? ? ) k j=0 ? ? j r max + (1 ? ?)? ln |A| 1 ? ? 1.</formula><p>Proof. Thanks to Thm. 1, M-VI(?,? ) produces the same sequence of policies that MD-VI(? ,? ) with ? = ?? and ? = (1 ? ?)? , and a sequence of q-functions related by q k = q k ? ?? ln ? k (q k being the q-functions computed by MD-VI(? ,? )). Thm. 2 of Vieillard et al. <ref type="bibr" target="#b31">[32]</ref> thus readily applies, with</p><formula xml:id="formula_19">? = ? ? + ? = ?? ?? + (1 ? ?)? = ?,</formula><p>which gives the stated result.</p><p>We refer to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Sec. 4.2]</ref> for an extensive discussion of this bound, but we highlight the fact that it still shows a compensation of errors (through a moving average instead of the average of Cor. 1), something that is desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Details on related works</head><p>First, we relate M-VI to CVI. Recall Eq. <ref type="formula" target="#formula_0">(3)</ref>:</p><p>? k+1 = argmax ??? S A ?, q k + ? H(?) q k+1 = r + ?? ln ? k+1 + ?P ? k+1 , q k ? ? ln ? k+1 + k+1 .</p><p>From the Legendre-Fenchel transform, we have that</p><formula xml:id="formula_20">? k+1 = sm( q k ? ) = exp q k ? 1, exp q k ? ? ? ln ? k+1 = q k ? ? ln 1, exp q k ? .</formula><p>Injecting this into the evaluation step, we obtain</p><formula xml:id="formula_21">q k+1 = r + ?? ln ? k+1 + ?P ? k+1 , q k ? ? ln ? k+1 + k+1 = r + ?(q k ? ? ln 1, exp q k ? ) + ?P ? k+1 , q k ? (q k ? ? ln 1, exp q k ? ) + k+1 = r + ?P (? ln 1, exp q k ? ) + ?(q k ? ? ln 1, exp q k ? ) + k+1 ,</formula><p>which is exactly Eq. <ref type="formula" target="#formula_5">(5)</ref>, that is a CVI-like update.</p><p>It is a classic result that the sum-log-exp tends towards the hard maximum as the temperature goes to zero (this can be also derived from properties of the Legendre-Fenchel transform):</p><formula xml:id="formula_22">lim ? ?0 ? ln a exp q k (s, a) ? = max a q k (s, a).</formula><p>Using this, the limit of the previous CVI-like update is</p><formula xml:id="formula_23">q k+1 = r + ?P ? k+1 , q k + ?(q k ? ? k+1 , q k + k+1 ) with ? k+1 ? G(q k ),</formula><p>where we have used that max a q k (?, a) = ? k+1 , q k with ? k+1 ? G(q k ). This is exactly Eq. (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Thm. 2</head><p>This is indeed a corollary of Thm. 1. First, we handle the case ? &lt; 1. From Thm. 1, we know that M-VI(?,? ) produces the same sequence of policies that MD-VI(?? ,(1 ? ?)? ). From <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Thm. 2]</ref>, we now that without error q k = q k ? ?? ln ? k (recall that q k is the sequence of q-functions computed by MD-VI) converges to q (1??)? * and that ? k converges to ? (1??)? * (recall that both algorithms produce the same sequence of policies). From this, we can deduce the limit of q k , the sequence of q-function produced by Munchausen VI:</p><formula xml:id="formula_24">lim k?? q k = q (1??)? * + ?? ln ? (1??)? * .</formula><p>From basic properties of regularized MDPs <ref type="bibr" target="#b14">[15]</ref>, we know that</p><formula xml:id="formula_25">? (1??)? * = sm( q (1??)? * (1 ? ?)? ) ? ln ? (1??)? * = q (1??)? * (1 ? ?)? ? ln 1, exp q (1??)? * (1 ? ?)? .</formula><p>Therefore, we have that</p><formula xml:id="formula_26">lim k?? q k = q (1??)? * + ?? ln ? (1??)? * = q (1??)? * + ?? q (1??)? * (1 ? ?)? ? ln 1, exp q (1??)? * (1 ? ?)? = 1 + ? 1 ? ? q (1??)? * ? ?? 1 ? ? ln 1, exp q (1??)? * (1 ? ?)? .</formula><p>Noticing that the log-sum-exp does not depend on the actions, we obtain the stated result.</p><p>Next, we handle the case ? = 1. From Thm. 1, we know that M-VI(1,? ) produces the same sequence of policies that MD-VI(? ,0). From [32, Thm. 1], we now that without error q k = q k ? ?? ln ? k converges to q * and that ? k converges to ? * , the solutions of the unregularized MDP. To simplify and without much loss of generality, assume that this MDP admits a unique optimal policy. As q k = q k + ? ln ? k , taking the limit we get for any s ? S lim k?? q k (s, a) = q * (s, a) if ? * (a|s) = 1 ?? else .</p><p>With the adopted convention, this proves the result for the case ? = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional experimental details and results</head><p>This appendix provides a complete description of the Munchausen agents, it gives additional experimental details, and it proposes additional results and visualisations: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Detailed description of the Munchausen agents</head><p>All the agents follow a similar learning procedure, described as a pseudo-code in Alg. 1 for M-DQN. What changes is the loss that is optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-DQN.</head><p>Here, we recall the basic workings of M-DQN. It estimates a q-value through an online q-network q ? of weights ?. Every C steps, the weights are copied to a target network q? of weights?. Transitions (s t , a t , r t , s t+1 ) are stored in fixed-sized FIFO replay buffer. To collect them, M-DQN interacts with the environment using the policy G ? (?), the policy that is ?-greedy with respect to q ? . M-DQN uses (as DQN) a decay on ? to favour exploration in the beginning of the learning. Each  M-IQN. IQN is a distributional method. It does not estimate directly a q-function, but the distribution of the discounted cumulative rewards, a so-called z-function. Precisely, the z-function z ? ? R S?A of a policy ? is a random quantity defined, for each s, a ? S ? A as:</p><formula xml:id="formula_27">z ? (s, a) = ? t=0</formula><p>? t r(s t , a t ), with a t ? ?(?|s t ) and s t+1 ? P (?|s t , a t ) for s 0 = s and a 0 = a. Algorithm 1 Munchausen DQN Require: T ? N * the number of environment steps, C ? N * the update period, F ? N * the interaction period. Initialize ? at random</p><formula xml:id="formula_28">B = {} ? = ? for t = 1 to T do Collect a transition b = (s t , a t , r t , s t+1 ) from G e (?) B ? B ? {b} if t mod F == 0 then On a random batch of transitions B t ? B, update ? with one step of SGD on L m-dqn , see (7) end if if k mod C == 0 then ? ? ? end if end for return G 0 (?)</formula><p>The q-function can be directly related to it with</p><formula xml:id="formula_29">q ? (s, a) = E [z ? (s, a)] .</formula><p>A remarkable result is that z ? satisfies a Bellman equation, similarly to q ? , and thus can be estimated with TD. Here, we give a quick overview of IQN, and explain how we modified it. We refer to Dabney et al. <ref type="bibr" target="#b10">[11]</ref> for an exact derivation and more details of the original algorithm. IQN estimates the quantile function of z at ? ? [0, 1], denoted z ? . The estimated q-value is thenq(s, a) = E ??U [0,1] [z ? (s, a)], this expectation being practically approximated by Monte Carlo. The TD error of IQN at step t, defined with ?, ? ? U [0,1] , is: TD IQN = r t + ?z ? (s t+1 , ?(s t+1 )) ? z ? (s t , a t ), with ?(s) = argmax a?Aq (s, a).</p><p>In practice, z ? is given by a target network, and z ? by an online network, to be optimized. The loss is then estimated as the empirical mean of the TD errors, by sampling ? and ? uniformly in [0, 1]. In M-IQN, we use an additional Munchausen term in TD error, TD M-IQN = r t + ? [? ln ?(a t |s t )] 0 l0 + ? a?A ?(a|s t+1 )(z ? (s t+1 , a) ? ? ln ?(a|s t+1 )) ? z ? (s t , a t ) with ?(?|s) = sm(q (s,?) ? ) (that is, the policy is softmax withq, the quantity with respect to which the original policy of IQN is greedy). We use the same parametrization for z as Dabney et al. <ref type="bibr" target="#b10">[11]</ref>, and all their provided hyperparameters, as implemented in Dopamine. We used the "Munchausen-RL parameters" from <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Custom log-sum-exp trick. Eq. 7 relies on computing a log-policy, so in our case the log-softmax of a q-values. Such computations are usually done using the "log-sum-exp trick", that allows for numerically stable operations by factorizing a maximum. This trick is widely used in software libraries, for example in TensorFlow <ref type="bibr" target="#b0">[1]</ref>, used to implement the experiments of this work. With this approach, we use the fact that ? ln ? k+1 = q k ? ? ln 1, exp q k ? , that can be unstable if ? is small. Thus, we compute the log-policy terms using a log-sum-exp-trick as</p><formula xml:id="formula_30">? ln ? k+1 = q k ? v k ? ? ln 1, exp q k ? v k ? ,</formula><p>where we defined v k ? R S as v k (s) = max a q k (s, a). This is more stable than the one implemented by default, because it takes into account the temperature coefficient.</p><p>Parameters. We provide the hyperparameters used in our algorithms in <ref type="table" target="#tab_2">Table 2</ref>. We denote neural networks structures as follow: Conv d a,b c is a 2D convolutional layer with c filters of size a ? b and of stride d, and FC n is a fully convolutional layer with n neurons. The parameters of the baseline agents are those reported in Dopamine (with the slight modification of considering 1-step returns instead of n-step returns for IQN, to match the original paper and the algorithm we modify). Munchausen-RL specific parameters ? (entropy temperature) 0.03 ? (Munchausen scaling term) 0.9 l 0 (clipping value) -1 AL specific parameters ? (advantage scaling term) 0.9</p><p>Environment details. We follow the procedures of Machado et al. <ref type="bibr" target="#b21">[22]</ref> to train on the ALE. Notably, we perform one training step (a gradient descent step) every 4 frames encountered in the environment. The state of an agent is the concatenation of the last 4 frames, sub-sampled to a shape of (84, 84), in gray levels. We refer to Machado et al. <ref type="bibr" target="#b21">[22]</ref> for details on the preprocessing.</p><p>Metrics. Here, we recall the definitions of the metrics used to compare algorithms. As an aggregating metric, we use the baseline-normalized score. Every 1M frames, we compute the undiscounted return averaged over the last 100 episodes a k , then we normalized it by a random score r and a baseline score b (score after training for 200M steps). The normalized score is then a k ?r |b?r| . We also use human-normalized scores, when we replace the baseline score by the score of a human. We used human scores reported by <ref type="bibr" target="#b22">[23]</ref>. For AirRaid, Carnival, ElevatorAction, JourneyEscape, and Pooyan, not considered in Mnih et al. <ref type="bibr" target="#b22">[23]</ref>, we averaged scores from game-play posted online by players. For a game-per-game metric, we compute the normalized improvement according to a basline. The "final score" of an agent is defined as the score averaged over the last 5M frames. The normalized improvement of a final score a w.r.t. the final score of a baseline b is a?b |b?r| . The maximum scores reported in <ref type="table">Table 3</ref> are the maximum scores over training, averaged over 100 episodes, averaged over 3 random seeds, obtained during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison of greedy and stochastic policies</head><p>Although M-DQN naturally produces stochastic policies, we used the ?-greedy one (with respect to q ? ), as explained in Sec. 4. This is motivated by the behaviour of some games. In some games, a random policy fails to gather rewards (as for example Venture or Enduro). The Q-network is initialized with small Q-values, close to zero. Even with the small temperature ? = 0 we consider, the resulting softmax policy is very close to uniform, and the M-DQN fails to collect rewards, and thus receives no signal to learn. On the converse, an ?-greedy exploration will have a more (randomly) structured exploration, as the scale of Q-values does not matter in this case. It then succeed to gather rewards, and to learn something. This is exemplified in <ref type="figure" target="#fig_7">Fig. 5</ref>, left, for the game Enduro.  On the converse, if the agent manage to get rewards, the M-DQN agent with a stochastic policy will perform more exploration, and a directed one, as it will chose more often actions with high Q-values, thanks to the softmax policy. Consequently, thanks to this less random exploration, it could perform better. We hypothesize that it is what happens for the game Seaquest, shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, right.</p><p>In <ref type="figure" target="#fig_8">Fig. 6</ref>, we provide the Human-normalized scores of both options, playing with an ?-greedy policy or with the more natural stochastic one. We observe that the stochastic policy is slightly better in median. Yet, it improves less games too, and we kept the ?-greedy policy for the core results. Improving the stochastic policy, maybe with an adaptive temperature or an adaptive ? parameter, is an interesting future direction of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Comparison of 1-step and 3-steps learning in M-IQN</head><p>The results in the papers are computed with a version of M-IQN that uses 3-steps learning, and compared to version of IQN that also uses 3-steps learning (as it is by default in the Dopamine library). For completeness, we evaluate M-IQN with 1-steps returns, and compre it to IQN with 1-step returns. The human-normalized scores for these algorithms are reported in <ref type="figure" target="#fig_9">Fig.7</ref>. Theses results show that (1) n-step learning and M-RL combine efficiently, as M-IQN 3-steps clearly outperforms M-IQN 1-step and (2) that M-IQN alone (with only 1-step returns) yields already high performances, and it particular outperforms -although by a tight margin -the Rainbow baseline, that uses 3-steps returns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Element of comparison with the original ALE setting</head><p>We explained in Sec. 4 the difference between the ALE setting we consider, more modern and more difficult, compared to the ALE setting often considered, for example for the seminal DQN <ref type="bibr" target="#b22">[23]</ref> or for Rainbow <ref type="bibr" target="#b17">[18]</ref>. The Rainbow baseline we consider <ref type="bibr" target="#b9">[10]</ref> is also not exactly the published one: even if the most important features are included, as deemed by Hessel et al. <ref type="bibr" target="#b17">[18]</ref>, it does not include all features (such as double Q-learning or dueling architecture). As a (partial) check, we also evaluated our Munchausen agents, M-DQN and M-IQN, as well as the baselines DQN, IQN and Rainbow, in a setting as close as possible to the one used for the baselines' publications. Notably, here we did not used sticky actions, making the environment deterministic, and we end an episode whenever the agent loses a life, instead of when it encounters a game-over. We also use hyperparameters provided in the original publications, the only difference being that we used a target update period of 10000 steps instead of 8000. We did so on the Asterix game, the results being depicted in <ref type="figure">Fig. 8</ref>.</p><p>On <ref type="figure">Fig. 8</ref>, left, we can observe DQN and M-DQN. The result for DQN is normal, despite the apparent "crash", see for example the training curves in <ref type="bibr" target="#b17">[18]</ref> (notice also that it is often the best scores over training which is reported, instead of the final one, as in our Tab. 3 or in the seminal DQN publication <ref type="bibr" target="#b22">[23]</ref>). We can observe that M-DQN performs much better than DQN, without falling, and that the score is close to the one of M-DQN in the more difficult setting (15k vs 19k in the more difficult setting).</p><p>On <ref type="figure">Fig. 8</ref>, right, we can observe Rainbow, IQN and M-IQN. All algorithms perform pretty well. For example, Rainbows reaches roughly 350k, comparable to the original publication <ref type="bibr" target="#b6">7</ref> . This is much more than in our setting, where Rainbow reaches only 18k, suggesting that the original setting is easier. We can also see that IQN works well (and somehow surprisingly better than in the original publication, compared to Rainbow), and that M-IQN works better than both IQN and Rainbow.</p><p>An interesting thing is to see how the methods degrades (roughly) when going from the agent is trained in the considered setting, compared to the original one. Rainbow goes from 350k to 18k (5% of the original scores), IQN goes from 350k to 33k (10%), while M-DQN goes from 15k to 17k (113%) and M-IQN goes from 350k to 50k (17%). This suggests that M-RL might be more stable over environments.</p><p>For sure, this discussion only holds for one game, and no general conclusion can be drawn. Yet, it suggests a few things, the ALE setting we consider is more difficult, among other advantages <ref type="bibr" target="#b21">[22]</ref>, the Rainbow baseline we consider is correct, and M-RL seems to be more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Additional results on the ablation study</head><p>We provide complementary results regarding the ablation study:</p><p>? <ref type="figure">Fig. 9</ref> p. 22 reports the Rainbow-normalized scores of the ablation (instead of the Humannormalized ones in the main paper, <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>? <ref type="figure" target="#fig_0">Fig. 11 p. 24</ref> shows the normalized improvements of all ablations with respect to DQN.</p><p>? <ref type="figure" target="#fig_0">Fig. 12</ref> p. 25 reports all learning curves an the 60 Atari games for the ablation. The Rainbow-normalized scores ( <ref type="figure">Fig. 9</ref>) confirms the Human-normalized ones <ref type="figure" target="#fig_2">(Fig. 3</ref>). The scores themselves are different (due to a different normalization), but the order of the different variations and their gaps is comparable. <ref type="figure" target="#fig_0">Fig. 11</ref> provides a summary of the per-game improvement, while <ref type="figure" target="#fig_0">Fig. 12</ref> provides all related learning curves ( <ref type="figure" target="#fig_0">Fig. 11</ref> summarizing what the results are after 200M frame). We can observe that M-DQN is not always the best performing agent. Yet, it is very often competitive with the best performing ablation (when M-DQN does not perform the best), and the ablation that surpasses M-DQN is highly game-dependent. Overall, M-DQN is consistently the best performing agent over the whole suite of games, as confirmed by <ref type="figure" target="#fig_2">Fig. 3</ref> or <ref type="figure">Fig. 9</ref> both in mean and median Rainbow and Human-normalized scores.</p><p>AL performs pretty well (even if less well than M-DQN). Yet, Munchausen-RL is more general, as it consists only in adding a scaled log-policy term to the reward. We've shown in the main paper how it can be readily applied to agents that does not even consider stochastic policies. On the converse, ALE relies heavily on being able to compute the maximum Q-value, something which could not be easily extended to continuous actions, contrary to the Munchausen principle. We let this as an interesting direction for future work. In both average and mean ( <ref type="figure" target="#fig_2">Fig. 3 and 9</ref>), Soft-DQN is the worst ablation, despite being much better in a few games (for example, Amidar or Jamesbond). Again, the temperature was not specifically tuned for Soft-DQN, but it is on par with the close literature (see discussion in Sec. 4). This suggests that the maximum entropy RL principle alone might not be sufficient, especially when one observes the significant improvement that the Munchausen term brings to it (or, implicitly, adding KL regularization to the entropy term). We also notice again that Adam DQN works surprisingly well, compared to the original DQN. This is a very interesting finding, and it suggests that Adam DQN should be considered as a better baseline than the seminal DQN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Additional comparison results</head><p>For completeness, we provides additional comparison results:</p><p>? In addition to the Human-normalized results of <ref type="figure" target="#fig_0">Fig. 1</ref>, we provide a Rainbow-normalized comparison of the Munchausen agents with respect to DQN, C51, IQN and Rainbow in <ref type="figure" target="#fig_0">Fig. 1</ref>. ? In addition to the per-game normalized improvement of a Munchausen agent with respect to its natural baseline ( <ref type="figure" target="#fig_3">Fig. 4)</ref>, we provide the per-game improvement for M-DQN over DQN, C51, IQN and Rainbow in <ref type="figure" target="#fig_0">Fig. 13</ref>, as well as the per-game improvement of M-IQN over the same baselines in <ref type="figure" target="#fig_0">Fig. 14</ref>  <ref type="table">Table 3</ref> p. 28. M-IQN obtains the most highest-ranking scores among all the considered baselines (including the human one). ? For completeness, we report all learning curves of the Munchausen agents and the considered baselines, for the full set of Atari games, in <ref type="figure" target="#fig_0">Fig. 15</ref>.</p><p>These additional results confirm the observations made in the main paper.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: Human-normalized mean scores. Right: Human-normalized median scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Action-gaps (Asterix).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Soft-DQN((1 ? ?)?) Soft-DQN(?) DQN Ablation study of M-DQN: Human-normalized mean (left) and median (right) scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Per-game improvement of M-DQN vs DQN (top) and of M-IQN vs IQN (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>Appx. B.1 provides a complete description of the Munchausen agents, as well as some additional details for the considered metrics (such as human scores for games not reported in the literature) and for the learning setting. ? Appx. B.2 discusses the difference between playing ?-greedy and stochastic policies for Munchausen DQN. ? Appx. B.3 discusses the diffrence between using 1-step or 3-steps returns in M-IQN. ? Appx. B.4 provides elements of comparison with the original ALE setting. ? Appx. B.5 provides complementary results for the ablation study. ? Appx. B.6 provides complementary comparison results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>F steps, M-DQN samples a random batch B of transitions from B and minimizes the following loss, based on the regression target of Eq. (2): L m-dqn (?) = (7)E B h r t + ? [? ln ??(a t |s t )] 0 l0 + ? a?A ??(a|s t+1 ) (q?(s t+1 , a) ? ? ln ??(a|s t+1 )) ? q ? (s t , a t ) ,with ?? = sm( q? ? ) and h the Huber loss function, with a paremeter x h , h(x) = x 2 if x &lt; x h else |x|. A pseudo-code detailing the learning procedure is given in Alg. 1.AL.We have shown in Sec. 3 that AL can be seen as a limiting case of M-DQN, in the limit ? ? 0. Yet, it cannot be obtained simply by setting ? = 0 in Alg. 1. Instead, we rewrite the minimized loss, according to Sec. 3. Each F steps, AL samples a random batch B of transitions from B and minimizes the loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>L</head><label></label><figDesc>al (?) =? B h r t + ? q?(s t , a t ) ? max a?A q?(s t , a) + max a?A q?(s t+1 , a) ? q ? (s t , a t ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Comparsion of M-DQN with a greedy (blue) or stochastic (orange) interaction policy. Left: Enduro. Right: Seaquest. On Enduro, the stochastic policy is not able to see any reward signal in the beginning, and learns nothing. On Seaquest, we see that it improves over the greedy policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Human-normalized scores of M-DQN greedy and stochastic, mean (left) and median right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Human normalized scores of M-IQN, IQN, and Rainbow with different n-steps returns, mean (left) and median right). M-IQN, IQN, and Rainbow use 3-steps, while the other two use 1-step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Scores of different agent on the game Asetrix, using the original ALE. left: M-DQN and DQN. right: Rainbow, IQN and M-IQN. Rainbow-normalized ablation study results. Left: mean. Right: median.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Rainbow-normalized scores. Left: mean. Right: median.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Per games N.I./DQN of the ablation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :Figure 15 :</head><label>1215</label><figDesc>All averaged training scores of the ablation. M-DQN in blue, AL in orange, Soft-DQN in green, DQN Adam in red, and DQN in dashed purple. All averaged training scores. M-DQN in blue, M-IQN in orange, IQN in dashed green, Rainbow in dashed red, DQN in dashed purple, and C51 in dashed brown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean/median Human/Rainbow-normalized scores at 200M frames, on the 60 games, averaged over 3 random seeds. In bold are the best of each column, and in blue over Rainbow. We also provide the number of improved games (compared to Human and Rainbow).</figDesc><table><row><cell></cell><cell cols="2">Human-normalized</cell><cell></cell><cell cols="2">Rainbow-normalized</cell><cell></cell></row><row><cell></cell><cell cols="6">Mean Median #Improved Mean Median #Improved</cell></row><row><cell>M-DQN</cell><cell>340%</cell><cell>124%</cell><cell>37</cell><cell>89%</cell><cell>92%</cell><cell>21</cell></row><row><cell>M-IQN</cell><cell>563%</cell><cell>165%</cell><cell cols="2">43 130%</cell><cell>109%</cell><cell>38</cell></row><row><cell cols="2">RAINBOW 414%</cell><cell>150%</cell><cell cols="2">43 100%</cell><cell>100%</cell><cell>-</cell></row><row><cell>IQN</cell><cell>441%</cell><cell>139%</cell><cell cols="2">41 105%</cell><cell>99%</cell><cell>27</cell></row><row><cell>C51</cell><cell>339%</cell><cell>111%</cell><cell>33</cell><cell>84%</cell><cell>70%</cell><cell>11</cell></row><row><cell>DQN</cell><cell>228%</cell><cell>71%</cell><cell>23</cell><cell>51%</cell><cell>51%</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This method augments the immediate rewards by the scaled logarithm of the policy computed by an RL agent. We applied this method to a simple variation of DQN, Soft-DQN, resulting in the M-DQN algorithm. M-DQN shows large performance improvements: it outperforms DQN on 53 of the 60 Atari games, while simply using a modification of the DQN loss. In addition, it outperforms the seminal distributional RL algorithm C51. We also extended the Munchausen idea to distributional RL, showing that it could DQN vs DQN:average improvement: 724.7%, median improvement: 45.0%, improved games: 53 / 60 IQN vs IQN:average improvement: 31.1%, median improvement: 8.7%, improved games: 40 / 60</figDesc><table><row><cell>N.I./DQN N.I./IQN</cell><cell>0 10 0 10 1 10 2 0 10 0</cell><cell>ElevatorAction M-ChopperCommand MontezumaRevenge Venture PrivateEye Hero Bowling Phoenix AirRaid MontezumaRevenge PrivateEye Bowling Jamesbond Tennis Hero Tutankham M-Carnival Venture</cell><cell>Robotank Amidar</cell><cell>Amidar StarGunner</cell><cell>Atlantis Alien</cell><cell>Pong Gravitar</cell><cell>Tennis Qbert</cell><cell>BeamRider KungFuMaster</cell><cell>JourneyEscape Kangaroo</cell><cell>Kangaroo Pitfall</cell><cell>KungFuMaster Carnival</cell><cell>StarGunner Atlantis</cell><cell>YarsRevenge Pong</cell><cell>Boxing Freeway</cell><cell>Freeway Boxing</cell><cell>Berzerk DoubleDunk</cell><cell>Jamesbond RoadRunner</cell><cell>CrazyClimber JourneyEscape</cell><cell>MsPacman Enduro</cell><cell>RoadRunner Phoenix</cell><cell>Pitfall Riverraid</cell><cell>Assault AirRaid</cell><cell>DoubleDunk FishingDerby</cell><cell>FishingDerby Asteroids</cell><cell>Alien VideoPinball</cell><cell>Krull Robotank</cell><cell>Qbert CrazyClimber</cell><cell>Riverraid Skiing</cell><cell>ChopperCommand MsPacman</cell><cell>DemonAttack Berzerk</cell><cell>NameThisGame Frostbite</cell><cell>Pooyan BankHeist</cell><cell>Seaquest Krull</cell><cell>Centipede BattleZone</cell><cell>Tutankham ElevatorAction</cell><cell>BankHeist YarsRevenge</cell><cell>BattleZone Centipede</cell><cell>IceHockey Assault</cell><cell>Enduro Seaquest</cell><cell>Gopher TimePilot</cell><cell>Gravitar Zaxxon</cell><cell>VideoPinball BeamRider</cell><cell>Zaxxon Pooyan</cell><cell>Breakout Gopher</cell><cell>SpaceInvaders WizardOfWor</cell><cell>UpNDown NameThisGame</cell><cell>Skiing SpaceInvaders</cell><cell>Asterix Solaris</cell><cell>WizardOfWor UpNDown</cell><cell>Solaris Breakout</cell><cell>Frostbite IceHockey</cell><cell>TimePilot Asterix</cell><cell>Asteroids DemonAttack</cell></row></table><note>be successfully combined with IQN to outperform the Rainbow baseline. Munchausen-DQN relies on theoretical foundations. To show that, we have studied an abstract Munchausen Value Iteration scheme and shown that it implicitly performs KL regularization. Notably, the strong theoretical results of [32] apply to M-DQN. By rewriting it in an equivalent ADP form, we have related our approach to the literature, notably to CVI, DPP and AL . We have shown that M-VI increases the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Parameters used for Munchausen RL agents.</figDesc><table><row><cell>Parameter</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. ? We provide a summary of all best scores (among training, averaged over 3 seeds), for all games on all agents, in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It appears that the benefits of distRL do not really come from RL principles, but rather from the regularizing effect of modelling a distribution and its role as an auxiliary task in a deep learning context<ref type="bibr" target="#b20">[21]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">By default, Dopamine's IQN uses 3-steps returns. We rather consider 1-step returns, as in<ref type="bibr" target="#b10">[11]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">To be on par with the literature, we keep using the published DQN as the baseline for other experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The setting is still not exactly the same, due to less enhancements in the Dopamine's Rainbow, a different codebase, but also a difference in the start (human start vs no-op for Rainbow, straight start for us), and possibly a different ROM, which cannot be checked.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg, Dandelion Man?</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum a posteriori policy optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic policy programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicen?</forename><surname>Mohammad Gheshlaghi Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilbert</forename><forename type="middle">J</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3207" to="3245" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Adri? Puigdom?nech Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vitvitskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13350</idno>
		<title level="m">Agent57: Outperforming the atari human benchmark</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reinforcement Learning Through Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Baird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>US Air Force Academy, US</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Increasing the action gap: New operators for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Samuel Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06110</idno>
		<title level="m">Dopamine: A research framework for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit quantile networks for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Stanczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action-gap phenomenon in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Massoud Farahmand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taming the noise in reinforcement learning via soft updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Pakman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Theory of Regularized Markov Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement Learning with Deep Energy-Based Policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent experience replay in distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Theoretical Analysis of Efficiency and Robustness of Softmax and Gap-Increasing Operators in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Kozuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comparative analysis of expected and distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Samuel</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Baron Munchhausen&apos;s Narrative of his Marvellous Travels and Campaigns in Russia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><forename type="middle">Erich</forename><surname>Raspe</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1785</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internation Conference on Representation Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximate modified policy iteration and its application to the game of Tetris</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gabillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Lesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1629" to="1676" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting the Softmax Bellman Operator: New Benefits and New Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep dynamic policy programming for robot control with raw images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihisa</forename><surname>Tsurumine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunduan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takamitsu</forename><surname>Matsubara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using a logarithmic mapping to enable lower discount factors in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Harm Van Seijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tavakoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14134" to="14144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Leverage the Average: an Analysis of Regularization in RL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Kozuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14089</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Momentum in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jch</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully parameterized quantile function for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6190" to="6199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
