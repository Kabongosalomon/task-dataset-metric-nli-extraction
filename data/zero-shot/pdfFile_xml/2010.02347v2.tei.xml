<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 LEARNING WITH INSTANCE-DEPENDENT LABEL NOISE: A SAMPLE SIEVE APPROACH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent YouTu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Zhu</surname></persName>
							<email>zwzhu@ucsc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
							<email>yifeigong@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent YouTu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent YouTu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>yangliu@ucsc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 LEARNING WITH INSTANCE-DEPENDENT LABEL NOISE: A SAMPLE SIEVE APPROACH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-annotated labels are often prone to noise, and the presence of such noise will degrade the performance of the resulting deep neural network (DNN) models. Much of the literature (with several recent exceptions) of learning with noisy labels focuses on the case when the label noise is independent of features. Practically, annotations errors tend to be instance-dependent and often depend on the difficulty levels of recognizing a certain task. Applying existing results from instance-independent settings would require a significant amount of estimation of noise rates. Therefore, providing theoretically rigorous solutions for learning with instance-dependent label noise remains a challenge. In this paper, we propose CORES 2 (COnfidence REgularized Sample Sieve), which progressively sieves out corrupted examples. The implementation of CORES 2 does not require specifying noise rates and yet we are able to provide theoretical guarantees of CORES 2 in filtering out the corrupted examples. This high-quality sample sieve allows us to treat clean examples and the corrupted ones separately in training a DNN solution, and such a separation is shown to be advantageous in the instance-dependent noise setting. We demonstrate the performance of CORES 2 on CIFAR10 and CI-FAR100 datasets with synthetic instance-dependent label noise and Clothing1M with real-world human noise. As of independent interests, our sample sieve provides a generic machinery for anatomizing noisy datasets and provides a flexible interface for various robust training techniques to further improve the performance. Code is available at https://github.com/UCSC-REAL/cores. * Equal contributions in alphabetical ordering. Hao leads experiments and Zhaowei leads theories. ? Corresponding authors: Y. Liu and Z. Zhu {yangliu,zwzhu}@ucsc.edu. 1 The proposed solution is primarily studied for the binary case in <ref type="bibr" target="#b7">Cheng et al. (2020)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks (DNNs) have gained popularity in a wide range of applications. The remarkable success of DNNs often relies on the availability of large-scale datasets. However, data annotation inevitably introduces label noise, and it is extremely expensive and time-consuming to clean up the corrupted labels. The existence of label noise can weaken the true correlation between features and labels as well as introducing artificial correlation patterns. Thus, mitigating the effects of noisy labels becomes a critical issue that needs careful treatment.</p><p>It is challenging to avoid overfitting to noisy labels, especially when the noise depends on both true labels Y and features X. Unfortunately, this often tends to be the case where human annotations are prone to different levels of errors for tasks with varying difficulty levels. Recent work has also shown that the presence of instance-dependent noisy labels imposes additional challenges and cautions to training in this scenario <ref type="bibr" target="#b22">(Liu, 2021)</ref>. For such instance-dependent (or feature-dependent, instance-based) label noise settings, theory-supported works usually focus on loss-correction which requires estimating noise rates <ref type="bibr" target="#b41">(Xia et al., 2020;</ref><ref type="bibr" target="#b5">Berthon et al., 2020)</ref>. Recent work by <ref type="bibr" target="#b7">Cheng et al. (2020)</ref> addresses the bounded instance-based noise by first learning the noisy distribution and then distilling examples according to some thresholds. 1 However, with a limited size of datasets, learning an accurate noisy distribution for each example is a non-trivial task. Additionally, the size and the quality of distilled examples are sensitive to the thresholds for distillation.</p><p>Departing from the above line of works, we design a sample sieve with theoretical guarantees to provide a high-quality splitting of clean and corrupted examples without the need to estimate noise rates. Instead of learning the noisy distributions or noise rates, we focus on learning the underlying clean distribution and design a regularization term to help improve the confidence of the learned classifier, which is proven to help safely sieve out corrupted examples. With the division between "clean" and "corrupted" examples, our training enjoys performance improvements by treating the clean examples (using standard loss) and the corrupted ones (using an unsupervised consistency loss) separately.</p><p>We summarize our main contributions: 1) We propose to train a classifier using a novel confidence regularization (CR) term and theoretically guarantee that, under mild assumptions, minimizing the confidence regularized cross-entropy (CE) loss on the instance-based noisy distribution is equivalent to minimizing the pure CE loss on the corresponding "unobservable" clean distribution. This classifier is also shown to be helpful for evaluating each example to build our sample sieve.2) We provide a theoretically sound sample sieve that simply compares the example's regularized loss with a closed-form threshold explicitly determined by predictions from the above trained model using our confidence regularized loss, without any extra estimates. 3) To the best of our knowledge, the proposed CORES 2 (COnfidence REgularized Sample Sieve) is the first method that is thoroughly studied for a multi-class classification problem, has theoretical guarantees to avoid overfitting to instance-dependent label noise, and provides high-quality division without knowing or estimating noise rates. 4) By decoupling the regularized loss into separate additive terms, we also provide a novel and promising mechanism for understanding and controlling the effects of general instancedependent label noise. 5) CORES 2 achieves competitive performance on multiple datasets, including CIFAR-10, CIFAR-100, and Clothing1M, under different label noise settings. Other related works In addition to recent works by <ref type="bibr" target="#b41">Xia et al. (2020)</ref>, <ref type="bibr" target="#b5">Berthon et al. (2020)</ref>, and <ref type="bibr" target="#b7">Cheng et al. (2020)</ref>, we briefly overview other most relevant references. Detailed related work is left to Appendix A. Making the loss function robust to label noise is important for building a robust machine learning model <ref type="bibr" target="#b49">(Zhang et al., 2016)</ref>. One popular direction is to perform loss correction, which first estimates transition matrix <ref type="bibr" target="#b30">(Patrini et al., 2017;</ref><ref type="bibr" target="#b35">Vahdat, 2017;</ref><ref type="bibr" target="#b42">Xiao et al., 2015;</ref><ref type="bibr">Zhu et al., 2021b;</ref><ref type="bibr" target="#b46">Yao et al., 2020b)</ref>, and then performs correction/reweighting via forward or backward propagation, or further revises the estimated transition matrix with controllable variations <ref type="bibr" target="#b40">(Xia et al., 2019)</ref>. The other line of work focuses on designing specific losses without estimating transition matrices <ref type="bibr" target="#b27">(Natarajan et al., 2013;</ref><ref type="bibr" target="#b44">Xu et al., 2019;</ref><ref type="bibr" target="#b24">Liu &amp; Guo, 2020;</ref><ref type="bibr" target="#b39">Wei &amp; Liu, 2021)</ref>. However, these works assume the label noise is instance-independent which limits their extension. Another approach is sample selection <ref type="bibr" target="#b14">(Jiang et al., 2017;</ref><ref type="bibr" target="#b11">Han et al., 2018;</ref><ref type="bibr" target="#b48">Yu et al., 2019;</ref><ref type="bibr" target="#b29">Northcutt et al., 2019;</ref><ref type="bibr" target="#b45">Yao et al., 2020a;</ref><ref type="bibr" target="#b38">Wei et al., 2020;</ref><ref type="bibr" target="#b52">Zhang et al., 2020a)</ref>, which selects the "small loss" examples as clean ones. However, we find this approach only works well on the instance-independent label noise. Approaches such as label correction <ref type="bibr" target="#b36">(Veit et al., 2017;</ref> or semi-supervised learning <ref type="bibr" target="#b19">(Li et al., 2020;</ref><ref type="bibr">Nguyen et al., 2019)</ref> also lack guarantees for the instancebased label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CORES : CONFIDENCE REGULARIZED SAMPLE SIEVE</head><p>Consider a classification problem on a set of N training examples denoted by D := {(x n , y n )} n? <ref type="bibr">[N ]</ref> , where [N ] := {1, 2, ? ? ? , N } is the set of example indices. Examples (x n , y n ) are drawn according to random variables (X, Y ) ? X ? Y from a joint distribution D. Let D X and D Y be the marginal distributions of X and Y . The classification task aims to identify a classifier f : X ? Y that maps X to Y accurately. One common approach is minimizing the empirical risk using DNNs with respect to the cross-entropy loss defined as</p><formula xml:id="formula_0">(f (x), y) = ? ln(f x [y]), y ? [K],</formula><p>where f x [y] denotes the y-th component of f (x) and K is the number of classes. In real-world applications, such as human-annotated images <ref type="bibr" target="#b17">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b51">Zhang et al., 2017)</ref> and medical diagnosis <ref type="bibr" target="#b0">(Agarwal et al., 2016)</ref>, the learner can only observe a set of noisy labels. For instance, human annotators may wrongly label some images containing cats as ones that contain dogs accidentally or irresponsibly. The label noise of each instance is characterized by a noise transition matrix T (X), where each element T ij (X) := P( Y = j|Y = i, X). The corresponding noisy dataset 2 and distribution are denoted by D := {(x n ,? n )} n?[N ] and D. Let 1(?) be the indicator function taking value 1 when the specified condition is satisfied and 0 otherwise. Similar to the goals in surrogate loss <ref type="bibr" target="#b27">(Natarajan et al., 2013)</ref>, L DMI <ref type="bibr" target="#b44">(Xu et al., 2019)</ref> and peer loss <ref type="bibr" target="#b24">(Liu &amp; Guo, 2020)</ref>, we aim to learn a classifier f from the noisy distribution D which also minimizes P(f (X) = Y ), (X, Y ) ? D.</p><p>Beyond their results, we attempt to propose a theoretically sound approach addressing a general instance-based noise regime without knowing or estimating noise rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CONFIDENCE REGULARIZATION</head><p>In this section, we present a new confidence regularizer (CR). Our design of the CR is mainly motivated by a recently proposed robust loss function called peer loss <ref type="bibr" target="#b24">(Liu &amp; Guo, 2020)</ref>. For each example (x n ,? n ), peer loss has the following form:</p><formula xml:id="formula_1">PL (f (x n ),? n ) := (f (x n ),? n ) ? (f (x n1 ),? n2 ),</formula><p>where (x n1 ,? n1 ) and (x n2 ,? n2 ) are two randomly sampled and paired peer examples (with replacement) for n. Let X n1 and Y n2 be the corresponding random variables. Note X n1 , Y n2 are two independent and uniform random variables being each x n , n ? [N ] and? n , n ? [N ] with probability 1 N respectively:</p><formula xml:id="formula_2">P(X n1 = x n | D) = P( Y n2 = y n | D) = 1 N , ?n ? [N ]. Let D Y | D be the distribution of Y n2 given dataset D.</formula><p>Peer loss then has the following equivalent form in expectation:</p><formula xml:id="formula_3">1 N n?[N ] E Xn 1 , Yn 2 | D [ (f (xn),?n)? (f (Xn 1 ), Yn 2 )] = 1 N n?[N ] (f (xn),?n)? n ?[N ] P(Xn 1 = x n | D)ED Y | D [ (f (x n ), Y )] = 1 N n?[N ] (f (xn),?n) ? ED Y | D [ (f (xn), Y )] .</formula><p>This result characterizes a new loss denoted by CA :</p><formula xml:id="formula_4">CA (f (x n ),? n ) := (f (x n ),? n ) ? E D Y | D [ (f (x n ), Y )].<label>(1)</label></formula><p>Though not studied rigorously by <ref type="bibr" target="#b24">Liu &amp; Guo (2020)</ref> </p><formula xml:id="formula_5">?E D Y | D [ (f (x n ), Y )]</formula><p>helps improve the confidence of the learned classifier. Inspired by the above observation, we define the following confidence regularizer:</p><formula xml:id="formula_6">Confidence Regularizer: CR (f (x n )) := ?? ? E D Y | D [ (f (x n ), Y )],</formula><p>where ? is positive and (?) refers to the CE loss. The prior probability P( Y | D) is counted directly from the noisy dataset. In the remaining of this paper, (?) indicates the CE loss by default.</p><p>Why are confident predictions important? Intuitively, when model fits to the label noise, its predictions often become less confident, since the noise usually corrupts the signal encoded in the clean data. From this perspective, encouraging confident predictions plays against fitting to label noise. Compared to instance-independent noise, the difficulties in estimating the instance-dependent noise rates largely prevent us from applying existing techniques. In addition, as shown by <ref type="bibr" target="#b26">Manwani &amp; Sastry (2013)</ref>, the 0-1 loss function is more robust to instance-based noise but hard to optimize with. To a certain degree, pushing confident predictions results in a differentiable loss function that approximates the 0-1 loss, and therefore restores the robustness property. Besides, as observed by <ref type="bibr" target="#b6">Chatterjee (2020)</ref> and <ref type="bibr">Zielinski et al. (2020)</ref>, gradients from similar examples would reinforce each other. When the overall label information is dominantly informative that T ii (X) &gt; T ij (X), DNNs will receive more correct information statistically. Encouraging confident predictions would discourage the memorization of the noisy examples (makes it hard for noisy labels to reduce the confidence of predictions), and therefore further facilitate DNNs to learn the (clean) dominant information.</p><p>CR is NOT the entropy regularization Entropy regularization (ER) is a popular choice for improving confidence of the trained classifiers in the literature <ref type="bibr" target="#b34">(Tanaka et al., 2018;</ref><ref type="bibr" target="#b47">Yi &amp; Wu, 2019)</ref>. Given a particular prediction probability p for a class, the ER term is based on the function ?p ln p, while our CR is built on ln p. Later we show CR offers us favorable theoretical guarantees for training with instance-dependent label noise, while ER does not. In Appendix C.1, we present both theoretical and experimental evidences that CR serves as a better regularizer compared to ER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONFIDENCE REGULARIZED SAMPLE SIEVE</head><p>Intuitively, label noise misleads the training thus sieving corrupted examples out of datasets is beneficial. Furthermore, label noise introduces high variance during training even with the existence of CR (discussed in Section 3.3). Therefore, rather than accomplishing training solely with CR , we will first leverage its regularization power to design an efficient sample sieve. Similar to a general sieving process in physical words that compares the size of particles with the aperture of a sieve, we evaluate the "size" (quality, or a regularized loss) of examples and compare them with some to-be-specified thresholds, therefore the name sample sieve. In our formulation, the regularized loss (f (x n ),? n ) + CR (f (x n )) is employed to evaluate examples and ? n is used to specify thresholds. Specifically, we aim to solve the sample sieve problem in (2).</p><formula xml:id="formula_7">Confidence Regularized Sample Sieve min f ?F , v?{0,1} N n?[N ] vn [ (f (xn),?n) + CR(f (xn)) ? ?n] s.t. CR(f (xn)) := ?? ? ED Y | D (f (xn), Y ), ?n := 1 K ??[K]</formula><p>(f (xn),?) + CR(f (xn)).</p><p>(2) ? v n ? {0, 1} indicates whether example n is clean (v n = 1) or not (v n = 0); ? ? n (mimicking the aperture of a sieve) controls which example should be sieved out;</p><p>?f is a copy of f and does not contribute to the back-propagation. F is the search space of f . Dynamic sample sieve The problem in (2) is a combinatorial optimization which is hard to solve directly. A standard solution to (2) is to apply alternate search iteratively as follows:</p><formula xml:id="formula_8">? Starting at t = 1, v (0) n = 1, ?n ? [N ].</formula><p>? Confidence-regularized model update (at iteration-t):</p><formula xml:id="formula_9">f (t) = arg min f ?F n?[N ] v (t?1) n [ (f (x n ),? n ) + CR (f (x n ))] ;</formula><p>(3)</p><formula xml:id="formula_10">? Sample sieve (at iteration-t): v (t) n = 1( (f (t) (x n ),? n ) + CR (f (t) (x n )) &lt; ? n,t ),<label>(4)</label></formula><p>where ? n,t = 1</p><formula xml:id="formula_11">K ??[K] (f (t) (x n ),?) + CR (f (t) (x n )), f (t)</formula><p>and v (t) refer to the specific classifier and weight at iteration-t. Note the values of CR (f (t) (x n )) and CR (f (t) (x n )) are the same. We keep both terms to be consistent with the objective in Eq. (2). In DNNs, we usually update model f with one or several epochs of data instead of completely solving (3). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the dynamic sample sieve, where the size of each example corresponds to the regularized loss and the aperture of a sieve is determined by ? n,t . In each iteration-t, sample sieve-  <ref type="figure">Figure 2</ref>: Loss distributions of training on CIFAR-10 with 40% symmetric noise (symm.) or 40% instance-based noise (inst.). The loss is given by (f (t) (x n ),? n ) + CR (f (t) (x n )) ? ? n,t as (4). CE Sieve represents the dynamic sample sieve with standard cross-entropy loss (without CR).</p><p>t "blocks" some corrupted examples by comparing a regularized example loss with a closed-form threshold ? n,t , which can be immediately obtained given current modelf <ref type="bibr">(t)</ref> and example (x n ,? n ) (no extra estimation needed). In contrast, most sample selection works <ref type="bibr" target="#b11">(Han et al., 2018;</ref><ref type="bibr" target="#b48">Yu et al., 2019;</ref><ref type="bibr" target="#b38">Wei et al., 2020)</ref> focus on controlling the number of the selected examples using an intuitive function where the overall noise rate may be required, or directly selecting examples by an empirically set threshold <ref type="bibr">(Zhang &amp; Sabuncu, 2018)</ref>. Intuitively, the specially designed thresholds ? n,t for each example should be more accurate than a single threshold for the whole dataset. Besides, the goal of existing works is often to select clean examples while our sample sieve focuses on removing the corrupted ones. On a high level, we follow a different philosophy from these sample selection works. We coin our solution as COnfidence REgularized Sample Sieve (CORES 2 ).</p><p>More visualizations of the sample sieve In addition to <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL GUARANTEES OF CORES 2</head><p>In this section, we theoretically show the advantages of CORES 2 . The analyses focus on showing CORES 2 guarantees a quality division, i.e. v n = 1(y n =? n ), ?n, with a properly set ?. To show the effectiveness of this solution, we call a model prediction on x n is better than random guess if f xn [y n ] &gt; 1/K, and call it confident if f xn [y] ? {0, 1}, ?y ? [K], where y n is the clean label and y is an arbitrary label. The quality of sieving out corrupted examples is guaranteed in Theorem 2.</p><p>Theorem 2. The sample sieve defined in (4) ensures that clean examples (x n ,? n = y n ) will not be identified as being corrupted if the model f (t) 's prediction on x n is better than random guess.</p><p>Theorem 2 informs us that our sample sieve can progressively and safely filter out corrupted examples, and therefore improves division quality, when the model prediction on each x n is better than random guess. The full proof is left to Appendix B.3. In the next section, we provide evidences that our trained model is guaranteed to achieve this requirement with sufficient examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DECOUPLING THE CONFIDENCE REGULARIZED LOSS</head><p>The discussion of performance guarantees of the sample sieve focuses on a general instance-based noise transition matrix T (X), which can induce any specific noise regime such as symmetric noise and asymmetric noise <ref type="bibr" target="#b15">(Kim et al., 2019;</ref><ref type="bibr" target="#b19">Li et al., 2020)</ref>. Note the feature-independency was one critical assumption in state-of-the-art theoretically guaranteed noise-resistant literatures <ref type="bibr" target="#b27">(Natarajan et al., 2013;</ref><ref type="bibr" target="#b24">Liu &amp; Guo, 2020;</ref><ref type="bibr" target="#b44">Xu et al., 2019)</ref>  </p><formula xml:id="formula_12">while we do not require. Let T ij := E D|Y =i [T ij (X)], ?i, j ? [K].</formula><formula xml:id="formula_13">E D (f (X), Y ) + CR (f (X)) = Term-1 T ? E D [ (f (X), Y )] + Term-2 ? ? E D? [ (f (X), Y )] + j?[K] i?[K] P(Y = i)E D|Y =i [(U ij (X) ? ?P( Y = j)) (f (X), j)] Term-3 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_14">T := min j?[K] T jj ,? := j?[K] ? j P(Y = j), ? j := T jj ? T , U ij (X) = T ij (X), ?i = j, U jj (X) = T jj (X) ? T jj , and ED ? [ (f (X), Y )] := 1(? &gt; 0) j?[K] ? j P(Y =j) ? E D|Y =j [ (f (X), j)].</formula><p>Equation <ref type="formula" target="#formula_13">(5)</ref> provides a generic machinery for anatomizing noisy datasets, where we show the effects of instance-based label noise on the CR regularized loss can be decoupled into three additive terms: Term-1 reflects the expectation of CE on clean distribution D, Term-2 shifts the clean distribution by changing the prior probability of Y , and Term-3 characterizes how the corrupted examples (represented by U ij (X)) might mislead/mis-weight the loss, as well as the regularization ability of CR (represented by ?P( Y = j)). In addition to the design of sample sieve, this additive decoupling structure also provides a novel and promising perspective for understanding and controlling the effects of generic instance-dependent label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GUARANTEES OF THE SAMPLE SIEVE</head><p>By decoupling the effects of instance-dependent noise into separate additive terms as shown in Theorem 3, we can further study under what conditions, minimizing the confidence regularized CE loss on the (instance-dependent) noisy distribution will be equivalent to minimizing the true loss incurred on the clean distribution, which is exactly encoded by Term-1. In other words, we would like to understand when Term-2 and Term-3 in (5) can be controlled not to disrupt the minimization of Term-1. Our next main result establishes this guarantee but will first need the following two assumptions. Assumption 1. (Y * = Y ) Clean labels are Bayes optimal (Y * := arg max i?[K] P(Y = i|X)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 2. (Informative datasets) The noise rate is bounded as</head><formula xml:id="formula_15">T ii (X) ? T ij (X) &gt; 0, ?i ? [K], j ? [K], j = i, X ? D X .</formula><p>Feasibility of assumptions: 1) Note for many popular image datasets, e.g. CIFAR, the label of each feature is well-defined and the corresponding distribution is well-separated by human annotation. In this case, each feature X only belongs to one particular class Y . Thus Assumption 1 is generally held in classification problems <ref type="bibr" target="#b21">(Liu &amp; Tao, 2015)</ref>. Technically, this assumption could be relaxed. We use this assumption for clean presentations. 2) Assumption 2 shows the requirement of noise rates, i.e., for any feature X, a sufficient number of clean examples are necessary for dominant clean information. For example, we require T ii (X) ? T ij (X) &gt; 0 to ensure examples from class i are informative <ref type="bibr" target="#b23">(Liu &amp; Chen, 2017)</ref>.</p><p>Before formally presenting the noise-resistant property of training with CR , we discuss intuitions here. As discussed earlier in Section 2.1, our CR regularizes the CE loss to generate/incentivize confident prediction, and thus is able to approximate the 0-1 loss to obtain its robustness property. More explicitly, from (5), CR affects Term-3 with a scale parameter ?. Recall that U ij (X) = T ij (X), ?i = j, which is exactly the noise transition matrix. Although we have no information about this transition matrix, the confusion brought by U ij (X) can be canceled or reversed by a sufficiently large ? such that U ij (X) ? ?P( Y = j) ? 0. Intuitively, with an appropriate ?, all the effects of U ij (X), i = j can be reversed, and we will get a negative loss punishing the classifier for predicting class-j when the clean label is i. Formally, Theorem 4 shows the noise-resistant property of training with CR and is proved in Appendix B.4. Theorem 4. (Robustness of the Confidence Regularized CE Loss) With Assumption 1 and 2, when</p><formula xml:id="formula_16">max i,j?[K],X?D X Uij(X) P( Y = j) ? ? ? min P( Y =i)&gt;P( Y =j),X?D X Tii(X) ? Tij(X) P( Y = i) ? P( Y = j) ,<label>(6)</label></formula><formula xml:id="formula_17">minimizing E D [ (f (X), Y ) + CR (f (X))] is equivalent to minimizing E D [ (f (X), Y )].</formula><p>Theorem 4 shows a sufficient condition of ? for our confidence regularized CE loss to be robust to instance-dependent label noise. The bound on LHS ensures the confusion from label noise could be canceled or reversed by the ? weighted confidence regularizer, and the RHS bound guarantees the model with the minimized regularized loss predicts the most frequent label in each feature w.p. 1.</p><p>Theorem 4 also provides guidelines for tuning ?. Although we have no knowledge about T ij (X), we can roughly estimate the range of possible ?. One possibly good setting of ? is linearly increasing with the number of classes, e.g. ? = 2 for 10 classes and ? = 20 for 100 classes.</p><p>With infinite model capacity, minimizing E D [ (f (X), Y )] returns the Bayes optimal classifier (since CE is a calibrated loss) which predicts on each x n better than random guess. Therefore, with a suf-</p><formula xml:id="formula_18">ficient number of examples, minimizing E D [ (f (X), Y ) + CR (f (X))]</formula><p>will also return a model that predicts better than random guess, then satisfying the condition required in Theorem 2 to guarantee the quality of sieved examples. Further, since the Bayes optimal classifier always predicts clean labels confidently when Assumption 1 holds, Theorem 4 also guarantees confident predictions. With such predictions, the sample sieve in <ref type="formula" target="#formula_10">(4)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING WITH SIEVED SAMPLES</head><p>We discuss the necessity of a dynamic sample sieve in this subsection. Despite the strong guarantee in expectation as shown Theorem 4, performing direct Empirical Risk Minimization (ERM) of the regularized loss is likely to return a sub-optimal solution. Although Theorem 4 guarantees the equivalence of minimizing two first-order statistics, their second-order statistics are also important for estimating the expectation when examples are finite.</p><formula xml:id="formula_19">Intuitively, Term-1 T ? E D [ (f (X), Y )]</formula><p>primarily helps distinguish a good classifier from a bad one on the clean distribution. The existence of the leading constant T reduces the power of the above discrimination, as effectively the gap between the expected losses become smaller as noise increases (T will decrease). Therefore we would require more examples to recognize the better model. Equivalently, the variance of the selection becomes larger. In Appendix C.2, we also offer an explanation from the variance's perspective. For some instances with extreme label noise, the ? satisfying Eqn. <ref type="formula" target="#formula_16">(6)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Now we present experimental evidences of how CORES 2 works. 5</p><p>Datasets: CORES 2 is evaluated on three benchmark datasets: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b16">(Krizhevsky et al., 2009</ref>) and Clothing1M <ref type="bibr" target="#b42">(Xiao et al., 2015)</ref>. Following the convention from Xu et al. <ref type="formula" target="#formula_4">(2019)</ref>, we use ResNet34 for CIFAR-10 and CIFAR-100 and ResNet50 for Clothing1M.</p><p>Noise type: We experiment with three types of label noise: symmetric, asymmetric and instancedependent label noise. Symmetric noise is generated by randomly flipping a true label to the other possible labels w.p. ? <ref type="bibr" target="#b15">(Kim et al., 2019)</ref>, where ? is called the noise rate. Asymmetric noise is generated by flipping the true label to the next class (i.e., label i ? i+1, mod K) w.p. ?. Instancedependent label noise is a more challenging setting and we generate instance-dependent label noise following the method from <ref type="bibr" target="#b41">Xia et al. (2020)</ref> (See Appendix D.3 for details). In expectation, the noise rate ? for all noise regimes is the overall ratio of corrupted examples in the whole dataset.</p><p>Consistency training after the sample sieve: Let ? be the last iteration of CORES 2 . Define  </p><formula xml:id="formula_20">L(? ) := {n|n ? [N ], v (? ) n = 1}, H(? ) := {n|n ? [N ], v (? ) n = 0}, D L(? ) := {(x n ,<label>?</label></formula><formula xml:id="formula_21">label noise. F-score := 2?Pre?Re Pre+Re , where Pre := n?[N ] 1(vn=1,yn=?n) n?[N ] 1(vn=1)</formula><p>, and Re :</p><formula xml:id="formula_22">= n?[N ] 1(vn=1,yn=?n) n?[N ] 1(yn=?n) . L(? )}, D H(? ) := {(x n ,? n ) : n ? H(? )}. Thus D L(? ) is sieved as clean examples and D H(? ) is fil- tered out as corrupted ones. Examples (x n ,? n ) ? D L(? ) lead the training direction using the CE loss as n?L(? ) (f (x n ),? n ).</formula><p>Noting the labels in D H(? ) are supposed to be corrupted and can distract the training, we simply drop them. On the other hand, feature information of these examples encodes useful information that we can further leverage to improve the generalization ability of models.</p><p>There are different ways to use this unsupervised information, in this paper, we chose to minimize the KL-divergence between predictions on the original feature and the augmented feature to make predictions consistent. This is a common option as chosen by <ref type="bibr" target="#b18">Li et al. (2019)</ref>, <ref type="bibr" target="#b43">Xie et al. (2019)</ref>, and <ref type="bibr">Zhang et al. (2020b)</ref>. The consistency loss function in epoch</p><formula xml:id="formula_23">-t is n?H(? ) KL (f (x n ),f (t) (x n,t )), wheref (t)</formula><p>is a copy of the DNN at the beginning of epoch-t but without gradients. Summing the classification and consistency loss yields the total loss. See Appendix D.1 for an illustration.</p><p>Other alternatives: Checking the consistency of noisy predictions is only one possible way to leverage the additional information after sample sieves. Our basic idea of first sieving the dataset and then treating corrupted examples differently from clean ones admits other alternatives. There are many other possible designs after sample sieves, e.g., estimating transition matrix using sieved examples then applying loss-correction <ref type="bibr" target="#b30">(Patrini et al., 2017;</ref><ref type="bibr" target="#b35">Vahdat, 2017;</ref><ref type="bibr" target="#b42">Xiao et al., 2015)</ref>, making the consistency loss as another regularization term and retraining the model <ref type="bibr">(Zhang et al., 2020b)</ref>, correcting the sample selection bias in clean examples and retraining <ref type="bibr" target="#b7">(Cheng et al., 2020;</ref><ref type="bibr" target="#b8">Fang et al., 2020)</ref>, or relabeling those corrupted examples and retraining, etc. Additionally, clustering methods on the feature space <ref type="bibr" target="#b25">Luo et al., 2020)</ref> or high-order information <ref type="bibr">(Zhu et al., 2021a)</ref> can also be exploited along with the dynamic sample sieve. Besides, the current structure is ready to include other techniques such as mixup <ref type="bibr" target="#b50">(Zhang et al., 2018)</ref>.</p><p>Quality of our sample sieve: <ref type="figure" target="#fig_3">Figure 3</ref> shows the F-scores of sieved clean examples with training epochs on the symmetric and the instance-based label noise. F-score quantifies the quality of the sample sieve by the harmonic mean of precision (ratio of actual cleans examples in sieved clean ones) and recall (ratio of sieved cleans examples in actual clean ones). We compare CORES 2 with Co-teaching and Co-teaching+. Note the F-scores of CORES 2 and Co-teaching are consistently high on the symmetric noise, while CORES 2 achieves higher performance on the challenging instancebased label noise, especially with the 60% noise rate where the other two methods have low F-scores.</p><p>Experiments on CIFAR-10, CIFAR-100 and Clothing1M: In this section, we compare CORES 2 with several state-of-the-art methods on CIFAR-10 and CIFAR-100 under instance-based, symmetric and asymmetric label noise settings, which is shown on <ref type="table" target="#tab_5">Table 1 and Table 2</ref>. CORES 2 denotes that we apply consistency training on the corrupted examples after the sample sieve. For a fair comparison, all the methods use ResNet-34 as the backbone. By comparing the performance of CE on the symmetric and the instance-based label noise, we note the instance-based label noise is a more challenging setting. Even though some methods (e.g., L DMI ) behaves well on symmetric and asymmetric label noise, they may reach low test accuracies on the instance-based label noise, especially when the noise rate is high or the dataset is more complex. However, CORES 2 consistently works well on the instance-based label noise and adding the consistency training gets better results. Table 3 verifies CORES 2 on Clothing1M, a dataset with real human label noise. Compared to the other   approaches, CORES 2 also works fairly well on the Clothing1M dataset. See more experiments in Appendix D. We also provide source codes with detailed instructions in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>This paper introduces CORES 2 , a sample sieve that is guaranteed to be robust to general instancedependent label noise and sieve out corrupted examples, but without using explicit knowledge of the noise rates of labels. The analysis of CORES 2 assumed that the Bayes optimal labels are the same as clean labels. Future directions of this work include extensions to more general cases where the Bayes optimal labels may differ from clean labels. We are also interested in exploring different possible designs of robust training with sieved examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>The appendices are organized as follows. Section A presents the full version of related works. Section B details the proofs for our theorems. Section C supplements other necessary evidences to justify CORES 2 . Section D shows more experimental details and results.</p><p>A FULL VERSION OF RELATED WORKS Learning with noisy labels has observed exponentially growing interests. Since the traditional crossentropy (CE) loss has been proved to easily overfit noisy labels <ref type="bibr" target="#b49">(Zhang et al., 2016)</ref>, researchers try to design different loss functions to handle this problem. There were two main perspectives on designing loss functions. Considering the fact that outputs of logarithm functions in the CE loss grow explosively when the prediction f (x) approaches zero, some researchers tried to design bounded loss functions <ref type="bibr" target="#b2">(Amid et al., 2019;</ref><ref type="bibr" target="#b10">Gong et al., 2018;</ref><ref type="bibr" target="#b9">Ghosh et al., 2017)</ref>. To avoid relying on fine-tuning of hyper-parameters in loss functions, a meta-learning method was proposed bt <ref type="bibr" target="#b32">Shu et al. (2020)</ref> to combine the above four loss functions together. However, simply considering loss function values without discussing the noise type and the corresponding statistics could not be noise-tolerant as defined by <ref type="bibr" target="#b26">Manwani &amp; Sastry (2013)</ref>. As a complementary, others started from noise types and tried to design noise-tolerant loss functions. Based on the assumption that label noise only depends on the true class (a.k.a. feature-independent or label-dependent), an unbiased loss function called surrogate loss <ref type="bibr" target="#b27">(Natarajan et al., 2013)</ref>, an information-based loss function called L DMI <ref type="bibr" target="#b44">(Xu et al., 2019)</ref>, and a new family of loss functions to punish agreements between classifiers and noisy datasets called peer loss <ref type="bibr" target="#b24">(Liu &amp; Guo, 2020)</ref> were proposed. They proved theoretically that training DNNs using their loss functions on feature-independent noisy datasets was equivalent to training CE on the corresponding unobservable clean datasets. However, surrogate loss focused on the binary classifications and required knowing noise rates. L DMI and peer loss does not require knowing noise rates while L DMI may not be easy for extension and multi-class classification of peer loss requires particular transition matrices.</p><p>The correction approach is also popular in handling label noise. Previous works <ref type="bibr" target="#b30">(Patrini et al., 2017;</ref><ref type="bibr" target="#b35">Vahdat, 2017;</ref><ref type="bibr" target="#b42">Xiao et al., 2015)</ref> assumed the feature-independent noise transition matrix was given or could be estimated and attempted to use it to correct loss functions. For example, <ref type="bibr" target="#b30">Patrini et al. (2017)</ref> first estimated the noise transition matrix and then relied on it to correct forward or backward propagation during training. However, without a set of clean examples, the noise transition matrix could be hard to estimate correctly. Instead of correcting loss functions, some methods directly corrected labels <ref type="bibr" target="#b36">(Veit et al., 2017;</ref>, whereas it might introduce extra noise and damage useful information. Recent works <ref type="bibr" target="#b41">(Xia et al., 2020;</ref><ref type="bibr" target="#b5">Berthon et al., 2020)</ref> extended loss-correction from the limited feature-independent label noise to part-dependent or a more general instance-dependent noise regime while they relied heavily on the noise rate estimation.</p><p>Sample selection <ref type="bibr" target="#b14">(Jiang et al., 2017;</ref><ref type="bibr" target="#b11">Han et al., 2018;</ref><ref type="bibr" target="#b48">Yu et al., 2019;</ref><ref type="bibr" target="#b45">Yao et al., 2020a;</ref><ref type="bibr" target="#b38">Wei et al., 2020)</ref> mainly focused on exploiting the memorization of DNNs and treating the "small loss" examples as clean ones, while they only focused on feature-independent label noise. <ref type="bibr" target="#b7">Cheng et al. (2020)</ref> tried to distill some examples relying on the predictions using the surrogate loss function <ref type="bibr" target="#b27">(Natarajan et al., 2013)</ref>. Note estimating noise rates are necessary for both applying surrogate loss and determining the threshold for distillation. The sample selection methods could be implemented with some semi-supervised learning techniques to improve the performance, where the corrupted examples were treated as unlabeled data <ref type="bibr" target="#b19">(Li et al., 2020;</ref><ref type="bibr">Nguyen et al., 2019)</ref>. However, the training mechanisms of these methods were still based on the CE loss, which could not be guaranteed to avoid overfitting to label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF FOR THEOREMS</head><p>In this section, we firstly present the proof for Theorem 3 (our main theorem) in Section B.1, which provides a generic machinery for anatomizing noisy datasets. Then we will respectively prove Theorem 1 in Section B.2, Theorem 2 in Section B.3, and Theorem 4 in Section B.4 according to the order they appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 PROOF FOR THEOREM 3</head><p>Theorem 3. (Main Theorem: Decoupling the Expected Regularized CE Loss) In expectation, the loss with CR can be decoupled as three separate additive terms:</p><formula xml:id="formula_24">E D (f (X), Y ) + CR (f (X)) = Term-1 T ? E D [ (f (X), Y )] + Term-2 ? ? E D? [ (f (X), Y )] + j?[K] i?[K] P(Y = i)E D|Y =i [(U ij (X) ? ?P( Y = j)) (f (X), j)] Term-3 ,<label>(7)</label></formula><p>where T := min j?[K] T jj ,? :</p><formula xml:id="formula_25">= j?[K] ? j P(Y = j), ? j := T jj ? T , U ij (X) = T ij (X), ?i = j, U jj (X) = T jj (X) ? T jj , and ED ? [ (f (X), Y )] := 1(? &gt; 0) j?[K] ? j P(Y =j) ? E D|Y =j [ (f (X), j)].</formula><p>Proof. The expected form of traditional CE loss on noisy distribution D can be written as</p><formula xml:id="formula_26">E D [ (f (X), Y )] = j?[K] i?[K] P(Y = i)E D|Y =i [T ij (X) (f (X), j)] = j?[K] i?[K] P(Y = i)T ij E D|Y =i [ (f (X), j)] + j?[K] i?[K] P(Y = i)Cov D|Y =i (T ij (X), (f (X), j)).</formula><p>The first term could be transformed as:</p><formula xml:id="formula_27">j?[K] i?[K] P(Y = i)T ij E D|Y =i [ (f (X), j)] = j?[K] ? ? T jj P(Y = j)E D|Y =j [ (f (X), j)] + i?[K],i =j T ij P(Y = i)E D|Y =i [ (f (X), j)] ? ? =T E D [ (f (X), Y )] +?E D? [ (f (X), Y )] + j?[K] i?[K],i =j T ij P(Y = i)E D|Y =i [ (f (X), j)], where T := min j?[K] T jj ,? := j?[K] ? j P(Y = j), ? j := T jj ? T , and E D? [ (f (X), Y )] := j?[K] ?j P(Y =j) ? E D|Y =j [ (f (X), j)], if? &gt; 0, 0 if? = 0. Then E D [ (f (X), Y )] =T ED[ (f (X), Y )] +?ED ? [ (f (X), Y )] + j?[K] i?[K],i =j TijP(Y = i)E D|Y =i [ (f (X), j)], + j?[K] i?[K] P(Y = i)Cov D|Y =i (Tij(X), (f (X), j)) =T ED[ (f (X), Y )] +?ED ? [ (f (X), Y )] + j?[K] i?[K],i =j TijP(Y = i)E D|Y =i [ (f (X), j)], + j?[K] i?[K],i =j P(Y = i)E D|Y =i [(Tij(X) ? Tij)( (f (X), j) ? E D|Y =i [ (f (X), j)])] + j?[K] P(Y = j)E D|Y =j [(Tjj(X) ? Tjj)( (f (X), j) ? E D|Y =j [ (f (X), j)])] =T ED[ (f (X), Y )] +?ED ? [ (f (X), Y )] + j?[K] i?[K],i =j P(Y = i)E D|Y =i [(Tij(X) ? Tij)( (f (X), j) ? E D|Y =i [ (f (X), j)]) + Tij (f (X), j)] + j?[K] P(Y = j)E D|Y =j [(Tjj(X) ? Tjj)( (f (X), j) ? E D|Y =j [ (f (X), j)])] =T ED[ (f (X), Y )] +?ED ? [ (f (X), Y )] + j?[K] i?[K],i =j P(Y = i)E D|Y =i [Tij(X) (f (X), j)] + j?[K] P(Y = j)E D|Y =j [(Tjj(X) ? Tjj) (f (X), j)] =T ED[ (f (X), Y )] +?ED ? [ (f (X), Y )] + j?[K] i?[K] P(Y = i)E D|Y =i [Uij(X) (f (X), j)], where U ij (X) = T ij (X), ?i = j, U jj (X) = T jj (X) ? T jj .</formula><p>The expected form of CR on noisy distribution D can be written as</p><formula xml:id="formula_28">E D [ CR (f (x i ))] = ??E D E D Y | D [ (f (x i ), Y )] = ?? D P( D)E D Y | D [ (f (x i ), Y )] = ?? j?[K] P( Y = j)E D X [ (f (x i ), j)] = ? j?[K] i?[K] P(Y = i)E D|Y =i [?P( Y = j) (f (x i ), j)].</formula><p>Thus the expected form of the new regularized loss is</p><formula xml:id="formula_29">E D (f (X), Y ) + CR (f (x i )) = T E D [ (f (X), Y )] +?E D? [ (f (X), Y )] + j?[K] i?[K] P(Y = i)E D|Y =i [(U ij (X) ? ?P( Y = j)) (f (X), j)].<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 PROOF FOR THEOREM 1</head><p>Theorem 1. For CA (?), solutions satisfying f xn [i] &gt; 0, ?i ? [K] are not locally optimal at (x n ,? n ).</p><p>Proof. Let (?) be the CE loss. Note this proof does not rely on whether the data distribution is clean or not. We use D to denote any data distribution and D to denote the corresponding dataset. This notation applies only to this proof. For any data distribution D, we have</p><formula xml:id="formula_30">E D (f (X), Y ) ? E D Y |D [ (f (x n ), Y )] =E D [ (f (X), Y )] ? E D Y [E D X [ (f (X), Y )]] = ? D X dx y?[K] P(x, y) ln f x [y] + D X dx y?[K] P(x)P(y) ln f x [y] = ? D X dx y?[K] ln f x [y][P(x, y) ? P(x)P(y)].</formula><p>The dynamical analyses are based on the following three assumptions: A1. The model capacity is infinite (i.e., it can realize arbitrary variation). A2. The model is updated using the gradient descent algorithm (i.e. updates follow the direction</p><formula xml:id="formula_31">of decreasing E D [ (f (X), Y )] ? E D Y [E D X [ (f (X), Y )]]).</formula><p>A3. The derivative of network function ?f (x;w) ?wi is smooth (i.e. the network function has no singular point), where w i 's are model parameters.</p><p>Denote the variations of f x [y] during one gradient descent update by ? y (x). From Lemma 1, it can be explicitly written as</p><formula xml:id="formula_32">? y (x) = f x [y] ? ? D X dx y ?[K] [P(x , y ) ? P(x )P(y )] i?[K] G i (x, y)G i (x , y ),<label>(9)</label></formula><p>where ? is the learning rate,</p><formula xml:id="formula_33">G i (x, y) = ? ?g y (x) ?w i + y ?[K] f x [y ] ?g y (x) ?w i ,</formula><p>and g y (x) is the network output before the softmax activation. i.e. .</p><p>With ? y (x), the variation of the regularized loss is</p><formula xml:id="formula_34">?E D [ (f (X), Y ) + CR ] = ? D X dx P(x) y?[K] ? y (x) P(y|x) ? P(y) f x [y] .<label>(10)</label></formula><p>If the training reaches a steady state (a.k.a. local optimum), we have ?E D [ (f (X), Y ) + CR ] = 0.</p><p>To check the property of this variation, consider the following example. For a particular x 0 , define</p><formula xml:id="formula_35">F (x 0 ) := y?[K] ? y (x 0 ) P(y|x 0 ) ? P(y) f x0 [y] .</formula><p>Split the labels y into the following two sets (without loss of generality, we ignore the P(y|x 0 ) ? P(y) = 0 cases): Let B (x 0 ) be a -neighbourhood of x 0 . Since f x [y] is continuous, we can set ? y (x) = 1 2 (1 + cos ? x?x0 )? y (x 0 ), ?x ? B (x 0 ) and 0 otherwise. The coefficient 1 2 (1 + cos ? x?x0 ) is added so that the continuity of f x [y] preserves. This choice will lead to ?E D [ (f (X), Y ) + CR ] &lt; 0. Therefore, for any CA (f (x n ), y n ) with solution f xn [i] &gt; 0, ?i ? [K], we can always find a decreasing direction, indicating the solution is not (steady) locally optimal. Note D can be any distribution in this proof. Thus the result holds for the noisy distribution D.</p><formula xml:id="formula_36">Lemma 1. ? y (x) = f x [y] ? ? D X dx y ?[K] [P(x , y ) ? P(x )P(y )] i?[K] G i (x, y)G i (x , y ).</formula><p>Proof. We need to take into account the actual form of activation function, i.e., the softmax function, as well as the SGD algorithm to demonstrate the correctness of this lemma. The variation ? y0 (x 0 ) is caused by the change in network parameters {w i }, i.e.,</p><formula xml:id="formula_37">? y0 (x 0 ) = i?[K] ?f x0 [y 0 ] ?w i ?w i ,<label>(11)</label></formula><p>where ?w i are determined by the SGD algorithm</p><formula xml:id="formula_38">?w i = ? ? ?E D [ (f (X), Y ) + CR ] ?w i =? x,y P(x, y) ? P(x)P(y) f x [y] ?f x [y] ?w i .</formula><p>Plugging back to (11) yields</p><formula xml:id="formula_39">? y0 (x 0 ) = ? x,y P(x, y) ? P(x)P(y) f x [y] i?[K] ?f x0 [y 0 ] ?w i ?f x [y] ?w i .</formula><p>To proceed, we need to expand ?fx[y] ?wi . Taking into account the activation function, one has</p><formula xml:id="formula_40">f x [y] = exp(g y (x)) y ?[K] exp(g y (x))</formula><p>, where g y (x) refers to the network output before passed to the activation function. Recall that, by our assumption, derivatives ?f (x;w) ?wi are not singular. Now we have</p><formula xml:id="formula_41">?f x [y] ?w i = ?e ?gy(x) ?w i 1 y ?[K] e ?g y (x) + e ?gy(x) ? ?w i 1 y ?[K] e ?g y (x) = ?e ?gy(x) y ?[K] e ?g y (x) ?g y (x) ?w i + e ?gy(x) y ?[K] e ?g y (x) 2 y ?[K] e ?g y (x) ?g y (x) ?w i =f x [y] ? ? ? ?g y (x) ?w i + y ?[K] f x [y ] ?g y (x) ?w i ? ? .</formula><p>For simplicity, we can rewrite the above result as</p><formula xml:id="formula_42">?f x [y] ?w i = f x [y]G i (x, y), where G i (x, y) := ? ?g y (x) ?w i + y f x [y ] ?g y (x) ?w i</formula><p>is a smooth function.</p><p>Combining all the above gives ? y0 (x 0 ) as follows.</p><formula xml:id="formula_43">? y0 (x 0 ) = f x0 [y 0 ] ? ? x,y [P(x, y) ? P(x)P(y)] i G i (x 0 , y 0 )G i (x, y) B.3 PROOF FOR THEOREM 2</formula><p>Theorem 2. The sample sieve defined in (4) ensures that clean examples (x n ,? n = y n ) will not be identified as being corrupted if the model f (t) 's prediction on x n is better than random guess.</p><p>Proof. Let y n be the true label corresponding to feature x n . For a clean sample, we have? n = y n .</p><p>Consider an arbitrary DNN model f . With the CE loss, we have (f (x n ), y n ) = ? ln(f xn [y n ]). According to Equation (4) in the paper, the necessary and sufficient condition of v n &gt; 0 is</p><formula xml:id="formula_44">(f (x n ),? n ) + CR (f (x n )) &lt; ? n ? ? ln(f xn [y n ]) &lt; ? 1 K y?[K] ln(f xn [y]) ? ? ln(f xn [y n ]) &lt; ? 1 K ? 1 y?[K],y =yn ln(f xn [y]).</formula><p>By Jensen's inequality we have</p><formula xml:id="formula_45">? ln 1 ? f xn [y n ] K ? 1 = ? ln y?[K],y =yn f xn [y] K ? 1 ? ? 1 K ? 1 y?[K],y =yn ln(f xn [y]).</formula><p>Therefore, when (sufficient condition)</p><formula xml:id="formula_46">? ln(f xn [y n ]) &lt; ? ln 1 ? f xn [y n ] K ? 1 ? f xn [y n ] &gt; 1 K ,</formula><p>we have v n &gt; 0. Inequality f xn [y n ] &gt; 1 K indicates the model prediction is better than random guess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 PROOF FOR THEOREM 4</head><p>Before proving Theorem 4, we need to show the effect of adding Term-2 to Term-1 in (5). Let X &lt; 0.5 be the measure of separation among classes w.r.t feature X in distribution D, i.e., P(Y = Y * |X) = 1 ? X , (X, Y ) ? D, where Y * := arg max i?[K] P(Y = i|X) is the Bayes optimal label. Let D be the shifted distribution by adding Term-2 to Term-1 and Y be the shifted label. Then P(X|Y ) = P(X|Y ), ?(X, Y ) ? D, (X, Y ) ? D but P(Y ) may be different from P(Y ). Lemma 2 shows the invariant property of this label shift.</p><p>Lemma 2. Label shift does not change the Bayes optimal label of feature X when X &lt; min ?i,j?[K] Tjj Tii+Tjj .</p><p>Proof. Consider the shifted distribution D . Let</p><formula xml:id="formula_47">T E D [ (f (X), Y )] +?E D? [ (f (X), Y )] = CE D [ (f (X), Y )], where E D [ (f (X), Y )] := j?[K] P(Y = j)E D |Y =j [ (f (X), j)],</formula><p>and</p><formula xml:id="formula_48">P(Y = j) := T jj P(Y = j) C ,</formula><p>where C := j?[K] T jj P(Y = j) is a constant for normalization. For each possible Y = i, we have P(Y = i|X) ? [0, X ] ? {1 ? X }, X &lt; 0.5. Thus</p><formula xml:id="formula_49">P(X|Y = i) = P(Y = i|X)P(X) P(Y = i) ? [0, X P(X) P(Y = i) ] ? { P(X)(1 ? X ) P(Y = i) }.</formula><p>Compare D and D, we know there is a label shift <ref type="bibr" target="#b1">(Alexandari et al., 2020;</ref><ref type="bibr" target="#b33">Storkey, 2009)</ref>, where P(X|Y = i) = P(X|Y = i) but P(Y ) and P(Y ) may be different. To ensure the label shift does not change the Bayes optimal label, we need</p><formula xml:id="formula_50">Y * = arg max i?[K] P(Y = i|X) = arg max i?[K] P(X|Y = i)P(Y = i) P(X) , (X, Y ) ? D.</formula><p>One sufficient condition is</p><formula xml:id="formula_51">X P(Y = i) P(Y = i) &lt; (1 ? X )P(Y = j) P(Y = j) ? X &lt; min ?i,j?[K]</formula><p>T jj T ii + T jj With Lemma 2, Assumption 1, and Assumption 2, we present the proof for Theorem 4 as follows.</p><p>Theorem 4. (Robustness of the Confidence Regularized CE Loss) With Assumption 1 and 2, when</p><formula xml:id="formula_52">max i,j?[K],X?D X U ij (X) P( Y = j) ? ? ? min P( Y =i)&gt;P( Y =j),X?D X T ii (X) ? T ij (X) P( Y = i) ? P( Y = j)</formula><p>,</p><formula xml:id="formula_53">minimizing E D [ (f (X), Y ) + CR (f (X))] is equivalent to minimizing E D [ (f (X), Y )].</formula><p>Proof. It is easy to check X = 0, ?X ? D X when Assumption 1 holds. Thus adding Term-2 to Term-1 in (5) does not change the Bayes optimal label. With Assumption 1, the Bayes optimal classifier on the clean distribution should satisfy f * (X)</p><formula xml:id="formula_54">[Y ] = 1, ?(X, Y ) ? D. On one hand, when ? ? max i,j?[K],X?D X U ij (X)/P( Y = j), we have ? ij (X) := U ij (X) ? ?P( Y = j) ? 0, ?i, j ? [K], X ? D X .</formula><p>In this case, minimizing the regularization term results in confident predictions. On the other hand, to make it unbiased to clean results, ? could not be arbitrarily large. We need to find the upper bound on ? such that f * also minimizes the loss defined in the latter regularization term. Assume there is no loss on confident true predictions and there is one miss-prediction on example (x n , y n = j 1 ), i.e., the prediction changes from the Bayes optimal prediction f xn [j 1 ] = 1 to f xn [j 2 ] = 1, j 2 = j 1 . Compared to the optimal one, the first two terms in the right side of (5) is increased by T j1,j1 0 , where 0 &gt; 0 is the regret of one confident wrong prediction. Accordingly, the last term in the right side of (5) is increased by (? j1,j1 (X) ? ? j1,j2 (X)) 0 . It is supposed that</p><formula xml:id="formula_55">T j1,j1 0 + (? j1,j1 (x n ) ? ? j1,j2 (x n )) 0 ? 0, ?j 1 , j 2 ? [K],</formula><p>which is equivalent to</p><formula xml:id="formula_56">?(P( Y = j 1 ) ? P( Y = j 2 )) ? T j1,j1 (x n ) ? T j1,j2 (x n ), ?j 1 , j 2 ? [K]. Thus ? ? min P( Y =j1)&gt;P( Y =j2),X?D X T j1,j1 (X) ? T j1,j2 (X) P( Y = j 1 ) ? P( Y = j 2 )</formula><p>.</p><p>By mathematical inductions, it can be generalized to the case with multiple miss-predictions in the CE term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C OTHER JUSTIFICATIONS</head><p>In this section, we first compare CR and entropy regularization in Section C.1 and highlight our superiority with both theoretical and experimental evidence, then show an example for explaining the variances incurred by label noise in Section C.2, and provide the risk bound in Section C.3 for training with the sieved examples that satisfy Corollary 1. For simplicity, we consider two-class classification problem. Suppose for a given feature x, the probability of x belonging to class 1 is p. The entropy regularization (ER) can be written as:</p><formula xml:id="formula_57">R ER (p) = ?(p ln p + (1 ? p) ln(1 ? p)),<label>(12)</label></formula><p>while our regularization term is written as:</p><formula xml:id="formula_58">R CR (p) = ln p + ln(1 ? p).<label>(13)</label></formula><p>We have the following proposition:</p><p>Proposition 1. CR regularizes models stronger than the entropy regularization in terms of gradients.</p><p>Proof. First notice that both R ER and R CR are symmetric functions around p = 0.5. Thus we can only consider the situation where 0 &lt; p &lt; 0.5. The gradients w.r.t p are:</p><formula xml:id="formula_59">?R ER (p) ?p = ?(ln p ? ln(1 ? p)) = ln( 1 p ? 1), and ?R CR (p) ?p = 1 p ? 1 1 ? p .</formula><p>Now we compare the absolute value of two gradients. When 0 &lt; p &lt; 0.5, it is easy to check</p><formula xml:id="formula_60">?R ER (p) ?p = ln( 1 p ? 1) &lt; 1 p ? 2 &lt; 1 p ? 1 1 ? p = ?R CR (p) ?p ,</formula><p>and both gradients are larger than 0. Therefore, CR has larger gradients than the entropy regularization, i.e., CR has stronger regularization ability than ER.</p><p>We can also draw a figure to show this phenomenon. <ref type="figure" target="#fig_7">Figure 4</ref> shows the value of R CR and R ER with respect to p. We can see the gradient of our regularization is larger than entropy regularization, resulting in a more confident prediction. We also perform an experiment to further show the evidence. <ref type="table" target="#tab_9">Table 4</ref> records comparison results which show our regularization achieves higher accuracy compared to the entropy term.</p><formula xml:id="formula_61">C.2 CALCULATING var D ( (f * D (X), Y )) AND var D [ (f * D (X), Y ) + CR (f * D (X))]</formula><p>Consider optimal classifier f *  </p><formula xml:id="formula_62">For var D [ (f * D (X), Y ) + CR (f * D (X))]</formula><p>, we know the loss (f * D (x n ),? = y n ) = min , and the loss</p><formula xml:id="formula_63">(f * D (x n ),? = y n ) = max . Note CR = (K ? 1) max + min K for each example. The expectation is E D [ (f * D (X), Y ) + CR (f * D (X))] = ? max + (1 ? ?) min + CR . Thus the variance is var D [ (f * D (X), Y ) + CR (f * D (X))] =?( max + CR ? (? max + (1 ? ?) min + CR )) 2 + (1 ? ?)( min + CR ? (? max + (1 ? ?) min + CR )) 2 =?(1 ? ?)( max ? min ) 2 .</formula><p>We know in this example,</p><formula xml:id="formula_64">var D [ (f * D (X), Y ) + CR (f * D (X))] = ?(1 ? ?)( max ? min ) 2 var D ( (f * D (X), Y )) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 ANALYSIS FOR THE RISK BOUND</head><p>Let D L * and D L * be the set and the distribution of the sieved clean examples according to Corollary 1. We know they are supposed to contain only clean examples.</p><formula xml:id="formula_65">Define R D (f ) := E D [ (f (X), Y )], f * D := arg min f R D (f ), R D L * ,? (f ) := 1 |L * | n?L * [?(x n ) (f (x n ),? n )], f D L * ,? := arg min f ?F R D L * ,? (f ), where ?(X) := P D (X)/P D L * (X) stands for the importance of each example to correct sample bias such that R D (f ) = E D L * [?(X) (f (X), Y )].</formula><p>The weight ?(X) can be estimated by kernel mean matching <ref type="bibr" target="#b13">(Huang et al., 2007)</ref> and its DNN adaption <ref type="bibr" target="#b8">(Fang et al., 2020)</ref>. Let D L * ,X be the marginal distribution of D L * on X. For example, with a particular kernel ?(X), the optimization problem is:</p><formula xml:id="formula_66">min ?(X) E D X [?(X)] ? E D L * ,X [?(X)?(X)] s.t.</formula><p>?(X) &gt; 0 and E D L * ,X [?(X)] = 1.</p><p>Note the selection of kernel ?(?) is non-trivial, especially for complicated features. See <ref type="bibr" target="#b8">(Fang et al., 2020)</ref> for a detailed DNN solutions.</p><p>Corollary 2 provides a risk bound for minimizing CE after sample sieve.</p><p>Corollary 2. If ? ? is [0, b]-valued, then for any ? &gt; 0, with probability at least 1 ? ?, we have</p><formula xml:id="formula_67">R D (f D L * ,? ) ? R D (f * D ) ? 2R(? ? ? F) + 2b log(1/?) 2|L * | , where the Rademacher complexity R(? ? ? F) := E D L * ,? [sup f ?F 2 |L * | n?L * ? n ?(x n ) (f (x n ),? n )]</formula><p>and {? n?L * } are independent Rademacher variables.</p><p>Proof. The sieved clean examples may be biased due to the covariate shift caused by instancebased label noise. One solution to such shift is re-weighting D L * to match D using importance re-weighting. Particularly, we need to estimate parameters ?(X) such that</p><formula xml:id="formula_68">R D (f ) = R D L * ,? (f ) := E D L * [?(X) (f (X), Y )].</formula><p>With the optimal ?(X), the ERM should be changed a?</p><formula xml:id="formula_69">f D L * ,? := arg min f ?F R D L * ,? (f ), where R D L * ,? (f ) := 1 |L * | n?L * [?(x n ) (f (x n ),? n )].</formula><p>Via Hoeffding's inequality, ?f , w.p. at least 1 ? ?, we have</p><formula xml:id="formula_70">| R D L * ,? (f ) ? R D L * ,? (f )| ? R( ? F) + 2b ln(1/?) 2|L * | .</formula><p>Following the basic Rademacher bound <ref type="bibr" target="#b4">(Bartlett &amp; Mendelson, 2002)</ref> on the maximal deviation between the expected empirical risks:</p><formula xml:id="formula_71">R D (f D L * ,? ) ? R D (f * D ) =R D L * ,? (f D L * ,? ) ? R D L * ,? (f * D L * ,? ) = R D L * ,? (f D L * ,? ) ? R D L * ,? (f * D L * ,? ) + R D L * ,? (f D L * ,? ) ? R D L * ,? (f D L * ,? ) + R D L * ,? (f * D L * ,? ) ? R D L * ,? (f * D L * ,? ) ?0 + 2 max f ?F | R D L * ,? (f ) ? R D L * ,? (f )| ?2R(? ? ? F) + 2b ln(1/?) 2|L * | , where the Rademacher complexity R(? ? ? F) := E D L * ,? [sup f ?F 2 |L * | n?L * ? n ?(x n ) (f (x n ),? n )]</formula><p>and {? n?L * } are independent Rademacher variables. Therefore, we get Corollary 2.</p><p>Corollary 2 informs us that, theoretically, the sample sieve is biased and ?(X) is necessary to correct the selection bias. However, the error induced by estimating ?(X) may degrade the performance. In addition, it is easy to check the optimal solution of performing direct ERM on the sieved clean examples is the same as f * D in expectation when Assumption 1 holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE DETAILS AND RESULTS FOR EXPERIMENTS</head><p>We firstly show our training framework in Section D.1, then show implementation details and discussions in Section D.2. The algorithm for generating the instance-dependent label noise is provided in Section D.3. We show more experiments in Section D.4 and the ablation study in Section D.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 ILLUSTRATION OF THE TRAINING FRAMEWORK</head><p>Our experiments follows the framework shown in <ref type="figure" target="#fig_8">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration-t</head><p>Model Update</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample sieve</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency training</head><p>Remove Label</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epoch-t</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Data Augmentation</head><p>Low Loss High Loss </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 IMPLEMENTATION DETAILS AND MORE ANALYSIS</head><p>Implementation details on CIFAR-10 and CIFAR-100 with instance-based label noise: The basic hyper-parameters settings for CIFAR-10 and CIFAR-100 are listed as follows: mini-batch size (64), optimizer (SGD), initial learning rate (0.1), momentum (0.9), weight decay (0.0005), number of epochs (100) and learning rate decay (0.1 at 50 epochs). Standard data augmentation is applied to each dataset. CORES 2 and baseline share the same hyper-parameters setting except for ? and ? in equation 2. When perform CORES 2 , We first train network on the dataset for 10 warm-up epochs with only CE (Cross Entropy) loss. Then ? is linearly increased from 0 to 2 for next 30 epochs and kept as 2 for the rest of the epochs. The data selection is performed at the 30 epoch and ? n,t is set to 1 K ??[K] (f (t) (x n ),?) + CR (f (t) (x n )) in epoch-t as the paper suggests.</p><p>When performing CORES 2 , we used the sieved result at epoch-40. It is worth noting that at that time, the sample sieve may not reach the highest test accuracy. However, the division property brought by the confidence regularizer works well at that time. We use the default setting from UDA <ref type="bibr" target="#b43">(Xie et al., 2019)</ref> to apply efficient data augmentation.</p><p>Implementation details on Clothing-1M: We train the network for 120 epochs on 1 million noisy training images. Batch-size is set to 32. The initial learning rate is set as 0.01 and reduced by a factor of 10 at 30, 60, 90 epochs. For each epoch, we sample 1000 mini-batches from the training data while ensuring the (noisy) labels are balanced. Mixup strategy is employed to further avoid the overfitting problem <ref type="bibr" target="#b50">(Zhang et al., 2018;</ref><ref type="bibr" target="#b19">Li et al., 2020)</ref>. ? is set to 0 at first 80 epochs, and linearly increased to 0.4 for next 20 epochs and kept as 0.4 for the rest of the epochs. It is worth noting that Clothing-1M actually does not satisfy our Assumption 2 since the class "Knitwear" (denoted by class-i) and the class "Sweater" (denoted by class-j) can not satisfy T ii (X) ? T ij (X) &gt; T ii ? T jj . Note consistency training is not implemented on Clothing-1M.</p><p>More analysis on ?: The value of ? mainly affects the sample sieve in CORES 2 . From Theorem 3 and Theorem 4 in the paper, when ? is set to be small, we do not have the good division property. When ? is set to be large, the training is biased to the CE term. <ref type="figure" target="#fig_9">Figure 6</ref>  2: Sample instance flip rates q n from the truncated normal distribution N (?, 0.1 2 , [0, 1]); 3: Sample W ? R S?K from the standard normal distribution N (0, 1 2 ); for n = 1 to N do 4: p = x n ? W // Generate instance dependent flip rates. The size of p is 1 ? K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>p yn = ?? // Only consider entries different from the true label 6: p = q n ? softmax(p) // Let qn be the probability of getting a wrong label 7: p yn = 1 ? q n // Keep clean w.p. 1 ? qn 8:</p><p>Randomly choose a label from the label space as noisy label? n according to p; end for Output: 9: Noisy examples (x i ,? n ) N n=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 GENERATING THE INSTANCE-DEPENDENT LABEL NOISE</head><p>In this section, we introduce how to generate instance-based label noise which is illustrated in Algorithm 1. Note this algorithm follows the state-of-the-art method <ref type="bibr" target="#b41">(Xia et al., 2020)</ref>. Define the noise rate (the global flipping rate) as ?. First, in order to control ? but without constraining all of the instances to have a same flip rate, we sample their flip rates from a truncated normal distribution N(?, 0.1 2 , [0, 1]), where [0, 1] indicates the range of the truncated normal distribution. Second, we sample parameters W from the standard normal distribution for generating instance-dependent label noise. The size of W is S ? K, where S denotes the length of each feature. For each instance (x n , y n ), we use Step 5 and Step 6 to ensure that the probability of getting a wrong label is q n .</p><p>Step 7 ensures the sum of all the entries of p is 1.</p><p>Suppose there are two features: x i and x j where x i = x j . Then the possibility p of these two features, calculated by x ? W , from the Algorithm 1, would be exactly the same. Thus the label noise is strongly instance-dependent.  <ref type="bibr" target="#b31">(Reed et al., 2014)</ref> 82.9 58.4 Forward T <ref type="bibr" target="#b30">(Patrini et al., 2017)</ref> 83.1 59.4 Co-teaching+ <ref type="bibr" target="#b48">(Yu et al., 2019)</ref> 88.2 84.1 Mixup <ref type="bibr" target="#b50">(Zhang et al., 2018)</ref> 92.3 77.6 P-correction <ref type="bibr" target="#b47">(Yi &amp; Wu, 2019)</ref> 92.0 88.7 Meta-Learning <ref type="bibr" target="#b18">(Li et al., 2019)</ref> 92.0 88.8 M-correction <ref type="bibr">(Arazo et al., 2019) 93.8 91.9</ref> DivideMix <ref type="bibr" target="#b19">(Li et al., 2020)</ref> 95.7 94.4 CORES 2 95.9 94.5  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 MORE EXPERIMENTS ON CIFAR-10 AND TINY-IMAGENET</head><p>In this section, we compare CORES 2 with more methods on CIFAR-10 and Tiny-Imagenet. <ref type="table" target="#tab_10">Table  5</ref> records the comparison results with recent benchmark methods. <ref type="table" target="#tab_11">Table 6</ref> compares CORES 2 with other methods on Tiny-ImageNet. Both tables show that CORES 2 achieves competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 ABLATION STUDY</head><p>CORES 2 (without consistency training): By optimizing loss in (2), the model can be forced to concentrate only on clean examples. Thus even without consistency training, the network trained by CORES 2 is also noise-robust. <ref type="table" target="#tab_12">Table 7</ref> compares CORES 2 with other noise-robust methods which do not apply semi-supervised setting in the framework. We can see CORES 2 still achieves the best performance among all the methods.</p><p>CORES 2 without confidence regularization or dynamic data selection: The loss in equation 2 consists of data selection strategy and confident regularization term. To see how they influence the final accuracy, we perform the ablation study to show their effect on <ref type="table" target="#tab_13">Table 8</ref>. The first row of <ref type="table" target="#tab_13">Table  8</ref> corresponds to the traditional CE loss. The second row corresponds to the sample sieve with CE loss. The third row is the typical CORES 2 . The last row is CORES 2 . We can see both the dynamic sample sieve in (4) and the confidence-regularized model update in <ref type="formula">(3)</ref> show positive effects on the final accuracy, which suggests the rationality of CORES 2 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Dynamic sample sieves. Green circles are clean examples. Red hexagons are corrupted examples. The crucial components in (2) are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, we visualize the superiority of our sample sieve with numerical results as Figure 2. The sieved dataset is in the form of two clusters of examples. Particularly, from Figure 2(b) and Figure 2(f), we observe that CE suffers from providing a good division of clean and corrupted examples due to overfitting in the final stage of training. On the other hand, with CR , there are two distinct clusters and can be separated by the threshold 0 as shown in Figure 2(d) and Figure 2(h). Comparing Figure 2(a)-2(d) with Figure 2(e)-2(h), we find the effect of instance-dependent noise on training is indeed different from the symmetric one, where the instance-dependent noise is more likely to cause overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>F-score comparisons on CIFAR10 under symmetric (Symm.) and instance-based (Inst.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>f x [y] = exp(g y (x)) y ?[K] exp(g y (x))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Y x0;? = {y : P(y|x 0 ) ? P(y) &lt; 0} and Y x0;+ = {y : P(y|x 0 ) ? P(y) &gt; 0}. By assigning ? y (x 0 ) = a y &lt; 0, ?y ? Y x0;? and ? y (x 0 ) = b y &gt; 0, ?y ? Y x0;+ , one finds F (x 0 ) &gt; 0 since f x0 [y] &gt; 0. Note we have an extra constraint y ? y (x 0 ) = 0 to ensure y?[K] f x0 [y] = 1 after update. It is easy to check our assigned a y and b y could maintain this constraint by introducing a weight N ab to scale b y as follows. y?Y? a y + N ab y?Y+ b y = 0, b y = N ab b y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>D := arg min f E D [ (f (X), Y )]. Let max be the upper bound of the (?) loss, and min be the lower bound of the (?) loss. Denote ? by the over noise rate (ratio of corrupted examples in all examples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>ForFigure 4 :</head><label>4</label><figDesc>var D ( (f * D (X), Y )), we know the loss (f * D (x n ), y n ) = min for each example. Thus the variance is var D ( (f * D (X), Y )) = 0. Comparing our regularization with entropy regularization .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>One example of CORES 2 . L(t): Indices of sieved clean examples. H(t): Indices of sieved corrupted examples. D L(t) := {(x n ,? n ) : n ? L(t)}, D H(t) := {(x n ,? n ) : n ? H(t)}, D X,H(? ) := {x n : n ? H(? )}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>visualize this phenomenon. It can be seen that in the left and right figure, many clean examples and corrupted examples overlap together located in the left and right clusters, respectively. Analyzing how the value of ? influences the division. We set ? = 0.5, 2, 10 for lower, proper, and higher beta settings, respectively.Algorithm 1 Instance-Dependent Label Noise GenerationInput: 1: Clean examples (x n , y n ) N n=1 ; Noise rate: ?; Size of feature: 1 ? S; Number of classes: K. Iteration:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Theorem 3 explicitly shows the contributions of clean examples, corrupted examples, and CR during training. See Appendix B.1 for the proof.</figDesc><table /><note>Theorem 3. (Main Theorem: Decoupling the Expected Regularized CE Loss) In expectation, the loss with CR can be decoupled as three separate additive terms:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>will achieve 100% precision on both clean and corrupted examples. This guaranteed division is summarized in Corollary 1: Corollary 1. When conditions in Theorem 4 hold, with infinite model capacity and sufficiently many examples, CORES 2 achieves v n = 1(y n =? n ), ?n ? [N ], i.e., all the sieved clean examples are effectively clean.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>in Theorem 4 may not exist. In such case, these instances cannot be properly used and other auxiliary techniques are necessary (e.g., sample pruning).Sieving out the corrupted examples from the clean ones allows us a couple of better solutions. First, we can focus on performing ERM using these sieved clean examples only. We derive the risk bound for training with these clean examples in Appendix C.3. Secondly, leveraging the sample sieve to distinguish clean examples from corrupted ones provides a flexible interface for various robust training techniques such that the performance can be further improved. For example, semisupervised learning techniques can be applied (see section 4 for more details).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Comparison of test accuracies on clean datasets under instance-based label noise. = 0.2 ? = 0.4 ? = 0.6 ? = 0.2 ? = 0.4 ? = 0.6</figDesc><table><row><cell cols="2">Method ? Cross Entropy 87.16</cell><cell>Inst. CIFAR10 75.16</cell><cell>44.64</cell><cell>58.72</cell><cell>Inst. CIFAR100 41.14</cell><cell>25.29</cell></row><row><cell>Forward T (Patrini et al., 2017)</cell><cell>88.08</cell><cell>82.67</cell><cell>41.57</cell><cell>58.95</cell><cell>41.68</cell><cell>22.83</cell></row><row><cell>LDMI (Xu et al., 2019)</cell><cell>88.80</cell><cell>82.70</cell><cell>70.54</cell><cell>58.66</cell><cell>41.77</cell><cell>28.00</cell></row><row><cell>Lq (Zhang &amp; Sabuncu, 2018)</cell><cell>86.45</cell><cell>69.02</cell><cell>32.94</cell><cell>58.18</cell><cell>40.32</cell><cell>23.13</cell></row><row><cell>SCE (Wang et al., 2019)</cell><cell>89.11</cell><cell>72.04</cell><cell>44.83</cell><cell>59.87</cell><cell>41.76</cell><cell>23.41</cell></row><row><cell>Co-teaching (Han et al., 2018)</cell><cell>88.66</cell><cell>69.50</cell><cell>34.61</cell><cell>43.03</cell><cell>23.13</cell><cell>7.07</cell></row><row><cell>Co-teaching+ (Yu et al., 2019)</cell><cell>89.04</cell><cell>69.15</cell><cell>33.33</cell><cell>41.84</cell><cell>24.40</cell><cell>8.74</cell></row><row><cell>JoCoR (Wei et al., 2020)</cell><cell>88.71</cell><cell>68.97</cell><cell>30.27</cell><cell>44.28</cell><cell>22.77</cell><cell>7.54</cell></row><row><cell>Peer Loss (Liu &amp; Guo, 2020)</cell><cell>89.33</cell><cell>81.09</cell><cell>73.73</cell><cell>59.92</cell><cell>45.76</cell><cell>33.61</cell></row><row><cell>CORES 2</cell><cell>89.50</cell><cell>82.84</cell><cell>79.66</cell><cell>61.25</cell><cell>47.81</cell><cell>37.85</cell></row><row><cell>CORES 2</cell><cell>95.42</cell><cell>88.45</cell><cell>85.53</cell><cell>72.91</cell><cell>70.66</cell><cell>63.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Comparison of test accuracies on clean datasets under symmetric/asymmetric label noise.</figDesc><table><row><cell>Method</cell><cell cols="8">Symm. CIFAR10 ? = 0.4 ? = 0.6 ? = 0.2 ? = 0.3 ? = 0.4 ? = 0.6 ? = 0.2 ? = 0.3 Asymm. CIFAR10 Symm. CIFAR100 Asymm. CIFAR100</cell></row><row><cell>Cross Entropy</cell><cell>81.88</cell><cell>74.14</cell><cell>88.59</cell><cell>86.14</cell><cell>48.20</cell><cell>37.41</cell><cell>59.20</cell><cell>51.40</cell></row><row><cell>MAE (Ghosh et al., 2017)</cell><cell>61.63</cell><cell>41.98</cell><cell>59.67</cell><cell>57.62</cell><cell>7.68</cell><cell>6.45</cell><cell>11.16</cell><cell>8.97</cell></row><row><cell>Forward T (Patrini et al., 2017)</cell><cell>83.27</cell><cell>75.34</cell><cell>89.42</cell><cell>88.25</cell><cell>53.04</cell><cell>41.59</cell><cell>64.86</cell><cell>64.72</cell></row><row><cell>Lq (Zhang &amp; Sabuncu, 2018)</cell><cell>87.13</cell><cell>82.54</cell><cell>89.33</cell><cell>85.45</cell><cell>61.77</cell><cell>53.16</cell><cell>66.59</cell><cell>61.45</cell></row><row><cell>LDMI (Xu et al., 2019)</cell><cell>83.04</cell><cell>76.51</cell><cell>89.04</cell><cell>87.88</cell><cell>52.32</cell><cell>40.00</cell><cell>60.04</cell><cell>52.82</cell></row><row><cell>NLNL (Kim et al., 2019)</cell><cell>92.43</cell><cell>88.32</cell><cell>93.35</cell><cell>91.80</cell><cell>66.39</cell><cell>56.51</cell><cell>63.12</cell><cell>54.87</cell></row><row><cell>SELF (Nguyen et al., 2019)</cell><cell>91.13</cell><cell>-</cell><cell>93.75</cell><cell>92.42</cell><cell>66.71</cell><cell>-</cell><cell>70.53</cell><cell>65.09</cell></row><row><cell>CORES 2</cell><cell>93.76</cell><cell>89.78</cell><cell>95.18</cell><cell>94.67</cell><cell>72.22</cell><cell>59.16</cell><cell>75.19</cell><cell>73.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The best epoch (clean) test accuracy for each method on Clothing1M.<ref type="bibr" target="#b30">Patrini et al., 2017)</ref> <ref type="bibr" target="#b11">(Han et al., 2018)</ref> <ref type="bibr" target="#b38">(Wei et al., 2020)</ref> <ref type="bibr" target="#b44">(Xu et al., 2019)</ref> (Xia et al., 2020) (our)   </figDesc><table><row><cell>CE (Baseline) (Acc. Method 68.94</cell><cell>Forward T 70.83</cell><cell>Co-teaching 69.21</cell><cell>JoCoR 70.30</cell><cell>LDMI 72.46</cell><cell>PTD-R-V 71.67</cell><cell>CORES 2 73.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In Advances in neural information processing systems, pp. 8778-8788, 2018. Zizhao Zhang, Han Zhang, Sercan O Arik, Honglak Lee, and Tomas Pfister. Distilling effective supervision from severe label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9294-9303, 2020b.</figDesc><table><row><cell>Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-</cell></row><row><cell>dependent label noise. In The IEEE Conference on Computer Vision and Pattern Recognition</cell></row><row><cell>(CVPR), June 2021a.</cell></row><row><cell>Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when</cell></row><row><cell>learning with noisy labels. arXiv preprint arXiv:2102.05291, 2021b.</cell></row><row><cell>Piotr Zielinski, Shankar Krishnan, and Satrajit Chatterjee. Explaining memorization and general-</cell></row><row><cell>ization: A large-scale study with coherent gradients. arXiv preprint arXiv:2003.07422, 2020.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Comparing CR with ER on CIFAR-10.</figDesc><table><row><cell>Method</cell><cell>0.2</cell><cell>Symm 0.4</cell><cell>0.6</cell><cell>0.1</cell><cell>Asymm 0.2</cell><cell>0.3</cell></row><row><cell>Baseline</cell><cell cols="6">86.98 81.88 74.14 90.69 88.59 86.14</cell></row><row><cell cols="7">Baseline + ER 87.61 83.84 80.55 91.36 89.61 87.47</cell></row><row><cell cols="7">Baseline + CR 90.70 88.29 82.10 92.41 91.02 90.53</cell></row><row><cell cols="4">C.1 COMPARING CR WITH ENTROPY REGULARIZATION</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the results reported by DivideMix<ref type="bibr" target="#b19">(Li et al., 2020)</ref> on CIFAR-10. All methods use Pre-ResNet18 as the backbone. The last epoch test accuracy for each method is reported. The noise rate is defined as the probability of replacing the label with other labels including the true label.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Symm 0.2 0.5</cell></row><row><cell></cell><cell>CE</cell><cell>82.7 57.9</cell></row><row><cell>Bootstrap</cell><cell></cell><cell></cell></row><row><cell>CIFAR-10</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The best epoch accuracy for each method on Tiny-ImageNet.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Method</cell><cell cols="2">Symm 0.2 0.5</cell></row><row><cell></cell><cell></cell><cell>MAE (Ghosh et al., 2017)</cell><cell>2.36</cell><cell>1.22</cell></row><row><cell cols="2">Tiny-ImageNet ResNet18</cell><cell cols="3">GCE (Zhang &amp; Sabuncu, 2018) 69.84 66.31 MentorNet (Jiang et al., 2017) 59.12 53.83</cell></row><row><cell></cell><cell></cell><cell>CORES 2</cell><cell cols="2">73.47 71.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Comparing CORES 2 (without consistency training) with other noise-robust methods on CIFAR-10. 81.88 74.14 90.69 88.59 86.14 Forward T (Patrini et al., 2017) 88.11 83.27 75.34 90.11 89.42 88.25 Truncated Lq (Zhang &amp; Sabuncu, 2018) 89.70 87.62 82.70 90.43 89.45 87.10 L DMI (Xu et al., 2019) 88.74 83.04 76.51 90.28 89.04 87.88 CORES 2 (without consistency training) 90.70 88.29 82.10 92.41 91.02 90.53</figDesc><table><row><cell>Method</cell><cell>0.2</cell><cell>Symm 0.4</cell><cell>0.6</cell><cell>0.1</cell><cell>Asymm 0.2</cell><cell>0.3</cell></row><row><cell>Cross Entropy</cell><cell>86.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Analysis of each component of CORES 2 on CIFAR-10. All the methods use ResNet-34. 81.44 74.63 90.18 88.43 87.27 ? ? 90.15 86.98 78.36 91.59 90.89 88.51 ? 90.70 88.29 82.10 92.41 91.02 90.53 95.73 93.76 89.78 96.05 95.18 94.67</figDesc><table><row><cell cols="2">Sample Sieve Data selection Regularization</cell><cell>Consistency training</cell><cell>0.2</cell><cell>Symm 0.4</cell><cell>0.6</cell><cell>0.1</cell><cell>Asymm 0.2</cell><cell>0.3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>86.67</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this paper, the noisy dataset refers to a dataset with noisy examples. A noisy example is either a clean example (whose label is true) or a corrupted example (whose label is wrong).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Detailed conditions for Theorem 1 are specified at the end of our main contents. 4 Our observation can also help partially explain the robustness property of peer loss<ref type="bibr" target="#b24">(Liu &amp; Guo, 2020)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The logarithmic function in CR is adapted to ln(fx[y] + 10 ?8 ) for numerical stability.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning statistical models of phenotypes using noisy labeled training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Podchiyska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veena</forename><surname>Banda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiffany</forename><forename type="middle">I</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">P</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">E</forename><surname>Minty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elsie</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1166" to="1173" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum likelihood with biascorrected calibration is hard-to-beat at label shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Alexandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML &apos;20</title>
		<meeting>the 37th International Conference on Machine Learning, ICML &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust bi-tempered logistic loss based on bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Manfred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14987" to="14996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11238</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Confidence scores make instance-dependent label-noise learning possible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03772</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coherent gradients: An approach to understanding generalization in gradient descent-based optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satrajit</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML &apos;20</title>
		<meeting>the 37th International Conference on Machine Learning, ICML &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking importance weighting for deep learning under distribution shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decomposition-based evolutionary multiobjective optimization to self-paced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiguang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="288" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJgExaVtwr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The importance of understanding instance-level noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine-learning aided peer prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference on Economics and Computation</title>
		<meeting>the 2017 ACM Conference on Economics and Computation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="63" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Peer loss functions: Learning from noisy labels without knowing noise rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML &apos;20</title>
		<meeting>the 37th International Conference on Machine Learning, ICML &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A bi-level formulation for label noise learning with spectral cluster discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijing</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2605" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<idno type="arXiv">arXiv:1910.01842</idno>
		<title level="m">Self: Learning to filter noisy labels with selfensembling</title>
		<editor>Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00068</idno>
		<title level="m">Confident learning: Estimating uncertainty in dataset labels</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning adaptive loss for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06482</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">When training and test sets are different: characterizing learning transfer. Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5596" to="5605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13726" to="13735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">When optimizing $f$-divergence is robust with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=WesiCoRVQ15" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6838" to="6849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07836</idno>
		<title level="m">Dacheng Tao, and Masashi Sugiyama. Parts-dependent label noise: Towards instance-dependent label noise</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Unsupervised data augmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">L_dmi: An information-theoretic noiserobust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03388</idno>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Searching to exploit memorization effect in learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML &apos;20</title>
		<meeting>the 37th International Conference on Machine Learning, ICML &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dual T: Reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7260" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving crowdsourced label quality using noise correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1675" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-paced robust learning for leveraging clean labels in noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanglan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tien</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6853" to="6860" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
