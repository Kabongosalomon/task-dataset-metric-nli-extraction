<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VISION TRANSFORMERS IN 2022: AN UPDATE ON TINY IMAGENET</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">M</forename><surname>Huynh</surname></persName>
							<email>e2huynh@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VISION TRANSFORMERS IN 2022: AN UPDATE ON TINY IMAGENET</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent advances in image transformers have shown impressive results and have largely closed the gap between traditional CNN architectures. The standard procedure is to train on large datasets like ImageNet-21k and then finetune on ImageNet-1k. After finetuning, researches will often consider the transfer learning performance on smaller datasets such as CIFAR-10/100 but have left out Tiny ImageNet. This paper offers an update on vision transformers' performance on Tiny ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The ViT paper <ref type="bibr" target="#b2">(Dosovitskiy et al., 2020)</ref> showed that transformers can be applied to image classification tasks. However ViT was pretrained on the JFT-300M dataset <ref type="bibr" target="#b12">(Sun et al., 2017)</ref>, Google's internal dataset of 300 million images. Thus the issue of training efficiency and data availability was apparent. DeiT <ref type="bibr" target="#b13">(Touvron et al., 2020)</ref> was a response to that and showed that a way to alleviate the data-hungry nature of transformers was with a rigorous training schedule and knowledge distillation. As such, it became possible to train a vision transformer using ImageNet-21k <ref type="bibr">(Ridnik et al., 2021)</ref> and further finetuning on ImageNet-1k <ref type="bibr" target="#b11">(Russakovsky et al., 2014)</ref>. Subsequent image transformers, like CaiT <ref type="bibr">(Touvron et al., 2021)</ref> and Swin <ref type="bibr" target="#b9">(Liu et al., 2021b)</ref>, closely follow the blueprint laid out by DeiT.</p><p>In addition to ImageNet-1k, these studies perform transfer learning tests on CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b4">(Krizhevsky, 2009</ref>). However, every paper has failed to include Tiny ImageNet <ref type="bibr" target="#b6">(Le &amp; Yang, 2015)</ref>. Tiny ImageNet is a subset of ImageNet-1k with 100,000 images and 200 classes that was first introduced in a computer vision course at Stanford. Since its inception, few papers have used this dataset in their benchmarks.</p><p>That being said, a study has been done by <ref type="bibr">Lee et al. (2021)</ref> where they propose modifications to vision transformers to improve the accuracy training from scratch on Tiny ImageNet. But in reality, transfer learning is a much more common and stronger technique when it comes to accuracy. As such, there is no modern research done to evaluate vision transformers on Tiny ImageNet. This paper address that gap and will report the accuracy of ViT, DeiT, CaiT, and Swin transformer using a training regiment similar to DeiT's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EXPERIMENTAL SETTING</head><p>All vision transformers are taken from the timm library <ref type="bibr" target="#b17">(Wightman, 2019)</ref>. I trained each model using a Nvidia RTX 3070 (8GB memory) and an 8-core CPU. The size of the models were chosen based on being within the neighborhood of 10 to 60 minutes to train per epoch. As a refresher, I report the accuracies of the transformers on ImageNet-1k in  <ref type="table" target="#tab_0">Table 1</ref>: Results of ViT, DeiT, CaiT, and Swin on ImageNet-1k, CIFAR-100, and CIFAR-10. All models are finetuned on 384x384 resolution. The numbers for ImageNet-1k are taken from the timm library and the rest are from the original papers. -D signifies distillation. The authors of the Swin transformer did not report the accuracy on CIFAR-10 and CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DATA AUGMENTATION</head><p>The data augmentation techniques used mainly reflect that of DeiT's. I use Mixup <ref type="bibr" target="#b19">(Zhang et al., 2017)</ref> and Cutmix <ref type="bibr" target="#b18">(Yun et al., 2019)</ref> with a probability of 0.8 and 1.0 respectively, and Random Erasing <ref type="bibr" target="#b21">(Zhong et al., 2017)</ref> with a 0.25 probability. I train using the full image resized to 384x384 resolution with bicubic interpolation in both training and testing, as suggested by <ref type="bibr" target="#b13">Touvron et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">REGULARIZATION &amp; OPTIMIZER</head><p>For regularization, I employ label smoothing with an of 0.1 and a stochastic depth of 0.1 I train each model for 30 epochs, using 128 batch size. With the image resolution being 384x384, 8 gigabytes of video memory is not enough to load both the model and batch into GPU memory. As such, gradient accumulation was required to train with a 128 batch size.</p><p>The optimizer of choice is AdamW at an initial learning rate of 10 ?3 with cosine decay and weight decay of 0.05.  Swin The large Swin transformer achieves state-of-the-art accuracy of 91.35%, beating the previous by 0.33%. Applying a window to multi-headed self-attention (MSA) and a shifted window to MSA proves to be effective. Swin continues to impress among the vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>ViT The ancestor of vision transformers, ViT, falls behind its advancements. The timm library reports that ViT outperforms DeiT by a significant margin on ImageNet-1k and <ref type="bibr" target="#b13">Touvron et al. (2020)</ref> reports that ViT-L performs worse then a distilled DeiT-B so perhaps there is an advancement in training in the timm library I'm not aware of.</p><p>DeiT The base distilled DeiT achieves a respectable accuracy of 87.29% while training the fastest by a large margin. The power of knowledge distillation is evident as it performs better then ViT-L and trains the fastest by a large margin.</p><p>CaiT CaiT has an accuracy of 86.74%. Considering that CaiT-S/36 is based off a DeiT-S, the accuracy is expected. A CaiT-M/36 model would likely surpass the DeiT model but CaiT took the longest to train despite the parameter count and number of FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PARAMETERS AND FLOPS ARE NOT CREATED EQUALLY</head><p>CaiT shows that parameter count and FLOPs are not indicative of model efficiency. Despite CaiT-S/36 having the lowest parameter count and FLOPs, it reports the lowest throughput and trains the slowest, see <ref type="table" target="#tab_4">Table 3</ref>. Instead, there is a trend with layer count and throughput. The size of the embedding is another thing to consider but the number of layers seems to be the main determining factor considering CaiT's small embedding size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TUNING THE TRAINING PROCEDURE</head><p>This section describes the experiments I did with the training procedure setup to reach the final setup described in Section 2 and achieve a final validation accuracy of: 91.35%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HYPERPARAMETERS</head><p>Optimizers For AdamW, I tried a combination of learning rates and weight decay in the range of [3.10 ?3 , 10 ?3 , 7.10 ?4 , 5.10 ?4 ] and [0.2, 0.05, 0.01] respectively. I found the setting of 10 ?3 learning rate and 0.05 weight decay to work the best.</p><p>Perturbed optimizers like SAM <ref type="bibr" target="#b3">(Foret et al., 2020)</ref>, ASAM <ref type="bibr" target="#b5">(Kwon et al., 2021)</ref>, and PUGD <ref type="bibr">(Tseng et al., 2021)</ref> were also considered. Initial testing showed that SAM, ASAM, and PUGD increased the training time of an epoch by 85% while only having accuracies around 60-70% for the first 5 epochs. In comparison, AdamW has 89% accuracy after the first epoch. As a result, I decided not to train with these optimizers.</p><p>SGD was also tested with a learning rate of 10 ?2 , weight decay of 10 ?5 , and momentum of 0.9, with and without nesterov momentum. <ref type="figure" target="#fig_0">Figure 1</ref> shows that SGD converges faster then AdamW, likely due to the fact that I use a higher learning rate for SGD. On the other hand, AdamW is more variable then SGD perhaps because of the adaptive nature of AdamW. Either way, SGD falls short of AdamW in terms of accuracy but nesterov momentum performs better than vanilla momentum (91.21% vs 91.1%).  There are 2 types of cropping I tried, Random Resized Crop (RRC) and Simple Random Crop (SRC) <ref type="bibr" target="#b15">(Touvron et al., 2022)</ref>. RRC is the standard method of cropping where the crop is just a random portion of the image resized to a given resolution. SRC is a square crop along the x-axis which maintains the aspect ratio while having a bigger fraction of the image. The main goal of SRC is that it is more likely to have the label in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATION STUDY</head><p>Model EMA has shown that in some cases it boosts accuracy and in some cases it hurts accuracy. <ref type="bibr" target="#b9">Liu et al. (2021b)</ref> reports that Model EMA does not help but it is still worth trying on a dataset-bydataset basis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper has shown that vision transformers transfer well onto Tiny ImageNet which makes sense as it is a subset of ImageNet-1k. Two standout architectures are DeiT and Swin. DeiT reports a respectable accuracy while training the fastest by a significant margin and Swin achieves state-ofthe-art accuracy, beating the previous by 0.33%. Future work could be done on even more vision transformers. SwinV2 <ref type="bibr" target="#b8">(Liu et al., 2021a)</ref> improves on Swin and scales up Swin using self-supervised learning techniques and post normalization among other things. Another is MiniViT <ref type="bibr" target="#b20">(Zhang et al., 2022)</ref>, who uses a combination of weight sharing and weight distillation to drastically reduce the parameter count and increase accuracy of vision transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Convergence of AdamW vs SGDAutoAugment was a widely-used data augmentation technique before the introduction of RandAugment. Research has since favored the simplicity of RandAugment but AutoAugment is still comparable to RandAugment and is still considered when designing training schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="3">ImageNet-1k CIFAR-100 CIFAR-10</cell></row><row><cell>ViT-L/16 DeiT-B/16-D CaiT-M/36 Swin-L/4</cell><cell>87.08 85.43 86.05 87.15</cell><cell>94.04 91.40 93.10 -</cell><cell>99.38 99.20 99.40 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Analysis of ViT, DeiT, CaiT, and Swin accuracy and training efficiency on Tiny ImageNet. I report the highest validation accuracy obtained during training. Throughput is measured with a batch size of 32. This is largest possible batch size that can fit on all models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of various model size metrics with throughput.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This section describes the effects of removing various data augmentation and regularization techniques as well as trying different ones to determine what the optimal training setup is for Swin on Tiny ImageNet.</figDesc><table /><note>I define the default training setup to be: Random Augment (Cubuk et al., 2019), Random Erasing, Mixup, CutMix, Stochastic Depth, and Label Smoothing. Experimentation was done by simply removing one factor at a time and comparing their performance with the default setting. In addition to those techniques, there were 3 more techniques I experimented with: AutoAugment (Cubuk et al., 2018), image cropping, and Model EMA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 4for detailed results. RandAugment was the only data augmentation technique in the default setting to technically decrease training accuracy. Without RandAugment, the model trains to a 91.35% accuracy which is within margin of error of the default setting. However, the training patterns exhibited were attractive. The model was able to maintain its top accuracy for multiple epochs whereas the default setting peaks at 91.34% and plateaus at around 91.20%. For that reason, I decided to remove RandAugment. The other augmentation and regularization methods proved to be beneficial towards learning.AutoAugment and Model EMA show a decrease in accuracy.<ref type="bibr" target="#b9">Liu et al. (2021b)</ref> mentions that Model EMA does not increase performance, so my findings fall in line with the original paper. Whereas AutoAugment, I did not put in the time to try to optimize the parameters so perhaps with some tuning it could be useful.Both cropping methods aren't effective in this environment. It's likely due to the fact that some form of cropping was utilized during ImageNet-21k pretraining and ImageNet-1k finetuning. At that point, cropping is not required as the model has already been strictly regularized.</figDesc><table><row><cell>4.3 ABLATION RESULTS</cell><cell></cell></row><row><cell>I provide As such, the final training setup is as follows:</cell><cell></cell></row><row><cell>? AdamW with 10 ?3 learning rate and 0.05 weight decay</cell><cell></cell></row><row><cell>? 30 epochs, cosine decay, and 128 batch size</cell><cell></cell></row><row><cell>? Mixup</cell><cell></cell></row><row><cell>? CutMix</cell><cell></cell></row><row><cell>? Random Erasing</cell><cell></cell></row><row><cell>? Stochastic Depth of 0.1</cell><cell></cell></row><row><cell>? Label smoothing of 0.1</cell><cell></cell></row><row><cell>Change</cell><cell>Accuracy</cell></row><row><cell>Default</cell><cell>91.34</cell></row><row><cell>Remove RandAugment</cell><cell>91.35</cell></row><row><cell>Remove Rand-Erasing</cell><cell>91.28</cell></row><row><cell>Add Simple Random Crop</cell><cell>91.26</cell></row><row><cell>Remove Stochastic Depth</cell><cell>91.25</cell></row><row><cell>Remove Label Smoothing</cell><cell>91.25</cell></row><row><cell>Replace RA with AutoAugment</cell><cell>91.25</cell></row><row><cell>Remove Mixup</cell><cell>91.11</cell></row><row><cell>Add Random Resized Crop</cell><cell>91.06</cell></row><row><cell>Remove CutMix</cell><cell>91.04</cell></row><row><cell>Add Model EMA</cell><cell>90.83</cell></row><row><cell cols="2">Table 4: Comparison of the effects of various data augmentation and regularization techniques. RA stands for Rand-Augment.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Practical data augmentation with no separate search. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.13719" />
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
		<ptr target="http://arxiv.org/abs/1805.09501" />
		<title level="m">Learning augmentation policies from</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.01412" />
		<imprint>
			<date type="published" when="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ASAM: adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongseop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunseo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kwon</forename><surname>Choi</surname></persName>
		</author>
		<idno>abs/2102.11600</idno>
		<ptr target="https://arxiv.org/abs/2102.11600" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vision transformer for small-size datasets. CoRR, abs/2112.13492, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Seung Hoon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung Cheol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.13492" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Swin transformer V2: scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2111.09883</idno>
		<ptr target="https://arxiv.org/abs/2111.09883" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2103.14030</idno>
		<ptr target="https://arxiv.org/abs/2103.14030" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.10972" />
		<imprint/>
	</monogr>
	<note>Imagenet-21k pretraining for the masses. CoRR, abs/2104.10972, 2021</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0575" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era. CoRR, abs/1707.02968</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.02968" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2012.12877</idno>
		<ptr target="https://arxiv.org/abs/2012.12877" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers. CoRR, abs/2103.17239, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.17239" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deit iii: Revenge of the vit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07118</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Update in unit gradient. CoRR, abs/2110.00199, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hsun</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu-Hsueh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin-Jye</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Zeng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.00199" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.04899" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>abs/1710.09412</idno>
		<ptr target="http://arxiv.org/abs/1710.09412" />
		<title level="m">mixup: Beyond empirical risk minimization. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Minivit: Compressing vision transformers with weight multiplexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinnian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation. CoRR, abs/1708.04896</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.04896" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
