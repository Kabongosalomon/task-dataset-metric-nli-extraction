<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangzheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of Illinois at Urbana-Champaign ? Temple University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wenpeng.yin@temple.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of Illinois at Urbana-Champaign ? Temple University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
							<email>muhaoche@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ? University of Illinois at Urbana-Champaign ? Temple University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This work presents LITE</head><p>, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types. 1</p><p>Chrysler is a sea bird.</p><p>Hypotheses by FALSE labels 91% Confidence given the premise (entailment score)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15%</head><p>Model Inference NLI Heavy Rain is set to come out on February 23rd , but it is said that there will be a playable demo available on the 11th .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank over</head><p>? Heavy Rain is a game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypotheses by all labels from label space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label words with most confident hypotheses</head><p>Heavy Rain is a dog.</p><p>Chrysler is a corporation.</p><p>Chrysler is a sea bird.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity typing, inferring the semantic types of the entity mentions in text, is a fundamental and long-lasting research problem in natural language understanding, which aims at inferring the semantic types of the entities mentioned in text. The resulted type information can help with grounding human language components to real-world concepts <ref type="bibr" target="#b1">(Chandu et al., 2021)</ref>, and provide valuable prior knowledge for natural language understanding tasks such as entity linking <ref type="bibr" target="#b18">(Ling et al., 2015;</ref><ref type="bibr" target="#b26">Onoe and Durrett, 2020</ref>), question answering <ref type="bibr" target="#b42">(Yavuz et al., 2016)</ref>, and information extraction <ref type="bibr" target="#b17">(Koch et al., 2014)</ref>. Prior studies have mainly formulated the task as a multi-way classification problems <ref type="bibr" target="#b8">(Wang et al., 2021;</ref><ref type="bibr" target="#b46">Zhang et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020a;</ref><ref type="bibr" target="#b15">Hu et al., 2020)</ref>.</p><p>However, earlier efforts for entity typing are far from enough for representing real-world scenarios, where types of entities can be extremely diverse. Accordingly, the community has recently paid much attention to more fine-grained modeling of types for entities. One representative work is the Ultra-fine Entity Typing (UFET) benchmark created by <ref type="bibr" target="#b5">Choi et al. (2018)</ref>. The task seeks to search for the most appropriate types for an entity among over ten thousand free-form type candidates. The drastic increase of types enforces us to doubt if the multi-way classification framework is still suitable for UFET. In this context, two main issues are noticed from prior work. First, prior studies have not tried to understand the target types since most classification systems converted all types into indices. Without knowing the semantics of types, it is hard to match an entity mention to a correct type especially when there is not sufficient annotated data for each type. Second, existing entity typing systems are far behind the desired capability in real-world applications in which any open-form types can appear. Specifically, those pre-trained multi-way classifiers cannot recognize types that are unseen in training, especially when there is no reasonable mapping from existing types to unseen type labels, unless the classifiers are re- To alleviate the aforementioned challenges, we propose a new learning framework that seeks to enhance ultra-fine entity typing with indirect supervision from natural language inference (NLI) <ref type="bibr" target="#b6">(Dagan et al., 2006)</ref>. Specifically, our method LITE , namely (Language Inference based Typing of Entities), treats each entity-mentioning sentence as a premise in NLI. Using simple, template-based generation techniques, a candidate type is transformed into a textual description and is treated as the hypothesis in NLI. Based on the premise sentence and a hypothesis description of a candidate type, the entailment score given by an NLI model is regarded as the confidence of the type. On top of the pre-trained NLI model, LITE conducts a learning-to-rank objective, which aims at scoring hypotheses of positive types higher than the hypotheses of sampled negative types. Finally, the label candidates whose hypotheses obtain scores above a threshold are given as predictions by the model. Technically, LITE benefits ultra-fine entity typing from three perspectives. First, the inference ability of a pre-trained NLI model can provide effective indirect supervision to improve the prediction of type information. Second the hypothesis, as a type description, also provides a semantically rich representation of the type, which further benefits few-shot learning with insufficient labeled data. Moreover, to handle the dependency of type labels in different granularities, we also utilize the inference ability of NLI model to learn that the finer label hypothesis of an entity mention entails its general label hypothesis. Experimental results on the UFET benchmark <ref type="bibr" target="#b5">(Choi et al., 2018)</ref> show that LITE drastically outperforms the recent state-of-the-art (SOTA) systems <ref type="bibr" target="#b8">(Dai et al., 2021;</ref><ref type="bibr" target="#b24">Onoe et al., 2021;</ref> without any need of distantly supervised data as they do. In addition, our LITE also yields the best performance on traditional (less) fine-grained entity typing tasks. 2 What's more, since we adopt a learning-to-rank objective to optimize the inference ability of LITE rather than classification on a specified label space, it is feasible to apply the trained model across different typing data sets. We therefore test its transferability by training on UFET and evaluate on traditional fine-grained benchmarks to get promising results. Moreover, we also examined the time efficiency of LITE, and discussed about the trade-off between training and inference costs in comparison with prior methods.</p><p>To summarize, the contributions of our work are three-folds. First, to our knowledge, this is the first work that uses NLI formulation and NLI supervision to handle entity typing. As a result, our system is able to keep the labels' semantics and encode the label dependency effectively. Second, our system offers SOTA performance on both ultrafine entity typing and regular fine-grained typing tasks, being particularly strong at predicting zeroshot and few-shot cases. Finally, we show that our system, once trained, can also work on different test sets which are free to have unseen types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Entity Typing. Traditional entity typing was introduced and thoroughly studied by <ref type="bibr" target="#b19">Ling and Weld (2012)</ref>. One main challenge that earlier efforts have focused on was to obtain sufficient training data to develop the typing model. To do so, automatic annotation has been commonly used in the a series of works <ref type="bibr" target="#b13">(Gillick et al., 2014;</ref><ref type="bibr" target="#b19">Ling and Weld, 2012;</ref><ref type="bibr" target="#b45">Yogatama et al., 2015)</ref>. Later works were developed for further improvement by modeling the label dependency with a hierarchy-aware loss <ref type="bibr" target="#b33">(Ren et al., 2016;</ref><ref type="bibr" target="#b40">Xu and Barbosa, 2018)</ref>. External knowledge from knowledge bases has also been introduced to capture the semantic relations or relatedness of type information <ref type="bibr" target="#b16">(Jin et al., 2019;</ref><ref type="bibr" target="#b7">Dai et al., 2019;</ref><ref type="bibr" target="#b23">Obeidat et al., 2019)</ref>. <ref type="bibr" target="#b11">Ding et al. (2021)</ref> adopt prompts to model the relationship between entities and type labels, which is similar to our template-based type description generation. However, their prompts are intended for label generation from masked language models while our templates realize the supervision from NLI.</p><p>More recently, <ref type="bibr" target="#b5">Choi et al. (2018)</ref> proposed the ultra-fine entity typing (UFET) task which involved free-form type labeling to realize the opendomain label space with much more comprehensive coverage of types. As the UFET tasks nontrivial learning and inference problems, several methods have been explored by more effectively modeling the structure of the label space. <ref type="bibr" target="#b39">Xiong et al. (2019)</ref> utilized a graph propagation layer to impose label-relation bias in order to capture type dependencies implicitly. <ref type="bibr" target="#b25">Onoe and Durrett (2019)</ref> trained a filtering and relabeling model with the human annotated data to denoise the automatically generated data for training. <ref type="bibr" target="#b24">Onoe et al. (2021)</ref> introduced box embeddings <ref type="bibr" target="#b35">(Vilnis et al., 2018)</ref> to represent the dependency among multiple levels of type labels as topology of axis-aligned hyperrectangles (boxes). To further cope with insufficient training data, <ref type="bibr" target="#b8">Dai et al. (2021)</ref> used pretrained language model for augmenting (noisy) training data with masked entity generation. Different to their strategy of augmenting training data, our approach generates type descriptions to leverage indirect supervision from NLI which requires no more data samples.</p><p>Natural Language Inference and Its Applications. Early approaches towards NLI problems were based on studying lexical semantics and syntactic relations <ref type="bibr" target="#b6">(Dagan et al., 2006)</ref>. Following research then introduced deep-learning methods into this task to capture contextual semantics. <ref type="bibr" target="#b28">Parikh et al. (2016)</ref> utilize Bi-LSTM <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997)</ref> to encode the input tokens and use attention mechanism to capture substructures of input sentences. Most recent works develop end-to-end trained NLI models that leverage pre-trained language models <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref> for sentence pair representation and large learning resources <ref type="bibr" target="#b0">(Bowman et al., 2015;</ref><ref type="bibr" target="#b38">Williams et al., 2018)</ref> for training.</p><p>Specifically, since pre-trained NLI models benefit generalizable logical inference, current literature has also proposed to leverage NLI models to improve prediction tasks with insufficient training labels, including zero-shot and few-shot text classification <ref type="bibr" target="#b43">(Yin et al., 2019)</ref>. <ref type="bibr" target="#b34">Shen et al. (2021)</ref> adopted RoBERTa-large-MNLI  to calculate the document similarity for document multi-class classification.  proposed to verify the output of a QA system with NLI models by converting the question and answer into a hypothesis and extracting textual evidence from the reference document as the premise.</p><p>Recent works by <ref type="bibr" target="#b44">Yin et al. (2020)</ref> and <ref type="bibr" target="#b37">White et al. (2017)</ref> are particularly relevant to this topic, which utilize NLI as a unified solver for several text classification tasks such as co-reference resolution and multiple choice QA in few-shot or fully-supervised manner. Yet our work handles a learning-to-rank objective for inference in a large candidate space, which not only enhances learning under a data-hungry condition, but also is free to be adapted to infer new labels that are unseen to training. <ref type="bibr" target="#b44">Yin et al. (2020)</ref> also proposed an approach to transform co-reference resolution task into NLI manner and we modified it as one of our template generation methods, which is discussed in ?3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce the proposed method for (ultra-fine) entity typing with NLI. We start with the preliminary of problem definition and the overview of our NLI-based entity typing framework ( ?3.1), followed by technical details of type Templates Type Descriptions Premise-Hypothesis Pairs for NLI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Taxonomic Statement</head><p>Jay is a producer. Premise: "Jay is currently working on his Spring 09 collection, . . . " Hypothesis: "Jay is a producer."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Explanation</head><p>In this context, career at a com--pany is referring to duration.</p><p>Premise: "No one expects a career at a company any more, . . . " Hypothesis: "In this context, career at a company is referring to duration."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Substitution</head><p>Musician knows how to make a hip-hop record sound good.</p><p>Premise: "He knows how to make a hip-hop record sound good." Hypothesis: "Musician knows how to make a hip-hop record sound good." description generation ( ?3.2), label dependency modeling ( ?3.3), learning objective ( ?3.4) and inference ( ?3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Problem Definition. The input of an entity typing task is a sentence s and an entity mention of interest e ? s. This task aims at typing e with one or more type labels from the label space L. For instance, in "Jay is currently working on his Spring 09 collection , which is being sponsored by the YKK Group.", the entity "Jay" should be labeled as person, designer or creator instead of organization or location.</p><p>The structure of the label space L can vary. For example, in some benchmarks like OntoNotes <ref type="bibr" target="#b13">(Gillick et al., 2014)</ref>, labels are provided in canonical form and strictly depend on their ancestor types. In this case, a type label bridge appears as /location/transit/bridge. However, in benchmarks like FIGER <ref type="bibr" target="#b19">(Ling and Weld, 2012)</ref>, partial labels have a dependency with their ancestors while the others are free-form and uncategorized. For instance, label film is given as /art/film but currency appears as a single word. For our primary task, for ultra fine-grained entity typing, the UFET benchmark <ref type="bibr" target="#b5">(Choi et al., 2018)</ref> provides no ontology of the labels and the label vocabulary consists of freeform words only. In this case, film star and person can appear independently in an annotation set with no dependency information provided.</p><p>Overview of LITE. Given a sentence with at least an entity mention, LITE treats the sentence as the premise in NLI, and then learns to type the entity in three consecutive steps ( <ref type="figure" target="#fig_0">Fig. 1</ref>). First, LITE employs a simple, low-cost template-based technique to generate a natural language description for a type candidate. This type description is treated as the hypothesis in NLI. For this step, we explore with three different description generation templates ( ?3.2). Second, to capture label dependency, whether or not the type ontology is provided, LITE consistently generates type descriptions for any ancestors of the original type label on the previous sentence and learns their logical dependencies ( ?3.3). These two steps create positive cases of type descriptions for the entity mention in the previous sentence. Last, LITE finetunes a pre-trained NLI model with a learning-torank objective that ranks the positive case(s) over negative-sampled type descriptions according to the entailment score ( ?3.5). During the inference phase, given another sentence that mentions an entity to be typed, our model predicts type that leads to the hypothetical type description with the highest entailment score. In this way, LITE can effectively leverage indirect supervision signals of a (pre-trained) NLI model to infer the type information of a mentioned entity.</p><p>We hereby describe the technical details of training and inference steps of LITE in the rest of the section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Type Description Generation</head><p>Given each sentence s with an annotated entity mention e, LITE first generates a natural language type description T (a) for the type label annotation a. The description will later act as a hypothesis in NLI. Specifically, we consider several generation technique to obtain such type descriptions, for which the details are described as follows.</p><p>? Taxonomic statement. The first template directly connects the entity mention and the type label with an "is-a" statement, i..e. "[ENTITY] is a [LABEL]".</p><p>? Contextual explanation. The second template generates a declarative sentence which adds a context-related connective. The generated type description is in the form of "In this context, [ENTITY] is referring to [LABEL]".</p><p>? Label substitution. <ref type="bibr" target="#b44">Yin et al. (2020)</ref> proposed to transform co-reference resolution problem into NLI manner by replacing the pronoun mentions with candidate entities. Inspired by their transformation, this technique directly replaces the [ENTITY] in the original sentence with <ref type="bibr">[LABEL]</ref>. Therefore, the NLI model will treat the modified sentence with a "type mention" as the hypothesis of the original sentence with the entity mention.</p><p>As shown in Tab. 1, each template provides a semantically meaningful way to connect the entity with label. In this way, the inference ability of an NLI model can be leveraged to capture the relationship of entity and label, given the original entity-mentioning sentence as the premise. Particularly, we have also tried automatic template generation method proposed by <ref type="bibr" target="#b12">Gao et al. (2021)</ref>, which has led to the adoption of the contextual explanation template. Such a template technique adopts the pre-trained text-to-text Transformer T5 <ref type="bibr" target="#b32">(Raffel et al., 2020)</ref> to generate prompt sentences for fine-tuning language models. In our case, T5 mask tokens are added between the sentence, the entity and the label. Since T5 is trained to fill in the blanks within its input, the output tokens can be used as the template for our type description. For example, given the sentence "Anyway, Nell is their new singer, and I would never interrupt her show.", the entity Nell and the annotations (singer, musician, person), we can formulate the input to T5 as "Anyway, Nell is their new singer, and I would never interrupt her show. &lt;X&gt; Nell &lt;Y&gt; singer &lt;Z&gt;". T5 will then fill in the placeholders &lt;X&gt;, &lt;Y&gt;, &lt;Z&gt; and output "... I would never interrupt her show. In fact, Nell is a singer." We observe that most of the generated templates given by T5 have appeared as the format where a prepositional phrase (e.g. in fact, in this context, in addition, etc.) followed by a statement such as "[ENTITY] is a [LABEL]" or "[EN-TITY] became [LABEL]". Accordingly, we select the above contextual explanation template, which is the most representative pattern observed in the generations.</p><p>In the training process, we use one of the three templates to generate the hypotheses, for which the same template will also be used to obtain the candidate hypotheses in inference. According to our preliminary results on dev set, the taxonomic statement generation generally gives better performance than the others under most settings, for which the analysis is presented in ?4.3. Thus, the main experimentation is reported as the configuration where LITE uses the type descriptions based on taxonomic statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling Label Dependency</head><p>The rich entity type vocabulary may form hierarchies that enforce logical dependency among labels of different specificity. Hence, we extend the generation process of type description to better capture such the label dependency. In detail, for a specific type label that LITE has generated a type description, if there are ancestor types, we not only generate descriptions for each of the ancestor types, but also conduct learning among these type descriptions. The descendant type description would act as the premise and the ancestor type description would act as the hypothesis. For instance, in OntoNotes <ref type="bibr" target="#b13">(Gillick et al., 2014)</ref> or FIGER <ref type="bibr" target="#b19">(Ling and Weld, 2012)</ref>, suppose a sentence mentions the entity London and is labeled as /location/city, if the taxonomic statement based description generation is used, LITE will yield descriptions for both levels of types, i.e. "London is a city" and "London is a location". In such a case, the more fine-grained type description "London is a city" can act as the premise of the more coarse-grained description "London is a location", so as to help capturing the dependency between two labels "city" and "location". Such paired type descriptions are added to training and will be captured by the dependency loss L d as being described in ?3.4. This technique to capture label dependency can be easily adapted to tasks where a type ontology is unavailable, but each instance is directly annotated with multiple type labels of different specificity. Particularly for the UFET task <ref type="bibr" target="#b5">(Choi et al., 2018)</ref>, while no ontology is provided for the label space, the task separates the type label vocabulary into different specificity, i.e. general, fine and ultrafine ones. Since its annotation to an entity from a sentence includes multiple labels of different specificity, we can still utilize the aforementioned dependency modeling method. For instance, an entity Mike Tyson may be simultaneously labeled as person (general), sportsman (fine), and boxer (ultra-fine). Similar to using an ontology, each pair of descendant and ancestor descriptions among the three generations "Mike Tyson is a sportsman", "Mike Tyson is a person" and "Mike Tyson is a sportsman" are also added to training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Objective</head><p>Let L be the type vocabularies, the learning objective of LITE is to conduct learning-to-rank on top of the NLI model. Given a sentence s with mentioned entity e, we use P to denote all true type labels of e that may include the original label and any induced ancestor labels as described in ?3.3. Then, for each label p ? P whose type description is generated as H(p) by one of the techniques in ?3.2, the NLI model calculates the entailment score ?(s, H(p)) ? [0, 1] for the premise s and hypothesis H(p). Meanwhile, negative sampling randomly selects a false label p ? L \ P . Following the same procedure above, the entailment score ?(s, H(p )) is obtained for the premise s and the negative-sample hypothesis H(p ). The margin ranking loss for an annotated training case is then defined as</p><formula xml:id="formula_0">L t = [?(s, H(p )) ? ?(s, H(p)) + ?] + .</formula><p>[x] + denotes the positive part of the input x (i.e. max(x, 0)) and ? is a non-negative constant.</p><p>We also similarly define a ranking loss to model the label dependency. Still given the above annotated sentence s and the set of all true type labels P , as described in ?3.3, for any exiting pair of ancestor type p an and descendant type p de from P , the training phase also captures the entailment relation between their descriptions. This process regards H(p de ) as the premise and H(p an ) as the hypothesis, and the NLI model therefore yields an entailment score ?(H(p de ), H(p an )). The label dependency loss is then defined as the following ranking loss</p><formula xml:id="formula_1">L d = [?(H(p de ), H(p an )) ? ?(H(p de ), H(p an )) + ?] + ,</formula><p>where p an is negative-sampled type label.</p><p>The eventual learning objective is to optimize the following joint loss:</p><formula xml:id="formula_2">L = 1 |S| s?S 1 |P s | p?Ps L t + ?L d</formula><p>where S denotes the dataset containing sentences with typed entities, and P s denotes the set of true labels on an entity of the sentence instance s. In this way, all annotations of each entity mention will be involved in training. ? here is a nonnegative hyper-parameter that controls the influence of dependency modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference</head><p>The inference phase of LITE performs ranking on descriptions for all type labels from the vocabulary. For any given sentence s mentioning an entity e, LITE accordingly generates a type description for each candidate type label. Then, taking the sentence s as the premise, the finetuned NLI model ranks the hypothetical type descriptions according to their entailment scores. Finally, LITE selects the type label whose description receives the highest entailment score, or predicts with a threshold of entailment scores in cases where multi-label prediction is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we present the experimental evaluation for LITE framework, based on both UFET ( ?4.1) and traditional (less) fine-grained entity typing tasks ( ?4.2). In addition, we also conduct comprehensive ablation studies to understand the effectiveness of the incorporated techniques( ?4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ultra-Fine Entity Typing</head><p>We use the UFET benchmark created by <ref type="bibr" target="#b5">Choi et al. (2018)</ref> for evaluation. The UFET dataset consists of two parts. (i) Human-labeled data (L): 5,994 instances split into train/dev/test by 1:1:1 (1,998 for each); (ii) Distant supervision data (D): including 5.2M instances that are automatically labeled by linking entity to KB, and 20M instances generated by headword extraction. We follow the original design of the benchmark to evaluate loose macro-averaged precision (P), recall (R) and F1.</p><p>Training Data. In our approach, the supervision can come from the MNLI data (NLI) <ref type="bibr" target="#b38">(Williams et al., 2018)</ref>, distant supervision data (D) and the human-labeled data (L). Therefore, we investigate the best combination of training data by exploring the following different training pipelines:</p><p>? LITE NLI : Pre-train on MNLI 3 , then predict directly, without any tuning on D or L;</p><p>? LITE L : Only fine-tune on L;</p><p>? LITE NLI+L : Pre-train on MNLI, then fine-tune on L;</p><p>? LITE D+L : Pre-train on D, then fine-tune on L;</p><p>? LITE NLI+D+L : First pre-train on MNLI, then on D, finally fine-tune on L.</p><p>Model Configurations. Our system is first initialized as RoBERTa-large  and AdamW <ref type="bibr" target="#b22">(Loshchilov and Hutter, 2018</ref>) is used to optimize the model. The hyper-parameters as well as the output threshold are tuned on the dev set: batch size 16, pre-training (D) learning rate 1e-6, fine-tuning (L) learning rate 5e-6, margin ?=0.1 and ?=0.05. The pre-training epochs are limited to 5 that are enough considering the large size of pretraining data. The fine-tuning epochs are limited to 2,000; models are evaluated every 30 epochs on dev and the best model is kept to conduct inference on test.</p><p>Baselines. We compare LITE with the following strong baselines. Except for LRN which is merely trained on the human annotated data, all the other baselines incorporate the distant supervision data as extra training resource.</p><p>? UFET-biLSTM <ref type="bibr" target="#b5">(Choi et al., 2018)</ref> represents words using the GloVe embedding <ref type="bibr" target="#b29">(Pennington et al., 2014)</ref> and captures semantic information of sentences, entities as well as labels with a bi-LSTM and a character-level CNN. It also learns a type label embedding matrix to operate inner product with the context and mention representation for classification.</p><p>? LabelGCN <ref type="bibr" target="#b39">(Xiong et al., 2019)</ref> improves UFET-biLSTM by stacking a GCN layer on the top to capture the latent label dependency.</p><p>? LDET (Onoe and Durrett, 2019) applies ELMo embeddings <ref type="bibr" target="#b30">(Peters et al., 2018)</ref> for word representation and adopts LSTM as its sentence and mention encoders. Similar to UFET-biLSTM, it learns a matrix to compute inner product with each input representation for classification. Besides, LDET also trains a filter and relabeler to fix the label inconsistency in the distant supervision training data.  <ref type="table">Table 2</ref>: Results on the ultra-fine entity typing task. LITE series are equipped with the Taxonomic Statement template. "w/o label dependency" is applied to the "NLI+L" setting. The F1 result by LITE NLI+L is statistically significant (p-value &lt; 0.01 in t-test) in comparison with the best baseline result by MLMET. classification is fulfilled by computing the intersection of the input text and type boxes.</p><p>? LRN  encodes the context and entity with BERT-base-uncased. Then two LSTM-based auto-regression network captures the context-label relation and the label-label relation via attention mechanisms respectively in order to generate labels. They simultaneously construct bipartite graphs for sentence tokens, entities and generated labels to do relation reasoning and predict more labels.</p><p>? MLMET <ref type="bibr" target="#b8">(Dai et al., 2021)</ref>, the prior SOTA system, first generates additional distant supervision data by BERT Masked Language Model, then stacks a linear layer on BERT to learn the classifier on the union label space.   <ref type="table">Table 4</ref>: Examples of two sources of distant supervision data (one from entity linking, the other from head word extraction). In the right "Labels" column, correct types are boldfaced while incorrect ones are in grey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>the entailment scheme and the indirect supervision from NLI.</p><p>The bottom block in Tab. 2 further explores the best combination of available training data. First, training on MNLI (i.e., LITE NLI ) alone does not provide promising results. This could be due to that the MNLI does not generalize well to this UFET task. LITE L removes the supervision from NLI as compared to LITE NLI+L , causing a noticeable performance drop. In addition, the comparison between LITE NLI+L and LITE D+L illustrates that the MNLI data, as an out-of-domain resource, even provides more beneficial supervision than the distant annotations. To our knowledge, this is already the first work that shows rather than relying on gathering distant supervision data in the (entity-mentioning context, type) style, it is possible to find more effective supervision from other tasks (e.g., from entailment data) to boost the performance. However, when we incorporate the distant supervision data (D) into LITE NLI+L , the new system LITE NLI+D+L performs worse. We present more detailed analyses in ?4.3.</p><p>In addition, we also investigate the contribution of label dependency modeling by removing it from LITE NLI+L . As results shown in Tab. 2, incorpo-rating label dependency helps improve the recall with a large margin (from 46.6 to 48.9) despite a minor drop for the precision, leading to notable overall improvement in F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-grained Entity Typing</head><p>In addition to UFET, we are also interested in (i) the effectiveness of our LITE to entity typing tasks with much fewer types, and (ii) if our learned LITE model from the ultra-fine task can be used for inference on other entity typing tasks, which often has unseen types, even without further tuning. To the end, we evaluate LITE on OntoNotes <ref type="bibr" target="#b13">(Gillick et al., 2014)</ref> and FIGER <ref type="bibr" target="#b19">(Ling and Weld, 2012)</ref>, two popular fine-grained entity typing benchmarks.</p><p>OntoNotes contains 3.4M automatically labeled entity mentions for training and 11k manually annotated instances that are split into 8k for dev set and 2k for test set. Its label space consists of 88 types and one more other type. In inference, LITE outputs other if none of the 88 types is scored over the threshold described in ?3.5. FIGER contains 2M data samples labeled with 113 types. The dev set and test set include 1,000 and 562 samples respectively. Within its label space, 82 types have a dependency relation with their ancestor or descendant types while the other 30 types are uncategorized free-form words.</p><p>Results. Tab. 3 reports baseline results as well as results of two variants of LITE: one is pretrained on UFET and directly transfer to predict on the two target benchmarks, the other conducts task-specific training on the target benchmark after pre-training on MNLI. The task-specific training variant outperforms respective prior SOTA on both benchmarks (OntoNotes: 86.4 vs. 85.4 in macro-F1, 80.9 vs. 80.4 in micro-F1; FIGER: 86.7 vs. 84.9 in macro-F1, 83.3 vs. 81.5 in micro-F1). An interesting advantage of LITE lies in its transferability across benchmarks. Tab. 3 demonstrates that our LITE (pre-trained on UFET) offers competitive performance on both OntoNotes and FIGER even with only zero-shot transfer (it even exceeds the "task-specific training" version on OntoNotes). 4 Although there are disjoint type labels between these two datasets and UFET, there exist manually-crafted mappings from UFET labels to them (e.g. "musician" to "/person/artist/music"). In this way, traditional multi-way classifiers still work across the datasets after type mapping though we do not prefer human-involvement in real-world applications. To further test the transferability of LITE, a more challenging experimental setting for zeroshot type prediction is conducted and analyzed in ?4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Through the following analyses, we try to answer following questions: (i) Why did not the distant supervision data help (as Tab. 2 indicates)? (ii) How effective is each type description template (Tab. 1)? (iii) With the NLI-style formulation and the indirect supervision, does LITE generalize better for zero-shot and few-shot prediction? Is trained LITE transferable to new benchmarks with unseen types? (iv) On which entity types does our model perform better, and which ones remain challenging? (vi) How efficient is LITE?</p><p>Distant Supervision Data. As Tab. 2 indicates, adding distant supervision data in LITE NLI+D+L even leads to a drop of 3.2% absolute score in F1 from LITE NLI+L . This should be due to the fact that the distant supervision data (D) are overall noisy <ref type="bibr" target="#b25">(Onoe and Durrett, 2019)</ref>. Tab. 4 lists some frequent and typical problems that exist in D based on entity linking and head-word extraction. In general, they will lead to two problems.</p><p>On the one hand, a large number of false positive types are introduced. Considering the example (a) in Tab. 4, the state Connecticut is labeled as author, cemetery and person. For the example (c), hash brown is labeled as brown, turning the concept of food into color. Additionally, the headword method is short in capturing the semantics. In the example (d), number is falsely extracted as the type for a number of short stories because of the preposition "of".</p><p>On the other, such distant supervision may not comprehensively recall positive types. For instance, examples (b) and (e) are both about the entity "film" where the recalled types are correct. However, in the human annotated data, entity "film" may also be labeled as ("film", "art", "movie", "show", "entertainment", "creation"). In this situation, those missed positive types (i.e., "movie", "show", "entertainment" and "creation") will be selected by the negative sampling process of LITE and therefore negatively influence the performance. The comparison between LITE NLI+L and LITE D+L can further justify the superiority of the indirect supervision from NLI over that from the distant supervision data.</p><p>Type Description Templates. Tab. 5 reveals how template choices affect the typing performance. It is obvious that taxonomic statement outperforms the other two under all of the three training settings. The contextual explanation template yields close, while worse results but the label substitution leads to more noticeable F1 drop. This may result from the absence of entity mention in hypothesis by label substitution. For instance, in "Soft eye shields are placed on the babies to protect their eyes.", LITE with label substitution generates related but incorrect type labels such as treatment, attention or tissue.</p><p>Few-&amp; Zero-shot Prediction. In ?4.2, we discussed about transferring LITE trained on UFET to other fine-grained entity typing benchmarks. Nevertheless, since UFET labels are still inclusive of them with mapping, we conducted fur-   ther experiment in which portions of UFET training labels are randomly filtered out so that 40% of the testing labels are unseen in training. We then investigated the LITE NLI+L performance on test types which have zero or a few labeled examples in the training set. <ref type="figure">Fig. 2</ref> shows the results of LITE NLI+L and the strongest baseline, MLMET. Note that while the held-out set of type labels are completely unseen to LITE, the full type vocabulary is however provided for MLMET during its LM-based data augmentation process in this experiment.</p><p>As shown in the results, it is as expected that the performance on more frequent labels are better than on rare labels. LITE NLI+L outperforms ML-MET on all the listed frequency of labels which reveals the strong low-shot prediction performance of our model. Particularly, on the extremely challenging zero-shot labels, LITE NLI+L drastically exceeds MLMET by 32.9% vs. 10.8% in F1. Hence, it is demonstrated that the NLI-based entity typing succeeds in more reliably representing and inferring rare and unseen entity types.</p><p>The main difference between the NLI framework and multi-way classifiers is NLI makes use of the semantics of input text as well as the label text; conventional classifiers, however, only model the semantics of input text. Encoding the semantics of labels' side is particularly beneficial when the type set is super large and many types lack training data. When some test labels are filtered out in the training process, LITE still performs well with its inference manner but classifiers (like MLMET) fail to recognize the semantics of unseen labels merely with their features. In this way, LITE maintains high performance when transfers across benchmarks with disjoint type vocabularies.</p><p>Case Study. We randomly sampled 100 labels on which LITE improves MLMET by at least 50% in F1 and here are the recognized typical patterns:</p><p>? Contextual inference (28%): In case (a) of Tab. 6, considering the information "wining the   <ref type="figure">Figure 2</ref>: Performance comparison of our system LITE and the prior SOTA system, MLMET, on the filtered version of UFET for zero-shot and few-shot typing. The zero-shot labels correspond to the 40% test set type labels that are unseen in training. We also report the performance on other few-shot type labels.</p><p>50-meter backstroke gold medal", LITE successfully types her with swimmer in addition to athlete that is given by MLMET.</p><p>? Coreference (20%): In case (b), LITE correctly refers the pronoun entity it to "apology" but MLMET merely captures local information "tv network airing" to obtain the label words event, message.</p><p>? Hypernym (19%): In the case (c), even if there is no mention of furniture in the text, LITE gives a high confidence score to this type that is a hypernym of mechanical desks. Nevertheless, MLMET only get trivial answers such as desk, object.</p><p>On the other hand, we also sampled 100 labels on which MLMET performs better and it can be concluded that LITE falls short mainly in following scenarios:</p><p>? Multiple nominal words (30%): In the sample (d) of Tab. 6, due to ambiguous meaning of the type hypothesis "basketball and baseball is a basketball", LITE fails to predict the groundtruth label basketball.</p><p>? Clause (28%) Instance (e) illustrates a common situation when clauses are included in the entity mention, where the effectiveness of type descriptions is harmed. The clausal information distracts LITE from focusing on the key part of the entity.</p><p>Prediction on Different Categories of Entity Mentions. We also investigated the prediction of LITE on three different categories of entity mentions from the UFET test data: named entities, pronouns and nominals. For each category of mentions, we randomly sample 100 instances and the performance comparison against MLMET is reported in Tab. 7. According to the results, LITE consistently outperforms MLMET on all three categories of entities and the improvement on nominal phrases (46.2% vs 43.5% in F1) is most significant. This partly aligns with the capability of making inference based on noun hypernyms, as being discussed in Case Study. Meanwhile, typing on nominals seeks to be more challenging than on the other two categories of entities, which, from our observation, is mainly due to two reasons. First, Nominal phrases with multiple words are more difficult to capture by the language model in general. Second, nominals are sometimes less concrete than pronouns and named entities, hence LITE also generates more abstract type labels. For example, LITE has labeled the drink in an instance as substance, which is too abstract and is not recognized by human annotators.</p><p>Time Efficiency. In general, LITE has much less training cost, of around 40 hours, than the previous strongest (data-augmentation-based) model ML-MET, which requires over 180 hours, on the UFET task. 5 During the inference step, it takes about 35 seconds per new sentence for our model to do inference with a fixed type vocabulary of over 10,000 different labels while a common multi-way classifier merely requires around 0.2 seconds. In fact, such a big difference in inference cost results from encoding longer texts and multiple encoding calculation for the same text. It can be accelerated by modifying the encoding model structure which will be discussed in ?5. However, LITE is much more efficient on dynamic type vocabulary. It requires almost no re-calculation when new, unmappable labels are added to an existing type set but multi-way classifiers need re-training with an extended classifier every time (e.g. over 180 hours by the previous SOTA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We propose a new model LITE that leverages indirect supervision from NLI to type entities in texts. Through template-based type hypothesis generation, LITE formulates the entity typing task as a language inference task and meanwhile the semantically rich hypothesis remedy the data scarcity problem in the UFET benchmark.</p><p>Besides, the learning-to-rank objective further help LITE with generalized prediction across benchmarks with disjoint type sets. Our experimental results illustrate that LITE promisingly offer SOTA on UFET, OntoNotes and FIGER, and yields strong performance on zero-shot and few-shot types.</p><p>LITE pretrained on UFET also yields strong transferability by outperforming SOTA baselines when directly make predictions on OntoNotes and FIGER.</p><p>For future research, as mentioned in ?4.3, we first plan to investigate ways to accelerate LITE by utilizing a late-binding cross-encoder <ref type="bibr" target="#b27">(Pang et al., 2020)</ref> for linear-complexity NLI, and incorporating high-dimensional indexing techniques like ball trees in inference. To be specific, the premise and hypotheses can first be encoded respectively and the resulting representations can later be used to evaluate the confidence score of premise-hypothesis representation pairs through a trained network. With little expected loss in performance, LITE can still maintain its feature of strong transferability and zero-shot prediction.</p><p>In addition, we plan to extend NLI-based indirect supervision to information extraction tasks such as relation extraction and event extraction. Incorporating abstention-awareness <ref type="bibr" target="#b10">(Dhamija et al., 2018)</ref> for handling unknown types is another meaningful direction. Besides, <ref type="bibr" target="#b31">Poliak et al. (2018)</ref> recasted diverse types of reasoning dataset including NER, relation extraction and sentiment analysis into NLI structure, which we plan to incorporate as extra indirect supervision for LITE to further enhance the robustness of entity typing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Entity typing by LITE with indirect supervision from NLI. trained to include those new types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Type description instances of three templates. Entity mentions are boldfaced and underlined while label words are only boldfaced.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.</head><label></label><figDesc>Tab. 2 compares LITE with baselines, in which LITE adopts the taxonomic statement template (i.e. "[ENTITY] is a [LABEL]").Overall, LITE NLI+L demonstrates SOTA performance over other baselines, outperforming the prior top system MLMET<ref type="bibr" target="#b8">(Dai et al., 2021)</ref> with 1.5% absolute improvement on F1. Recall that MLMET built a multi-way classifier on the its newly collected distant supervision data and the human-labeled data, our LITE optimizes a textual entailment scheme on the entailment data (i.e., MNLI) and the human-labeled entity typing data. This comparison verifies the effectiveness of using</figDesc><table><row><cell></cell><cell></cell><cell cols="2">OntoNotes</cell><cell>FIGER</cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell cols="2">macro-F1 micro-F1</cell><cell cols="2">macro-F1 micro-F1</cell></row><row><cell cols="2">Hierarchy-Typing (Chen et al., 2020b)</cell><cell>73.0</cell><cell>68.1</cell><cell>83.0</cell><cell>79.8</cell></row><row><cell cols="2">Box4Types (Onoe and Durrett, 2020)</cell><cell>77.3</cell><cell>70.9</cell><cell>79.4</cell><cell>75.0</cell></row><row><cell cols="2">DSAM (Hu et al., 2020)</cell><cell>83.1</cell><cell>78.2</cell><cell>83.3</cell><cell>81.5</cell></row><row><cell cols="2">SEPREM (Xu et al., 2021)</cell><cell>-</cell><cell>-</cell><cell>86.1</cell><cell>82.1</cell></row><row><cell cols="2">MLMET (Dai et al., 2021)</cell><cell>85.4</cell><cell>80.4</cell><cell>-</cell><cell>-</cell></row><row><cell>LITE</cell><cell>pre-trained on NLI+UFET NLI+task-specific training</cell><cell>86.6 86.4</cell><cell>81.4 80.9</cell><cell>80.1 86.7</cell><cell>74.7 83.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results for fine-grained entity typing. All LITE model results are statistically significant (p-value &lt; 0.05 in t-test) in comparison with the best baseline results by MLMET on OntoNotes and by SEPREM on FIGER.</figDesc><table><row><cell>Data Source</cell><cell>Sentence</cell><cell>Labels</cell></row><row><cell></cell><cell>(a) From 1928-1929 , he enrolled in graduate coursework at Yale University in New</cell><cell>location, author, province,</cell></row><row><cell>Entity Linking</cell><cell>Haven , Connecticut.</cell><cell>cemetery, person</cell></row><row><cell></cell><cell>(b) Once Upon Andalasia is a video game based on the film of the same name.</cell><cell>art, film</cell></row><row><cell></cell><cell>(c) You can also use them in casseroles and they can be grated and fried if you want</cell><cell>brown</cell></row><row><cell></cell><cell>to make hash browns.</cell><cell></cell></row><row><cell>Head Word</cell><cell>(d) He has written a number of short stories in different fictional worlds, including</cell><cell>number</cell></row><row><cell></cell><cell>Dragonlance, Forgotten Realms, Ravenloft and Thieves' World.</cell><cell></cell></row><row><cell></cell><cell>(e) Despite obvious parallels and relationships , video art is not film.</cell><cell>film</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>50.6 45.4 49.9 47.4 27.5 56.4 37.0 Contextual Explanation 50.8 49.2 50.2 45.3 48.5 46.8 26.9 55.4 36.2 Label Substitution 47.4 49.3 48.3 42.5 50.7 46.2 24.8 59.3 35.0</figDesc><table><row><cell></cell><cell></cell><cell cols="2">LITE NLI+L</cell><cell></cell><cell cols="2">LITE NLI+D+L</cell><cell></cell><cell>LITE D+L</cell><cell></cell></row><row><cell>Templates</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Taxonomic Statement</cell><cell cols="2">52.4 48.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Behavior of different type description templates under three training settings.</figDesc><table><row><cell></cell><cell>Input</cell><cell>True Labels</cell><cell>Prediction</cell></row><row><cell></cell><cell>(a) The University of California commu-</cell><cell>athlete, person,</cell><cell>LITE:</cell></row><row><cell></cell><cell>nications major gave her mother a fitting</cell><cell>swimmer*, contestant,</cell><cell>athlete, person, swimmer*, female, student, winner</cell></row><row><cell>exceeds MLMET LITE</cell><cell>present, surprising herself by winning the 50-meter backstroke gold medal. (b) The apology is being viewed as a wa-tershed in Australia , with major television networks airing it live and crowd gathering around huge screens in the city. (c) A drawing table is also sometimes called a mechanical desk because , for</cell><cell>scholar, child event, apology*, plea, regret object, desk, furniture*, board,</cell><cell>MLMET: athlete, person, child, adult, female, mother, woman LITE: event, apology*, ceremony, happening, concept MLMET: event, message LITE: object, desk, furniture*</cell></row><row><cell></cell><cell>several centuries , most mechanical desks</cell><cell>desk, table</cell><cell>MLMET:</cell></row><row><cell></cell><cell>were drawing tables.</cell><cell></cell><cell>object, desk, computer</cell></row><row><cell>MLMET exceeds LITE</cell><cell>(d) He attended the University of Virginia , where he played basketball and baseball ; his brother Bill also played baseball for the University. (e) The manner in which it was confirmed however smacked of an acrimonious end to the relationship between club and player with Chelsea.</cell><cell>basketball*, baseball, fun, action, activity, contact sport, game, sport, athletics, ball game, ball, event manner*, way, concept, style, method</cell><cell>LITE: activity, game, sport, event, ball game, ball, athletics MLMET: activity, game, sport, event, basketball* LITE: event MLMET: manner*, event</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Case Study of labels on which LITE improves MLMET or MLMET outperforms LITE. Correct predictions are in blue and * indicates the representative label words for the discussed pattern.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>LITE NLI+L 58.6 55.5 57.0 51.2 57.5 54.2 45.3 47.1 46.2 MLMET 58.3 54.4 56.3 57.2 50.0 53.4 49.5 38.9 43.5</figDesc><table><row><cell></cell><cell cols="2">Named Entity</cell><cell></cell><cell>Pronoun</cell><cell></cell><cell></cell><cell>Nominal</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison of LITE and prior SOTA, MLMET, on named entity, pronoun and nominal entities respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MLMET</cell><cell>LITE</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell>38.9</cell></row><row><cell></cell><cell></cell><cell>32.9</cell><cell>33.6</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell>27.3</cell></row><row><cell>F-1 Score</cell><cell>20</cell><cell></cell><cell>23.5</cell></row><row><cell></cell><cell>10</cell><cell>10.8</cell></row><row><cell></cell><cell>0</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0 shot</cell><cell>1~5 shot</cell><cell>6~10 shot</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Label Frequency</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that although these more traditional entity typing tasks are termed as "fine-grained entity typing", their typing systems are much less fine-grained than that of UFET.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This is obtained from huggingface.co/ roberta-large-mnli</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">LITE pre-trained on UFET performs worse on FIGER than LITE with task-specific training. The main reason could be that a larger portion of FIGER test data comes with an entity of proper noun to be labeled with more compositional types, such as government agency, athlete, sports facility, which has appeared much less on UFET.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">All the time estimations are given by experiments on a commodity server with a TITAN RTX. Training and evaluation batch sizes are maximized to 16 or 128 for LITE and MLMET respectively.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors appreciate the reviewers and editors for their insightful comments and suggestions. The authors would also like to thank Hongliang Dai and Yangqiu Song from the Hong Kong University of Science and Technology for sharing the resources and implementation of MLMET, and thank Eunsol Choi from the University of Texas at Austin for sharing the full UFET distant supervision data.</p><p>This material is partly supported by the National Science Foundation of United States Grant IIS 2105329, and the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Grounding &apos;grounding&apos; in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Khyathi Raghavi Chandu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.375</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4283" to="4305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Can NLI models verify QA systems&apos; predictions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.324</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3841" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical entity typing via multi-level learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.749</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8465" to="8475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical entity typing via multi-level learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.749</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8465" to="8475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving fine-grained entity typing with entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1643</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing with weak supervision from a masked language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1790" to="1799" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reducing network agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Akshay Raj Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 32nd Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9175" to="9186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Promptlearning for fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Context-dependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diversified semantic attention model for fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing via hierarchical multi graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiansi</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1502</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4969" to="4978" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Type-aware distantly supervised relation extraction with linked arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1203</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1901" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00141</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing via label reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4611" to="4622" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Description-based zero-shot fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasha</forename><surname>Obeidat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Shahbazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1087</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling fine-grained entity types with box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mc-Callum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2051" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to denoise distantly-labeled data for entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2407" to="2417" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interpretable entity representations through largescale typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.54</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="612" to="624" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FAST-MATCH: Accelerating the inference of BERTbased text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.568</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6459" to="6469" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collecting diverse natural language inference problems for sentence representation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Steven</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tax-oClass: Hierarchical multi-label text classification using only class names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4239" to="4249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic embedding of knowledge graphs with box lattice measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.121</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1405" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inference is everything: Recasting semantic resources into a unified evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="996" to="1005" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imposing label-relational inductive bias for extremely fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="773" to="784" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural fine-grained entity type classification with hierarchy-aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
	<note>New Orleans, Louisiana. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Syntax-enhanced pre-trained model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.420</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Long Papers; Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5412" to="5422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving semantic parsing via answer type inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1404</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3914" to="3923" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Universal natural language processing with limited annotations: Try few-shot textual entailment as a start</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.660</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8229" to="8239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Embedding methods for fine grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
