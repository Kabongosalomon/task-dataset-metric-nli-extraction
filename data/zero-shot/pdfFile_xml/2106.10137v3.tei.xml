<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martine</forename><surname>Toering</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Gatopoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">BrainCreators B. V</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Stol</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">BrainCreators B. V</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Tao</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instance-level contrastive learning techniques, which rely on data augmentation and a contrastive loss function, have found great success in the domain of visual representation learning. They are not suitable for exploiting the rich dynamical structure of video however, as operations are done on many augmented instances. In this paper we propose "Video Cross-Stream Prototypical Contrasting", a novel method which predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. Specifically, we alternate the optimization process; while optimizing one of the streams, all views are mapped to one set of stream prototype vectors. Each of the assignments is predicted with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. As a result, more efficient video embeddings with ingrained motion information are learned, without the explicit need for optical flow computation during inference. We obtain state-of-the-art results on nearestneighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of this paper is self-supervised representation learning for video. Visual representation learning methods based on instance-level contrasting have significantly reduced the gap with supervised learning in image-based tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b64">65]</ref> and video <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b26">27]</ref>. These contrastive learning frameworks require an augmentation module that obtains multiple views of one instance, and a loss function that contrasts between augmented views of instances. The objective can be viewed as instance discrimination: producing higher similarity scores between augmentations of the same instances, rather than with those that belong to dif- <ref type="bibr">Figure 1</ref>. RGB and optical flow are used as two streams in the training of one stream by predicting consistent prototype assignments from features. By also alternating the training, we transfer knowledge cross-stream from motion (flow) to appearance (RGB) useful for downstream video tasks with optional optical flow. ferent ones (negative examples). As a result, the methods rely heavily on data augmentation in order to learn powerful representations. Furthermore, a vast amount of negative examples has to be obtained which often relies on either memory banks <ref type="bibr" target="#b28">[29]</ref> or large batch sizes <ref type="bibr" target="#b11">[12]</ref>.</p><p>To adopt these techniques into the video domain efficiently, we make the following observations. First, we notice that though video also provides natural augmentation with viewpoint changes, illumination (jittering) and deformation, still spatiotemporal coherence and motion are not explicitly used. We are inspired by the two-streams hypothesis for vision processing in the brain <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b21">22]</ref>, suggesting two pathways: the ventral stream involved in object recognition and the dorsal stream locating objects and recognizing motion. Motion without appearance information can be a rich source of information for humans <ref type="bibr" target="#b32">[33]</ref>, however more recent works propose that it is likely that the streams interact <ref type="bibr" target="#b44">[45]</ref>. Second, we believe instance-level contrastive learning is inefficient and neglects the use of semantic similarity between instances. Low similarity scores are produced for a large pool of negative pairs regardless of their semantic similarity, resulting in undesirable distances between samples in the embedding. To resolve this, several works have explored alternatives to random sampling for negative examples <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, such as hard negative mining <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b55">56]</ref>. We are instead interested in leaving instance-level comparisons and include mappings to prototypes (defined as representatives of semantically similar groups of features), providing a possible benefit on video representations without any potential costs from distance searches in the data.</p><p>In this work, we present a novel self-supervised method called Video Cross-Stream Prototypical Contrasting (ViCC) where we consider RGB and optical flow as distinct views for video contrastive learning, to influence appearance and motion learning respectively. The two input streams and spatiotemporal augmentations are united into one framework. In each iteration of the optimization of one stream, views are assigned to a set of prototypes and assignments are subsequently predicted from the features, see <ref type="figure">Figure 1</ref>.</p><p>Our contributions can be summarized as below. ? We introduce a novel visual-only self-supervised learning framework for video that contrasts using sets of views from two streams (RGB and flow). We demonstrate the benefits of operating on stream prototypes over contrastive instance learning, avoiding unnecessary comparisons and hence computations, while improving accuracy. ? We propose a new training mechanism for video, in which RGB and flow streams are interconnected in two ways: prototypes are predicted from both streams and the optimization process is alternated. As motion information is transferred to the RGB model, we can discard the optical flow network in deployment scenarios depending on speed and efficiency requirements. ? We perform extensive ablation studies to provide an indepth analysis of our method. Our result reaches stateof-the-art on UCF101 <ref type="bibr" target="#b58">[59]</ref> and HMDB51 <ref type="bibr" target="#b35">[36]</ref> on the two backbones S3D <ref type="bibr" target="#b73">[74]</ref> and R(2+1)D <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Contrastive instance learning. Instance discrimination considers each sample as its own class in the data. As such a classifier becomes computationally infeasible fast, <ref type="bibr" target="#b72">[73]</ref> use noise-contrastive estimation <ref type="bibr" target="#b23">[24]</ref> and a memory bank to store representations as their pool of negative samples. Other solutions include the work from Chen et al. <ref type="bibr" target="#b12">[13]</ref>, which retrieves more negative samples by using large batch sizes. He et al. <ref type="bibr" target="#b28">[29]</ref> propose a momentum encoder with a dynamic dictionary look-up. Another line of work contrasts between the global image and local patches <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b29">30]</ref>. Our method instead uses complementary modalities as main views and intuitively learns its own positive and negative examples from both feature spaces through the prototypes. Contrasting is done between instances and prototypes, going beyond instance-level learning while avoiding the need for substantial batch sizes <ref type="bibr" target="#b11">[12]</ref> or large memory banks <ref type="bibr" target="#b28">[29]</ref>.</p><p>Clustering in latent space. Combining clustering with representation learning to obtain pseudo-labels has been proposed in various self-supervised learning settings <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41]</ref>. Asano et al. <ref type="bibr" target="#b2">[3]</ref> propose a solution of degen-erate solutions by casting clustering into an instance of the optimal transport problem. Caron et al. <ref type="bibr" target="#b7">[8]</ref> use this clustering setup in a contrastive learning setting by enforcing consistency between different views, comparing cluster assignments instead of individual features. Furthermore, an online clustering and simultaneous feature learning mechanism was proposed in <ref type="bibr" target="#b79">[80]</ref>. Our objective is most similar to <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b40">[41]</ref>, aligning cluster assignments for augmented instances in an online manner. However, we apply our method on video, use augmentation in the form of optical flow and alternate the training of models and prototypes to incorporate information in both streams.</p><p>Self-supervised video and distillation. Advances in 3DConvNets <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b63">64]</ref> have driven video research forwards. Self-supervised approaches exploring pretext tasks are often based on the temporal domain, such as the order of frames or clips <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b74">75]</ref>, learning the arrow of time <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b71">72]</ref> or pace <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b3">4]</ref>. Pretext tasks that were previously explored in the image domain have been proposed and extended <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. Other approaches include leveraging the consistency in frames by temporal correspondence <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, tracking patches <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref>, future frame prediction <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b66">67]</ref> or future feature prediction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Multiple works explore optical flow for self-supervision <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref>. A cross-stream approach was first proposed by <ref type="bibr" target="#b49">[50]</ref>. As opposed to them, we use contrastive learning without dense trajectories. Mahendran et al. <ref type="bibr" target="#b43">[44]</ref> use optical flow as supervision for RGB. Tian et al. <ref type="bibr" target="#b61">[62]</ref> first explore the use of RGB and optical flow as views for contrastive learning. Most similar to our work are <ref type="bibr" target="#b61">[62]</ref> and <ref type="bibr" target="#b26">[27]</ref>, which both employ RGB and flow in a two-stream manner for contrastive learning. Han et al. <ref type="bibr" target="#b26">[27]</ref> use an alternated training process and samples hard positive examples from the other stream. Different from these works, we do not employ instance-level contrastive learning. As we use prototype mappings of our features and subsequently predict feature assignments, our streams leverage a stronger interplay. We also incorporate informed negative examples from both streams through our prototypes and we do not use a momentum encoder <ref type="bibr" target="#b28">[29]</ref>. As optical flow computation can be computationally expensive, several works avoid flow computation during inference while utilizing it during training, e.g. through knowledge distillation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b19">20]</ref> which is related to our work. Our proposed method instead keeps two streams and leverages an alternated optimization process to perform a form of distillation through contrastive learning, avoiding the need for optical flow while still enabling its optional use.</p><p>Multi-modal approaches. Video allows for a multimodal approach by using information such as audio <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and text <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61]</ref> to learn from correspondence between modalities. Alwassel et al. <ref type="bibr" target="#b0">[1]</ref> use a cross-modal audiovideo iterative clustering and relabeling algorithm. Asano et al. <ref type="bibr" target="#b1">[2]</ref> employ both RGB and audio in a simultaneous clustering and representation learning setting, following <ref type="bibr" target="#b2">[3]</ref>. Our method strictly speaking does not leverage multiple modalities as we use an optical flow representation originally extracted from the RGB representation, without introducing any external information. However, our work similarly leverages the interplay of complementary information and could therefore be used alternatively as a multi-modal approach, e.g. leveraging audio in addition to optical flow in order to improve representations further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We first introduce preliminaries on instance-level contrastive learning in Section 3.1. We explain how we can use RGB and optical flow separately to predict and learn prototypes following <ref type="bibr" target="#b7">[8]</ref> in Section 3.2. Finally, we introduce our contribution which consists of the cross-stream interplay and the steps of our algorithm in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Contrastive instance learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref> can be defined as a self-supervised learning method which contrasts in the latent space by maximizing agreement between different augmented views of the same data instances. Three key components in this framework are i) a data augmentation module that transforms a given sample x into two views x i and x j by applying separate transformations t and t ? sampled from the set of augmentations T , ii) the embedding function f (?) consisting of an encoder and a small MLP projection head that extracts feature vectors z i and z j from views, and iii) a contrastive loss function that contrasts between x i and a set {x k } of augmented pairs that includes our positive pair. Given a dataset X = {x 1 , x 2 , ..., x n }, we aim to learn a function f (?) that maps X to Z = {z 1 , z 2 , ..., z n }. The contrastive loss objective for a positive pair (i, j), referred to as the InfoNCE loss <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b11">12]</ref>, is then given by</p><formula xml:id="formula_0">L InfoNCE (z i , z j ) = ? log exp(z i ? z j /? ) k? =i exp(z i ? z k /? ) ,<label>(1)</label></formula><p>where ? is the temperature hyperparameter and z i ? z j refers to the dot product between normalized vectors, i.e. cosine similarity. The final loss is computed for all available positive pairs. Given a positive pair, a sufficiently large number of negative examples in {x k } needs to be available for which storage of features besides the mini-batch is often needed. The contrastive learning mechanism also neglects to take into account the informativeness of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Predicting stream prototype assignments</head><p>In our proposed method we avoid instance-level contrasting by using for each stream a set of prototypes in our contrasting. Furthermore, we extend the augmentation module by considering RGB frames and optical flow as views. Mathematically, given a video clip x we first consider the two streams as views, obtaining x = {x 1 , x 2 } which describe a RGB and a flow sample respectively. The objective is to learn the stream representations z 1 = f 1 (x 1 ) and z 2 = f 2 (x 2 ) through learning their encodings f 1 (?) and f 2 (?). Each of the encoders has a set of K trainable prototype vectors, {c 1 1 , ..., c 1 K } ? C 1 and {c 2 1 , ..., c 2 K } ? C 2 , implemented as a linear layer in the networks.</p><p>Consider only the training of one encoder f s on its own stream s where s ? {1, 2}. We denote the corresponding prototype set as matrix C s with columns c s 1 , ..., c s k . Given input sample x s , we obtain two augmented versions</p><formula xml:id="formula_1">{x s i , x s j }.</formula><p>After applying the encoder f s (?) we obtain features {z s i , z s j }. The features are mapped to the set of prototypes C s to obtain cluster assignments {q s i , q s j }, as detailed in the following section. The features and assignments are subsequently used in the following prediction loss:</p><formula xml:id="formula_2">L Single-stream s (z s i , z s j ) = l s z s j , q s i + l s z s i , q s j . (2)</formula><p>Each of the terms represents the cross-entropy loss between the stream prototype assignment q and the probability obtained by a softmax on the similarity between z and C s :</p><formula xml:id="formula_3">l s z s j , q s i = ? k q s,(k) i log exp(z s i ? c s k /? ) k ? exp(z s i ? c s k ? /? ) ,<label>(3)</label></formula><p>where ? is a temperature hyperparameter. The objective is to maximize the agreement of prototype assignments from multiple views of one sample (RGB or flow). Features are contrasted indirectly through comparing their prototype assignments. The total loss of training the encoder f s on its own stream is taken over all videos and pairs of data augmentations, minimized with respect to both f s and C s .</p><p>Learning stream prototype assignments. The assignments {q s i , q s j } are computed by matching features {z s i , z s j } to prototypes C s . In essence, we need to consider the crossentropy for assigning each z to C s and perform a mapping to assign labels automatically. Optimizing q directly leads to degeneracy. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> a uniform split of the features across prototypes is enforced, which avoids the collapse of assignments to one prototype. Given our feature vectors Z whose columns are z 1 , ..., z B , we map them to C s and optimize using an Optimal Transport <ref type="bibr" target="#b51">[52]</ref> solver the mapping Q = q 1 , .., q B :</p><formula xml:id="formula_4">max Q?Q Tr (Q T C T s Z) + ?H(Q),<label>(4)</label></formula><p>where H(Q) is the entropy of Q which acts as a regularizer. The ? parameter controls the uniformity of the assignment where a low value helps to avoid collapse. Following <ref type="bibr" target="#b7">[8]</ref>, we restrict the transportation polytope to mini-batches: </p><formula xml:id="formula_5">Q = {Q ? R K?B | Q1 B = 1 K 1 K , Q T 1 K = 1 B 1 K },<label>(5)</label></formula><formula xml:id="formula_6">1 i , z 1 j , z 2 i , z 2 j ,</formula><p>which are in turn assigned to either RGB or flow prototype vectors, depending on which stream s ? {1, 2} is optimized. Next, the stream prototype assignments</p><formula xml:id="formula_7">q 1 i , q 1 j , q 2 i , q 2 j</formula><p>are predicted using features only from the three other views. The encoder and prototypes from the optimized stream are updated by backpropagation, while the other encoder remains fixed.</p><p>where 1 K denotes a vector of all ones with dimension K. We preserve soft assignments Q * and the solution of the transportation polytope, solved efficiently using the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b16">[17]</ref> can be written as follows:</p><formula xml:id="formula_8">Q * = Diag(?) exp 1 ? C T s Z Diag(?),<label>(6)</label></formula><p>where ? and ? denote renormalization vectors such that Q results in a probability matrix <ref type="bibr" target="#b16">[17]</ref>. As the amount of batch features B is usually smaller than the number of prototypes K, we increase or available features B by adopting a queue mechanism that stores features from previous iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning cross-stream</head><p>We are now interested in using information from both streams for each encoder. Consider again the encoder f s and prototypes C s from the stream s that is optimized in one alternation. We now add a second stream t where t ? {1, 2} and s? =t. We use the encoder f t with frozen weights and obtain samples {x t i , x t j } and features {z t i , z t j }. By matching these features to prototypes C s , the assignments {q t i , q t j } are obtained. Given f s , C s , and f t , all initialized with prior representations learned on their own stream, the loss function for the prediction problem consist of four main parts:</p><formula xml:id="formula_9">L Cross-stream s z s i , z s j , z t i , z t j = L s z s i , z s j , z t j , q t i + L s z s i , z s j , z t i , q t j + L s z s j , z t i , z t j , q s i + L s z s i , z t i , z t j , q s j ,<label>(7)</label></formula><p>where the function L s measures the fit between three features z and an assignment q. For instance, the first of the L s terms is given by:</p><formula xml:id="formula_10">L s z s i , z s j , z t j , q t i = l s z s i , q t i + l s z s j , q t i + l s z t j , q t i .<label>(8)</label></formula><p>The total loss function therefore consist of 12 terms. Each of the terms l s again represents the cross-entropy between one feature z and one assignment q, e.g.:</p><formula xml:id="formula_11">l s (z s i , q t i ) = ? k q t,(k) i log exp(z s i ? c s k /? ) k ? exp(z s j ? c s k ? /? ) ,<label>(9)</label></formula><p>where we predict the assignment q t i from stream t (obtained by matching corresponding feature z t i to the prototypes C s ) using one of the augmented features z s i from stream s. In summary, we predict assignments from each of the four views using features originating from three views, see <ref type="figure" target="#fig_0">Figure 2</ref>. In the prediction of each q, we avoid the use of the feature z where s is equal to t (same stream) and i is equal to j (same augmentation). This setup forces the features to capture the same information by predicting consistent assignments from them. The total loss for cross-stream training on stream s is taken over all videos and pairs of augmentations, optimized with respect to f s and C s .</p><p>Alternation. The optimization process from this section is then performed vice versa on the other stream. For example, we first optimize our RGB encoder f 1 and the corresponding prototypes C 1 as our f s and C s using views from both f s (RGB) and f t (flow). Next, we optimize our flow encoder f 2 and prototypes C 2 as our f s and C s , and use RGB as our f t . See the appendix for detailed pseudocode.</p><p>ViCC Algorithm. Our complete algorithm is structured as follows. Stage 1) Single-stream. In the first stage, the two encoders f 1 and f 2 and their prototypes C 1 and C 2 are initialized from scratch and trained using their own input stream, following Equation 2. Stage 2) Cross-stream. In the second stage, cross-stream, the two models are trained in an alternating fashion using input from both streams. In one alternation, one of the streams s with encoder f s and prototypes C s is encouraged to predict mappings consistently following Equation 7, leveraging complementary information from the other stream through assigning views from f t to C s . Both the prototype mappings and the alternation process in our cross-stream mechanism serve as means for transferring knowledge from motion (flow) to RGB.</p><p>Inference. At the inference stage, depending on speed vs. accuracy requirements, both the RGB model f 1 trained with ViCC self-supervision can be used for downstream tasks as well as both RGB f 1 and flow f 2 by averaging predictions from the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>We use two datasets for our experiments: HMDB51 <ref type="bibr" target="#b35">[36]</ref> and UCF101 <ref type="bibr" target="#b58">[59]</ref>. UCF101 consists of 13K videos over 101 human action classes. HMDB51 is another widely used action recognition dataset and contains around 7K videos over 51 action classes. UCF101 and HMDB51 are both divided into three train/test splits. For self-supervised training we use UCF101 training split 1 without class labels. For downstream evaluation we use UCF101 and HMDB51 and evaluate on split 1 for both datasets, following prior work <ref type="bibr" target="#b26">[27]</ref>.</p><p>Data preprocessing. From the source videos at 25fps, input video clips are extracted at random time stamps. Our input video clips have a spatial resolution of 128?128 pixels. We use clips of 32 frames as input, without temporal downsampling for S3D. For R(2+1)D and R3D, we use input clips of 16 frames with temporal downscaling at rate 2. For optical flow, we use the widely used TV-L1 algorithm <ref type="bibr" target="#b78">[79]</ref> and follow practice in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref> for preprocessing. This means that we truncate large vectors with more than 20 in both channels, transform the values to range [0, 255] and append a third channel of 0s. Random cropping, horizontal flipping, Gaussian blur and color jittering are used in a frame-consistent manner on RGB and flow clips following recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>. For temporal augmentation we take clips at different time stamps with 50% probability.</p><p>Implementation and training. As our base encoder architecture we use S3D <ref type="bibr" target="#b73">[74]</ref>. We also test our method with the R(2+1)D-18 <ref type="bibr" target="#b63">[64]</ref> architecture, following recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, and the R3D-18 <ref type="bibr" target="#b27">[28]</ref> backbone. We use a 2-layer MLP projection head during self-supervised training that projects the backbone output to 128 dimensional space following SimCLR <ref type="bibr" target="#b11">[12]</ref>. In line with SwaV <ref type="bibr" target="#b7">[8]</ref>, we employ a linear layer updated by backpropagation as the prototype implementation. The projection head and the prototype layer are removed for downstream evaluation. During self-supervised training, we use a queue that consists of 1920 features. We use K=300 as the number of prototypes. The single-stream stage consists of 300 epochs. Next, the cross-steam stage is initialized with models from the singlestream stage and is trained for two cycles. In one crossstream cycle, we first train RGB for 100 epochs and then flow for 100 epochs, each time taking the newest models, following CoCLR <ref type="bibr" target="#b26">[27]</ref>. We run all our experiments with 4 Titan RTX GPUs with a batch size of 48.</p><p>Evaluation methods. We evaluate the quality of our learned video representation using two downstream video understanding tasks: nearest neighbour video retrieval and action recognition. In the former, retrieval is performed without any supervised finetuning. We follow common protocol <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b74">75]</ref> by using videos form the test set as queries for k nearest-neighbour (kNN) retrieval in the training set. We report Recall at k (R@K) where we mark the retrieval as correct if a video of the same class appears among the top kNN. In the latter downstream task, we initialize with our representation and evaluate two settings: linear probe and finetuning. For linear probe, we freeze the entire network and add a linear classifier. For finetuning, the entire network with linear layer is trained end-to-end. We report Top-1 accuracy for both settings. Data augmentation similar to the self-supervision stage is used except for Gaussian blur. At inference we follow the ten-crop procedure, where the center crop, four corners and the horizontal flipped version of these crops are obtained. The moving-window approach is used for taking clips followed by averaging the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model ablations</head><p>Impact of training stages. In <ref type="table" target="#tab_0">Table 1</ref> results are shown for several stages of our method in order to evaluate the improvement that cross-stream (Stage 1) has over singlestream (Stage 2). We report action recognition and nearestneighbour video retrieval on UCF101 split 1 and include <ref type="bibr" target="#b26">[27]</ref> as our baseline model, as it uses the contrastive instance loss on RGB and flow with additional positive examples. Training settings are kept identical across selfsupervised models. All methods, including single-stream, are trained on an equal amount of epochs (500 in total). Evaluated on nearest-neighbour retrieval, we observe that our RGB-1 network gains a significant performance benefit when learning and predicting from optical flow in stage 2, shown as RGB-2 (62.1% vs. 40.0%). Furthermore, when combining predictions from the RGB-2 model and the Flow-2 model, both trained with cross-stream, we obtain a further performance boost shown as ViCC-R+F-2 (65.1% vs. 62.1%). We outperform <ref type="bibr" target="#b26">[27]</ref> on retrieval by +9.5%, demonstrating the benefit of cross-stream prototype contrasting in ViCC. In linear probe downstream classification, our RGB-2 model again outperforms the RGB-1 one by a significant margin (72.2% vs. 49.2%). When end-to-end finetuned our self-supervised RGB-2 outperforms RGB-1 (84.3% vs. 81.8%). Further improvement is found by combining the predictions of the two streams, obtaining the result for R+F (90.5% vs. 84.3%). Here, our performance for R+F is on par with the RGB model from <ref type="bibr" target="#b26">[27]</ref>. As our cross-stream phase consists of cycles in which we alternate the training of streams, we further analyse the performance progress on video retrieval across training phases in <ref type="figure" target="#fig_1">Figure 3</ref>. We show the evolution from single-stream to crossstream for both models, where cross-stream consists of two  <ref type="table">Table 2</ref>. Ablations on streams used as views for assignment and prediction. We report Top-1 accuracy on action recognition finetuning on UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of prototypes</head><p>Method 100 300 1000</p><p>ViCC-RGB-2 83.5 84.3 83.9 ViCC-R+F-2 89.2 90.5 90.0 <ref type="table">Table 3</ref>. Impact of number of prototypes. We report Top-1 accuracy on action recognition finetuning on UCF101. cycles in which RGB and Flow are trained alternately. It can be seen that representations for both models continue to improve after one cycle, indicating that the alternating scheme is beneficial for ViCC representations. Ablations on stream views. We perform an ablation study on our model by investigating the importance of the streams used as views for both prediction and assignment. We first consider the number of features for prediction, where the normal setting is to use all other available views from streams s and t for prediction of each assignment q.</p><p>We now study the setting where we use two features for prediction originating only from the other stream t. <ref type="table">Table 2</ref> shows results for both settings, reporting Top-1 accuracy on UCF101 action recognition using the finetuning protocol. We find that using two features results in a slightly worse performance overall, suggesting that more views are beneficial for prediction of the assignments despite originating from the same stream as the assignment. The second setting that we evaluate is only using the other stream t for assignment, where we map only the two features from stream t to prototypes C s . Note, the prediction is performed as normal, using all other available views. Both models are again slightly underperforming compared to using all views. The results for stream views in both settings suggest that the information used from the other stream in ViCC cross-stream training is of more significance than its own stream. Indeed, we find that ViCC is robust against changes in views from its own stream as it almost performs in line with results using all views for both prediction and assignment. Impact of number of prototypes. We evaluate the impact of the number of stream prototypes K. Explored previously by <ref type="bibr" target="#b7">[8]</ref> on ImageNet <ref type="bibr" target="#b17">[18]</ref>, they found no significant impact on performance when varying the prototypes by several orders of magnitude using a sufficiently large amount of prototypes. In <ref type="table">Table 3</ref>, we show results on varying the number of prototypes to K={100, 1000}. We observe a slightly worse result for both settings for the RGB model and the R+F model. As we find no significant impact on the performance, our results are in line with previous work suggesting that the soft prototype mappings used for contrasting in ViCC are not necessarily a self-labeling approach similar to other pseudo-labeling approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b75">76]</ref>, despite the usefulness in contrasting for representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art</head><p>In this section, we compare our method with selfsupervised methods on action classification and video retrieval, reporting our models from the cross-stream stage.</p><p>Action recognition. We compare with several selfsupervised methods on action recognition in <ref type="table">Table 4</ref>, displaying our results for two backbone architectures. We organized the methods by backbone and include settings such as resolution (Res), number of frames and number of parameters (Param) for a fairer comparison. We include several methods pretrained on larger training datasets for both visual-only and multi-modal methods. In the following, we compare with visual-only modality on the same training set, with visual-only on larger datasets, and with  <ref type="table">Table 4</ref>. Comparison with prior self-supervised works on video action recognition on UCF101 and HMDB51 for finetuning and linear probe. We report Top-1 accuracy and compare with self-supervision pretraining on UCF101. In grey color we show larger pretraining datasets such as K-400 <ref type="bibr" target="#b9">[10]</ref> and multi-modal datasets (where T is text, A is audio).</p><p>multi-modal approaches on end-to-end finetuning. First, we significantly outperform previous approaches pretrained on UCF101 when considering the visual modality (V). On the S3D backbone, our R+F model (obtained by averaging RGB and flow predictions) achieves a Top-1 accuracy of 90.5% on UCF101 and a Top-1 accuracy of 62.2% on HMDB51. Our approach outperforms the best model of Han et al. <ref type="bibr" target="#b26">[27]</ref> by 3.2% on UCF101 and by 3.5% on HMDB51. We also achieve better performance than Pace Pred <ref type="bibr" target="#b68">[69]</ref>, which uses the S3D-G <ref type="bibr" target="#b73">[74]</ref> backbone, on both UCF101 and HMDB51. Using the R(2+1)D backbone, we obtain a Top-1 accuracy of 82.8% on UCF101 and a Top-1 accuracy of 52.4% on HMDB51 for RGB. When combining RGB and Flow predictions (R+F), we obtain 88.8% and 61.5% on the datasets respectively. We outperform VCOP <ref type="bibr" target="#b41">[42]</ref>, VCOP <ref type="bibr" target="#b74">[75]</ref>, PRP <ref type="bibr" target="#b76">[77]</ref> by a wide margin for both our models. With the R+F model we obtain a 7.2% increase on UCF101 and a 15.1% increase over RTT <ref type="bibr" target="#b30">[31]</ref>, underlined in the table as the second-best result. ViCC models therefore consistently outperform previous works on both backbones and evaluation datasets, where optical flow provides only an optional performance boost. Comparing against visual-only information using larger training sets, we outperform methods that use Kinetics (K-400) pretraining on HMDB51, using UCF101 pretraining, such as Pace Pred <ref type="bibr" target="#b67">[68]</ref> for R(2+1)D and SpeedNet <ref type="bibr" target="#b3">[4]</ref> for S3D-G. We also perform better on HMDB51 than some multi-modal ap-proaches that use text <ref type="bibr" target="#b45">[46]</ref> and audio <ref type="bibr" target="#b2">[3]</ref> for similar resolution, number of frames and backbone. Finally, comparing against methods on linear probe, we outperform CoCLR <ref type="bibr" target="#b26">[27]</ref> on the same training dataset by a significant margin. Nearest-neighbour retrieval. Next, we compare with selfsupervised approaches on nearest-neighbour clip retrieval in <ref type="table" target="#tab_2">Table 5</ref>. All methods are pretrained on UCF101. We also report results on R3D for a fairer comparison. Our ViCC approach outperforms all previous approaches by a significant margin on UCF101 and HMDB51 for both backbone networks R(2+1)D and S3D. Our R3D models outperforms previous methods with the same backbone significantly. We achieve a Top-1 Recall of 65.1% on UCF101 using the S3D backbone, outperforming the previous best by 9.2%. On HMDB51, we achieve a Top-1 Recall of 29.7%, which is a 8.8% increase on previous best. With the R(2+1)D backbone, we obtain a Top-1 Recall of 58.6% on UCF101 and 25.3% on HMDB51 for RGB, and 59.9% and 28.3% respectively for R+F. Compared to other self-supervised works apart from the second-best, the margins are significantly wider. We conclude that our cross-stream self-supervision model RGB learns useful motion features without needing optical flow during test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Nearest-neighbour retrieval</head><p>In <ref type="figure">Figure 4</ref>, we visualize query video clips from the UCF101 test set with its Top-3 nearest-neighbours from UCF101 HMDB51</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Year Backbone Modality R@1 R@5 R@10 R@20 R@1 R@5 R@10 R@20 Top-3 Nearest-neighbours Cross-stream (ViCC-RGB-2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swing</head><p>Swing Swing <ref type="figure">Figure 4</ref>. Nearest-neighbour retrieval results with our representations. The query video from the UCF101 test set is shown on the left, the top-3 nearest neighbours from the UCF101 training set on the right. Each video is visualized with 2 frames and we show results for single-stream (RGB-1) and cross-stream (RGB-2). The action label is shown above the video (not used during training), where green denotes the correct label and red denotes an incorrect result. Best viewed in color.</p><p>the UCF101 training set, retrieved using the ViCC representation without labels. The ground truth action labels are included above the video clips. We visualize results for single-stream (RGB-1) and cross-stream (RGB-2). Our qualitative results further support the benefit of crossstream training, showing that it helps to retrieve videos from the same semantic categories compared to single-stream, despite significant changes in appearance and background (e.g. Swing and WalkingWithDog). More difficult is the retrieval for the query video from class BlowDryHair, but we again observe that cross-stream training improves retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present the Video Cross-Stream Prototypical Contrasting (ViCC) framework for self-supervised representation learning. We demonstrate the advantages of using similar semantic groupings of RGB and flow views over methods that use instance-level contrastive learning, avoiding redundant comparisons and improving performance. By learning through predicting consistent prototype assignments from views originating from both streams, ViCC effectively transfers knowledge from the motion representation to appearance and vice versa. We demonstrate state-of-the-art performance on downstream video recognition tasks using visual-only self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Example code for ViCC</head><p>Here, we provide pseudocode in PyTorch-like style for the implementation of the cross-stream stage of ViCC-RGB. For the definition of the function sinkhorn that describes the Sinkhorn-Knopp algorithm we refer to <ref type="bibr" target="#b7">[8]</ref>.</p><p>Pseudocode for ViCC-RGB-2 in PyTorch-like style B. Implementation Details B.1. Implementation and Training SGD with LARS <ref type="bibr" target="#b77">[78]</ref> is used as the optimizer. A learning rate of 0.6, a weight decay of 10 ?6 and a cosine learning rate schedule with a final learning rate of 6 ? 10 ?4 are chosen. The temperature ? is set to 0.1, the Sinkhorn regularization parameter ? is set to 0.05 and we perform 3 iterations of the Sinkhorn-Knopp algorithm. We use batch shuffle <ref type="bibr" target="#b28">[29]</ref> to avoid the model exploiting local intra-batch information leakage for trivial solutions. For single-stream, the prototypes are frozen during the first 100 epochs of training. For cross-stream, the prototypes are directly updated from the start of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Queue</head><p>To store additional features for use in the assignment to prototypes, we employ a queue in line with <ref type="bibr" target="#b7">[8]</ref>. With 4 GPUs and a total batch size of 48 ? 4 = 192, we adopt a queue of size 1920 to store features from the last 10 batches. The queue is introduced when the evolution of features is slowing down, i.e. when the decrease of the loss function is moderate. For single-stream RGB (RGB-1) we introduce the queue at 150 epochs and for Flow-1 we introduce the queue at 200 epochs. For the cross-stream stage, we introduce the queue at 25 epochs in each alternation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Analysis of Prototypes</head><p>This section focuses on further analysis of the prototypes. The main purpose of the prototype sets in ViCC is to guide the contrasting of groups of views from streams in each iteration. In combination with the relatively stable performance observed when varying the number of prototypes, we conjecture that the prototypes are not a pseudo-labeling approach similar to other methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b75">76]</ref>. Despite this intuition and our use of soft assignments, we investigate the prototypes by visualizing video samples assigned to the same prototypes when rounding the assignments. We also evaluate the rounded prototype assignments from several of our self-supervised stages on standard cluster evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Visualization of Prototypes</head><p>In <ref type="figure">Figure 5</ref> we show the hard assignment of video samples to random prototypes. Video samples with the highest similarity scores to the prototype clusters are visualized. Prototype scores are indicated on the samples and the ground truth class labels of the samples are indicated below the groups. We can observe that video samples assigned to the same prototypes share semantic similarity and even belong to the same action class, despite the fact that class labels are not used during ViCC training. The prototypes seem effective at grouping together views from the same semantic class label, as the samples visualized are all from the same class. These semantically similar sets in ViCC thereby provide an advantage for video representation learning over methods that use contrastive instance learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Cluster evaluation</head><p>In this section, we evaluate the hard assignment of our prototype sets with standard cluster evaluation measures as done in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref>. Although the ground truth number of clusters is not known in advance for self-supervised training, we set the number of prototypes to K=101 for evaluation purposes only to match the number of class labels for UCF101. The Hungarian algorithm <ref type="bibr" target="#b36">[37]</ref> is then used to match selfsupervised labels to the ground truth labels to obtain accuracy (Acc). We also report the Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), mean entropy per cluster (where the optimal number is 0) and mean maximal purity per cluster as defined in <ref type="bibr" target="#b2">[3]</ref>. For example, the NMI ranges from 0 (no mutual information) to 100% (implying perfect correlation between self-supervised labels and the ground truth labels). <ref type="table" target="#tab_3">Table 6</ref> shows that our prototypes from the cross-stream stage (RGB-2 and Flow-2) obtain better performance on all measures compared to prototypes learned only on their own stream (RGB-1 and Flow-1), achieving e.g. a higher NMI, lower mean entropy per cluster and higher mean maximal purity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. T-SNE Visualization</head><p>In this section, we visualize ViCC representations of the UCF101 test set using the t-SNE clustering algorithm <ref type="bibr" target="#b65">[66]</ref> to project features to 2D. For clarity, only 10 random ac-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ViCC-RGB-1</head><p>ViCC-RGB-2 tion classes are visualized with a limited amount of random features for each class. <ref type="figure" target="#fig_3">Figure 6</ref> shows the t-SNE visualization of features extracted from single-stream (RGB-1)  <ref type="table">Table 7</ref>. Impact of queue size. We report Top-1 accuracy on action recognition finetuning on UCF101.</p><p>and cross-stream (RGB-2) trained using the same number of epochs (500). It can be observed that the inter-class distance between certain classes such as CricketBowling and GolfSwing is increased from RGB-1 to RGB-2. Moreover, the intra-class distance is reduced for classes FrisbeeCatch, BasketballDunk and ApplyEyeMakeup, which can be attributed to the benefit of motion learning from the flow encoder in cross-stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Impact of queue size</head><p>We investigate the effect of the queue size on performance. The queue is used in the assignment of features to K prototypes. In theory, using more features in each iteration on top of the current batch should result in a more accurate assignment for the Sinkhorn-Knopp algorithm. Results for queue sizes {3840, 1920, 0} are shown in <ref type="table">Table 7</ref>. We report Top-1 accuracy on action recognition on UCF101 finetuning. For queue size 3840, we observe that the larger queue size is not necessary or beneficial for UCF101 selfsupervised pretraining, as the differences in performance are minimal. We also find that using no queue almost performs on par with our default queue size of 1920. We conjecture that our mini-batches may already provide enough features for ViCC self-supervision on UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. More comparison with self-supervised works on action recognition</head><p>In <ref type="table">Table 8</ref> we list more results from self-supervised methods evaluated on action recognition. Results for the additional backbone R3D-18 <ref type="bibr" target="#b27">[28]</ref> are included. We achieve better performance than several methods that use the R3D backbone. Our overall best result on the S3D backbone still outperforms almost all methods pretrained on UCF101. We also outperform several methods pretrained on the larger dataset K-400, and achieve competitive performance compared to CVRL <ref type="bibr" target="#b54">[55]</ref>.  <ref type="table">Table 8</ref>. Comparison with prior self-supervised works on video action recognition on UCF101 and HMDB51 for finetuning and linear probe. We report Top-1 accuracy, compare with self-supervision pretraining on UCF101 and additionally report results on backbone R3D <ref type="bibr" target="#b27">[28]</ref>. In grey color we show larger pretraining datasets such as K-400 <ref type="bibr" target="#b9">[10]</ref> and multi-modal datasets (where T is text, A is audio).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Video Cross-Stream Prototypical Contrasting. Two different augmented samples are obtained for both RGB and flow. The encoders f1 and f2 map samples from RGB and flow respectively to obtain features z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Retrieval performance progress on our training phases. RGB and flow are subsequently optimized in one cross-stream cycle, where a dotted line indicates no optimization. We report Top-1 Recall (R@1) on UCF101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>#</head><label></label><figDesc>rgb_model: encoder network for RGB # flow_model: encoder network for flow, frozen # temp: temperature for rgb, flow in loader: # B samples # two augmented versions for two streams rgb_i, flow_i = aug(rgb_i, flow_i) rgb_j, flow_j = aug(rgb_j, flow_j) # get RGB and flow embeddings: 2B x D z_rgb = cat(rgb_model(rgb_i), rgb_model(rgb_j)) z_flow = cat(flow_model(flow_i), flow_model(flow_j)) # get similarity with prototypes C_rgb, C_rgb in D x K sim_rgb_i, sim_rgb_j = mm(z_rgb, C_rgb) sim_flow_i, sim_flow_j = mm(z_flow, C_rgb) # compute assignments with torch.no_grad(): q_rgb_i, q_rgb_j, q_flow_i, q_flow_j = sinkhorn(sim_rgb_i), sinkhorn(sim_rgb_j), sinkhorn(sim_flow_i), sinkhorn(sim_flow_j) # convert similarity scores to probabilities p_rgb_i, p_rgb_j, p_flow_i, p_flow_j = softmax(sim_rgb_i / temp), softmax(sim_rgb_j / temp), softmax(sim_flow_i / temp), softmax(sim_flow_j / temp) # predict cluster assignments using three other views l_rgb_i = q_rgb_i * log(p_rgb_j) + q_rgb_i * log(p_flow_i) + q_rgb_i * log(p_flow_j) l_rgb_j = q_rgb_j * log(p_rgb_i) + q_rgb_j * log(p_flow_i) + q_rgb_j * log(p_flow_j) l_flow_i = q_flow_i * log(p_rgb_i) + q_flow_i * log(p_rgb_j) + q_flow_i * log(p_flow_j) l_flow_j = q_flow_j * log(p_rgb_i) + q_flow_j * log(p_rgb_j) + q_flow_j * log(p_flow_i) # combine for total loss for rgb model loss = -1/4 * (1/3 * l_rgb_i + 1/3 * l_rgb_j + 1/3 * l_flow_i + 1/3 * l_flow_j) # optimizer update and normalize prototypes loss.backward() update(rgb_model.params), update(C_rgb) with torch.no_grad(): C_rgb = normalize(C_rgb, dim=0, p=2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>T-SNE visualization of the feature representations of UCF101 test set after 500 epochs of ViCC training. On the top RGB-1 single-stream is shown and on the bottom RGB-2 crossstream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Improvement of ViCC from single-stream (stage 1) to cross-stream (stage 2) evaluated on action recognition and nearestneighbour retrieval on UCF101.<ref type="bibr" target="#b26">[27]</ref> is included as a baseline comparison. R+F denotes the result obtained by averaging predictions of RGB and flow models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Classification</cell><cell>Retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Linear Finetune No labels</cell></row><row><cell>Method</cell><cell cols="2">Stage Input</cell><cell>Acc</cell><cell>Acc</cell><cell>R@1</cell></row><row><cell>ViCC-RGB-1</cell><cell>1</cell><cell>RGB</cell><cell>49.2</cell><cell>81.8</cell><cell>40.0</cell></row><row><cell>ViCC-Flow-1</cell><cell>1</cell><cell>Flow</cell><cell>71.9</cell><cell>87.9</cell><cell>55.5</cell></row><row><cell>ViCC-RGB-2</cell><cell>2</cell><cell>RGB</cell><cell>72.2</cell><cell>84.3</cell><cell>62.1</cell></row><row><cell>ViCC-Flow-2</cell><cell>2</cell><cell>Flow</cell><cell>75.5</cell><cell>88.7</cell><cell>59.7</cell></row><row><cell>CoCLR [27]</cell><cell>2</cell><cell>R+F</cell><cell>72.1</cell><cell>87.3</cell><cell>55.6</cell></row><row><cell>ViCC-R+F-2</cell><cell>2</cell><cell>R+F</cell><cell>78.0</cell><cell>90.5</cell><cell>65.1</cell></row><row><cell></cell><cell cols="5">Streams for prediction Streams for assignment</cell></row><row><cell>Method</cell><cell>s + t</cell><cell>t</cell><cell></cell><cell>s + t</cell><cell>t</cell></row><row><cell cols="2">ViCC-RGB-2 84.3</cell><cell>83.8</cell><cell></cell><cell>84.3</cell><cell>84.1</cell></row><row><cell>ViCC-R+F-2</cell><cell>90.5</cell><cell>90.2</cell><cell></cell><cell>90.5</cell><cell>90.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>OPN [40]</cell><cell></cell><cell>2017</cell><cell>VGG</cell><cell>V</cell><cell>19.9</cell><cell>28.7</cell><cell>34.0</cell><cell>40.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ST Order [5]</cell><cell cols="2">2018 CaffeNet</cell><cell>V</cell><cell>25.7</cell><cell>36.2</cell><cell>42.2</cell><cell>49.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ST-Puzzle [35]</cell><cell>2019</cell><cell>R3D</cell><cell>V</cell><cell>19.7</cell><cell>28.5</cell><cell>33.5</cell><cell>40.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VCOP [75]</cell><cell></cell><cell>2019</cell><cell>R3D</cell><cell>V</cell><cell>14.1</cell><cell>30.3</cell><cell>40.4</cell><cell>51.1</cell><cell>7.6</cell><cell>22.9</cell><cell>34.4</cell><cell>48.8</cell></row><row><cell cols="2">Pace Pred [69]</cell><cell>2020</cell><cell>R3D</cell><cell>V</cell><cell>23.8</cell><cell>38.1</cell><cell>46.4</cell><cell>56.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Var. PSP [14]</cell><cell>2020</cell><cell>R3D</cell><cell>V</cell><cell>24.6</cell><cell>41.9</cell><cell>51.3</cell><cell>62.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RTT [31]</cell><cell></cell><cell>2020</cell><cell>R3D</cell><cell>V</cell><cell>26.1</cell><cell>48.5</cell><cell>59.1</cell><cell>69.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ViCC-RGB (ours)</cell><cell></cell><cell>R3D</cell><cell>V</cell><cell>50.3</cell><cell>70.9</cell><cell>78.7</cell><cell>85.6</cell><cell>22.7</cell><cell>46.2</cell><cell>60.9</cell><cell>74.1</cell></row><row><cell cols="2">ViCC-R+F (ours)</cell><cell></cell><cell>R3D</cell><cell>V</cell><cell>52.1</cell><cell>71.7</cell><cell>79.8</cell><cell>86.0</cell><cell>25.2</cell><cell>48.1</cell><cell>61.1</cell><cell>72.7</cell></row><row><cell cols="2">MemDPC [26]</cell><cell>2020</cell><cell>R2D3D</cell><cell>V</cell><cell>20.2</cell><cell>40.4</cell><cell>52.4</cell><cell>64.7</cell><cell>7.7</cell><cell>25.7</cell><cell>40.6</cell><cell>57.7</cell></row><row><cell>VCP [43]</cell><cell></cell><cell cols="2">2020 R(2+1)D</cell><cell>V</cell><cell>19.9</cell><cell>33.7</cell><cell>42.0</cell><cell>50.5</cell><cell>6.7</cell><cell>21.3</cell><cell>32.7</cell><cell>49.2</cell></row><row><cell>CoCLR [27]</cell><cell></cell><cell>2020</cell><cell>S3D</cell><cell>V</cell><cell>55.9</cell><cell>70.8</cell><cell>76.9</cell><cell>82.5</cell><cell>26.1</cell><cell>45.8</cell><cell>57.9</cell><cell>69.7</cell></row><row><cell cols="2">ViCC-RGB (ours)</cell><cell></cell><cell>R(2+1)D</cell><cell>V</cell><cell>58.6</cell><cell>76.2</cell><cell>83.1</cell><cell>89.0</cell><cell>25.3</cell><cell>50.4</cell><cell>64.0</cell><cell>77.5</cell></row><row><cell cols="2">ViCC-R+F (ours)</cell><cell></cell><cell>R(2+1)D</cell><cell>V</cell><cell>59.9</cell><cell>77.6</cell><cell>84.6</cell><cell>90.6</cell><cell>28.3</cell><cell>52.7</cell><cell>65.3</cell><cell>77.0</cell></row><row><cell cols="2">ViCC-RGB (ours)</cell><cell></cell><cell>S3D</cell><cell>V</cell><cell>62.1</cell><cell>77.1</cell><cell>83.7</cell><cell>87.9</cell><cell>25.5</cell><cell>49.6</cell><cell>61.9</cell><cell>72.5</cell></row><row><cell cols="2">ViCC-R+F (ours)</cell><cell></cell><cell>S3D</cell><cell>V</cell><cell>65.1</cell><cell>80.2</cell><cell>85.4</cell><cell>89.8</cell><cell>29.7</cell><cell>54.6</cell><cell>66.0</cell><cell>76.2</cell></row><row><cell>Query</cell><cell cols="6">Top-3 Nearest-neighbours Single-stream (ViCC-RGB-1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Swing</cell><cell cols="2">Swing</cell><cell>Swing</cell><cell></cell><cell cols="2">TrampolineJumping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WalkingWithDog</cell><cell cols="2">WalkingWithDog</cell><cell>CliffDiving</cell><cell></cell><cell cols="2">PlayingFlute</cell><cell></cell><cell cols="2">WalkingWithDog</cell><cell cols="2">WalkingWithDog</cell><cell>WalkingWithDog</cell></row><row><cell>BlowDryHair</cell><cell cols="2">Haircut</cell><cell>Benchpress</cell><cell></cell><cell cols="2">Shavingbeard</cell><cell></cell><cell>BlowDryHair</cell><cell></cell><cell cols="2">HeadMassage</cell><cell>HeadMassage</cell></row></table><note>. Comparison with self-supervised methods on nearest-neighbour video retrieval. All self-supervised methods are pretrained on UCF101 split 1. We show results on Top-k Recall (R@k) for k={1, 5, 10, 20} on UCF101 split 1 and HMDB51 split 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Visualization of rounded assignments to random ViCC prototypes using videos from UCF101. Samples with high similarity scores (visualized on the samples) to the prototypes are shown. The ground truth labels of all the video samples are included below (not used during training). Cluster evaluation of ViCC prototypes when rounding the assignments evaluated on the UCF101 test set.</figDesc><table><row><cell>0.93</cell><cell>0.82</cell><cell>0.81</cell><cell>0.86</cell><cell>0.86</cell><cell>0.88</cell><cell>0.83</cell><cell>0.87</cell><cell>0.90</cell><cell>0.93</cell></row><row><cell>0.83</cell><cell>0.89</cell><cell>0.93</cell><cell>0.82</cell><cell>0.87</cell><cell>0.84</cell><cell>0.94</cell><cell>0.82</cell><cell>0.94</cell><cell>0.95</cell></row><row><cell>0.90</cell><cell>0.91</cell><cell>0.83</cell><cell>0.84</cell><cell>0.89</cell><cell>0.95</cell><cell>0.92</cell><cell>0.80</cell><cell>0.95</cell><cell>0.93</cell></row><row><cell>0.92</cell><cell>0.93</cell><cell>0.93</cell><cell>0.82</cell><cell>0.92</cell><cell>0.91</cell><cell>0.65</cell><cell>0.78</cell><cell>0.76</cell><cell>0.61</cell></row><row><cell cols="2">BoxingPunchingBag</cell><cell>PoleVault</cell><cell></cell><cell>PlayingDhol</cell><cell></cell><cell>Bowling</cell><cell></cell><cell>Mixing</cell><cell></cell></row><row><cell>Figure 5. Method</cell><cell cols="3">Acc NMI ARI Entropy Max Purity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ViCC-RGB-1 32.3 62.5 16.4</cell><cell>1.6</cell><cell>36.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ViCC-Flow-1 34.4 63.1 17.6</cell><cell>1.5</cell><cell>39.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ViCC-RGB-2 40.8 67.8 24.5</cell><cell>1.4</cell><cell>45.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ViCC-Flow-2 40.3 67.0 23.5</cell><cell>1.4</cell><cell>45.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank Prof. dr. Cees G.M. Snoek for the helpful comments and feedback. The presentation of this paper at the conference was financially supported by the Amsterdam ELLIS Unit, Qualcomm and the Master AI program of the University of Amsterdam.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-Supervised Learning by Cross-Modal Audio-Video Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SpeedNet: Learning the Speediness in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>B?chler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Parametric Instance Classification for Unsupervised Visual Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Clustering for Unsupervised Learning of Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">A Short Note about Kinetics-600. ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Vggsound: A Large-Scale Audio-Visual Dataset. In ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improved Baselines with Momentum Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-Supervised Spatio-Temporal Representation Learning Using Variable Playback Speed Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02692</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lin Yen-Chen, Antonio Torralba, and Stefanie Jegelka. Debiased Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MARS: Motion-Augmented RGB Stream for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nieves</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-Supervised Video Representation Learning With Odd-One-Out Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Motion-Augmented Self-Training for Video Recognition at Smaller Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Karmanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01646</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">David</forename><surname>Melvyn A Goodale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Spatiotemporally Coherent Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video Representation Learning by Dense Predictive Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memoryaugmented Dense Predictive Coding for Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selfsupervised Co-training for Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video Representation Learning by Recognizing Temporal Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Givi</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-Supervised Spatiotemporal Feature Learning via Video Rotation Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hard Negative Mixing for Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Pion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-Supervised Video Representation Learning with Space-Time Cubic Puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estibaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MAST: A Memory-Augmented Self-supervised Tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised Learning for Video Correspondence Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning by Sorting Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prototypical Contrastive Learning of Unsupervised Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploring Relations in Untrimmed Videos for Self-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Qing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02711</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross Pixel Optical Flow Similarity for Self-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two visual streams for perception and action: Current trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1391" to="1396" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Pretext-Invariant Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved Dense Trajectory with Cross Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsunori</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Multi-modal Self-Supervision from Generalized Data Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Computational optimal transport: With applications to data science. Foundations and Trends? in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Seeing the Arrow of Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyndsey</forename><forename type="middle">C</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Evolving Losses for Unsupervised Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatiotemporal Contrastive Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">Contrastive Learning with Hard Negative Samples. In ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Two visual systems: Brain mechanisms for localization and discrimination are dissociated by tectal and cortical lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multiclass N-pair Loss Objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">D3D: Distilled 3D Networks for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning Video Representations using Contrastive Bidirectional Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Contrastive Multiview Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Representation Learning with Contrastive Predictive Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Generating Videos with Scene Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-supervised Spatio-temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Selfsupervised Video Representation Learning by Pace Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Representations using Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning Correspondence from the Cycle-Consistency of Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning and Using the Arrow of Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instancelevel Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Self-Supervised Spatiotemporal Learning via Video Clip Order Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">ClusterFit: Improving Generalization of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Video Playback Rate Perception for Self-supervisedSpatio-Temporal Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Large Batch Training of Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A Duality Based Approach for Realtime TV-L1 Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM-Symposium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Yew Soon Ong, and Chen Change Loy. Online Deep Clustering for Unsupervised Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dance with Flow: Two-in-One Stream Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaojiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Unsupervised Learning from Video with Deep Neural Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11954</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In ArXiv Preprint</note>
	<note>Max Sobol Mark, and Daniel Yamins</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
