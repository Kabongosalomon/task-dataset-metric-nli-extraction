<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Focus-aware Positional Queries for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>He</surname></persName>
							<email>haoyu.he@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
							<email>zizheng.pan@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<email>jing.liu1@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">JD Explore Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
							<email>bohan.zhuang@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Focus-aware Positional Queries for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the latest top semantic segmentation approaches are based on vision Transformers, particularly DETR-like frameworks, which employ a set of queries in the Transformer decoder. Each query is composed of a content query that preserves semantic information and a positional query that provides positional guidance for aggregating the query-specific context. However, the positional queries in the Transformer decoder layers are typically represented as fixed learnable weights, which often encode dataset statistics for segments and can be inaccurate for individual samples. Therefore, in this paper, we propose to generate positional queries dynamically conditioned on the cross-attention scores and the localization information of the preceding layer. By doing so, each query is aware of its previous focus, thus providing more accurate positional guidance and encouraging the cross-attention consistency across the decoder layers. In addition, we also propose an efficient way to deal with high-resolution cross-attention by dynamically determining the contextual tokens based on the low-resolution cross-attention maps to perform local relation aggregation. Our overall framework termed FASeg (Focus-Aware semantic Segmentation) provides a simple yet effective solution for semantic segmentation. Extensive experiments on ADE20K and Cityscapes show that our FASeg achieves state-of-the-art performance, e.g., obtaining 48.3% and 49.6% mIoU respectively for single-scale inference on ADE20K validation set with ResNet-50 and Swin-T backbones, and barely increases the computation consumption from Mask2former. Source code will be made publicly available at https://github.com/zip-group/FASeg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation aims at assigning each pixel in an image with a semantic class label. As vision Transformers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25]</ref> have gained much popularity, semantic segmentation with Transformers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b34">35]</ref> outperforms purely convolutional neural network (CNN) based segmentors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref> and becomes the dominant architecture.</p><p>Recently, many segmentors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref> follow DETR-like frameworks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38]</ref> to learn a set of queries that are progressively refined with the backbone features in the stacked Transformer decoder layers for the final prediction. Although promising, these segmentors suffer from two limitations that can deteriorate the performance. First, the positional queries in the DETR-like decoder are represented as randomly initialized parameters, which is not a good choice. Specifically, in DETRlike frameworks, each query is formed by a content query and a positional query. As studied in <ref type="bibr" target="#b32">[33]</ref>, ? Corresponding author Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the positional queries provide positional guidance for aggregating the query-specific context from the encoder output features in the cross-attention layer. In this case, the positional queries formed as randomly initialized learnable parameters tend to encode the average statistics across the dataset. However, for an individual sample, the dataset statistics are inaccurate positional guidance, which may hamper the segmentation performance. Second, the prior arts have not considered the consistency for the cross-attention across decoder layers, which is important for locating and segmenting foreground regions in a smooth and progressive way. Towards these issues, recent detector DAB-DETR <ref type="bibr" target="#b21">[22]</ref> integrates the concept of conditional computing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref> into the Transformer decoder and encodes the predicted bounding box positions into positional queries as dynamic anchors. Another detector SMCA <ref type="bibr" target="#b16">[17]</ref> generates Gaussian distributed weight map from the previous bounding box prediction to modulate cross-attention with consistency among the decoder layers. However, both DAB-DETR <ref type="bibr" target="#b21">[22]</ref> and SMCA <ref type="bibr" target="#b16">[17]</ref> are delicately designed for detection tasks and heavily depend on the predicted bounding boxes as the anchor positions, which are not directly applicable to semantic segmentation because segmenting semantic regions require encoding fine details such as edges and boundaries, which cannot be well described by bounding boxes.</p><p>In this paper, we propose a simple yet effective query design for semantic segmentation, namely Dynamic Focus-aware Positional Queries (DFPQ) to tackle the above-mentioned issues. Specifically, we propose to encode the cross-attention scores and the localization information of the preceding layer in the decoder's positional queries. Therefore, the previous focus for each query provides reasonable positional guidance for the current layer. Compared with the original randomly initialized learnable positional queries <ref type="bibr" target="#b3">[4]</ref>  <ref type="figure">(Figure 1 (a)</ref>) that are shared among decoder layers and tend to encode dataset statistics, our focus-aware positional queries <ref type="figure">(Figure 1 (b)</ref>) are data-dependent, and thus provide a more accurate positional guidance for the cross-attention layer. Besides, our query design encourages the cross-attention consistency across the decoder layers so that the preceding layers can help identify and locate the target segments in the current layer smoothly and progressively without dependence on the anchor points.</p><p>In addition, we propose an efficient method to handle high-resolution cross-attention and mine details for segmenting small regions from the high-resolution feature maps (1/4 ? 1/4 of the original image size). Considering performing cross-attention on high-resolution feature maps requires a formidable amount of memory footprints and computational complexity, e.g., 7G extra floating-point operations with 512 ? 512 resolution for the input image, we propose to encode token affinity only on the areas of high-resolution feature maps that are indicated to be important in the low-resolution attention maps. In this way, fine-grained details are learned efficiently with significantly reduced memory and computations.</p><p>Our main contributions can be summarized as follows:</p><p>? We make the first attempt to present a simple yet effective query formulation for semantic segmentation. We propose to dynamically generate the positional queries in Transformer decoder layers based on the cross-attention maps of the preceding layer, which facilitates local context aggregation from the target regions suggested by the cross-attention operations and encourages cross-attention consistency across the Transformer decoder layers to segment the target progressively.</p><p>? We propose an efficient high-resolution cross-attention that only models partial areas in high-resolution feature maps that are identified to be important in the low-resolution crossattention maps, thereby enriching segmentation details while exhibiting less memory footprint and computational cost.</p><p>? By simply incorporating our dynamic focus-aware position queries and efficient fine-grained cross-attention into the latest Mask2former <ref type="bibr" target="#b6">[7]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention maps</head><p>(a) (b) <ref type="figure">Figure 1</ref>: (a) The original randomly initialized positional queries <ref type="bibr" target="#b3">[4]</ref> as network parameters, where the positional queries are shared among the Transformer decoder layers and tend to encode dataset statistics recording the likely positions for the semantic regions. (b) The proposed dynamic focusaware queries, where the learnable queries are dynamically generated considering the cross-attention scores and the localization information of the preceding layer and thereby providing more accurate positional guidance.</p><p>three parts: a backbone, a neck, and a head (decoder) that is attached after the backbone or neck, where both the backbone and the head can be implemented by Transformers. Therefore, the recent advances can also be roughly split into the three categories corresponding to improving the three parts. The first category <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref> aims at providing a better backbone to encode more representative features, mostly improving the self-attention mechanism in Transformers. For example, Swin Transformer <ref type="bibr" target="#b22">[23]</ref> proposes an efficient local window self-attention to reduce the computational complexity as well as insert locality. The second category <ref type="bibr" target="#b34">[35]</ref> endeavors to provide a better neck, which usually incorporates multi-scale features with the popular feature pyramid network (FPN) <ref type="bibr" target="#b20">[21]</ref> or pyramid scene parsing (PSP) <ref type="bibr" target="#b41">[42]</ref> architecture. For instance, segformer <ref type="bibr" target="#b34">[35]</ref> simplifies FPN under the Transformer backbone to integrate multi-level features with different resolutions. The last category <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41]</ref> improves the Transformer head, which learns a set of queries and refines them with the backbone features in the Transformer decoder layers. For example, very recent work employs multi-scale features in the Transformer head, e.g., PFD <ref type="bibr" target="#b25">[26]</ref>, SenFormer <ref type="bibr" target="#b2">[3]</ref> and Mask2former <ref type="bibr" target="#b6">[7]</ref> fuse multi-scale information after the Transformer decoder, concurrently in the Transformer decoder, and sequentially in the Transformer decoder, respectively.</p><p>Our work aligns with the last category to improve the Transformer decoder and keep the other parts untouched. Different from the prior work, we are the first work improving the query designs for the semantic segmentation. We propose a simple yet effective DFPQ with the idea of conditional computing <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>, which provides more accurate positional guidance as well as maintain crossattention consistency among the decoder layers.</p><p>Positional encodings for Transformers. Both self-attention and cross-attention for Transformers are permutation-equivalent. Therefore, positional encodings play an essential role in introducing the order of the sequence. In general, the positional encodings include: absolute positional encodings that are generated with sinusoidal functions <ref type="bibr" target="#b31">[32]</ref> or being entirely learnable parameters <ref type="bibr" target="#b17">[18]</ref>; relative positional encodings that encode distances between the input tokens <ref type="bibr" target="#b10">[11]</ref>; and conditional positional encodings, which are dynamically generated, e.g., <ref type="bibr" target="#b8">[9]</ref> generates positional encodings conditioned on the neighborhood locality information with depth-wise convolution which can deal with arbitrary sequence length. In the same spirit, our DFPQ also encodes data-dependent positional information, and thus it delivers higher segmentation accuracy with query-specific positional guidance. In contrast to <ref type="bibr" target="#b8">[9]</ref>, we tailor our DFPQ to DETR-like frameworks, which are generated conditioned on the cross-attention scores and the localization information from the preceding layer. Besides, our DFPQ introduces locality that is not restricted by the convolutional inductive bias as <ref type="bibr" target="#b8">[9]</ref>.</p><p>When solely pre-training Transformer backbones, the positional encodings are generally seen as a part of the feature embeddings and directly be added with patch features after patchifying the image <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>. In the cross-attention layers of DETR-like frameworks, both the encoder output features and the content queries require positional encodings, but the positional encodings in crossattention only serve to adjust attention maps, which we refer the readers to Section 3 for details. In this case, the positional queries provide explicit positional guidance for aggregating the query-specific context in the cross-attention layer. Recent detectors <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b21">22]</ref> encode anchor positions into positional queries to ease the optimization difficulty. Contrarily, we propose to encode the cross-attention scores and localization information into the positional queries, which are more suitable for segmenting semantic regions that cannot be well described by bounding boxes. Moreover, our DFPQ also encourages the cross-attention consistency among the Transformer decoder layers which has not been considered by the previous positional encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary: Cross-attention in DETR</head><p>For the Transformer decoder in DETR-like frameworks <ref type="bibr" target="#b3">[4]</ref>, cross-attention is a basic module which learns to update the decoder feature embeddings with the encoder output features. With N , D, H and W respectively denoting the number of queries, the hidden dimensions, the height, and the width of the encoder output features, we denote the queries as Q ? R N ?D and keys as K ? R HW ?D for the cross-attention layer. In DETR models, both Q and K contain a content part and a positional part. Denoting the content queries (decoder feature embeddings) as Q c and positional queries (e.g., learnable positional queries) as Q p , we have Q = Q c + Q p . Similarly, denoting the Transformer encoder output features as K c and the positional encodings for the Transformer encoder output features as K p , we have K = K c + K p . Then, the cross-attention operation can be formulated as</p><formula xml:id="formula_0">Crs-Attention(Q, K, V ) = softmax QK T ? D V ,<label>(1)</label></formula><p>where the values are denoted by V ? R HW ?D . Note that K c and V are usually shared in the DETR-like frameworks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref> and we omit all the linear projections and bias terms for simplicity. From Eq. (1), we can interpret the cross-attention as aggregating features from the encoder output feature map based on the dot-product similarity between the queries and keys. Since both the content part and the positional part for Q and K are involved in calculating the attention map, the resulting attention map considers both the content and positional similarities, where the content similarity contributes to mining the co-relation between the decoder embedding and the encoder features and the positional similarity provides a query-specific positional guidance.</p><p>4 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation</head><p>The positional queries Q p play an essential role for cross-attention in DETR models as they provide positional guidance on the encoder output features to explicitly modulate attentions <ref type="bibr" target="#b21">[22]</ref>. Before introducing our DFPQ, we first discuss the motivation for our work by presenting two desired properties for positional queries in the semantic segmentation task.</p><p>The first desired property is being data-dependent. Recall that the original DETR-like frameworks randomly initialize Q p as learnable network parameters, where Q p is shared among decoder layers. Therefore the positional queries tend to encode dataset statistics recording the likely positions for semantic regions. Such positional queries are content-agnostic and prone to encode dataset bias, which is inaccurate for individual samples. In contrast, data-dependent positional queries can provide more accurate query-specific positional guidance in the cross-attention layer. Recent detector DAB-DETR <ref type="bibr" target="#b21">[22]</ref> carefully re-designs the positional queries to dynamically introduce explicit positional guidance on the neighborhood area around the anchor predicted by the previous layer. However, it is specifically designed for the detection task and depends on the anchor position. In contrast, the positional queries for the semantic segmentation task require finer details (e.g., edges and boundaries) which cannot be simply summarized in a position and a scale with an anchor.</p><p>The second desired property is maintaining proper consistency among attended regions from the preceding cross-attention layers in the Transformer decoder. In DETR-like frameworks, decoder layers are sequentially stacked to aggregate encoder output features via the cross-attention mechanism with a prediction head attached after. Thus, the attention map in the cross-attention layer of the preceding layer is helpful for 1) identifying local foreground regions and shapes; 2) locating and providing guidance on pixels, especially when the corresponding encoder features preserve the semantics of different granularity levels <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7]</ref>. For example, once an object is recognized in low-resolution feature maps, the positional query can guide the subsequent layers to refine details with high-resolution feature maps. To maintain the attention consistency in the decoder layers, one solution is to enforce the attention patterns to have a small Wasserstein distance <ref type="bibr" target="#b0">[1]</ref>. However, it is hard to design the margin since too similar attentions in pyramid feature maps make it difficult to segment regions at different scales or in different shapes. with the attention map A t?1 followed by a projection function h. H t?1 and W t?1 are the height and width for encoder output features in the (t ? 1)-th decoder layer, respectively. Here we omit the bias term B for simplicity. (b) The overall framework for our FASeg. In addition to Mask2former <ref type="bibr" target="#b6">[7]</ref>, we incorporate dynamic focus-aware positional queries to provide proper positional guidance (blue arrow) and efficient high-resolution cross-attention to enrich segmentation details (orange arrow). Here "Top-k" selects the top-k pixels indicated by the previous cross-attention scores of the low-resolution Transformer decoder layer. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-attention</head><formula xml:id="formula_1">+ + ! " ! ! ? ? !"# !"# # " # "$% Class Mask Backbone Top-k Pixel Deco der Transformer Decoder 3? (a) (b) ? ! ! .. ! "&amp;% ? "&amp;% + ! ! ? # " ! " " " .. .. .. .. ?(') !"# !"# ? ..</formula><p>In a nutshell, we deem that a reasonable positional query for the semantic segmentation task should have the following properties:</p><p>(1) Be data-dependent to serve as accurate positional guidance for feature aggregation in each cross-attention layer; (2) Maintain proper attention consistency with the preceding cross-attention layers among the Transformer decoder layers without introducing too strong consistency constraints. Therefore, the former layer's focus can help identify and locate the target segment.</p><p>In this work, we find that a simple design is to encode the cross-attention scores and the localization information of the previous layer into the positional queries, which we term as Dynamic Focus-aware Positional Queries (DFPQ). Being data-dependent, DFPQ learns the proper consistency with several simple non-linear projection layers and guides the cross-attention on where to attend. Moreover, as attended areas contain explicit information for semantic segmentation around the foreground <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref>, our DFPQ that is aware of the previous focus helps to adaptively aggregate the information for each query at the subsequent stacked decoder layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dynamic Focus-aware Positional Queries</head><p>We depict cross-attention with our dynamic focus-aware positional queries in <ref type="figure" target="#fig_0">Figure 2</ref> (a). In essence, since K p preserves positional information for each pixel in encoder output features, we form our dynamic focus-aware positional queries by aggregating K p as indicated by A in the previous cross-attention layer, which can be formulated as</p><formula xml:id="formula_2">Q t p = h(A t?1 K t?1 p + B),<label>(2)</label></formula><p>where t is the index of the t-th Transformer decoder layer, A t?1 ? R N ?HW is the cross-attention map from the (t ? 1)-th Transformer decoder layer, h is a two-layer non-linear projection with ReLU non-linearity in between and B ? R N ?D is learnable network parameters. In this case, we first linearly combine K t?1 p with the cross-attention scores A t?1 to aggregate the positional information from the positional encodings of the encoder output features according to the focus of the previous decoder layer. Next, we add a bias term and apply a non-linear projection to learn how to employ such positional information to minimize loss via back propagation. We visualize some cross-attention attention maps with and without our DFPQ in <ref type="figure">Figure 3</ref> and observe that our DFPQ helps generate more compact and consistent attention maps focusing on the target segments.  <ref type="figure">Figure 3</ref>: Comparison of cross-attention attention maps for Mask2former <ref type="bibr" target="#b6">[7]</ref> with and without DFPQ for semantic segmentation, where the attention maps without DFPQ 1) are scattered and cannot focus on the target region (row 1, layer 8 and row 3, layer 8); mix up different segments (row 1, layers 2 and 4). While, the attention maps with DFPQ are more compact and consistent. The target foreground is reflected in the red box. We visualize the early layers (layers 2, 3 and 4) and deep layers (layers 8 and 9) out of the nine Transformer decoder layers. The attention scores are normalized within the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficient High-resolution Cross-attention</head><p>As demonstrated by the prior arts <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20]</ref>, high-resolution encoder features are important for segmenting small regions. However, modeling cross-attention between decoder embeddings and highresolution encoder features requires a unbearable amount of memory footprints and computational cost. In this case, we propose an efficient High-Resolution Cross-Attention (HRCA) to mine details from high-resolution feature maps with acceptable memory burden. Specifically, we first select top-k pixels within the feature maps with the highest attention scores from the low-resolution feature maps. We then map these areas to the high-resolution feature map positions in a top-down manner and only perform cross-attention on these positions. Formally, including the top-k pixels regarding to the attention scores in the previous low-resolution attention map into set ?, the efficient high-resolution cross-attention can be formulated as</p><formula xml:id="formula_3">HRCA(Q, K, V , ?) = softmax QK T ? D V ,<label>(3)</label></formula><p>where K = g (K, f (?)), V = g(V , f (?)), f (?) is the mapping that maps the low-resolution positions to the corresponding high-resolution positions (e.g., upsampling operation) and g is the indexing operation. In this way, we only perform cross-attention on the informative areas for high-resolution feature maps, thereby having affordable memory consumption and computational complexity.</p><p>Actually, our HRCA is closely related to the previous sparse attention methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29</ref>] that attend to partial of the keys and values instead of the entire sequence. Differently, these methods are designed for self-attention layers and often rely on neighboring local relations regarding to the grid features, while our HRCA is designed for the cross-attention layers where queries are not explicitly defined with the local neighborhood information. In this case, the previous sparse self-attention methods are not directly applicable to the cross-attention layers. One similar work to our HRCA is the RCDA module <ref type="bibr" target="#b32">[33]</ref> which is a representative sparse cross-attention method that decouples cross-attention into a row-wise and a column-wise attention to reduce the memory and computation cost. We include the comparison between our HRCA and RCDA <ref type="bibr" target="#b32">[33]</ref> in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Focus-aware Semantic Segmentation</head><p>Built upon our DFPQ and HRCA, we develop our FASeg with the recent universal image segmentation framework Mask2former <ref type="bibr" target="#b6">[7]</ref>. The overview of our FASeg is depicted in <ref type="figure" target="#fig_0">Figure 2 (b)</ref>. The modifications are mainly in the Transformer decoder and we refer the readers to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> for details of the three Maskformer components: a backbone, a pixel decoder, and a Transformer decoder.</p><p>First, we apply DFPQ (Section 4.2) for each cross-attention layer to obtain more accurate positional guidance for the cross-attention layer and encourage attention consistency among the cross-attention layers in the decoder. Note that in the first Transformer decoder layer, we obtain the positional queries by performing average pooling on the predicted foreground mask from the auxiliary prediction head before the Transformer decoder <ref type="bibr" target="#b6">[7]</ref>. Besides, we introduce an extra decoder layer modeling high-resolution cross-attention (1/4 ? 1/4 of the original image size) to enrich the segmentation details at the end of the Transformer decoder, which is also repeated three times. To alleviate the burden on the peak-time memory footprints and the computational complexity, we employ HRCA (Section 4.3) to restrict the attended area as indicated by the attention scores from the previous low-resolution attention map for cross-attention. With the two simple modifications, the performance gain over the original Mask2former is significant (See Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Implementation details. We adopt the same training settings and implementation details as in Mask2former <ref type="bibr" target="#b6">[7]</ref> except that we relax the masked threshold from 0.5 to 0.15. For our efficient high-resolution cross-attention in Section 4.3, we select |?| = HW/32 from the low-resolution feature maps (1/32 ? 1/32 of the original image size). By default, we train our models with a batch size of 16 on 8 NVIDIA V100 GPUs and evaluate the trained models with a single RTX 3090 GPU. We adopt ResNet <ref type="bibr" target="#b18">[19]</ref> and Swin Transformer <ref type="bibr" target="#b22">[23]</ref> as the pre-trained backbones for training. For ResNet <ref type="bibr" target="#b18">[19]</ref>, we use the ResNet-50 (R50) variant. For Swin Transformer <ref type="bibr" target="#b22">[23]</ref>, we use the Swin-T and Swin-L backbones where Swin-L is pre-trained on ImageNet-22k <ref type="bibr" target="#b11">[12]</ref>. For the ablation experiments, unless specified otherwise, we adopt all training settings the same as the default settings of FASeg with ResNet-50 <ref type="bibr" target="#b18">[19]</ref> backbone on ADE20K val <ref type="bibr" target="#b43">[44]</ref> with 150 categories. Our FASeg also makes some additional modifications from Mask2former <ref type="bibr" target="#b6">[7]</ref>. First, we empirically find that different implementations of K p affect the performance of FASeg significantly, which we will analysis in Section 5.2. In this case, we implement K p with more powerful learnable absolute positional encoding <ref type="bibr" target="#b17">[18]</ref> and conditional positional encoding <ref type="bibr" target="#b8">[9]</ref> in all decoder layers. Second, Mask2former <ref type="bibr" target="#b6">[7]</ref> introduces masked attention to restrict the cross-attention to be within only the local foreground regions predicted by the previous layer. However, we find that too small attention regions diminish the performance gain for our DFPQ. In this case, we relax the threshold for masked attention in <ref type="bibr" target="#b6">[7]</ref> from 0.5 to 0.15, which therefore enlarges the predicted local foreground regions. We will show the effect of the threshold in Section 5.2.</p><p>Datasets. We conduct our experiments on ADE20K <ref type="bibr" target="#b43">[44]</ref> and Cityscapes <ref type="bibr" target="#b9">[10]</ref>. ADE20K <ref type="bibr" target="#b43">[44]</ref> is one of the most challenging large-scale datasets for semantic segmentation, which covers 150 finegrained semantic concepts, where the training set and validation set contain 20,210 and 2,000 images, respectively. Cityscapes <ref type="bibr" target="#b9">[10]</ref> is an urban street-view dataset with high-resolution images from 50 cities with 19 semantic classes, which consists of 2,975 images for the training set and 2,725 images for the validation set.</p><p>Evaluation metrics. We use single-scale (s.s.) and multi-scale (m.s.) mean Intersection over Union (mIoU) <ref type="bibr" target="#b14">[15]</ref> as the evaluation metric to measure the model performance. We also compare models with their model size (number of parameters) and computational complexity with Floating-point Operations (FLOPs) to evaluate the efficiency for these models. For ablation studies on HRCA, we also show the training time memory consumption. For ADE20K <ref type="bibr" target="#b43">[44]</ref> and Cityscapes <ref type="bibr" target="#b9">[10]</ref>, we calculate FLOPs with fixed 512 ? 512 and 1024 ? 2048 image size respectively.</p><p>Compared methods. We compare our method with the SOTA semantic segmentation methods including DeepLab V3+ <ref type="bibr" target="#b5">[6]</ref>, UperNet <ref type="bibr" target="#b33">[34]</ref>, Maskformer <ref type="bibr" target="#b7">[8]</ref>, SenFormer <ref type="bibr" target="#b2">[3]</ref>, PFD <ref type="bibr" target="#b25">[26]</ref> and Mask2former <ref type="bibr" target="#b6">[7]</ref>. Among them, SenFormer <ref type="bibr" target="#b2">[3]</ref>, PFD <ref type="bibr" target="#b25">[26]</ref> and Mask2former <ref type="bibr" target="#b6">[7]</ref> are proposed within the past few months. We refer the readers to Section 2 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We compare our FASeg with state-of-the-art semantic segmentation methods on ADE20K val <ref type="bibr" target="#b43">[44]</ref> ( <ref type="table" target="#tab_2">Table 1</ref>) and Cityscapes val [10] <ref type="table" target="#tab_3">(Table 2)</ref>. Specifically, we show FASeg with two K p implementations: learnable absolute positional encoding <ref type="bibr" target="#b17">[18]</ref> and conditional positional encoding <ref type="bibr" target="#b8">[9]</ref>.</p><p>From  <ref type="figure">Figure 4</ref>: Qualitative results on the ADE20K val <ref type="bibr" target="#b43">[44]</ref> with Swin-L backbone and single-scale inference. Compared to Mask2former <ref type="bibr" target="#b6">[7]</ref>, our FASeg predicts masks with finer details (highlighted in red boxes) and more accurate predictions (highlighted in orange boxes). Best viewed in color.  <ref type="bibr" target="#b3">[4]</ref> 47.2 46.9 Learnable absolute pos <ref type="bibr" target="#b17">[18]</ref> 47.0 47.4 Conditional pos <ref type="bibr" target="#b8">[9]</ref> 47.3 48.3 superiority of our FASeg. We also visualize some qualitative results on ADE20K val <ref type="bibr" target="#b43">[44]</ref> (with Swin-L backbone, single-scale inference) in <ref type="figure">Figure 4</ref>, where FASeg shows finer details and more accurate classification predictions than Mask2former <ref type="bibr" target="#b6">[7]</ref>.</p><p>For the comparison on Cityscapes in <ref type="table" target="#tab_3">Table 2</ref>, we observe that with the ResNet-50 backbone, our FASeg outperforms all the SOTA methods under desirable FLOPs. Surprisingly, FASeg even achieves comparable results with the methods employing the ResNet-101 backbone, which further demonstrates the effectiveness of our method.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>Effect of K p . K p is the positional encoding for the Transformer encoder output features. For our FASeg, since our DFPQ is generated directly from the cross-attention scores A t?1 and K t?1 p of the previous layer, the implementation of K p differs significantly on the performance. We investigate the effect of K p in <ref type="table" target="#tab_5">Table 3</ref>. We can observe that for Mask2former <ref type="bibr" target="#b6">[7]</ref>, different K p have similar performance. However, more powerful K p affects greatly on the performance of our FASeg. For instance, FASeg with conditional positional encoding <ref type="bibr" target="#b8">[9]</ref> achieves a new state-of-the-art performance of 48.3% mIoU with the ResNet-50 backbone. It is demonstrated that our DFPQ can benefit from more powerful K p implementations and provide more accurate and meaningful positional guidance in Transformer decoder layers.</p><p>Effectiveness of each module. We investigate the effectiveness of each module on ADE20k val with ResNet-50 backbone in <ref type="table" target="#tab_6">Table 4</ref>. As illustrated in implementation details, Mask2former <ref type="bibr" target="#b6">[7]</ref> restricts the attention to a relatively small area with a high threshold. However, we empirically find that the small attention area diminishes the performance gain for DFPQ, and we propose to incorporate DFPQ with a low masked attention threshold. In this case, we start from the original Mask2former <ref type="bibr" target="#b6">[7]</ref> and relax the masked attention threshold from 0.5 to 0.15 in row 2. We can notice a mIoU drop from 47.2% to 46.6%, indicating that the original Mask2former still requires a high masked attention threshold to ease the optimization difficulty (introduced by the uniform attention scores as illustrated in <ref type="bibr" target="#b44">[45]</ref>). Next, from row 2 to row 3, we observe that our DFPQ contributes remarkably to the performance by 0.8%. Finally, the proposed HRCA contributes to another 0.2% mIoU gain in row 4.</p><p>DFPQ vs. other positional queries. We investigate the effectiveness of our DFPQ and compare it with other learnable queries variants on ADE20K val <ref type="bibr" target="#b43">[44]</ref> with 150 categories in <ref type="table" target="#tab_7">Table 5</ref>. Here we adopt all the other settings the same as our FASeg with learnable absolute K p and the R50 backbone, and only differ the positional queries for all the competitors. Specifically, we compare with three settings: 1) the original learnable positional queries that are randomly initialized <ref type="bibr" target="#b3">[4]</ref>; 2) positional queries fixed as pre-defined grid anchor points akin to Anchor-DETR <ref type="bibr" target="#b32">[33]</ref>; 3) positional queries dynamically generated from the center of the instance mask predicted by the previous layer similar to <ref type="bibr" target="#b21">[22]</ref>. We find that our DFPQ outperforms all the competitors by a large margin. For example, our DFPQ achieves 0.6% higher mIoU than dynamic anchor positional queries.</p><p>HRCA vs. other efficient cross-attention methods. In <ref type="table" target="#tab_8">Table 6</ref>, we investigate the effectiveness of the proposed HRCA and compare it with other efficient cross-attention methods by only differing the cross-attention on the Transformer decoder layers modeling high-resolution feature maps on FASeg with learnable absolute K p and the R50 backbone. We compare our HRCA with randomly sampled top-k pixels in the low-resolution attention map ? that |?| is the same as HRCA. We also compare with RCDA <ref type="bibr" target="#b32">[33]</ref> that the cross-attention is operated on the entire high-resolution feature maps. We can observe that with similar training time memory and FLOPs, our HRCA significantly outperforms both counterparts, which demonstrates the superiority of our HRCA for efficiently identifying and utilizing the contextual tokens in the high-resolution feature maps to improve segmentation performance.</p><p>Effect of |?| in HRCA. We then investigate how |?| affects the performance, memory consumption and computational complexity in <ref type="table" target="#tab_9">Table 7</ref> on FASeg with learnable absolute K p and the R50 backbone. |?| determines the number of contextual tokens used in attention as introduced in Section 4.3. Here we measure the memory consumption by the training time memory with a batch size of 4 on a single GPU. In standard cross-attention layers, cross-attention attends to the entire feature maps from the encoder, in which case |?| = HW . We can observe that when |?| = HW/32 , our efficient high-resolution cross-attention achieves 0.6% higher mIoU with 861M less memory and 6G fewer FLOPs. We conjecture that the sparse property <ref type="bibr" target="#b15">[16]</ref> has reduced the redundancy in high-resolution feature maps in our HRCA and leads to both higher performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed a novel query design for the DETR style segmentation framework, where the positional queries are dynamically generated and conditioned on the cross-attention scores and the localization information of the preceding layer. Our query design delivers more accurate attention regions and consistent cross-attention, thereby generating more accurate semantic segmentation results. Besides, we have presented an efficient cross-attention method to deal with high-resolution feature maps, which enriches segmentation details with significantly reduced memory footprint and computation burden. Extensive experiments have demonstrated the effectiveness of our FASeg.</p><p>Future work. For the future work, we will explore adapting our query design to more segmentation tasks. Since K p has much influence on the final results, we will also explore more implementations of K p to seek for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Effect of Low-resolution Feature Maps for HRCA</head><p>In HRCA, we identify the most informative pixel positions within the low-resolution feature maps, then map these positions to the high-resolution feature maps and only perform cross attention on these positions. We keep the number of positions in the high-resolution feature maps the same and investigate the effect of the low-resolution feature maps in <ref type="table" target="#tab_10">Table 8</ref> on FASeg with learnable absolute K p and the R50 backbone. We can observe that more coarse-grained feature maps yield better results, where we conjecture the reason is that coarse-grained feature maps contain more context information which is helpful for locating the informative pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visualization</head><p>We visualize sample predictions of our FASeg model with Swin-L backbone on ADE20K val with multi-scale inference in <ref type="figure" target="#fig_1">Fig. 5</ref>. We can observe that FASeg generates consistent predictions that align with the ground truth. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Cross-attention with our dynamic focus-aware positional queries. The generation of Q t p is included in the dashed box, where t is the index of the t-th Transformer decoder layer. We derive Q t p by multiplying the positional encodings for the encoder output features K t?1 p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on the ADE20K val<ref type="bibr" target="#b43">[44]</ref>. First and third columns are ground truth. Second and forth columns are prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons with the state-of-the-art methods. We report both single-scale (s.s.) and multi-scale (m.s.) inference results on ADE20K val<ref type="bibr" target="#b43">[44]</ref> with 150 categories.</figDesc><table><row><cell>Method</cell><cell cols="6">Backbone Crop size MIoU s.s. (%) MIoU m.s. (%) Params. FLOPs</cell></row><row><cell>UperNet [34]</cell><cell>R50</cell><cell>512?512</cell><cell>42.1</cell><cell>-</cell><cell>67M</cell><cell>238G</cell></row><row><cell>DeepLab V3+ [6]</cell><cell>R50</cell><cell>512?512</cell><cell>44.0</cell><cell>44.9</cell><cell>44M</cell><cell>177G</cell></row><row><cell>SenFormer [3]</cell><cell>R50</cell><cell>512?512</cell><cell>44.7</cell><cell>45.2</cell><cell>144M</cell><cell>179G</cell></row><row><cell>Maskformer [8]</cell><cell>R50</cell><cell>512?512</cell><cell>44.5</cell><cell>46.7</cell><cell>41M</cell><cell>53G</cell></row><row><cell>PFD [26]</cell><cell>R50</cell><cell>512?512</cell><cell>45.6</cell><cell>48.3</cell><cell>74M</cell><cell>-</cell></row><row><cell>Mask2former [7]</cell><cell>R50</cell><cell>512?512</cell><cell>47.2</cell><cell>49.2</cell><cell>44M</cell><cell>71G</cell></row><row><cell>FASeg w/ learnable absolute K p</cell><cell>R50</cell><cell>512?512</cell><cell>47.6 (+0.4)</cell><cell>49.3 (+0.1)</cell><cell>51M</cell><cell>72G</cell></row><row><cell>FASeg w/ conditional K p</cell><cell>R50</cell><cell>512?512</cell><cell>48.3 (+1.1)</cell><cell>49.3 (+0.1)</cell><cell>51M</cell><cell>72G</cell></row><row><cell>UperNet [34]</cell><cell>Swin-T</cell><cell>512?512</cell><cell>44.4</cell><cell>46.1</cell><cell>60M</cell><cell>236G</cell></row><row><cell>SenFormer [3]</cell><cell>Swin-T</cell><cell>512?512</cell><cell>46.0</cell><cell>-</cell><cell>144M</cell><cell>179G</cell></row><row><cell>Maskformer [8]</cell><cell>Swin-T</cell><cell>512?512</cell><cell>46.7</cell><cell>48.8</cell><cell>42M</cell><cell>55G</cell></row><row><cell>PFD [26]</cell><cell>Swin-T</cell><cell>512?512</cell><cell>48.3</cell><cell>49.6</cell><cell>74M</cell><cell>-</cell></row><row><cell>Mask2former [7]</cell><cell>Swin-T</cell><cell>512?512</cell><cell>47.7</cell><cell>49.6</cell><cell>47M</cell><cell>74G</cell></row><row><cell>FASeg w/ learnable absolute K p</cell><cell>Swin-T</cell><cell>512?512</cell><cell>48.5 (+0.2)</cell><cell>49.8 (+0.2)</cell><cell>54M</cell><cell>75G</cell></row><row><cell>FASeg w/ conditional K p</cell><cell>Swin-T</cell><cell>512?512</cell><cell>49.6 (+1.3)</cell><cell>51.3 (+1.7)</cell><cell>54M</cell><cell>75G</cell></row><row><cell>UperNet [34]</cell><cell>Swin-L</cell><cell>640?640</cell><cell>52.1</cell><cell>53.5</cell><cell>234M</cell><cell>647G</cell></row><row><cell>SenFormer [3]</cell><cell>Swin-L</cell><cell>640?640</cell><cell>53.1</cell><cell>54.2</cell><cell>314M</cell><cell>546G</cell></row><row><cell>Maskformer [8]</cell><cell>Swin-L</cell><cell>640?640</cell><cell>54.1</cell><cell>55.6</cell><cell>212M</cell><cell>375G</cell></row><row><cell>PFD [26]</cell><cell>Swin-L</cell><cell>640?640</cell><cell>56.0</cell><cell>57.2</cell><cell>242M</cell><cell>-</cell></row><row><cell>Mask2former [7]</cell><cell>Swin-L</cell><cell>640?640</cell><cell>56.1</cell><cell>57.3</cell><cell>215M</cell><cell>403G</cell></row><row><cell>FASeg w/ learnable absolute K p</cell><cell>Swin-L</cell><cell>640?640</cell><cell>56.4 (+0.3)</cell><cell>57.3 (+0.0)</cell><cell>228M</cell><cell>405G</cell></row><row><cell>FASeg w/ conditional K p</cell><cell>Swin-L</cell><cell>640?640</cell><cell>56.3 (+0.2)</cell><cell>57.7 (+0.4)</cell><cell>228M</cell><cell>405G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons with the state-of-the-art methods. We report single-scale (s.s.) inference results on Cityscapes val<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4">Backbone MIoU s.s. (%) Params. FLOPs</cell></row><row><cell>Maskformer [8]</cell><cell>R50</cell><cell>78.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Senformer [3]</cell><cell>R50</cell><cell>78.8</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepLab V3+ [3]</cell><cell>R50</cell><cell>79.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Mask2former [7]</cell><cell>R50</cell><cell>79.4</cell><cell>44M</cell><cell>526G</cell></row><row><cell>Maskformer [8]</cell><cell>R101</cell><cell>79.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Mask2former [7]</cell><cell>R101</cell><cell>80.1</cell><cell>67M</cell><cell>628G</cell></row><row><cell>SenFormer [3]</cell><cell>R101</cell><cell>80.3</cell><cell>-</cell><cell>-</cell></row><row><cell>FASeg w/ learnable absolute K p</cell><cell>R50</cell><cell>80.3</cell><cell>67M</cell><cell>533G</cell></row><row><cell>FASeg w/ conditional K p</cell><cell>R50</cell><cell>80.5</cell><cell>67M</cell><cell>533G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>, we can observe that under the similar amount of parameters and FLOPs, our FASeg consistently outperforms the SOTA methods. Specifically, FASeg with conditional K p achieves 48.3%, 49.6%, and 56.3% mIoU for single-scale testing and 49.3%, 51.3%, and 57.7% for multi-scale testing on R50, Swin-T and Swin-L backbone, respectively, reaching a new SOTA performance. We can also observe that FASeg achieves significant performance gain over Mask2former<ref type="bibr" target="#b6">[7]</ref> with only marginally increased number of parameters and FLOPs. For instance, FASeg with conditional K p and Swin-T backbone outperforms the Mask2former counterpart for single-scale and multi-scale inference respectively by 1.9% mIoU and 1.7% mIoU with only 1G extra FLOPs, demonstrating the</figDesc><table><row><cell>GT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effect of K p on ADE20K val<ref type="bibr" target="#b43">[44]</ref> with 150 categories. "pos" is short for positional encoding. Here we investigate the effect on both Mask2former<ref type="bibr" target="#b6">[7]</ref> and our FASeg using ResNet-50 backbone.</figDesc><table><row><cell>K p</cell><cell>Mask2former [7] mIoU s.s (%) FASeg mIoU s.s (%)</cell></row><row><cell>Sinusoidal pos</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for FASeg w/ learnable absolute K p . We report single-scale (s.s.) inference results on ADE20K val<ref type="bibr" target="#b43">[44]</ref> with 150 categories.</figDesc><table><row><cell cols="2">Row Relax masked attention DFPQ HRCA MIoU s.s. (%)</cell></row><row><cell>1</cell><cell>47.2</cell></row><row><cell>2</cell><cell>46.6</cell></row><row><cell>3</cell><cell>47.4</cell></row><row><cell>4</cell><cell>47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons between DFPQ with other positional queries variants on ADE20Kval<ref type="bibr" target="#b43">[44]</ref> with 150 categories.</figDesc><table><row><cell>Method</cell><cell>mIoU s.s.(%)</cell></row><row><cell>Learnable parameters</cell><cell>46.9</cell></row><row><cell>Fixed anchor positional queries</cell><cell>46.6</cell></row><row><cell>Dynamic anchor positional queries</cell><cell>47.0</cell></row><row><cell>DFPQ</cell><cell>47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons between our HRCA with other efficient cross-attention methods on ADE20K val<ref type="bibr" target="#b43">[44]</ref> with 150 categories.</figDesc><table><row><cell>Method</cell><cell cols="3">mIoU s.s. (%) Training memory FLOPs</cell></row><row><cell>Random ?</cell><cell>46.7</cell><cell>8,843M</cell><cell>72G</cell></row><row><cell>RCDA</cell><cell>46.9</cell><cell>8,581M</cell><cell>72G</cell></row><row><cell>HRCA</cell><cell>47.6</cell><cell>8,906M</cell><cell>72G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Effect of |?| in efficient high-resolution cross-attention on ADE20K val<ref type="bibr" target="#b43">[44]</ref> with 150 categories.</figDesc><table><row><cell>|?|</cell><cell cols="3">mIoU s.s. (%) Training memory FLOPs</cell></row><row><cell>HW</cell><cell>47.0</cell><cell>9,767M</cell><cell>78G</cell></row><row><cell>HW/16</cell><cell>47.3</cell><cell>8,939M</cell><cell>72G</cell></row><row><cell>HW/32</cell><cell>47.6</cell><cell>8,906M</cell><cell>72G</cell></row><row><cell>HW/64</cell><cell>47.5</cell><cell>8,846M</cell><cell>71G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Effect of low-resoluiton features for HRCA on ADE20K val [44] with 150 categories.</figDesc><table><row><cell cols="2">Low-level features mIoU s.s. (%)</cell></row><row><cell>1/8</cell><cell>46.4</cell></row><row><cell>1/16</cell><cell>46.5</cell></row><row><cell>1/32</cell><cell>47.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We organize our appendix as follows.</p><p>? In Section A, we analysis the effect of the low-resolution feature maps in HRCA.</p><p>? In Section B, we visualize sample predictions of our FASeg.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient self-ensemble framework for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bousselham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thibault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Machireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13280</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01527</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3621" to="3630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semask: Semantically masked transformers for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12782</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DAB-DETR: Dynamic anchor boxes are better queries for DETR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable visual transformers with hierarchical pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pyramid fusion transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04019</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7262" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quadtree attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anchor detr: Query design for transformer-based detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficient detr: improving end-to-end object detector with dense prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m">Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">K-net: Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deformable {detr}: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
