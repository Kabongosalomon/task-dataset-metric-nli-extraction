<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Training in High Dimensions via Block Coordinate Geometric Median Descent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Acharya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abolfazl</forename><surname>Hashemi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Amazon Search 3 Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Amazon Search 3 Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ufuk</forename><surname>Topcu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Training in High Dimensions via Block Coordinate Geometric Median Descent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Geometric median (Gm) is a classical method in statistics for achieving a robust estimation of the uncorrupted data; under gross corruption, it achieves the optimal breakdown point of 0.5. However, its computational complexity makes it infeasible for robustifying stochastic gradient descent (SGD) for high-dimensional optimization problems. In this paper, we show that by applying Gm to only a judiciously chosen block of coordinates at a time and using a memory mechanism, one can retain the breakdown point of 0.5 for smooth non-convex problems, with non-asymptotic convergence rates comparable to the SGD with Gm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider smooth non-convex optimization problems with finite sum structure:</p><formula xml:id="formula_0">min x?R d f (x) := 1 n n i=1 f i (x) .<label>(1)</label></formula><p>Mini-batch SGD is the de-facto method for optimizing such functions [RM51, <ref type="bibr" target="#b10">Bot10,</ref><ref type="bibr" target="#b84">TBA86]</ref> which proceeds as follows: at each iteration t, it selects a random batch D t of b samples, obtains gradients g (i) t = ?f i (x t ), ?i ? D t , and updates the parameters using iterations of the form:</p><formula xml:id="formula_1">x t+1 := x t ? ?g (t) ,g (t) = 1 |D t | i?Dt g (t)</formula><p>i .</p><p>(2)</p><p>In spite of its strong convergence properties in the standard settings [MB11, DGBSX12, LZCS14, GDG + 17, KMN + 16, <ref type="bibr" target="#b99">YHCL12]</ref>, it is well known that even a small fraction of corrupt samples can lead SGD to an arbitrarily poor solution <ref type="bibr" target="#b3">[BBC11,</ref><ref type="bibr" target="#b12">BTN00]</ref>. This has motivated a long line of work to study robust optimization in presence of corruption [BGS + 17, AAZL18, WLCG20, XKG19]. While the problem has been studied under a variety of contamination models, in this paper, we study the robustness properties of the first-order method (2) under the strong and practical gross contamination model (See Definition 1) [Li18, DK19, DKK + 19b, DKK + 19a] which also generalizes the popular Huber's contamination model and the byzantine contamination framework <ref type="bibr" target="#b39">[Hub92,</ref><ref type="bibr" target="#b53">LSP82]</ref>.</p><p>In particular, the goal of this work is to design an efficient first-order optimization method to solve (1), which remains robust even when 0 ? ? &lt; 1/2 fraction of the gradient estimates g <ref type="bibr">(i)</ref> t are arbitrarily corrupted in each batch D t , without any prior knowledge about the malicious samples. Note that, by letting the corrupt estimates to be arbitrarily skewed, this corruption model is able to capture a number of important and practical scenarios including corruption in feature (e.g., existence of outliers) , corrupt gradients (e.g., hardware failure, unreliable communication channels during distributed training) and backdoor attacks [CLL + 17, LZS + 18, GLDGG19, BNL12, MGR18, TLM18] (See Section 7).</p><p>Robust SGD via Geometric Median Descent <ref type="bibr">(GmD)</ref>. Under the gross corruption model (Definition 1), the vulnerability of mini-batch SGD can be attributed to the linear gradient aggregation step (2) [BGS + 17, AAZL18, YCKB18, XKG19]. In fact, it can be shown that no linear gradient aggregation strategy can tolerate even a single grossly corrupted update, i.e., they have a breakdown point (Definition 2) of 0. To see this, consider the single malicious gradient g (j)</p><formula xml:id="formula_2">t = ? i?Dt\j g (i)</formula><p>t . One can see that this single corrupted gradient results in the average to become 0, which in turn means mini-batch SGD gets stuck at the initialization. An approach for robust optimization may be to find an estimateg such that with high probability g? 1 G g i ?G g i is small even in presence of gross-corruption <ref type="bibr" target="#b26">[DK19]</ref>. In this context, geometric median (Gm) (Definition 3) is a well studied rotation and translation invariant robust estimator with optimal breakdown point of 1/2 even under gross corruption [LR + 91, M + 15, CLM + 16]. Due to this strong robustness property, SGD with Gm-based gradient aggregation (GmD) has been widely studied in robust optimization literature [AAZL18, CSX17, PKH19, WLCG20]. Following the notation of (2) the update step of GmD can be written as:</p><formula xml:id="formula_3">x t+1 := x t ? ?g (t) ,g (t) = Gm({g (t) i }) ?i ? [b]</formula><p>(3)</p><p>Despite the strong robustness guarantees of Gm, the computational cost of calculating approximate Gm is prohibitive, especially in high dimensional settings. For example, the best known result [CLM + 16] uses a subroutine that needs O(d/ 2 ) computations to find an -approximate Gm. Despite recent efforts in [VZ00, Wei37, CT89, CMMP13, CLM + 16, PKH19] to design computationally tractable Gm(?), given that in practical large-scale optimization settings such as training deep learning models the number of parameters (d) is large (e.g., d ? 60M for AlexNet, d ? 175B for GPT-3). GmD remains prohibitively expensive <ref type="bibr" target="#b19">[CWCP18,</ref><ref type="bibr" target="#b68">PKH19]</ref> and with limited applicability.</p><p>Overview of Our Algorithm (BgmD). In this work, we leverage coordinate selection strategies to reduce the cost of GmD and establish Block co-ordinate Gm Descent (BGmD) (Algorithm 1). BGmD is a robust optimization approach that can significantly reduce the computational overhead of Gm based gradient aggregation resulting in nearly two orders of magnitude speedup over GmD on standard deep learning training tasks, while maintaining almost the same level of accuracy and optimal breakdown point 1/2 even in presence of gross corruption.</p><p>At a high level, BGmD selects a block of 0 &lt; k ? d important coordinates of the gradients; importance of a coordinate is measured according to the largest directional derivative measured by the squared 2 norm across all the samples (Algorithm 2). The remaining (d ? k) dimensions are discarded and gradient aggregation happens only along these selected k directions. This Implies the Gm subroutine is performed only over gradient vectors in R k (a significantly lower dimensional subspace). Thus, when k d, this approach provides a practical solution to deploy Gm-based aggregation in high dimensional settings 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Block Gm Descent (BGmD)</head><p>Initialize: estimate: x 0 ? R d , step-size: ?, memory:m 0 = 0, Block Coordinate Selection operator: C k (?), Geometric Median operator: Gm(?) for epochs t = 0, . . . , until convergence do</p><formula xml:id="formula_4">Select samples D t = {i 1 , . . . , i b } Obtain : g (i) t := ?f i (x t ), ?i ? D t Let G t ? R b?d s.t. each row G t [i, :] = g (i) t G t [i, :] = ?G t [i, :] +m t ?i ? [b] (Memory Augmentation) ? t := C k (G t ) (Select top k columns via Algo 2) M t = G t ? ? t (Compute Residuals) m t+1 = 1 b 0?i?b M t [i, :] (Update memory) g t := Gm(? t ) (Robust Aggregation in R k ) x t+1 := x t ?g t (Global model update) end Algorithm 2 Block Coordinate Selection Strategy Input: G t ? R n?d , k for coordinates j = 0, . . . , d-1 do s j ? G t [:, j] 2 (norm along each dimension) end Sample set I k of k dimensions with probabilities proportional to s j C k (G) i,j?I k = G i,j , C k (G) i,j / ?I k = 0 Return: C k (G)</formula><p>The motivation behind reducing the dimension of the gradient vectors is that in many scenarios most of the information in the gradients is captured by a subset of the coordinates <ref type="bibr" target="#b73">[SCCS19]</ref>. Hence, by the judicious block coordinate selection subroutine outlined in Algorithm 2 one can identify an informative low-dimensional representation of the gradients.</p><p>While Algorithm 2 identifies a representative block of the coordinates, aggressively reducing the dimension (i.e., k d) might lead to a significant approximation error, which in turn might lead to slower convergence [Nes12, NSL + 15] (i.e., more iterations), dwarfing the benefit from reduction in per iteration cost. To alleviate this issue, by leveraging the idea of Error Compensation [SFD + 14, <ref type="bibr" target="#b76">SK19,</ref><ref type="bibr" target="#b47">KRSJ19]</ref> in Algorithm 1 we introduce a memory augmentation mechanism. Specifically, at each iteration the residual error from dimensionality reduction is computed and accumulated in a memory vector i.e.,m t and in the subsequent iteration,m t is added back to the new gradient estimates. Our ablation studies show that such memory augmentation indeed ensures that the required number of iterations remain relatively small despite choosing k d (See <ref type="figure" target="#fig_6">Figure 6</ref>).</p><p>Contributions. The main contributions of this work are as follows:</p><p>? We propose BGmD (Algorithm 1), a method for robust optimization in high dimensions.</p><p>BGmD is significantly more efficient than the standard Gm-SGD method but is still able to maintain the optimal breakdown point 1/2 -first such efficient method with provable convergence guarantees.</p><p>? We provide strong guarantees on the rate of convergence of BGmD in standard non-convex scenarios including smooth non-convex functions, and non-convex functions satisfying the Polyak-?ojasiewicz Condition. These rates are comparable to those established for Gm-SGD </p><formula xml:id="formula_5">SGD O(bd) O(bd) O(bd) 0 CMD * [YZFL19, YCKB18] O(bd) O(bd) O(bd) 1/2 -?( d/b) GMD [CLM + 16, WLCG20] O(d ?2 + bd) 1/2 [DD20] O(db 2 min(d, b)) 1/4 BGmD (This Work) O(k ?2 + bd) 1/2</formula><p>under more restricting conditions such as strong convexity [CSX17, PKH19, WLCG20].</p><p>? Through extensive experiments under several common corruption settings ; we demonstrate that BGmD can be up to 3? more efficient to train than Gm-SGD on Fashion MNIST and CIFAR-10 benchmarks while still ensuring similar test accuracy and maintaining same level of robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Computationally Tractable Robust SGD. Robust optimization in the presence of gross corruption has received renewed impetus in the machine learning community, following practical considerations such as preserving the privacy of the user data and coping with the existence of adversarial  <ref type="bibr" target="#b44">KKSJ19]</ref> propose choosing the coordinates greedily according to norm-based selection criteria, a strategy known as the Gauss Southwell rule.</p><p>Remark 1. Note that, our approach (Algorithm 2) is closely related to the Greedy Gauss Southwell coordinate selection approach. In fact, it is immediate that for batch size b = 1 Gauss Southwell Co-ordinate Descent becomes a special case of BGmD.</p><p>Connection to Error Feedback Compensating for the loss incurred due to approximation through a memory mechanism is a common concept in the feedback control and signal processing literature (See <ref type="bibr" target="#b22">[DFT13]</ref> and references therein). [SFD + 14, Str15] adapt this to gradient compression (1Bit-SGD) to reduce the number of communicated bits in distributed optimization.</p><p>Recently, [SCJ18, SK19, KRSJ19] have analyzed this error feedback framework for a number of gradient compressors in the context of communication-constrained distributed training.</p><p>Remark 2. Note that our memory mechanism is inspired by this error feedback mechanism.</p><p>In fact, all the works on error feedback to compensate for gradient compression [SCJ18, <ref type="bibr" target="#b76">SK19,</ref><ref type="bibr" target="#b47">KRSJ19]</ref> are special case of our proposed memory mechanism when batch size b = 1 i.e. M t =m t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We first briefly recall some related concepts and state our main assumptions. Throughout, . denotes the 2 norm unless otherwise specified.</p><p>Definition 1 (Gross Corruption Model). Given 0 ? ? &lt; 1 2 and a distribution family D on R d the adversary operates as follows: n samples are drawn from D ? D. The adversary is allowed to inspect all the samples and replace up to ?n samples with arbitrary points.</p><p>Intuitively, this implies that (1 ? ?) fraction of the training samples are generated from the true distribution (inliers) and rest are allowed to be arbitrarily corrupted (outliers) i.e. ? := |B|/|G| = ? 1?? &lt; 1, where B and G are the sets of corrupt and good samples. In the rest of the paper, we will refer to a set of samples generated through this process as ?-corrupted.</p><p>Definition 2 (Breakdown Point). Finite-sample breakdown point <ref type="bibr" target="#b24">[DH83]</ref> is a way to measure the resilience of an estimator. It is defined as the smallest fraction of contamination that must be introduced to cause an estimator to break i.e. produce arbitrarily wrong estimates.</p><p>In the context Definition 1 we can say an estimator has the optimal breakdown point 1/2 if it is robust in presence of ?-corruption ? ? &lt; 1/2 or alternatively ? ? &lt; 1.</p><p>Definition 3 (Geometric Median). Given a finite collection of observations x 1 , x 2 , . . . x n defined over a separable Hilbert space X with norm ? the geometric median or the Fermet-Weber point [Hal48, WF + 29, M + 15, Kem87] is defined as:</p><formula xml:id="formula_6">x * = Gm({x i }) = arg min y?X g(x) := n i=1 y ? x i (4)</formula><p>We call a point x ? R d an -accurate geometric median if g(x) ? (1 + )g(x * ).</p><p>Assumption 1 (Stochastic Oracle). Each non-corrupt sample i ? G is endowed with an unbiased stochastic first-order oracle with bounded variance. That is,</p><formula xml:id="formula_7">E z?D i [g i (x, z)] = ?f i (x), E z?D i ?F i (x, z) 2 ? ? 2 (5) Assumption 2 (Smoothness). Each non-corrupt function f i is L-smooth, ?i ? G, f i (x) ? f i (y) + x ? y, ?f i (y) + L 2 x ? y 2 . ?x, y ? R d .<label>(6)</label></formula><p>Assumption 3 ( Polyak-?ojasiewicz Condition). The average of non-corrupt functions f := 1 G i?G f i (x) satisfies the Polyak-?ojasiewicz condition (PLC) with parameter ?, i.e.</p><formula xml:id="formula_8">?f (x) 2 ? 2?(f (x) ? f (x * )), ? &gt; 0 where x * = arg min x f (x), ?x ? R d . (7)</formula><p>We further assume that the solution set X * ? R d is non-empty and convex. Also note that ? &lt; L.</p><p>Simply put, the Polyak-?ojasiewicz condition implies that when multiple global optima exist, each stationary point of the objective function is a global optimum <ref type="bibr" target="#b69">[Pol63,</ref><ref type="bibr" target="#b46">KNS16]</ref>. This setting enables the study of modern large-scale ML tasks that are generally non-convex. Also note that ?-strongly convex functions satisfy PLC with parameter ? implying PLC is a weaker assumption than strong convexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Block Coordinate Geometric Median Descent (BGmD)</head><p>As discussed earlier, BGmD (Algorithm 1) involves two key steps: (i) Selecting a block of coordinates and run computationally expensive Gm aggregation over a low dimensional subspace and (ii) Compensating for the residual error due to block coordinate selection. In the rest of this section we will discuss these two ideas in more detail.</p><p>Block Selection Strategy. The key intuition why we might be able to select a small number of k coordinates for robust mean estimation is that in practical over-parameterized models, the main mass of the gradient is concentrated in a few coordinates [CCS + 19]. So, what would be the best strategy to select the most informative block of coordinates? Ideally, one would like to select the best k dimensions that would result in the largest decrease in training loss. However, this task is NP-hard in general [DK11, NW81, CGK + 00]. Instead, we adopt a simple and fast block coordinate selection rule: Consider G ? R b?d where each row corresponds to the transpose of the stochastic gradient estimate, i.e. G i,</p><formula xml:id="formula_9">: = g T i ? R 1?d , ?i ? [b]</formula><p>where b is number of samples (or batches/clients in case of distributed setting) participating in that iteration (2). Then, selecting k dimensions is equivalent to selecting k columns of G, which we select according to the norm of the columns. That is, we assign a score to each dimension proportional to the 2 norm (total mass along that coordinate) i.e. s j = G :,j 2 , for all j ? [d]. We then sample only k coordinates with with probabilities proportional to s j and discard the rest to find a set ? k of size k (see Algorithm 2). We show below that this method produces a contraction approximation to G. Lemma 1. Algorithm 2 yields a contraction approximation, i.e.,</p><formula xml:id="formula_10">E C k C k (G) ? G 2 F |x ? (1 ? ?) G 2 F , k d ? ? ? 1, where C k (G) i,j?? k = G i,j , and C k (G) i,j / ?? k = 0.</formula><p>It is also worth noting that without additional distributional assumption on G the lower bound on ?cannot be improved. 3 However, in practice the gradient vectors are extremely unlikely to be uniform [AHJ + 18] and thus active norm sampling is expected to to satisfy Lemma 1 with ? ? 1.</p><p>The Memory Mechanism. While descending along only a small subset of k coordinates at each iteration significantly improves the per iteration computational cost, a smaller value of k would also imply larger gradient information loss i.e., a smaller ? (Lemma 1). Intuitively, a restriction to a k-dimensional subspace results in a d k factor increase in the gradient variance <ref type="bibr" target="#b74">[SCJ18]</ref>.To mitigate this, we adopt a memory mechanism [SFD + 14, Str15, SCJ18, KRSJ19, SK19, DFT13] Our approach is as follows: Throughout the training process, we keep track of the residual errors in G t ? C k (G t ) viam t ? R d that we call memory. At each iteration t, it simply accumulates the residual error incurred due to ignoring d ? k dimensions, averaged over all the samples participating in that round. In the next iteration,m t is added back to all the the new gradient estimates as feedback. Following our Jacobian notation, the memory update is given as:</p><formula xml:id="formula_11">Memory Augmentation G t [i, :] = ?G t [i, :] +m t ?i ? [b] Memory Update M t = G t ? C k (G t ) ;m t+1 = 1 b 0?i?b M t [i, :]<label>(8)</label></formula><p>Intuitively, as the residual at each iteration is not discarded but rather kept in memory and added back in a future iteration, this ensures similar convergence rates as training in R d (see Theorem 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Computational Complexity Analysis</head><p>To theoretically understand the overall computational benefit of our proposed scheme BGmD over Gm-SGD, we first analyze the per epoch time complexity of both the algorithms. Combining this with the convergence rates derived in Theorems 1 and 2 provides a clear picture of the overall computational efficiency of BGmD.</p><p>Consider problem (1) with parameters x ? R d and SGD style iterations of the form (2) and batch size |D t | = b. Note that the difference between the iterations of SGD, Gm-SGD, and BGmD is primarily in the aggregation step, i.e., how they aggregate the updates communicated by samples (or batches) participating in training during that iteration.</p><p>At iteration t, let T (t) a denote the time to aggregate the gradients. Also let, T (t) b denote the time taken to compute the batch gradients (i.e., time to perform back propagation). Thus, the overall complexity of one training iteration is roughly O(T</p><formula xml:id="formula_12">(t) a + T (t) b ). Now, note that T (t) b</formula><p>is approximately the same for all the algorithms mentioned above and for methods like Gm-SGD T </p><formula xml:id="formula_13">( d 2 ) [CLM + 16, PKH19, AAZL18, CSX17]. In contrast, T (t) a for BGmD is O( k 2 + bd),</formula><p>where the first term in computational complexity is due to computation of Gm of R k -dimensional points. The second term is due to the coordinate sampling procedure.</p><p>That is, by choosing a sufficiently small block of coordinates i.e. k d, BGmD can result in significant savings per iteration. Based on this observation, one can derive the following Lemma.</p><formula xml:id="formula_14">Lemma 2. (Choice of k). Let k ? O( 1 F ? b 2 ) ? d.</formula><p>Then, given an -approximate Gm oracle, Algorithm 1 achieves a factor F speedup over Gm-SGD for aggregating b samples.</p><p>In most practical settings, the second term of b 2 in Lemma 2 is often negligible compared to the first term which implies significantly smaller per-iteration complexity for BGmD compared to that of Gm-SGD. Interestingly, in Theorem 1 we show that the rate of convergence and break-down point for both the methods is similar, which implies that overall BGmD is much more efficient in optimizing (1) than Gm-SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Convergence Guarantees of BGmD</head><p>We now analyze the convergence properties of BGmD as described in Algorithm 1. We state the results in Theorem 1 and Theorem 2 for general non-convex functions and functions satisfying PLC, respectively.</p><p>Theorem 1 (Smooth Non-convex). Consider the general case where the functions f i correspond to non-corrupt samples i ? G are non-convex and smooth (Assumption 2). Define,</p><formula xml:id="formula_15">R 0 := f (x 0 ) ? f (x * )</formula><p>where x * is the true optima and x 0 is the initial parameters. Run Algorithm 1 with compression factor k d ? ? ? 1 (Lemma 1), learning rate ? = 1/2L and ?approximate Gm(?) oracle in presence of ??corruption (Definition 1) for T iterations. Sample an iteration ? from 1, . . . , T uniformly at random. Then, it holds that:</p><formula xml:id="formula_16">E ?f (x ? ) 2 = O LR 0 T + ? 2 ? ?2 (1 ? ?) 2 + L 2 2 |G| 2 (1 ? ?) 2 .</formula><p>Theorem 2 (Non-convex under PLC). State the notation of Theorem 1.</p><formula xml:id="formula_17">Assume f = 1 G i?G f i (x) satisfies the Polyak-?ojasiewicz Condition (Assumption 3) with parameter ?.</formula><p>After T iterations Algorithm 1 with compression factor k d ? ? ? 1 (Lemma 1), learning rate ? = 1/4L and ?approximate Gm(?) oracle in presence of ??corruption (Definition 1) satisfies:</p><formula xml:id="formula_18">E x T ? x * 2 = O LR 0 ? 2 1 ? ? 8L T + ? 2 ? ?2 ? 2 (1 ? ?) 2 + L 2 2 ? 2 |G| 2 (1 ? ?) 2 ,<label>(9)</label></formula><p>for a global optimal solution x * ? X * . Here,</p><formula xml:id="formula_19">x T := 1 W T T ?1 t=0 w t x t with weights w t := (1 ? ? 8L ) ?(t+1) , W T := T ?1 t=0 w t .</formula><p>Theorem 1 and Theorem 2 state that Algorithm 1 with a constant stepsize convergences to a neighborhood of a first order stationary point. The radius of this neighborhood depends on two terms. The first term depends on the variance of the stochastic gradients as well as the effectiveness of the coordinate selection strategy through ?. The second term however depends on how accurate the Gm computation is performed in each iteration. As we demonstrate in our experiments typically the proposed coordinate selection strategy entails a ? ? 1. Therefore, we expect the radius of error to be negligible. Furthermore, both terms in the radius depend on the fraction of corrupted samples and as long as less than 50% of the samples are corrupted, BGmD attains convergence to a neighborhood of a stationary point. We also note that this rate matches the rate of Gm-SGD when the data is not i.i.d. (see e.g. [CSX17, AAZL18, WLCG20, DD20] and the references therein) while Algorithm 1 significantly reduces the cost of Gm-based aggregation. We further note that the convergence properties established in Theorem 2 are analogous to those of Gm-SGD. However, compared to the existing analyses that require strong convexity([AAZL18, WLCG20, DD20]), Theorem 2 only assumes PLC which as we discussed is a much milder condition. Furthermore, in absence of corruption (i.e., ? = 0), if the data is i.i.d., our theoretical analysis reveals that by setting ? = O(1/ ? T ) Algorithm 1 convergences at the rate of O(1/ ? T ) to the statistical accuracy. 4 This last result can be established by using the concentration of the median-of-the-means estimator [CSX17].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Proof Outline</head><p>Following [Sti18, KRSJ19, BDKD19], we start by defining a sequences of averaged quantities. Divergent from these works however, due to the adversarial setting being considered, we define these quantities over only the uncorrupted samples: Notice that the BGmD cannot compute the above average sequences over the uncorrupted samples, but it aims to approximate the aggregated stochastic gradients of the reliable clients, i.e. g t , via the Gm(?) oracle. Noting the definition of ? t in (10), the update can be thought of as being a perturbed sequence with the perturbation quantity z t :=g t ? Gm({? i t } i?G ). Next, using the closeness of the Gm(?) oracle to the true average we will establish the following lemmas to bound the perturbation.</p><formula xml:id="formula_20">g t = 1 |G| i?G g i t ,? t = E t [g t ] = 1 |G| i?G ?f i (x t ), m t = 1 |G| i?G M t [i, :], ? t = 1 |G| i?G ? i t , p t = 1 |G| i?G p i t = ? t g t + m t .<label>(10)</label></formula><p>Lemma 3 (Bounding the Memory). Consider the setting of Algorithm 1 in iteration t with compression factor ? (Lemma 1), learning rate ? and in presence of ??corruption (Definition 1). If further f i have bounded variance ? 2 (Assumption 1) we have</p><formula xml:id="formula_21">E m t 2 ? 4(1 ? ? 2 )? 2 ? 2 ? ?2 , ?i ? [n].<label>(11)</label></formula><p>Lemma 4 (Bounding the Perturbation). Consider the setting of Algorithm 1 in iteration t with compression factor ? (Lemma 1), learning rate ? and ?approximate Gm(?) oracle in presence of ??corruption (Definition 1). Under the assumption that function f i are smooth (Assumption 2),</p><formula xml:id="formula_22">E z t 2 ? 96? 2 ? 2 (1 ? ?) 2 1 + 4(1 ? ? 2 ) ? 2 + 2 2 |G| 2 (1 ? ?) 2 .<label>(12)</label></formula><p>With the bound in Lemma 4, we define the following perturbed virtual sequences for i ? G</p><formula xml:id="formula_23">x i t+1 =x t ? ?g i t ? z t ,x i 0 = x 0 ,x t+1 = 1 |G| i?Gx i t+1 =x t ? ?g t ? z t<label>(13)</label></formula><p>Again,x t can be though of as a perturbed version of the SGD iterates over only the good samples i ? G. Notice that BGmD does not compute the virtual sequence and this sequence is defined merely for the theoretical analysis. Therefore, it is essential to establish its relation to x t , i.e. the iterates of BGmD. We do so in Lemma 5.</p><p>Lemma 5 (Memory as a Delayed Sequence). Consider the setting of Algorithm 1 in iteration t. It holds that x t ?x t = m t .</p><p>The main challenge in showing this result is the presence of perturbations z t in the resilient aggregation that we adopt in Algorithm 1. Upon establishing this lemma, using smoothness we establish a bound on suboptimality of the model learned at each iteration of BGmD as a function of the perturbed virtual sequencex t .</p><p>Lemma 6 (Recursive Bounding of the Suboptimality). For any 0 &lt; ? &lt; 0.5 it holds that</p><formula xml:id="formula_24">E t [f (x t+1 )] ? f (x t ) ? 1 2 ? ? ? 2 ?f (x t ) 2 + 3?L 2 2 x t ? x t 2 + L? 2 E t g t 2 + L + 1 2?? + 1 2? E t z t 2 .<label>(14)</label></formula><p>The proofs are furnished by noting that the amount of perturbation can be bounded by using smoothness and the PLC assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Empirical Evidence</head><p>In this section, we describe our experimental setup and present our empirical findings and establish strong insights about the performance of BgmD. <ref type="table" target="#tab_3">Table 2</ref>, 3 and 4 provide a summary of the results. These experiments are performed in presence of increasing the fraction samples corrupted with impulse noise. At 40% corruption SGD diverged and thus does not appear in the plot. Observe that BGmD is able to maintain high accuracy in even presence of strong corruption and is at least GmD while attaining at least 3x speedup over GmD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setup</head><p>We use the following two important optimization setups for our experiments: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Corruption Simulation</head><p>We consider all three possible sources of error: corruption in features, corruption in labels, and corruption in communicated gradients. All the experiments are repeated for 0% (i.e. clean),  20% and 40% corruption levels, i.e, ? = 0, 0.2, 0.4, respectively (see Definition 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Corruption:</head><p>We consider corruption in the raw training data itself which can arise from different issues related to data collection. Particularly, adopting the corruptions introduced in [HD18], we use two noise models to directly apply to the corrupt samples:  (i) Additive: Gaussian noise z i ? N (0, 100) directly added to the image, and (ii) Impulse: Salt and Pepper noise added by setting 90% of the pixels to 0 or 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Corruption:</head><p>In distributed training over multiple machines the communicated gradients can be noisy, e.g., due to hardware issues or simply because some nodes are adversarial and aim to maliciously disrupt the training. Using standard noise models for gradient corruption [Fu98, XKG19, BWAA18] we directly corrupt the gradients in the following manner: (i) Additive: adversary adds random Gaussian noise z i ? N (0, 100) to the true gradient, and (ii)Scaled Bit Flip: corrupted gradients g c t are the scaled bit flipped version of the true gradient <ref type="bibr" target="#b13">[BWAA18]</ref> estimate; in particular, we use the following scale: g c t = ?100g t .</p><p>(c) Label Corruption: We simulate the important real world backdoor attack <ref type="bibr" target="#b78">[SS19,</ref><ref type="bibr" target="#b86">TTGL20]</ref> where the goal of the adversary is to bias the classifier towards some adversary chosen class. To simulate this behavior: at each iteration we flip the labels of randomly chosen ? fraction of the samples to a target label (e.g. in Fashion-MNIST we use 8:bag as the backdoor label).</p><p>Dynamic Corruption Strategy. In order to represent a wide array of real world adversarial scenarios we use the following dynamic corruption strategy: At each iteration, we randomly sample ? fraction of the samples malicious. Note that this implies none of the samples are  trustworthy and thus makes the robust estimation problem truly unsupervised. Note that, this is a strictly stronger and realistic simulation of corruption compared to existing literature where a fixed set of samples are treated as malicious throughout the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Discussion</head><p>Discussion on the Test Performance In <ref type="table" target="#tab_3">Table 2</ref> we observe that without corruption both BgmD and GmD are able to achieve similar accuracy as the baseline (i.e., SGD). Conversely, CmD has a significant sub-optimality gap even in the clean setting. We observe the same trend under different corruption models over various levels of corruption. When corruption is high, SGD starts to diverge after a few iterations. While CmD doesn't diverge, at higher level of corruptions its performance significantly degrades. On the other hand, both GmD and BGmD remain robust and maintain their test accuracy as expected from thier strong theoretical guarantees. Surprisingly, BGmD not only maintains similar robustness as GmD, in several experiments it even outperforms the performance of GmD. We hypothesize that this might be because of implicit regularization properties of BGmD in over-parameterized settings and leave it as a future problem.</p><p>Discussion on the Relative Speedup. The main goal of this work is to speed up Gm based robust optimization methods. To this end, we verify the efficiency of different methods via plotting the performance on test set as a function of the wall clock time. <ref type="figure" target="#fig_2">Figure 1, 2  and 3</ref> suggest that under a variety of corruption settings BgmD is able to achieve significant speedup over GmD often by more than 3x while maintaining similar (sometime even better) test performance as GmD.</p><p>Choice of k and the Role of Memory In order to study the importance the memory mechanism we train a 4-layer CNN on MNIST <ref type="bibr" target="#b49">[LBBH98]</ref> dataset while retaining 0 &lt; ? ? 1 fraction of coordinates, i.e., k = ?d. <ref type="figure" target="#fig_6">Figure 6</ref> shows that the training runs that used memory (denoted by M) were able to achieve a similar accuracy while using significantly fewer coordinates. . In the context of robust optimization this would imply that performing robust gradient estimation in a low-dimensional subspace has little to no impact in the downstream optimization task especially in case of overparameterized models. Thus, by judiciously selecting a block of co-ordinates and performing robust gradient estimation only in this low-dimensional subspace is a practical approach to leverage highly robust estimators such as Gm even in high dimensional setting which was previously intractable. This means BGmD would be an attractive choice especially in overparameterized setting. In order to validate this, we perform a small experiment: we take a tiny 412 parameter CNN and a large 1.16M parameter CNN and train both on simple MNIST dataset. In addition to looking at the train and test performance we also compute relative residual error 1?? = G t ?C k (G t ) 2 F / G t 2 F Lemma 1 at each gradient step (iteration). We plot the relative residual, train and test performance for different values of k = ?d where 0 &lt; ? ? 1 in <ref type="figure" target="#fig_8">Figure 7</ref>  It is clear from these experiments that in over-parameterized settings ? ? 0 as training progresses potentially due to the inherent sparsity of overparameterized models. This implies our block selection procedure is often near lossless and thus BGmD even with aggressively small k can achieve near optimal performance. And since it is a few orders of magnitude faster than GmD -it is an attractive robust optimization algorithm in high dimensions. However, in the tiny CNN setting, as training progresses the gradients tend to become more uniform implying higher loss due to block co-ordinate selection. This implies, in these settings, BGmD needs to use a larger k in order to retain the optimal performance. However, note that in these small scale settings, GmD is not expensive and thus for robust optimization , a larger value of k or at the extreme k = d i.e. running GmD can be practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Results</head><p>The above empirical observations highlight the following insights: (a) For challenging corruption levels/models, Gm based methods are indeed superior while standard SGD or CmD can be significantly inaccurate, (b) In all of our reported experiments, BGmD was run with k set to 10% of the number of parameters. Despite using such a small ratio BGmD retains the strong performance of GmD.</p><p>(c) By judiciously choosing k, BGmD is more efficient than GmD, and (d) memory augmentation is vital for BGmD to attain a high accuracy while employing relatively small values of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed BGmD, a method for robust, large-scale, and high dimensional optimization that achieves the optimal statistical breakdown point while delivering significant savings in the computational costs per iteration compared to existing Gm-based strategies. BGmD employs greedy coordinate selection and memory augmentation which allows to aggressively select very few coordinates while attaining strong convergence properties comparable to Gm-SGD under standard non-convex settings. Extensive deep learning experiments demonstrated the efficacy of BGmD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Limitation</head><p>Currently, BGmD samples the coordinates for computing geometric median using just the norm of each coordinate in the gradient matrix. In large deep networks, the gradients should have more structure that cannot be captured just by the norm of coordinates. We leave further investigation into leveraging gradient structure for more effective coordinate selection for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Broader Impact</head><p>While the current work is more theoretical and algorithmic in nature, we believe that robust learning -the main problem addressed in the work -is a key requirement for deep learning systems to ensure that a few malicious data points do not completely de-rail the system and produce offensive/garbage output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing Proofs of BgmD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Lemma 1 (Sparse Approximation)</head><p>Lemma 1. Algorithm 2 yields a contraction approximation, i.e.,</p><formula xml:id="formula_25">E C k C k (G) ? G 2 F |x ? (1 ? ?) G 2 F , k d ? ? ? 1, where C k (G) i,j?? k = G i,j , and C k (G) i,j / ?? k = 0. Proof. Suppose, ? k = {? ? {1, 2, .</formula><p>.., d} : |?| = k} is the set of all possible subsets of cardinality k i.e. |? k | = d k . Also let the embeddings produced by random co-ordinate sampling and active norm sampling (Algorithm 2) are denoted by C r k (?) and C n k (?) respectively and let the i?th row G t [i, :] = g i t . Then, we can bound the reconstruction error in expectation ?G t ? R b?d as:</p><formula xml:id="formula_26">E C n k C n k (G t ) ? G t 2 F |G ? E C n k C r k (G t ) ? G t 2 |G t = n i=1 E C n k C r k (g i t ) ? g i t 2 |g i t = n i=1 1 |? k | ??? k d i=1 x 2 i I{i / ? ?} = n i=1 (1 ? k d ) g i t 2 = (1 ? k d ) G t 2 F</formula><p>This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Computational Complexity of BGmD iterates</head><p>Proposition 1. (Computational Complexity). Given an -approximate Gm oracle , each gradient aggregation of BGmD with block size k incurs a computational cost of: O( k 2 + bd). Proof. Let us first recall, one gradient aggregation operation: given b stochastic gradients g i ?i ? [b], the goal of the aggregation step is to computeg for subsequent iterations of the form x t+1 = x t ? ?g. Further, G t ? R b?d denote the gradient jacobian where i th row G t [i, :] corresponds to g i . Also assume that we have at our disposal an oracle Gm(?) that can compute an -approximate Gm</p><formula xml:id="formula_27">g i ? R d ?i ? [b] using O( d 2 ) compute [CLM + 16].</formula><p>Recall that BGmD (Algorithm 1) is composed of the following main steps:</p><p>? Memory Augmentation: At each gradient aggregation step, BgmD needs to add back the stored memorym t compensating for accumulated residual error incurred in previous iterations to G t such that G t [i, :] = ?G t [i, :] +m t . Note that, this is a row wise linear (addition) operation implying O(bd) associated cost.</p><p>? Active Norm Sampling: At each gradient aggregation step: BgmD selects k of the d coordinates i.e. k columns of G t using Algorithm 2. This requires computing the 2 norm distribution along the d columns, followed by sampling k of them proportional to the norm distribution. The computational complexity of computing Active Norm Sampling is O(bd) <ref type="bibr" target="#b94">[WS17]</ref>.</p><p>? Compute M t+1 : Further, memory needs to be updated to M t+1 for future iterates implying another row wise linear operation incurring O(bd) compute.</p><p>? Low Rank Gm: Note that BGmD needs to run Gm(?) over R k implying a cost of O( k 2 ). Putting it together, the total cost of computing gradient aggregation per iteration using Algorithm 1 is then O( k 2 + bd). This concludes the proof.</p><p>C Proof of Lemma 2</p><formula xml:id="formula_28">Lemma 2. (Choice of k) Let k ? O( 1 F ? b 2 ) ? d.</formula><p>Then, given an -approximate Gm oracle, Algorithm 1 achieves a factor F speedup over Gm-SGD for aggregating b samples.</p><p>Proof. First, note that: for one step of gradient aggregation Gm-SGD makes one call to Gm(?) oracle implying a O( d 2 ) computational cost per gradient aggregation. Now, let us assume, k = ?d where 0 &lt; ? ? 1 denotes the fraction of total gradient dimensions retained by BGmD. Then using Proposition 1 we can find a bound on ? such that gradient aggregation step of BGmD has a linear speedup over that of Gm-SGD by a linear factor F .</p><formula xml:id="formula_29">O( d 2 ) ? F ? O( k 2 + bd) ?O( 1 2 ) ? O( F ? 2 ) + O(F b) ?O(F ?) ? O(1) ? O(F b 2 ) ?O(?) ? O( 1 F ) ? O(b 2 )<label>(15)</label></formula><p>This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Detailed Statements of the convergence Theorems</head><p>We here state the full version of the main convergence theorems </p><formula xml:id="formula_30">= f (x 0 ) ? f (x * )</formula><p>where x * is the true optima and x 0 is the initial parameters. Run Algorithm 1 with compression factor ? (Lemma 1), learning rate ? = 1/2L and ?approximate Gm(?) oracle in presence of ??corruption (Definition 1) for T iterations. Then it holds that:</p><formula xml:id="formula_31">1 T T ?1 t=0 E ?f (x t ) 2 ? 8R 0 ?T + 8L?? 2 + 48L 2 ? 2 ? 2 (1 ? ? 2 ) ? 2 + 2304? 2 (1 ? ?) 2 1 + 4(1 ? ? 2 ) ? 2 + 48 2 ? 2 |G| 2 (1 ? ?) 2 = O LR 0 T + ? 2 ? ?2 (1 ? ?) 2 + L 2 2 |G| 2 (1 ? ?) 2 (16)</formula><p>Theorem 2 (Non-convex under PLC). Assume in addition to non-convex and smooth (Assumption 2) the functions f i correspond to non-corrupt samples also satisfy the Polyak-?ojasiewicz Condition (Assumption 3) with parameter ?. After T iterations Algorithm 1 with compression factor ? (Lemma 1), learning rate ? = 1/4L and ?approximate Gm(?) oracle in presence of ??corruption (Definition 1) satisfies:</p><formula xml:id="formula_32">E x T ? x * 2 ? 16(f (x 0 ) ? f * ) ? 2 ? 1 ? ?? 2 T + 16L?? 2 ? 2 + 80? 2 ? 2 L 2 (1 ? ? 2 ) ? 2 ? 2 + 3072? 2 ? 2 (1 ? ?) 2 1 + 4(1 ? ? 2 ) ? 2 + 64 2 ? 2 ? 2 |G| 2 (1 ? ?) 2 = O LR 0 ? 2 1 ? ? 8L T + ? 2 ? ?2 ? 2 (1 ? ?) 2 + L 2 2 ? 2 |G| 2 (1 ? ?) 2 (17)</formula><p>for a global optimal solution x * ? X * .</p><p>Here,x T := 1</p><formula xml:id="formula_33">W T T ?1 t=0 w t x t with weights w t := (1 ? ? 8L ) ?(t+1) , W T := T ?1 t=0 w t . E Useful Facts Fact 1. E i?A a i 2 ? |A| i?A E a i 2</formula><p>This can be seen as a consequence of the Jensen's inequality.</p><p>Fact 2 (Young's Inequality). For any ? &gt; 0,</p><formula xml:id="formula_34">E[ a, b ] ? ? 2 E a 2 + 1 2? E b 2<label>(18)</label></formula><p>This can be seen as a special case of the weighted AM-GM inequality.</p><p>Fact 3 (Lemma 2 in <ref type="bibr" target="#b82">[Sti19]</ref>). Let {a t } and {b t } be to non-negative sequences such that</p><formula xml:id="formula_35">a t+1 ? (1 ? r?)a t ? s?b t + c,<label>(19)</label></formula><p>where r, ?, s, c &gt; 0 and r? &lt; 1. Let w t = (1 ? r?) ?(t+1) and W T = T ?1 t=0 w t . Then the following holds:</p><formula xml:id="formula_36">s W T T ?1 t=0 b t w t + ra T ? a 0 ? (1 ? r?) T + c ? .<label>(20)</label></formula><p>Fact 4. Let f be a function that satisfies PLC (Assumption 3) with parameter ?. Then, f satisfies the quadratic growth condition [KNS16, Zha20]:</p><formula xml:id="formula_37">f (x) ? f * ? ? 2 x ? x p 2 (21)</formula><p>where x p is the projection of x onto the solution set X * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Proof of Lemma 3</head><p>Proof. The result is due to Lemma 5 in <ref type="bibr" target="#b5">[BDKD19]</ref> which is inspired by Lemma 3 in <ref type="bibr" target="#b47">[KRSJ19]</ref>. The proof relies on using the fact that the proposed norm sampling operator C k , as shown in Lemma 1 is contractive. Hence, we can use this property to derive a recursive bound on the norm of the memory. Using this result as well as the bounded stochastic gradient assumption results in a geometric sum that can be bounded by the RHS of (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proof of Lemma 4</head><p>Proof. Sinceg t is the -accurate Gm of {? i t } i , z t can be thought of as the -accurate Gm of </p><formula xml:id="formula_38">{? i t ? ? t } i .</formula><formula xml:id="formula_39">E z t 2 ? 8|G| (|G| ? |B|) 2 i?G E ? i t ? ? t 2 + 2 2 (|G| ? |B|) 2 .<label>(22)</label></formula><p>Next, we bound E ? i t ? ? t 2 using the properties of memory mechanism stated in Lemma 3 with H = 1 as:</p><formula xml:id="formula_40">E ? i t ? ? t 2 = 2E ? i t 2 + 2E ? t 2 ? 2E ? i t 2 + 2 |G| i?G E ? i t 2 ? 4 max i?G E ? i t 2 ,<label>(23)</label></formula><p>where we used Fact 1 twice. Next, we establish a bound on the norm of the communicated messages as follows. Add add and subtract p i t and use the update rule of the memory to obtain</p><formula xml:id="formula_41">E ? i t 2 = E ? i t + p i t ? p i t 2 = E p i t ?m t+1 2 = E ?g i t +m t ?m t+1 2 ? 3E ?g i t 2 + 3E m t 2 + 3E m t+1 2 , ? 3? 2 ? 2 + 3E m t 2 + 3E m t+1 2 ,<label>(24)</label></formula><p>by definition of p i t and Fact 1. Notice that using Lemma 3 with H = 1 we can uniformly bound the last two terms on the RHS of (24):</p><formula xml:id="formula_42">3E m t 2 + 3E m t+1 2 ? 24(1 ? ? 2 )? 2 ? 2 ? 2 .<label>(25)</label></formula><p>Therefore, we conclude</p><formula xml:id="formula_43">E ? i t 2 ? 3? 2 ? 2 + 24(1 ? ? 2 )? 2 ? 2 ? 2 .<label>(26)</label></formula><p>Therefore, by (23) and (26)</p><formula xml:id="formula_44">E ? i t ? ? t 2 ? 12? 2 ? 2 1 + 4(1 ? ? 2 ) ? 2 ,<label>(27)</label></formula><p>and the proof is complete by combining this last result with (22).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Proof of Lemma 5</head><p>Proof. We derive a recursive relation for the difference x t+1 ?x t+1 . It follows that</p><formula xml:id="formula_45">x t+1 = x t ?g t = x t ? ? t ? z t .<label>(28)</label></formula><p>On the other hand,x</p><formula xml:id="formula_46">t+1 =x t ? ?g t ? ?z t =x t ? ?g t ? z t =x t ? ?g t ? z t ,<label>(29)</label></formula><p>Collectively, (28) and (29) imply</p><formula xml:id="formula_47">x t+1 ?x t+1 = (x t ?x t ) + (?g t ? ? t ).<label>(30)</label></formula><p>Since y 0 =? 0 using induction yields</p><formula xml:id="formula_48">x t+1 ?x t+1 = t j=0 (?g t ? ? j ) = t j=0 (m j+1 ? m j ) = m t+1 ? m 0 = m t+1 ,<label>(31)</label></formula><p>where we used the fact that m 0 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Proof of Lemma 6</head><p>Proof. Recall that g t = 1</p><p>|G| i?G ?f i (x t , z i t ), i.e., the average of stochastic gradients ove uncorrupted samples at time t, and E[g t ] =? t . By the definition of L-smoothness (see Assumption 2), we have</p><formula xml:id="formula_49">f (x t+1 ) ? f (x t ) + ?f (x t ),x t+1 ?x t + L 2 x t+1 ?x t 2 = f (x t ) + ?f (x t ), ??g t ? z t + L 2 ?g t + z t 2 ? f (x t ) ? ? ?f (x t ), g t ? ?f (x t ), z t + L? 2 g t 2 + L z t 2 .<label>(32)</label></formula><p>Let E t denote expectation with respect to sources of randomness in computation of stochastic gradients at time t. Then,</p><formula xml:id="formula_50">E t [f (x t+1 )] ? f (x t ) ? ? ?f (x t ),? t ? E t [ ?f (x t ), z t ] + L? 2 E t g t 2 + LE t z t 2 .<label>(33)</label></formula><p>The first inner-product in (33) can be bounded according to</p><formula xml:id="formula_51">2 ?f (x t ),? t = ?f (x t ) 2 + ? t 2 ? ?f (x t ) ?? t 2 ? ?f (x t ) 2 ? ?f (x t ) ?? t 2 = ?f (x t ) 2 ? 1 |G| i?G ?f i (x t ) ? 1 |G| i?G ?f i (x t ) 2 ? ?f (x t ) 2 ? 1 |G| i?G ?f i (x t ) ? ?f i (x t ) 2 ? ?f (x t ) 2 ? L 2 x t ? x t 2 = ?f (x t ) 2 + L 2 x t ? x t 2 ? 2L 2 x t ? x t 2 ? ?f (x t ) 2 + ?f (x t ) ? ?f (x t ) 2 ? 2L 2 x t ? x t 2 = ?f (x t ) 2 + ?f (x t ) ? ?f (x t ) 2 ? 2L 2 x t ? x t 2 ? 1 2 ?f (x t ) 2 ? 2L 2 x t ? x t 2 ,<label>(34)</label></formula><p>where we employed Fact 1 and L-smoothness of each function several times. Therefore,</p><formula xml:id="formula_52">?? ?f (x t ),? t ? ? ? 4 ?f (x t ) 2 + ?L 2 x t ? x t 2 .<label>(35)</label></formula><p>We now bound the second inner-product. To this end,</p><formula xml:id="formula_53">?E t [ ?f (x t ), z t ] = ?E t [ ?f (x t ), z t ] + E t [ ?f (x t ) ? ?f (x t ), z t ] ? ?? ?f (x t ) 2 + 1 2?? E t z t 2 + E t [ ?f (x t ) ? ?f (x t ), z t ] ? ?? 2 ?f (x t ) 2 + 1 2?? E t z t 2 + 1 2? E t z t 2 + ? 2 ?f (x t ) ? ?f (x t ) 2 ? ?? 2 ?f (x t ) 2 + 1 2?? + 1 2? E t z t 2 + ?L 2 2 x t ?x t 2 ,<label>(36)</label></formula><p>where we used Fact 2 twice and to obtain the last inequality we employed the smoothness assumption. Here, 0 &lt; ? &lt; 0.5 is a parameter whose value will be determined later. Application of (35) and (36) in (33) yields</p><formula xml:id="formula_54">E t [f (x t+1 )] ? f (x t ) ? 1 2 ? ? ? 2 ?f (x t ) 2 + 3?L 2 2 x t ? x t 2 + L? 2 E t g t 2 + L + 1 2?? + 1 2? E t z t 2 .</formula><p>(37) J Proof of Theorem 1</p><p>Proof. Rearranging (37) and taking expectation with respect to the entire sources of randomness, i.e. the proposed coordinated sparse approximation and the randomness in computation of stochastic gradient in iterations 0, . . . , t ? 1, using the bounded SFO assumption yields</p><formula xml:id="formula_55">1 2 ? ? ? 2 E ?f (x t ) 2 ?E[f (x t )] ? E[f (x t+1 )] + 3?L 2 2 E x t ? x t 2 + L? 2 ? 2 + L + 1 2?? + 1 2? E z t 2 .<label>(38)</label></formula><p>Evidently, we need to bound two quantities in (38). Lemma 4 establishes a bound on E z t 2 while by Lemma 5</p><formula xml:id="formula_56">E x t ? x t 2 = E m t 2 ? 1 |G| i?G E m t 2 ? 4(1 ? ? 2 )? 2 ? 2 ? 2<label>(39)</label></formula><p>using Lemma 3 with H = 1 we get : </p><formula xml:id="formula_57">1 2 ? ? ? 2 E ?f (x t ) 2 ?E[f (x t )] ? E[f (x t+1 )] + L? 2 ? 2 + 6?L 2 (1 ? ? 2 )? 2 ? 2 ? 2 + L + 1 2?? + 1 2? 96? 2 ? 2 (1 ? ?) 2 1 + 4(1 ? ? 2 ) ? 2 + L + 1 2?? + 1 2? 2 2 |G| 2 (1 ? ?) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Proof of Theorem 2</head><p>Recall (38) in the proof of Theorem 1. Let a t := E[f (x t )] ? f * ? 0. Using Assumption 3 to bound the gradient terms in (38) yields</p><formula xml:id="formula_58">a t+1 ?a t ? 1 2 ? ? ??E[f (x t ) ? f * ] + 3?L 2 2 E x t ? x t 2 + L? 2 ? 2 + L + 1 2?? + 1 2? E z t 2 .<label>(41)</label></formula><p>The above result is not sufficient to complete the proof since a t is with respect to the virtual sequence. This difficulty, which is addressed for the first time in this paper, has been the main challenge in the proof of convergence of error compensated schemes with biased gradient compression under PLC. To deal with this burden, we revisit (33) to establish an alternative bound. Specifically, we to bound the first inner-product in (33) we instead establish ?? ?f (x t ),? t = ?? ?f (x t ), ?f (x t ) + ? ?f (x t ), ?f (x t ) ?? t</p><formula xml:id="formula_59">? ?? ?f (x t ) 2 + ?? 1 2 ?f (x t ) 2 + ? 2? ?f (x t ) ?? t 2 = ??(1 ? ? 1 2 ) ?f (x t ) 2 + ? 2? 1 ?f (x t ) ?? t 2<label>(42)</label></formula><p>where we used Fact 2 with parameter ? 1 &gt; 0 which will be determined later. Recall that by smoothness ?f (x t ) ?? t 2 ? L 2 x t ? x t 2 .</p><p>(43)</p><p>We now derive a bound for the second inner-product in (33). To do so, an application of Fact 2 yields</p><formula xml:id="formula_60">?E t [ ?f (x t ), z t ] ? ? 2 ? 2 ?f (x t ) 2 + 1 2?? 2 E t z t 2 .<label>(44)</label></formula><p>Therefore, in light of these new bounds we obtain</p><formula xml:id="formula_61">E t [f (x t+1 )] ? f (x t ) ? ? 1 ? ? 1 + ? 2 2 ?f (x t ) 2 + L? 2 E t g t 2 + L + 1 2?? 2 E t z t 2 + ?L 2 2? 1 x t ? x t 2 .</formula><p>(45)</p><p>Subtracting f * and taking expectation with respect to the entire sources of randomness yields</p><formula xml:id="formula_62">E[f (x t+1 )] ? f * ? E[f (x t )] ? f * ? ? 1 ? ? 1 + ? 2 2 E ?f (x t ) 2 + L? 2 ? 2 + L + 1 2?? 2 E z t 2 + ?L 2 2? 1 E x t ? x t 2 .<label>(46)</label></formula><p>The gradient term E ?f (x t ) 2 in (46) can be related to a t using the PL condition. Therefore</p><formula xml:id="formula_63">a t+1 ? 1 ? 2?? 1 ? ? 1 + ? 2 2 a t + L? 2 ? 2 + L + 1 2?? 2 E z t 2 + ?L 2 2? 1 E x t ? x t 2 .<label>(47)</label></formula><p>Multiply (41) and (47) by 1/2 and add the two to obtain</p><formula xml:id="formula_64">a t+1 ? 1 ? ?? 1 ? ? 1 + ? 2 2 a t + L? 2 ? 2 ? 1 2 ? ? ?? 2 E[f (x t ) ? f * ] + L + 1 4?? + 1 4?? 2 + 1 4? E z t 2 + 3 + 1 ? 1 ?L 2 4 E x t ? x t 2 .</formula><p>(48)</p><formula xml:id="formula_65">Define b t := E[f (x t ) ? f * ].<label>(49)</label></formula><p>Let ? = 1/4, ? 1 = 1/2, ? 2 = 1/2, and ? = 1/4L. Using Lemma 4 and (39), (48) simplifies to</p><formula xml:id="formula_66">a t+1 ? 1 ? ?? 2 a t ? ?? 8 b t + L? 2 ? 2 + 5L 2 4 ? 3 ? 2 4(1 ? ? 2 ) ? 2 + 192?? 2 (1 ? ?) 2 1 + 4(1 ? ? 2 ) ? 2 + 4 2 ?|G| 2 (1 ? ?) 2 .<label>(50)</label></formula><p>Thus we can apply Fact 3 to obtain</p><formula xml:id="formula_67">1 W T T ?1 t=0 b t w t ? 8(f (x 0 ) ? f * ) ?? 1 ? ?? 2 T + 8L?? 2 ? + 10L 2 ? ? 2 ? 2 4(1 ? ? 2 ) ? 2 + 1536? 2 ?(1 ? ?) 2 1 + 4(1 ? ? 2 ) ? 2 + 32 2 ?? 2 |G| 2 (1 ? ?) 2 ,<label>(51)</label></formula><p>where we used E[f (x i 0 )] = f (x 0 ). Finally, to obtain the stated result we invoke Fact 4 and the assumption that the solution set (i.e., the set of all stationary points) is convex. Specifically, by the quadratic growth condition and convexity of the Euclidean norm</p><formula xml:id="formula_68">T ?1 t=0 w t W T E[f (x t ) ? f * ] ? T ?1 t=0 w t W T E x t ? P X * (x t ) 2 ? ? 2 T ?1 t=0 w t W T E x t ? P X * (x t ) 2 ? ? 2 E T ?1 t=0 w t W T x t ? T ?1 t=0 w t W T P X * (x t ) 2 := ? 2 E x T ? x * 2 ,<label>(52)</label></formula><p>where P X * (x t ) is the projection of x t onto the solution set X * and x * := T ?1 t=0 wt W T P X * (x t ) ? X * by the assumption that the solution set is convex.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a . So we focus on T(t) a to study the relative computational cost of SGD, Gm-SGD, and BGmD. To computeg (t) vanilla SGD needs to compute average of b gradients in R d implying O(bd) cost. On the other had, Gm-SGD and its variants [AAZL18, CSX17, BCNW12] require computing -approximate Gm of b points in R d incurring per iteration cost of at least O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4</head><label></label><figDesc>setting ? = O(1/T ), BGmD convergences at the rate of O( log T T ) under PLC and no corruption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Robustness to Feature Corruption: We compare test set performance of different schemes as a function of wall clock time for training Fashion MNIST in i.i.d setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Robustness to Gradient Corruption: We train a CNN on Fashion MNIST in the i.i.d setting in presence of scaled bit flip corruption of stochastic gradients. Similar to Figure 1, BGmD achieves up to ? 3 ? x speedup over GmD while being robust. For high corruption %, CmD converges to sub-optimal solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Robustness to Label Corruption: We train a CNN on Fashion-MNIST in the i.i.d setting in presence of backdoor attack. BGmD achieves up to large speedup over GmD while being robust even at very high corruptions where SGD converges very slowly due to backdoor attack and converges to a sub-optimal point at high 40% corruption.CmD converges to sub-optimal solution. (a) Original Image (b) Gaussian Noise (c) Salt &amp; Pepper Noise (d) Gaussian Blur Feature Corruption: shows the effect of the perturbations added to image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Gradient Corruption:This Toy example in 2 dimensions visually demonstrates the superior robustness properties of Gm for robust mean estimation (e.g. estimating the aggregated gradient) for increasing fraction of (None to 45%) gradients corrupted with additive Gaussian noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>BGMD with different values of k and with / without memory on MNIST using CNN demonstrating the importance of memory augmentation Discussion on Over parameterization Intuitively, as a consequence of over-parameterization, large scale deep learning models are likely to have sparse gradient estimates [SCCS19]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Large Model: We train an over-parameterized (d = 1.16 M) parameter CNN on MNIST [LBBH98] dataset. (a) shows relative residual error as a function of iterations (gradient steps) (b) and (c) show the corresponding test and training performance. The residual error starts to approaches zero as training progresses even for very small values of k explaining negligible impact on training and test performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Tiny Model: Here we instead train a super tiny d = 412 parameter CNN on MNIST. (a),(b),(c) plotsrelative residual, train and test performance respectively. We can see that even in this setting (1 ? ?) &gt; k/d = ? as we remarked in Lemma 1. However, as training progresses the gradients become closer to uniform explaining the optimality gap of BGmD with small values of k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Theorem 1 (</head><label>1</label><figDesc>Smooth Non-convex). Consider the general case where the functions f i correspond to non-corrupt samples i ? G i.e. f = 1 G i?G f i (x) are non-convex and smooth (Assumption 2). Define, R 0 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>? = 1/4 and ? = 1 2L we obtain the stated result by averaging (40) over time and noting f * ? E[f (x T )] and f (x 0 ) = f (x 0 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of time complexity and robustness properties of different robust optimization methods. The bold quantities show a method achieves the theoretical limits. * CmD throughout the paper will refer to Co-ordinate wise median descent i.e. simply replacing the aggregation step of SGD by Cm.</figDesc><table><row><cell>Algorithm</cell><cell>Iteration Complexity Breakdown Point</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MNIST each data shard was assigned 750 samples. Each client was then randomly assigned 8 shards of data at the beginning of training implying with high probability no client had access to all classes of data ensuring heterogeneity [LSZ + 18, LHY + 19, DAH + 20, KKM + 20].Common to all the experiments, we used the categorical cross entropy loss plus an 2 regularizer with weight decay value of 1e?4. All the experiments are performed on a single 12 GB Titan Xp GPU. For reproducibility, all the experiments are run with deterministic CuDNN back-end. Further, each experiment is repeated 5 times with different random seeds and confidence interval is noted in theTables 2 4 and 3.</figDesc><table /><note>Homogeneous Samples We trained a moderately large LeNet [LBBH98] style CNN with 1.16M parameters on the challenging Fashion-MNIST [XRV17] dataset in the vanilla mini batch setting with 32 parallel mini-batches each with batch size 64. The training data was i.i.d among all the batches at each epoch. We use initial learning rate of 0.01 which was decayed by 1% at each epoch. Each experiment under this setting was run for 50 epochs (full passes over training data). We also train the same model on simple MNIST [LBBH98] dataset using mini-batch SGD with 10 parallel batches each of size 64. We use a learning rate 0.001 updated using cosine annealing scheduler along with a momentum 0.9. Heterogeneous Samples Our theoretical results (Theorem 1, 2) are established without any assumption on how the data is distributed across batches. We verify this by training an 18-layer wide ResNet [HZRS16] with 11.2M parameters on CIFAR 10 [KSH12] in a federated learning setting [MMR + 17]. In this setting, at each iteration 10 parallel clients compute gradients over batch sizes of 128 and communicate with the central server. The training data was distributed among the clients in a non i.i.d manner only once at the beginning of training. Each experiment was run for 200 epochs with an initial learning rate 0.1 , warm restarted via cosine annealing. To Simulate heterogeneous (non i.i.d) data distribution among clients we use the follow- ing scheme: The entire training data was first sorted based on labels and then contiguously divided into 80 equal data-shards D i i.e. for Fashion-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Homogeneous Training: 1.12M parameter CNN trained on Fashion MNIST in regular i.i.d. setting. For all corruption types, test accuracy of BGmD is similar to that of GmD and surprisingly, in some cases even higher. As expected, SGD often diverges under corruption (empty entries in the table). CmD performs sub-optimally as corruption is increased. 99?0.02 76.38?0.13 88.97?0.10 88.26?0.04 40 73.01?0.68 60.85?1.24 84.69?0.31 81.32?0.16</figDesc><table><row><cell></cell><cell>Corruption (%)</cell><cell>SGD</cell><cell>CmD</cell><cell>BGmD</cell><cell>GmD</cell></row><row><cell>Clean</cell><cell>-</cell><cell cols="3">89.39?0.28 83.82?0.26 89.25?0.19</cell><cell>88.98?0.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Gradient Attack</cell><cell></cell></row><row><cell>Bit Flip</cell><cell>20 40</cell><cell>--</cell><cell cols="3">84.20?0.02 88.42?0.16 88.07?0.05 82.33?1.60 85.67?0.09 85.57?0.09</cell></row><row><cell>Additive</cell><cell>20 40</cell><cell>--</cell><cell cols="3">72.55?0.16 87.87?0.33 87.24?0.16 41.04?1.13 88.29?0.01 83.89?0.08</cell></row><row><cell></cell><cell></cell><cell cols="2">Feature Attack</cell><cell></cell></row><row><cell>Additive Noise</cell><cell>20 40</cell><cell>--</cell><cell cols="3">82.38?0.13 86.76?0.03 86.63?0.04 78.54?0.65 82.27?0.06 81.23?0.03</cell></row><row><cell>Impulse Noise</cell><cell>20 40</cell><cell cols="4">79.18?6.47 82.59?0.60 86.91?0.36 86.23?0.03 -78.03?0.73 78.03?0.73 81.41?0.12</cell></row><row><cell></cell><cell></cell><cell cols="2">Label Attack</cell><cell></cell></row><row><cell>Backdoor</cell><cell>20</cell><cell>86.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Homogeneous Training:1.12M parameter CNN trained on MNIST in regular i.i.d.</figDesc><table><row><cell></cell><cell>Corruption (%)</cell><cell>SGD</cell><cell>CmD</cell><cell>BGmD</cell><cell>GmD</cell></row><row><cell>Clean</cell><cell>-</cell><cell cols="4">99.27?0.01 98.83?0.02 99.09?0.05 99.24?0.02</cell></row><row><cell></cell><cell></cell><cell cols="2">Gradient Attack</cell><cell></cell></row><row><cell>Bit Flip</cell><cell>20 40</cell><cell cols="4">9.51?1.77 98.79?0.01 99.06?0.02 98.98?0.01 9.60?2.04 93.69?0.09 97.89?0.05 98.11?0.12</cell></row><row><cell>Additive</cell><cell>20 40</cell><cell cols="4">9.68?0.11 94.26?0.03 98.61?0.01 98.69?0.01 9.74?0.12 91.86?0.03 97.78?0.27 92.78?0.04</cell></row></table><note>setting. For all corruption types, test accuracy of BGmD is similar to that of GmD and surprisingly, in some cases even higher. As expected, SGD fails to make progress under corruption . CmD performs sub-optimally as corruption is increased.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Heterogeneous Training: ResNet-18 trained on CIFAR-10 in a federated setting under two gradient attack models. We observe similar trends as Tables 2 and 3 -GmD and BGmD both remain robust at both 20% and 40% corruption maintaining high accuracy. At high corruptions BGmD achieves similar or even better performance than GmD while taking 3x less time. Interestingly, even in non-corrupt setting, BGmD outperforms SGD, perhaps due to the distributional shift introduced by non-i.i.d setting. Note that in i.i.d setting, our model achieves 94.8% accuracy with SGD</figDesc><table><row><cell cols="2">Corruption (%)</cell><cell>SGD</cell><cell>CmD</cell><cell>BGmD</cell><cell>GmD</cell></row><row><cell>Clean</cell><cell>-</cell><cell cols="4">82.29?1.32 85.50?1.43 84.82?0.76 85.65?0.48</cell></row><row><cell></cell><cell></cell><cell cols="2">Gradient Attack</cell><cell></cell></row><row><cell>Bit Flip</cell><cell>20 40</cell><cell>--</cell><cell cols="3">80.87?0.21 84.56?0.06 88.07?0.05 77.41?1.04 82.66?0.31 80.81?0.01</cell></row><row><cell>Additive</cell><cell>20 40</cell><cell cols="4">20.7?1.56 54.75?0.38 83.84?0.12 82.40?0.90 -23.35?6.13 82.79?0.68 79.46?0.24</cell></row><row><cell>(a) No corruption</cell><cell cols="2">(b) 10% Corruption</cell><cell cols="2">(c) 20% Corruption</cell><cell>(d) 40% Corruption</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Thus, by the classical robustness property of Gm (Theorem. 2.2 in [LR + 91]; see also [M + 15, CLM + 16, CSX17, LXC + 19, WLCG20] for similar adaptations) we have</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The notation k d implies that k is at least an order of magnitude smaller than d</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To see this, consider the case where each g (i) t is uniformly distributed along each coordinates. Then, the algorithm would satisfy Lemma 1 with ? = k d . In this scenario, the achievable bound is identical to the bound achieved via choosing the k dimensions uniformly at random.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Just to clarify, both these experiments are performed in clean setting</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Byzantine stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4613" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sarit Khirirat, and C?dric Renggli. The convergence of sparsified gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Konstantinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5973" to="5983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Even faster accelerated coordinate descent using non-uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Richt?rik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1110" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theory and applications of robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caramanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="501" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sample size selection in optimization methods for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gillian</forename><forename type="middle">M</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="155" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debraj</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepesh</forename><surname>Data</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Karakus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhas</forename><surname>Diggavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14668" to="14679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="334" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning with adversaries: Byzantine tolerant gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peva</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Stainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="119" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On distributed stochastic gradient descent for nonconvex functions in the presence of byzantines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikiran</forename><surname>Bulusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Khanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranay</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod K</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3137" to="3141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6389</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the convergence of block coordinate descent type methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luba</forename><surname>Tetruashvili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2037" to="2060" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust solutions of linear programming problems contaminated with uncertain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="424" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SignSGD: Compressed optimisation for non-convex problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar ; Pratik Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jennifer Chayes, Levent Sagun, and Riccardo Zecchina</title>
		<imprint>
			<publisher>Christian Borgs</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">124018</biblScope>
		</imprint>
	</monogr>
	<note>Entropysgd: Biasing gradient descent into wide valleys</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combinatorial feature selection problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesan</forename><surname>Guruswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sridhar Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Sahai ; Xinyun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05526</idno>
	</analytic>
	<monogr>
		<title level="m">and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="631" to="640" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings 41st Annual Symposium on Foundations of Computer Science</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geometric median in nearly linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Tat</forename><surname>Michael B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-eighth annual ACM symposium on Theory of Computing</title>
		<meeting>the forty-eighth annual ACM symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="9" to="21" />
		</imprint>
	</monogr>
	<note>CLM + 16</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Runtime guarantees for regression problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Hui Han Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th conference on Innovations in Theoretical Computer Science</title>
		<meeting>the 4th conference on Innovations in Theoretical Computer Science</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="269" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed statistical machine learning in adversarial settings: Byzantine gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Measurement and Analysis of Computing Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open questions concerning weiszfeld&apos;s algorithm for the fermat-weber location problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramaswamy</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Tamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="293" to="295" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Draco: Byzantine-resilient distributed training via redundant gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="903" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Faster non-convex federated learning via global and local momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dah + 20] Rudrajit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abolfazl</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ufuk</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topcu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04061</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Byzantine-resilient SGD in high dimensions on heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepesh</forename><surname>Data</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhas</forename><surname>Diggavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07866</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><forename type="middle">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tannenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Feedback control theory. Courier Corporation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal distributed online prediction using mini-batches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="165" to="202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The notion of breakdown point. A festschrift for Erich</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page">157184</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1102.3975</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05911</idno>
		<title level="m">Recent advances in algorithmic highdimensional robust statistics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust estimators in high-dimensions without the computational intractability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dkk + 19a] Ilias Diakonikolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="742" to="864" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sever: A robust meta-algorithm for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dkk + 19b] Ilias Diakonikolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1596" to="1606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nearest neighbor based greedy coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2160" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Penalized regressions: the bridge versus the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wenjiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="416" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5937</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Distributed robust learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gdg + 17] Priya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Badnets: Evaluating backdooring attacks on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="47230" to="47244" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Byzantine fault-tolerant distributed machine learning using stochastic gradient descent (sgd) and norm-based comparative gradient elimination (cge)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirupam</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><forename type="middle">H</forename><surname>Vaidya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04699</idno>
		<idno>arXiv:1911.09721</idno>
	</analytic>
	<monogr>
		<title level="m">Swanand Kadhe, Arya Mazumdar, and Kannan Ramchandran. Communication-efficient and byzantine-robust distributed learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>GMK + 19. Raj Kumar Maity</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">When cyclic coordinate descent outperforms randomized coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Gurbuzbalaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Asuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuri Denizcan</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Note on the median of a multivariate distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jbs Haldane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="414" to="417" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast coordinate descent methods with variable selection for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1064" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Making large-scale svm learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The median of a finite measure on a banach space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jhb Kemperman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical data analysis based on the L1-norm and related methods</title>
		<meeting><address><addrLine>Neuch?tel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="217" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scaffold: Stochastic controlled averaging for federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Kkm + 20] Sai Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda Theertha</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5132" to="5143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient greedy coordinate descent for composite problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Sai Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koloskova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2887" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<idno type="arXiv">arXiv:1609.04836</idno>
		<title level="m">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<editor>KMN + 16] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Linear convergence of gradient and proximal-gradient methods under the Polyak-?ojasiewicz condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9851</biblScope>
			<biblScope unit="page" from="795" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Error feedback fixes SignSGD and other gradient compression schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Sai Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rebjock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09847</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>+ 19] Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02189</idno>
		<title level="m">On the convergence of fedavg on non-iid data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Principled approaches to robust machine learning and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry Zheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Breakdown points of affine equivariant estimators of multivariate location and covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hendrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopuhaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="248" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The byzantine generals problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leslie Lamport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Shostak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="401" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Federated optimization in heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06127</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rsa: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Giannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1544" to="1551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient mini-batch training for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Backdoor embedding in convolutional neural network models via invisible perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoti</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sencun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10307</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Geometric median and robust estimation in banach spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Minsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2308" to="2335" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The hidden vulnerability of distributed learning in byzantium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>El Mahdi El Mhamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rouault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07927</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eider</forename><surname>Mmr + 17] Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficiency of coordinate descent methods on huge-scale optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="362" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Let&apos;s make block coordinate descent go fast: Faster greedy rules, message-passing, active-set complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issam</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08859</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and superlinear convergence. NSL + 15</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Coordinate descent converges faster with the gauss-southwell rule than random selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issam</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoyt</forename><surname>Koepke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1632" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Paved with good intentions: analysis of a randomized block kaczmarz method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deanna</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="page" from="199" to="221" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Maximizing submodular set functions: formulations and analysis of algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North-Holland Mathematics Studies</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1981" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="279" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sequential Minimal Optimization. A fast algorithm for training support vector machines</title>
	</analytic>
	<monogr>
		<title level="j">CiteSeerX</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4376</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Robust aggregation for federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Pillutla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harchaoui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13445</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Gradient methods for minimizing functionals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Teodorovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polyak</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zhurnal Vychislitel&apos;noi Matematiki i Matematicheskoi Fiziki</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="653" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Iteration complexity of randomized blockcoordinate descent methods for minimizing a composite function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Richt?rik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Tak??</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Block coordinate relaxation methods for nonparametric wavelet denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Sardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="379" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Understanding top-k sparsification in distributed deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Chun</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08772</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Sparsified SGD with memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4447" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Valiant ; Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04940</idno>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Resilience: A criterion for learning in the presence of arbitrary outliers</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">The error-feedback framework: Better rates for sgd with delayed gradients and compressed communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai Praneeth</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karimireddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05350</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Approximate steepest coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anant</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3251" to="3259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning with bad training data via iterative trimmed loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5739" to="5748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Accelerated mini-batch stochastic dual coordinate ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">On the nonasymptotic convergence of cyclic coordinate descent methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="576" to="601" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09767</idno>
		<title level="m">Local sgd converges fast and communicates little</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Unified optimal analysis of the (stochastic) gradient method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Scalable distributed dnn training using commodity gpu cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikko</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Distributed asynchronous deterministic and stochastic gradient optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Athans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on automatic control</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="803" to="812" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Spectral signatures in backdoor attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8000" to="8010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Data poisoning attacks against federated learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vale</forename><surname>Tolpegin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacey</forename><surname>Truex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Mehmet Emre Gursoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Research in Computer Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="480" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoon</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of optimization theory and applications</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">513</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A coordinate gradient descent method for nonsmooth separable minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoon</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="387" to="423" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Probabilistic logics and the synthesis of reliable organisms from unreliable components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automata studies</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="43" to="98" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The multivariate l1-median and associated data depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Vardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cun-Hui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1423" to="1426" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Sur le point pour lequel la somme des distances de n points donn?s est minimum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endre</forename><surname>Weiszfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tohoku Mathematical Journal, First Series</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="355" to="386" />
			<date type="published" when="1937" />
		</imprint>
	</monogr>
	<note>WF + 29</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Alfred Weber&apos;s theory of the location of industries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Joachim</forename><surname>Friedrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1929" />
			<publisher>The University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Federated variance-reduced stochastic gradient descent with robustness to byzantine attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="4583" to="4596" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Provably correct algorithms for matrix column subset selection with selectively sampled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5699" to="5740" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Zeno: Distributed stochastic gradient descent with suspicion-based fault-tolerance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indranil</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6893" to="6901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Waheed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bajwa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08098</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Bridge: Byzantine-resilient decentralized gradient descent. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Byzantinerobust distributed learning: Towards optimal statistical rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5650" to="5659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Large linear classification when data cannot fit in memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Asynchronous parallel greedy coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4682" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Byzantine-resilient stochastic gradient descent for distributed learning: A lipschitz-inspired coordinate-wise median approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 58th Conference on Decision and Control (CDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5832" to="5837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">New analysis of linear convergence of gradient-type methods via unifying error bound conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="371" to="416" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
