<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SNoRe: Scalable Unsupervised Learning of Symbolic Node Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Me?nar</surname></persName>
							<email>smeznar@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Lavra?</surname></persName>
							<email>nada.lavrac@ijs.si</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?ef</forename><surname>Stefan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?ef</forename><surname>Stefan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Jo?ef Stefan Institute</orgName>
								<address>
									<addrLine>Jamova 39</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Jo?ef Stefan Institute</orgName>
								<address>
									<addrLine>Jamova 39</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">International Postgraduate School Jamova 39</orgName>
								<orgName type="institution">University of Nova</orgName>
								<address>
									<addrLine>Gorica Glavni trg 8, Vipava</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country>Slovenia, Slovenia Bla??krlj</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Jo?ef Stefan Institute</orgName>
								<address>
									<addrLine>Jamova 39</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">International Postgraduate School Jamova 39</orgName>
								<address>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SNoRe: Scalable Unsupervised Learning of Symbolic Node Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SNoRe Editor: Sebastian Me?nar</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>node embedding</term>
					<term>feature construction</term>
					<term>symbolic learning</term>
					<term>interpretable ma- chine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from complex real-life networks is a lively research area, with recent advances in learning information-rich, low-dimensional network node representations. However, stateof-the-art methods are not necessarily interpretable and are therefore not fully applicable to sensitive settings in biomedical or user profiling tasks, where explicit bias detection is highly relevant. The proposed SNoRe (Symbolic Node Representations) algorithm is capable of learning symbolic, human-understandable representations of individual network nodes, based on the similarity of neighborhood hashes which serve as features. SNoRe's interpretable features are suitable for direct explanation of individual predictions, which we demonstrate by coupling it with the widely used instance explanation tool SHAP to obtain nomograms representing the relevance of individual features for a given classification. To our knowledge, this is one of the first such attempts in a structural node embedding setting. In the experimental evaluation on eleven real-life datasets, SNoRe proved to be competitive to strong baselines, such as variational graph autoencoders, node2vec and LINE. The vectorized implementation of SNoRe scales to large networks, making it suitable for contemporary network learning and analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Me?nar, Lavra? and?krlj</head><p>Costa et al. <ref type="formula">(2011)</ref>). By representing a real-life system as a network, it is possible to study network properties, such as the key network nodes, why they are relevant, how sets of nodes group together and how network nodes are classified <ref type="bibr" target="#b3">(Cai et al. (2018)</ref>; Bhagat et al.</p><p>(2011)). The latter task is the focus of this research. By using networks to represent reallife systems, we can further explore interactions between instances instead of conventional approaches that assume instance independence.</p><p>The problem of node classification has been already considered in the 1990s <ref type="bibr" target="#b6">(Farmer and Rodkin (1996)</ref>). However, it was popularized only in the recent years due to the increase in the available computational power. A well-known method capable of node classification is label propagation (Xiaojin and Zoubin <ref type="formula">(2002)</ref>), an algorithm that asynchronously assigns labels to neighboring nodes, eventually reaching an equilibrium state that corresponds to the final classification. Albeit efficient, label propagation and similar approaches operate in a relatively na?ve manner, not accounting for the rich structure of a given network that spans beyond simple neighborhoods. To mitigate this issue, novel representation learning methods emerged, offering efficient ways of constructing real-valued vector representations of individual nodes, suitable for down-stream learning such as classification.</p><p>Contemporary structural node representation algorithms are mostly concerned with the down-stream performance, with insufficient focus on the interpretability, which is of utmost importance when the user tries to understand why the system decided to classify a given instance the way it did. To mitigate this issue, we developed SNoRe, an algorithm that compares node neighborhoods and is capable of learning interpretable feature sets through symbolic expressions, describing a given node, which can be used to obtain explanations of individual predictions, being an improvement over state-of-the-art low-dimensional, blackbox node representations. The contributions of this work are summarized as follows:</p><p>? We propose SNoRe, an efficient algorithm capable of learning symbolic representations of nodes by accounting for global network topology.</p><p>? Theoretical and empirical comparisons with state-of-the-art indicate competitive performance, whilst offering the interpretability of individual predictions, explained by the contributions of the neighboring nodes.</p><p>? We show that SNoRe scales to real-life networks with tens of thousands of nodes, and does not require dedicated hardware for effective performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Networks can be used to model numerous real-world systems, spanning from biological protein interaction networks to social and transportation networks <ref type="bibr" target="#b17">(L? and Zhou (2011)</ref>;</p><p>? SNoRe is implemented as a simple-to-use Python library, transpiled to lower-level code via the Numba framework <ref type="bibr" target="#b15">(Lam et al. (2015)</ref>) for maximum efficiency. The implementation also features a highly efficient sparse implementation of the Hub Promoted Index (HPI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This section presents the state-of-the-art methods capable of solving the node classification task along with their properties. Note that there are two main settings for learning from networks, referred to as transductive and inductive learning. In the transductive learning setting, node classification is performed within the same network, where part of the network is initially labeled, whereas the remaining part is not. The task addresses the issue of extrapolating the information from the known part of the network to the unknown (unlabeled) part. Common examples of this task include gene function prediction and social network-based tasks, such as user profiling. On the other hand, in the inductive learning setting, independent networks are fed as input and are also classified on the network level. The focus of this work is on transductive learning.</p><p>The types of learning algorithms can further be split based on the information they are capable to exploit during learning. An algorithm can perform solely by exploiting the network structure, or can also incorporate potentially interesting features assigned to nodes or edges. The focus of this work is on structural classification with no assigned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structural node embedding</head><p>The notion of structural node embedding corresponds to the process of learning a given node's latent representation (most commonly real-valued), based on its neighborhood within a given network. The first branch of methods was inspired by the widely known word2vec algorithm <ref type="bibr" target="#b22">(Mikolov et al. (2013)</ref>): DeepWalk <ref type="bibr" target="#b27">(Perozzi et al. (2014)</ref>) was one of the first node representation learners, and remains state-of-the-art to this date. DeepWalk creates a network representation by using sequences of nodes representing random walks as input sentences for the word2vec algorithm. Random walks created in a depth-first search manner intuitively map nodes with similar second-order proximity close together.</p><p>Following similar ideas, methods such as node2vec, struc2vec, LINE, PTE, NetMF and others emerged, each considering additional network properties (e.g., network-topological properties) during representation learning. Algorithm node2vec <ref type="bibr" target="#b7">(Grover and Leskovec (2016)</ref>) uses hyper-parameters p and q to guide random walks. Parameter p dictates the return probability whereas q dictates the probability of exploration away from the previous node. If p and q are set to 1 we get the special case where the node2vec algorithm is equivalent to DeepWalk. LINE <ref type="bibr" target="#b34">(Tang et al. (2015b)</ref>) derives an objective function for first and second-order proximity that is computationally intensive and thus not scalable. The algorithm is then made scalable with the adoption of negative sampling. Function parameters of the classification model are optimized with asynchronous stochastic gradient descent.</p><p>NetMF <ref type="bibr" target="#b28">(Qiu et al. (2018)</ref>) is presented along with the theoretical analysis of Deep-Walk <ref type="bibr" target="#b27">(Perozzi et al. (2014)</ref>), node2vec <ref type="bibr" target="#b7">(Grover and Leskovec (2016)</ref>), LINE <ref type="bibr" target="#b34">(Tang et al. (2015b)</ref>) and PTE <ref type="bibr" target="#b33">(Tang et al. (2015a)</ref>), showing that all the aforementioned methods approximate matrix factorization and that the close forms of these matrices are intrinsically connected to the graph Laplacian. NetMF factorizes these closed form matrices, potentially offering consistent improvement in performance over the methods mentioned above.</p><p>Personalized Page Rank with Shrinking (PPRS) was introduced as a part of the HIN-MINE methodology <ref type="bibr" target="#b14">(Kralj et al. (2017)</ref>). This algorithm creates vectors representing personalized node ranks by using the power iteration. Such vectors can be used directly for learning purposes, or further compressed by an autoencoder <ref type="bibr">(?krlj et al. (2019a)</ref>), offering small compact representations trained in an end-to-end manner.</p><p>Similarly to these embedding algorithms, our proposed approach uses information about a given node's neighborhood to create a representation in unsupervised manner. Instead of creating a dense, latent embedding, our algorithm returns a sparse embedding where features represent nodes, which makes the result easily interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph neural networks</head><p>Since networks as such are not bound to a given coordinate system, direct input of e.g., adjacency matrices into neural networks proves to be problematic. As a result, in parallel with the aforementioned structural node embedding methods, which are useful for representation learning in domains with a well structured spatial structure (such as images), the area of graph neural networks (GNNs) ; <ref type="bibr" target="#b38">Wu et al. (2020)</ref>; <ref type="bibr" target="#b40">Xu et al. (2018)</ref>; ; ) emerged, conceived to tackle the problem of learning from unstructured domains.</p><p>Graph Neural Networks operate by passing feature information from the considered node's neighbors towards the node itself. During this process, the latent representation (embedding) of the node is obtained. The final representation is, in most cases, a result of gradient descent-based optimization. These algorithms are mostly divided into three subgroups: graph recurrent neural networks, graph convolutional neural networks and graph autoencoders. Graph recurrent neural networks try to capture and learn recursive and sequential patterns by taking advantage of recurrent neural networks. Graph convolutional neural networks learn local and global patterns trough designed convolution and readout functions.</p><p>Graph convolutional neural networks are divided into spectral and spatial based algorithms, based on how they define convolution. Spectral based algorithms define graph convolution using filters from graph signal processing, whereas the convolution in spatial algorithms relays on information propagation. Graph autoencoders are often used for unsupervised representation learning by assuming that the networks have low-rank structures that are potentially nonlinear ).</p><p>Graph convolutional networks (GCNs) (Kipf and Welling <ref type="formula">(2017)</ref>) are among the most influential works in graph-based deep learning since CNNs bridge the gap between spectral and spatial based graph convolutional neural networks. The GCN algorithm simplifies filtering by only focusing on first-order neighbors. Since the number of neighbors can vary, GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al. (2017)</ref>) samples a fixed amount of neighbors and aggregates them. The Graph attention network (GAT) <ref type="bibr" target="#b36">(Veli?kovi? et al. (2018)</ref>) further improves both previously mentioned approaches by introducing the attention mechanism. Attention mechanism allows the neural network to learn how much each neighbor contributes instead of assuming that all neighbors contribute the same amount (like in GraphSAGE) or that this amount is predetermined (like in GCN). Another interesting graph convolutional neural network is the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b41">(Xu et al. (2019)</ref>) that presents a readout function that uses summation and a multi-layer perceptron to provably achieve the maximum discriminative power.</p><p>A popular graph autoencoding algorithm is the Variational graph autoencoder (VGAE) (Kipf and Welling (2016)) that uses latent variables to create a representation for undirected networks. The algorithm encodes the network into mean and variance matrices and decodes them with the dot product. The parameters of the model are learned by minimizing the variational lower bound.</p><p>While Graph Neural Networks represent the state-of-the-art in node classification, they differ significantly from our approach as they usually use features that are not calculated from the network and classify nodes in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The SNoRe algorithm</head><p>In this section we first define some essential components and present the key ideas of the SNoRe algorithm (Section 3.1). The algorithm is divided into four steps: random walk generation (Section 3.2), random walk hashing (Section 3.3), feature selection (Section 3.4), and similarity calculation (Section 3.5). For each step, we present its description and its implementation. We also propose an extension of the algorithm that chooses the number of features based on the embedding size (Section 3.6), show an overview of the algorithm (Section 3.7) and present its theoretical properties (Section 3.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definitions and key ideas</head><p>Let us first define the key terms used throughout this paper.</p><p>Definition 1 (Network). A network is a tuple G = (N, E), where N represents the set of nodes and E represents the set of edges. An edge can be represented as an ordered pair (e.g., (n 1 , n 2 ) ? N ? N )-in this case the network is directed. Alternatively, an edge can be represented as a subset of size 2 (e.g., {n 1 , n 2 } ? N )-in this case the network is undirected.</p><p>For generality, we will use directed networks since we can also represent the undirected ones using the same formalism. We define a walk and a random walk as follows.</p><p>Definition 2 (Walk). A walk of length k in a directed network is any sequence of k nodes n 1 , n 2 , ..., n k ? N , so that each pair of consecutive nodes n i and n i+1 has a connection (n i , n i+1 ) ? E.</p><p>Definition 3 (Random walk). A random walk is a walk generated in such way that at step i, node n i+1 ? {a, (n i , a) ? E} is chosen with some probability.</p><p>The result of our algorithm is symbolic node embedding of a network defined as: Definition 4 (Symbolic node embedding). Symbolic node embedding of network G = (N, E) is a matrix M ? R |N |?d , where d is the dimension of the embedding. Such embedding is considered symbolic, when each column represents a symbolic expression, which-when evaluated against a given node's neighborhood information-returns an integer number representing a given node. This definition uses the term symbolic expression to describe the structure of data that can be easily interpreted by a human (i.e. the similarity between neighborhoods of nodes i and j).</p><p>Note that the above defined type of symbolic node embedding can also be referred to as propositionalization (see a recent review ) for more details).</p><p>We use the above definitions to outline the proposed SNoRe algorithm, illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. In the figure, we highlight a node and mark it red to present how an arbitrary node in the network gets embedded. The first step generates random walks, marked as a collection of red edges in <ref type="figure" target="#fig_0">Fig. 1</ref>. We then aggregate walks starting at the red node into a vector of node occurrences in step 2.</p><p>Step 3 then selects the features based on weights assigned by the PageRank algorithm <ref type="bibr" target="#b24">(Page et al. (1999)</ref>). In the final step, we generate Step 1 generates random walks that are then hashed in Step 2. These hashes are represented as sparse vectors and used to calculate the similarity between two node neighborhoods in Step 4, where the similarity is calculated between all nodes and the nodes that are chosen as features in Step 3 based on their PageRank score. the embedding of any given node by calculating cosine similarity between the hash values of nodes selected as features and the red (considered) node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Random walk generation</head><p>Sampling the neighborhood of a given node can give us information about the network structure and the connectivity patterns in its vicinity. We can sample the neighborhood using short random walks. These offer many advantages such as ease and parallelization of computation, bound for the distance of the farthest node and ease of representation.</p><p>The first step of the algorithm generates random walks and represents them with a data structure such as a list of visited nodes. We use the random walk generation scheme (and vectorized implementation) presented in <ref type="bibr">(?krlj et al. (2019b)</ref>). Let w ? R s , w 1 = 1 be the distribution vector, where s is the maximum length of the walk and w i denotes the probability that the walk is of length i. We sample random walk length i from w and create a random walk of length i using Algorithm 1. In line 4 of the algorithm we append the current node c (together with some information) to the walk representation structure. Function neighbor in line 5 returns a neighbor of the given node (randomly). This algorithm is repeated nw times for each node, giving us nw random walks per node.</p><p>In our implementation, we represented a random walk with a list of tuples denoting the node and step (n i , s i ) ? N ? {j, j = 0, ..., s}. The final random walk structure consists of concatenated random walk lists l i for each node separately.</p><p>Algorithm 1 Classical walk Input : Starting node n i , Walk length wl Output: Random walk structure Ws 1: c ? n i 2: Ws ? ? 3: for i = 0 to wl do 4:</p><p>Ws ? Ws ? c 5:</p><p>c ? neighbor(c) 6: end for 7: return Ws</p><p>The time and space complexity of computing the random walk structure is O(|N |?nw?s), where s represents the mean length of the walk. We get this time complexity because for each node we create nw walks that make s steps on average. Since only the random walk hashing step uses this representation of random walks, the space complexity can drop to a constant if we merge the first two steps by incrementally calculating the hash value after each walk. This way the walks do not have to be stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Random walk hashing</head><p>We represent the neighborhood of node n i numerically by hashing random walks starting in n i . The hashing function can incorporate different sources of information about the network to make a vector, h ? R dh , where dh is the dimension of hashing function output. Some examples of this can include: occurrences of nodes normalized, the number of nodes with some degree normalized or occurrences of the communities normalized. We will denote the hash value (vector) for the i-th node as h i .</p><p>Our implementation uses only neighborhood-level information about the network, i.e. how often a node appears in random walks that start at node n i . We also use threshold as the lower bound for occurrences. Any node that occurs less then length(l i ) ? times is not included in h i . The final hash is a normalized sparse row vector, where values represent how frequently an included node was encountered during a random walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature selection</head><p>Features of the node embedding created by our algorithm are symbolic expressions that can be easily interpreted. We use a subset of nodes as our features to satisfy this goal. The feature values represent the similarity between the neighborhoods of a given node and the node that represents the feature. We will use feature-map : N ? N as the function mapping feature index to the corresponding node.</p><p>Feature selection can be done in a supervised or unsupervised manner <ref type="bibr" target="#b29">(Saeys et al. (2007)</ref>). We focus on unsupervised feature selection so that the whole algorithm can remain unsupervised.</p><p>In feature selection, we want to select nodes that are important for the network structure. We assign a score to each node using the PageRank algorithm <ref type="bibr" target="#b24">(Page et al. (1999)</ref>), then sort them based on this score in the descending order, and select top d nodes as our features.</p><p>The PageRank algorithm computes a probability distribution pr ? R |N | , pr 1 = 1, where pr i approximates the probability of a random walker being at node i. When pr i is high, node i is more likely to be visited and therefore it is likely more important for the structure of the network. Let r ? R |N | represent a vector of PageRank values for each node. Let d j represent the degree of the j-th node. If the adjacency matrix of the considered network (A) is normalized as follows:</p><formula xml:id="formula_0">C ij = 1 d j ; A ij = 0 0; otherwise</formula><p>the computation of PageRank can be formulated as an eigenvalue problem:</p><formula xml:id="formula_1">r = Cr.</formula><p>For larger networks, the power iteration is used to approximate the final solution. This procedure first initializes r = [ 1 |N | , 1 |N | , . . . , 1 |N | ] T (i.e. a discrete uniform distribution), and iterates by computing:</p><formula xml:id="formula_2">r k+1 = Cr k ,</formula><p>until the difference between r k and r k+1 is smaller than some predetermined threshold ?. The final r represents the final collection of PageRank values considered in this work. Note that in practice, about 10-50 iterations are needed for convergence, making this method highly scalable. We chose PageRank as the scoring function used in feature selection because it is fast, unsupervised and gives a good approximation for node importance. This choice, however, is not very important since many times, choosing nodes randomly gives us only slightly worse results. This is especially true for sparse networks coupled with the extension of SNoRe we present in Section 3.6 since most if not all nodes are chosen. This extension uses feature ranking to estimate d such that the embedding we get is equivalent in size to a chosen dense embedding.</p><p>PageRank has a parameter, alpha, also known as the damping factor. In our work, we used the value 0.85 as the parameter since it is the default value in the NetworkX (Hagberg et al. <ref type="formula">(2008)</ref>) implementation we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Similarity calculation</head><p>The proposed SNoRe algorithm creates a symbolic node embedding matrix M , where row m i represents the similarity of the i-th node to the nodes chosen as features (pivot nodes). This similarity is calculated in the final step from hash values h i generated in the random walk hashing step. We compare the hash value h i of the i-th node to the hash value h feature-map(j) of the j-th pivot node.</p><p>The cosine similarity metric is defined such that it represents the cosine angle between two non-zero vectors:</p><formula xml:id="formula_3">cos sim(a, b) = |N | i=1 a i ? b i |N | i=1 a 2 i |N | i=1 b 2 i ,</formula><p>where a and b represent the two vectors 1 . The similarity score between two vectors without common features is 0, and between two vectors with the same angle is 1. This makes the similarity between two vectors easily interpretable. Further, since the score can be 0, this metric works well with sparse representations. Because of these properties, we use cosine similarity as our main similarity calculation metric. In Section 5.4 we further demonstrate the advantage of cosine similarity and show how different distance measures compare against it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Estimating the representation dimension</head><p>One of the key features of SNoRe is its ability to construct sparse representations of individual nodes. Compared to e.g., DeepWalk and similar methods, where the dimension is predetermined, SNoRe exploits the following theoretical insight to construct a high dimensional representation with the same (or lower) memory footprint than the comparative methods. As the dimensions in SNoRe can be computed independently (walks w.r.t. individual nodes are independent), this feature offers an iterative expansion of the representation until a sufficient number of e.g., floating-point values is obtained.</p><p>The following example demonstrates the mentioned functionality. Consider a situation where SNoRe is to be compared against a dense representation learning algorithm, which learns d dimensional representations of nodes. Assuming |N | instances, the total space required to store the representation can be denoted with ? = |N | ? d (floating-point values). The SNoRe algorithm constructs the representation requiring the same (or less) space in the following manner. We follow the first three steps of the algorithm to create hash values and the list of nodes sorted in the descending order by their PageRank score. We then add features incrementally calculating the similarity between the added feature and all nodes. After each calculation, we subtract the number of nonzero values for the feature from ? and return the created embedding when ? drops below zero.</p><p>During testing, we realized that the quality of the embedding is not affected much by small changes in the similarity score and that sometimes digitizing it helps classification (showcased in Appendix A). For this reason, we divide the interval</p><formula xml:id="formula_4">[0, 1] into b sub-intervals, where sub-interval i = [ i b , i+1 b )</formula><p>, and use them to discretize the similarity score between two hashes. We replace the similarity score M i,j with idx b , where idx denotes the index of subinterval containing M i,j . This allows us to store values using fewer bits and consequently create the embedding with more features that takes up the same amount of space.</p><p>The automatic sparse representation construction is outlined in lines 12-25 of Algorithm 2. This extension is presented as SNoRe with Size Dependent Features (SNoRe (SDF)) in Section 5. The behaviour of SNoRe and SNoRe (SDF) only differs in the number of nodes used in the final embedding, so we use the terms interchangeably in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">SNoRe overview</head><p>The pseudocode of SNoRe (with the final step of SNoRe (SDF)) is presented in Algorithm 2. Function SAMPLE takes a distribution vector described in Section 3.2 as the input and returns an integer representing walk length sampled from it. Function WALK returns a structure that represents a random walk and takes as arguments the starting node and the walk length. Function HASH returns the hash value of the inputted walks. Function PAGE RANK returns a sorted list of nodes based on their PageRank scores. Function SIM returns a number between 0 and 1 that represents the similarity between two hashes given as input (distance between the obtained walk distributions). for j = 1 to num walks do 4:</p><formula xml:id="formula_5">Algorithm 2 SNoRe (SDF) Input : Network G = (N, E), Length distribution w,</formula><formula xml:id="formula_6">walks i ? walks i ? WALK(N i , SAMPLE(w)) 5: end for 6: end for 7: h ? ? ;</formula><p>Random walk hashing. 8: for i = 1 to |N | do 9:</p><formula xml:id="formula_7">h i ? HASH(walks i ) 10: end for 11: feature-map ? PAGE RANK(G) ; Unsupervised feature ranking. 12: M ? [0] |N |?|N | 13: l ? 0 ; Embedding generation. 14: while ? ? 0 &amp; l &lt; |N | do 15: l ? l + 1 16: num ? 0 17: for j = 0 to |N | do 18: s ? SIM(h i , h feature-map(j) ) ;</formula><p>Similarity.</p><p>19:</p><formula xml:id="formula_8">M i,j ? round(s) b 20:</formula><p>if M i,j = 0 then 21:</p><formula xml:id="formula_9">num ? num + 1 22: end if 23: end for 24: ? ? ? ? num 25: end while 26: return M ? R |N |?l</formula><p>Lines 1-6 show the random walk generation step. The outer loop iterates over nodes and the inner loop over random walks for each node. In line 4 the generated random walk is transformed into a suitable representation and appended to the ones already generated.</p><p>In the implementation, we use memoization to sample walk lengths once and use them for all nodes instead of sampling the length of each walk independently. The generated walks are used in the random walk hashing step that is outlined in lines 7-10.</p><p>Hash values (vectors) are generated in the loop shown in lines 8-10. Since hashes are independent between nodes we parallelized this step in the implementation.</p><p>The version of the algorithm described in pseudocode also estimates the representation dimension as shown in Section 3.6. This is done in lines 11-25. Line 11 calculates the PageRank score of nodes and sorts them. The embedding is iteratively calculated in lines 14-25, adding one feature in each pass until ? &lt; 0 or we run out of features that can be added. We can see that the estimation also uses the similarity calculation step denoted in lines 17-23. The algorithm finishes in line 26 where it returns the embedding of size |N | ? l, with ? ? = |N | ? d floating point values. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Theoretical properties</head><p>For an algorithm to be useful it has to have time and space complexities that are not too resource-intensive. Using the definitions from the previous sections and the understanding of how the algorithm works we next derive the time and space complexities of SNoRe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.1">Time complexity</head><p>To present the time complexity we describe how each step of the algorithm behaves and sum the gathered complexities. We simultaneously describe the time complexity of random walk creation and the hashing step, since they can be implemented together efficiently as described in Section 3.2.</p><p>Random walk creation and hashing are computed in O(|N | ? nw ? s), since we need to create nw walks with an average of s steps for |N | nodes, whilst assuming that every step takes O(1) time. Hashing maintains this complexity since each of |N | ? nw walks needs O(s) time to be hashed.</p><p>The time complexity of feature selection depends mostly on the algorithm used for selecting the representative subset of nodes. For feature ranking we used the PageRank algorithm with time complexity O(c ? |E|), when networks are represented with a sparse adjacency matrix. In the aforementioned time complexity c represents the maximum number of iterations. We also need additional O(|N | ? log |N |) to sort feature scores and gather first d pivot nodes. This can be done more efficiently by only selecting top d pivot nodes, but we rank all nodes for use in the extension. To calculate the time complexity of the last step we focus on the time needed to calculate the similarity between two hash values since this has to be calculated |N | ? d times to create the final node embedding matrix. We use sparse implementation of the cosine similarity function with sparse vectors containing at most 1 non-zero values. Because of this, we need 1 time to compute the similarity between two hashes. Consequently we need O(|N | ? d ? 1 ) to calculate the similarity between each node and each feature.</p><p>The algorithm extension that is shown in Section 3.6 only impacts the size of d, since other used operations do not contribute significantly to time complexity and can be omitted because of this. Since we use nodes as features d ? |N | still holds. Summing the time complexity of all steps we get the following time complexity:</p><formula xml:id="formula_10">O(|N | ? nw ? s + c ? |E| + |N | ? log |N | + |N | ? d ? 1 ) = O(|N | ? (d ? 1 + nw ? s + log |N |) + c ? |E|)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.2">Space complexity</head><p>The space complexity can be calculated similarly to time complexity by considering the four parts of the algorithm and merging the random walk creation and hashing step. Furthermore, we need O(|E|) for the sparse adjacency matrix to represent the network. We can compute the random walk creation and hashing steps in O( |N | ) space. Since random walks and hash value calculation can be done for each node independently, we need O(nw ? s) space for random walk creation and O( 1 ) space to store the sparse vector that represents the hash value for this node. This holds because at most 1 values can be greater than the threshold . Since node occurrence is usually not uniform and many nodes occur more frequently than , the used space is usually smaller than this. We get the space complexity O( |N | ) for this two steps by concatenating hash representations of each node.</p><p>The space complexity of the feature selection depends on d and the space complexity of the algorithm used for feature selection. We use PageRank that uses O(E) space to store a sparse adjacency matrix. We also need O(d) to store the selected features.</p><p>The similarity calculation step creates a (sparse) matrix of size |N | ? d where d ? |N |. To calculate the similarity between two hashes we only need constant additional space. If we put the space complexity of all steps together we get the final space complexity:</p><formula xml:id="formula_11">O(|E| + |N | + |N | ? d).</formula><p>We further extend the analysis of space complexity with the algorithm extension in Section 3.6 since we generate a sparse matrix that uses less or equal than ? = |N | ? d space, where d is the dimension of a dense embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets and experimental setting</head><p>In this section, we describe the datasets used to evaluate the performance of the proposed embedding algorithm, the experimental setting and the baselines we used to compare the results with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The datasets used for the evaluation of the embedding algorithms consist of 11 real-world complex networks. The summary of the datasets is shown in <ref type="table" target="#tab_1">Table 1</ref>. This table shows that we use datasets that have different characteristics since they differ a lot in the number of nodes, edges, classes, and connected components. We also show visualizations of Cora and Pubmed datasets in <ref type="figure" target="#fig_1">Fig. 2</ref>. In the figure, target classes are represented using different colors. ? Ions <ref type="bibr">(?krlj et al. (2019a, 2018)</ref>) is a network of ion binding sites, linked by their structural similarity. The target class is the type of ion that binds to a given protein substructure (node).</p><p>? Cora (Lu and Getoor <ref type="formula">(2003)</ref>) is a network of scientific publications and the citations between them. The labels represent the topic categories of the publication.</p><p>? CiteSeer (Lu and Getoor <ref type="formula">(2003)</ref>) is a network of scientific publications and the citations between them. The labels represent the topic categories of the publication.</p><p>? Bitcoin Alpha (?krlj et al. <ref type="formula">(2019a)</ref>) is a network of Bitcoin transactions from the platform Bitcoin Alpha. The labels represent the level of trust in the transaction (integer range from -10 to 10).</p><p>? Homo sapiens (PPI) (as used in <ref type="bibr" target="#b7">(Grover and Leskovec (2016)</ref>)) is a network of the proteome, i.e. a set of proteins which interact with each other. The labels represent the protein functions.</p><p>? Wikipedia (Mahoney <ref type="formula">(2011)</ref>) is a network of co-occurrences of words in the first million bytes of the Wikipedia dump. The labels represent the part-of-speech tags.</p><p>? Bitcoin (?krlj et al. <ref type="formula">(2019a)</ref>) is a network of Bitcoin transactions from the platform Bitcoin OTC. The labels represent the level of trust in the transaction (integer range from -10 to 10).</p><p>? BlogCatalog <ref type="bibr" target="#b42">(Zafarani and Liu (2009)</ref>) is a network of social relationships on the Blogger website. The labels represent interests inferred from metadata provided by the authors.</p><p>? Coauthor-CS <ref type="bibr" target="#b30">(Shchur et al. (2018)</ref>) is a computer science co-authorship network where nodes represent authors and edges represent that two authors co-authored a paper. The labels represent the authors most active fields of study.</p><p>? Pubmed (as used in <ref type="bibr" target="#b37">(Wang and Leskovec (2020)</ref>)) is a network of scientific publications and the citations between them. The labels represent the topic categories of the publication.</p><p>? Coauthor-PHY <ref type="bibr" target="#b30">(Shchur et al. (2018)</ref>) is a physics co-authorship network, where nodes represent authors and edges represent that two authors co-authored a paper. The labels represent the authors' most active fields of study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setting</head><p>The conducted experiments focus on the multi-label node classification task. In the multilabel classification task, many different labels may be assigned to each instance. This can be formally looked at as a problem where we search for a function f : N ? {0, 1} g , where g is the number of labels. Labels where the value is 1, are assigned to the instance, whereas the ones with 0 are not. An example of such a task is the assignment of genres to a movie. When comparing the proposed method to the baselines, we evaluated the performance of a given embedding algorithm with the same methodology as in state-of-the-art papers such as node2vec <ref type="bibr" target="#b7">(Grover and Leskovec (2016)</ref>). The methodology is described below.</p><p>? We embedded a network's nodes to a low-dimensional representation.</p><p>? We made ten copies of the embedding with corresponding labels and shuffled each.</p><p>? We evaluated the performance on each copy using a training set of increasing size, i.e. from 10% to 90% classified by logistic regression. We classified each node into top k i classes based on the probability returned from the classifier, where k i represents the number of classes of a given node.</p><p>? We calculated micro and macro F1 scores and averaged the results for each percentage.</p><p>? We performed the described test for each embedding algorithm ten times.</p><p>The exception to this method of testing is the Label Propagation algorithm that does not use an embedding. To test it we ran the algorithm 100 times with the randomly selected training set of increasing size from 10% to 90%, similarly to how we tested the other embedding algorithms.</p><p>All experiments were conducted on a machine with 128 GB RAM, Intel(R) Xeon(R) Gold 6150 @ 2.7 GHz with a NVIDIA Tesla V100 SXM3 32 GB GPU. The approaches that consumed more than 128 GB of RAM were marked as unsuccessful and are shown as Out Of Memory (OOM) in the results. We added this constraint because we use medium-sized datasets for testing and the methods that need more memory would probably not scale well to larger networks.</p><p>As default parameters for SNoRe we use = 0.005, maximum walk length = 5, number of walks per node = 1024 and 2048 pivot nodes (d). For SNoRe (SDF) we use the same parameters except that we use d equivalent to a dense representation with 256 features (? = |N | ? 256). We have chosen 256 features because other embedding algorithms we tested use 32-bit floating-point numbers with 128 features whereas we use 16-bit floating-point values, making the size of the embedding the same. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compared the results of the proposed approach against the results of eight other baselines outlined below. Seven of these are embedding algorithms, the exception being Label Propagation that performs classification directly by propagating label information across the network structure.</p><p>? Random baseline creates an embedding of size |N | ? 64 with random numbers drawn from Unif(0, 1).</p><p>? Label Propagation (LP) <ref type="bibr" target="#b39">(Xiaojin and Zoubin (2002)</ref>) propagates labels of annotated nodes through the network until convergence or the maximum number of iterations. We used alpha = 0.9 as parameter.</p><p>? VGAE (Kipf and Welling <ref type="formula">(2016)</ref>) is a variational auto-encoder that uses latent variables to learn a model that can be interpreted. This auto-encoder is used mostly for link prediction. We used default parameters epochs = 200, learning rate = 0.01, 32-dim hidden layer and 16-dim latent variables in the experiments.</p><p>? Personalized Page Rank with Shrinking (PPRS). This variant of Personalized PageRank was developed as part of HINMINE methodology <ref type="bibr" target="#b14">(Kralj et al. (2017)</ref>). The algorithm, for each node, computes its representation by iteratively obtaining a discrete stationary distribution of walk visits. The shrinking offers additional speedups. We use probability threshold = 0.0005 and number of important = 1000 that are the default parameters for testing.</p><p>? DeepWalk <ref type="bibr" target="#b27">(Perozzi et al. (2014)</ref>) equates random walks to sentences. These sentences are used to learn the network representation using a simple language model-like procedure. We use default parameters: representation size = 128, walk length = 80, and the number of walks = 10 in the experiments.</p><p>? NetMF (SCD) (?krlj et al. <ref type="formula">(2020)</ref>) is the PyTorch <ref type="bibr" target="#b25">(Paszke et al. (2017)</ref>) re-implementation of the NetMF embedder <ref type="bibr" target="#b28">(Qiu et al. (2018)</ref>). NetMF tries to approximate the closed form of the DeepWalk's implicit latent walk matrix. The re-implementation is suitable for highly sparse matrices and is optimized for running on GPUs, offering substantial performance improvements. We use the default parameters: dimension = 128, window size = 10, rank = 248 and negative = 1 in the experiments.</p><p>? LINE <ref type="bibr" target="#b34">(Tang et al. (2015b)</ref>) is one of the first network embedding algorithms. It uses an objective function that preserves first and second-order proximities. We use default parameters: embedding dimension = 200 and the number of negative samples = 5 in the experiments.</p><p>? node2vec (Grover and Leskovec (2016)) learns a low dimensional representation of nodes that maximizes the likelihood of neighborhood preservation using random walks. We use default parameters: embedding dimension = 128, walk length = 80, number of walks = 10 and window size = 10 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We next present the results of the empirical evaluation. We begin with the classification results across the considered real-life datasets, followed by a series of ablation studies, where we explored SNoRe's behaviour in more detail, ranging from its explainability capabilities to behaviour w.r.t. different hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification results</head><p>Classification results are visualized in <ref type="figure" target="#fig_2">Fig. 3 and 4</ref>, as well as presented in tabular format, where average performances across different training percentages alongside the corresponding standard deviations are reported <ref type="table" target="#tab_3">(Tables 2 and 3)</ref>. It can be observed that the proposed SNoRe algorithm performs competitively, or even outperforms the considered baselines. We can see that SNoRe and its extension SNoRe (SDF) work well on co-authorship networks, citation networks, Cora and the Ions dataset. Their results on both co-authorship network are interesting, since F1 scores are low at first, but then they rise fast and achieve the best results out of all baselines when we use enough training instances. Our algorithm performs poorly compared to other baseline methods on datasets such as Wikipedia and BlogCatalog, where nodes with similar class do not necessarily have similar neighborhoods (homophily), which is potentially the case with the co-authorship datasets. We can see that all embedding algorithms perform similarly to the random baseline on both Bitcoin datasets. This potentially shows that some datasets may not be suitable for direct learning.</p><p>Similar results can be observed in the averaged results <ref type="table" target="#tab_3">(Tables 2 and 3)</ref>, indicating SNoRe and its extension offer the state-of-the-art performance, albeit offering fundamentally different representation learning capabilities (sparse and symbolic). The results of both SNoRe and SNoRe (SDF) in the averaged tables suffer because the classification accuracy drops when we use a small amount of data (less than 25%) for training. The table also shows that SNoRe performs the best on four datasets (same amount as NetMF (SCD)) in    the micro F1 metric while only achieving the best result on one dataset in the macro F1 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Statistical analysis</head><p>This section presents the statistical comparison of embedding algorithms by using average rank diagrams with critical distances <ref type="bibr" target="#b5">(Dem?ar (2006)</ref>) and also Bayesian comparisons (Benavoli et al. <ref type="formula">(2017)</ref>). Average rank diagrams are shown in <ref type="figure" target="#fig_3">Fig. 5 and 6</ref>. These diagrams display the mean rank of algorithms over all datasets along the horizontal line. The ranks used in these diagrams are assigned to the algorithms based on their best performing percentage on a given dataset. We assigned ranks in this way because we usually only want to classify a few new instances using models trained on larger amounts of labeled data. More diagrams showing the performance where ranks are assigned based on mean results over all percentages on a dataset can be found in Appendix B. The critical distance in the figures group classifiers whose rank is not significantly different (Friedman test with Nemenyi post-hoc correction).</p><p>We see that for both micro and macro F1 metric SNoRe (SDF) performs best out of all algorithms and that when a constant amount (2048) of pivot nodes are used, SNoRe performs observably worse being fifth overall in the macro F1 metric. We can see that NetMF (SCD), DeepWalk, node2vec, and both versions of SNoRe form a group of algorithms that are state-of-the-art and perform significantly better than the other embedding algorithms. Our claim that SNoRe performs similar to other state-of-the-art algorithms is further backed by the critical distance that groups both versions of SNoRe with node2vec, DeepWalk, and NetMF (SCD).</p><p>Bayesian variants of performance comparison classifiers were recently introduced as a way to combat the shortcomings of methods like null hypothesis significance testing (NHST) <ref type="bibr" target="#b0">(Benavoli et al. (2017)</ref>). We use the Bayesian variant of the hierarchical t-test to determine differences in performance of compared classifiers. This test distinguishes between three scenarios: two where one of the classifiers outperforms the other and the one in which the difference in classifier performance lies in the region of practical equivalence (rope). The size of rope is a free parameter set to 0.01 in our experiments, which means that two performances are considered the same if they differ by less then 0.01. An algorithm can be argued to perform significantly better, if p(algorithm) &gt; 0.95.</p><p>As Bayesian multiple classifier correction cannot be intuitively visualized for more than two classifiers, we show the comparison between SNoRe (SDF) and node2vec as well as Label Propagation in <ref type="figure">Fig. 7</ref>. The two comparisons are used to demonstrate the performance against a strong and a weak baseline. We chose node2vec as the strong baseline because it is a generalization of Deepwalk and thus covers both algorithms. The data used to make these comparisons was collected over all datasets using ten repetitions of ten-fold cross-validation.</p><p>The green dots in the triangles represent samples, obtained from the hierarchical model. As the sampling procedure is governed by the underlying data, green dots fall under one of the three categories; classifier one dominates (left), classifier two dominates (right), or the difference of the classifiers' performance lies in the region of practical equivalence (up). Upon model convergence, some areas of the triangle are more densely populated, showing higher probability that the classifier outperformed the other. We can see that in our  experiment SNoRe (SDF) significantly outperformed the Label Propagation algorithm in both micro and macro F1 metric, having almost all green dots in the far left corner. More interesting are the comparisons against node2vec where SNoRe still outperforms node2vec whose probability of outperforming SNoRe is only 2%. Here a lot of dots are in the region of practical equivalence showing that both algorithm perform similarly a lot of times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation study -parameter space exploration</head><p>Having shown that the default hyperparameter setting = 0.005, maximum walk length of 5, number of walks = 1024 and 2048 pivot nodes performs competitively to state-of-theart, we conducted additional experiments to better understand SNoRe's behaviour w.r.t. different parameter settings. In this section, we present only plots of micro F1 performance. Additional plots with macro F1 results can be found in Appendix C.</p><p>As the default hyperparameter value of , we chose 0.005 because we wanted hashes with at most 200 non-zero values. <ref type="figure" target="#fig_5">Fig. 8</ref> shows the impact of different number of pivot nodes. From the figure, we can extract two types of datasets. Those where the score rises gradually and those where the score is similar no matter the number of features. Most tested datasets can be easily put in one of those groups, the exception being the BlogCatalog dataset and Homo Sapiens (PPI) <ref type="figure">Figure 7</ref>: Pairwise Bayesian performance comparisons of selected classifiers. The probabilities following classifier names represent the probabilities that a given classifier outperforms the other. dataset where the score rises slowly. Since the score noticeably rises on the Homo Sapiens dataset, we would put it in the first group, whereas we would put the BlogCatalog dataset into the second one. Coincidentally, BlogCatalog is the only dataset where SNoRe performs significantly worse than some other state-of-the-art methods. From <ref type="figure" target="#fig_2">Fig. 3 and 4</ref> we can further observe that the results on datasets where the number of features does not affect the score are usually similar no matter which embedder we use and close in many cases to those of the random baseline. These might mean that these datasets are less susceptible to classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Representation dimension</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Maximum walk length</head><p>We show the effect of the maximum walk length parameter in <ref type="figure">Fig. 9</ref>. From the figure, we can see that the score increases when we increase the value of this parameter. The increase of the score is especially apparent in the Coauthor PHY and Coauthor CS datasets. On the other hand, scores on Homo Sapiens (PPI), Wikipedia, BlogCatalog, and both Bitcoin datasets do not change much when the maximum walk length parameter is changed. This groups the datasets into almost the same groups as those in Section 5.3.1, with the exception being the Homo Sapiens dataset, where it was not clear to which group the dataset belongs. <ref type="figure">Figure 9</ref>: Micro F1 plots showing the effect of maximum walk length parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNoRe</head><p>We have chosen 5 as the default value for the maximum walk length parameter. We have chosen this value because on most datasets, score does not significantly increase if we increase the parameter value, while the embedding time does. Execution time is explored further in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Number of random walks</head><p>We show the effect of the different number of random walks per node in <ref type="figure" target="#fig_0">Fig. 10</ref>. We can see that the classification score on most datasets does not change much when different values of this parameter are chosen. The change is most observable on Coauthor CS and Coauthor PHY datasets, where the parameter values of 32-64 work best.</p><p>We have chosen 1024 as the default value for the number of random walks per node parameter. We selected this value because it entails the maximum length needed across all datasets, and the execution is still fast. We also believe it is beneficial to create more walks since this gives us a greater chance to sample out relevant sub-graph structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation study -effect of different distances (metrics)</head><p>In our approach, we selected cosine similarity to calculate the distance between two vectors because it offers good results and works well with sparse representation in both calculation and final embedding matrix. Since the choice of this distance metric is arbitrary we next show how the choice of distance metric affects the results. <ref type="table">Table 4</ref> shows different distance metrics we compared using same, default parameters. In the formula for standardized Euclidean v i represents the variance of the i-th feature in the hash vector. These metrics where chosen because they represent different groups of distance metrics. Euclidean distance is a special case of Minkowski distance where p = 2. It measures the distance between two points in the Euclidean space. By taking the variance of each dimension into account during the calculation of Euclidean distance, we get the Standardized Euclidean distance that is usually more robust when dimensions are scaled differently. Canberra distance is mostly used in intrusion detection and computer security and is a metric suitable for when the data is scattered around the origin. Jaccard similarity works on binary data and calculates similarity based on whether a feature is present or not. This metric can also be generalized for use with numeric values. The last metric we considered is the Hub Promoted Index (HPI) that was designed originally for quantifying the topological overlap between pairs of nodes in a network <ref type="bibr" target="#b44">(Zhou et al. (2009)</ref>). We generalized the metric, as shown in <ref type="table">Table 4</ref>, to be used alongside the proposed hashing scheme.</p><p>The results between different distance metrics are shown in <ref type="figure" target="#fig_0">Fig. 11</ref> and 12. We can see that most metrics perform similarly on Bitcoin datasets, Citeseer, Cora, Homo sapiens, Ions and Pubmed. On BlogCatalog both Euclidean metrics and the HPI metric performed better than the other three. On both co-authorship datasets, cosine similarity performed <ref type="table">Table 4</ref>: Used distance metrics and their formulas. <ref type="figure" target="#fig_0">Figure 10</ref>: Micro F1 plots showing the effect of different number of random walks.</p><formula xml:id="formula_12">Metric Formula Cosine |N | i=1 a i ?b i |N | i=1 a 2 i |N | i=1 b 2 i Euclidean |N | i=1 (a i ? b i ) 2 Standardized Euclidean |N | i=1 (a i ?b i ) 2 v 2 i Canberra |N | i=1 |a i |?|b i | |a i |+|b i | Jaccard |N | i=1 a i =0 and b i =0 |N | i=1 a i =0 or b i =0 Hub Promoted Index (HPI) |N | i=1 a i =0 and b i =0 min( |N | i=1 a i =0, |N | i=1 b i =0) SNoRe</formula><p>worse than other metrics but the byte size of the embedding is significantly smaller since the embedding matrix is very sparse. Using SNoRe (SDF) where the size of representation is less than ? = |N | ? 128 we get results that are better than those of other metrics. Using different distance metrics also helps on the Wikipedia dataset where the score is a lot higher for the Jaccard, Canberra and HPI metrics. As it should be expected both the Euclidean and Standardized Euclidean distance perform very similarly since SNoRe's hash function already normalizes the obtained values.</p><p>It should also be noted that cosine similarity, HPI and Jaccard similarities give us sparse embeddings, which perform significantly better when compared to the embeddings calculated using other metrics of the same size in bytes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation study -evaluation time</head><p>In Section 3.8 we give the theoretical boundaries for time complexity. Here we give further empirical results and compare them to other baselines. The results between different baselines are shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. We can see that both SNoRe and SNoRe (SDF) need a similar amount of time to finish and are usually the fastest, just before NetMF (SCD). We can see that SNoRe (SDF) is the fastest on small datasets but needs a little more time then SNoRe on larger datasets where more features need to be chosen. A more extensive study of evaluation time for different number of pivot nodes, maximum walk length, number of random walks, and different distance metrics can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation study -explainability</head><p>In the final set of experiments, we demonstrate how SNoRe can be coupled with the existing model explanation approaches such as SHapley Additive exPlanations (SHAP) (Lundberg and Lee (2017);?trumbelj and Kononenko <ref type="formula">(2014)</ref>). SHAP is a game-theoretic approach used to explain any type of classification or regression model. The algorithm perturbs subsets of input features to take into account the interactions and redundancies between them. The explanation model can then be visualized, showing how the feature values of an instance impacted its classification.</p><p>We use the following methodology to explain how different feature values representing nodes impact how the classifier assigns a label to a node. This process will be showcased on the Pubmed dataset. First, we create the embedding and save indexes used as features. We then train the XGBoost model and input it to the SHAP tree explainer. We can then explain how different feature values impact an instance or create a summary of impact for all instances. With a summary, we can for example take the most impactful nodes, look at which articles they represent and look how they influence the assignment of classes. We created such a summary using SHAP library <ref type="bibr" target="#b20">(Lundberg et al. (2020)</ref>) and visualized the results in <ref type="figure" target="#fig_0">Fig. 14.</ref> In the figure, the features are already renamed to indexes of the node (feature index i is renamed to node feature-map(i)). Red and blue dots represent feature value, red being 1 and blue 0. We can see that usually, only high (non-zero) values impact how the model classifies a given instance since only those give information about nodes neighborhood. This can be seen in the figure, especially for the first three features of class 0. From the fourth feature in the summary table for class 0 (node 13757), we can see, that sometimes even low feature values (merely their presence) can have a big impact on the SNoRe classification. The plot in the bottom right of <ref type="figure" target="#fig_0">Fig. 14 shows</ref> how much impact a feature has on average. We can see that node 4149 has the biggest impact on classification of nodes and that usually when its value is high the node is classified to class 1.</p><p>Similarly, we can show which nodes impacted the classification of a single instance to explain why the node was classified as it was. This is further elaborated in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Ablation study -latent clustering with UMAP</head><p>We also look at how nodes cluster together using UMAP algorithm <ref type="bibr" target="#b21">(McInnes et al. (2018)</ref>) to transform embedding vectors into 2D space. We saved the embedding of SNoRe (SDF) and used the default parameters for the unsupervised UMAP algorithm to generate node positions as shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. The class to which the node belongs to is shown as colour in the plot and added only for visualization. In general, we see that the nodes that belong to the same class are embedded near each other as best seen on the Coauthor PHY dataset. On the Pubmed dataset, we can observe that the classes coloured red and blue cluster well together and that the green one is scattered all over the plot, not clustering well. Nodes in the Cora embedding cluster well, but the classes are close together and sometimes overlap. The worst example we show is on dataset Citeseer where nodes do not cluster well and where classes overlap a lot, but some clusters can still be seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this section we summarize the main results and their implications, and discuss the limitations of the proposed SNoRe approach.</p><p>As empirically shown in Section 5, SNoRe and SNoRe (SDF) outperform state-of-theart methods on most datasets and perform comparably or slightly worse on others (e.g., Homo sapiens, Wikipedia). Coupled with the ability to use different distance metrics, speed and explainability of the embedding, this algorithm provides a very good alternative to the state-of-the-art algorithms. We further back this claim in Section 5.2, where we show that SNoRe outperforms the strong baseline node2vec according to the pairwise Bayesian performance comparisons. In both execution time and classification results, we show that the proposed algorithm is scalable since it achieves best results on both the smallest and the largest dataset while using the same amount of space or less than the baselines we compared it to. This is further shown in Appendix D, where the effects of parameters on execution time are shown. We also show the importance of efficient implementation of sparse algorithms and why such implementations are crucial for the future.</p><p>By observing the classification results of embeddings that have a different number of pivot nodes, maximum walk length and number of walks per node we have observed another interesting phenomenon. On datasets where all baselines achieved results that were similar to the random baseline, the parameters did not matter and for example an embedding with 4 features achieved similar results to the one with 4096. This gives us the ability to judge how susceptible a dataset is for classification and to judge if SNoRe is the suitable embedding algorithm for the task.</p><p>While observing classification results between different parameters can give us an idea how susceptible a dataset is for classification, observing the number of features returned by SNoRe (SDF) can give us some insight into the structure of the network. This is most notable on the Wikipedia dataset where SNoRe (SDF) gives us a dense embedding since all nodes have at least one node in common. On the other hand, when using the same amount of space as a dense embedding on the Coauthor PHY dataset, our algorithm generates an embedding with all nodes used as features. This shows that the Coauthor PHY network is a lot more decentralized than the Wikipedia one.</p><p>SNoRe uses nodes as features, making it possible to explain the reasoning behind why an instance was classified in a certain way. This can be done with the use of tools such as SHAP and allows us to use this embedding algorithm in situations where explainability is crucial such as medicine.</p><p>In Section 5.7 we show that our algorithm creates an embedding that embeds nodes belonging to the same class, close together. We do this by using the UMAP algorithm to transform each instance into 2D space and by coloring the node w.r.t. the class they belong to. In the corresponding figure, we can easily see how nodes with the same class cluster on datasets Pubmed and Coauthor PHY and although a little less prevalent also on the other ones. Some of the limitations of our algorithm can be seen on datasets like Wikipedia and BlogCatalog, where the neighborhood of the node is not necessarily important and distinctive enough. Since the algorithm is modular this can probably be avoided sometimes by changing the hashing function is such a way that it better encodes the relevant network structure.</p><p>Although PageRank works very well on most networks, giving us features that give us good results, we cannot guarantee good features that span trough all the network will be chosen. This can drastically decreases the performance on some part of the network since some nodes may not have neighborhoods that overlap with the neighborhoods of the features.</p><p>The last problem to highlight is the number of features (pivot nodes) in the final embedding. A small number of features is usually not descriptive enough and therefore the embedding performs badly. On the other hand, having a large number of features may give good results but need longer to train the classifier. Related to this, many classifiers are not optimized for sparse matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Further work</head><p>We introduced a scalable unsupervised algorithm for learning symbolic node representations of networks. The algorithm is fast, achieves results that are comparable or better than those of state-of-art algorithms and can be interpreted when coupled with methods like SHAP.</p><p>This work offers extensive exploration, as well as a proof that symbolic representations, if learned based on a considered graph's global topology, offer a competitive paradigm to currently adopted black-box representation learning. The proposed SNoRe is freely available and highly optimized, and as such ready to be tested in many scenarios where black-box approaches are currently adopted -for example in bioinformatics where the canonical task of protein function prediction is commonly addressed.</p><p>Further, the current version of SNoRe, offers symbolic representations of individual nodes which are in principle understandable and inspectable. However, we believe that, as domain knowledge in the form of e.g., ontologies is many times present, the obtained representations could be further generalized in order to obtain even more informative descriptions of why a particular node has a given property of study. Hence, coupling SNoRe with existing work on relational reasoning will be explored as part of the future work.</p><p>The key focus of this paper revolved around the task of node classification. Analogous to how representations of links can be learned from e.g., DeepWalk embeddings, similar idea could be explored in the context of symbolic representations, offering explainable link prediction. Here, each link would be explained based on the presence of a particular collection of nodes in its neighborhood, offering a novel research venue applicable especially in high-risk scenarios such as the biomedical domain (for example, this ideas could be used to explore whether there is really an interaction between e.g., a pair of micro RNA molecules, and why this is the case?).</p><p>In further work, we plan to further explore how to incorporate different high-level network structures and the effect of different hashing functions. We also want to explore how different feature selection algorithms affect the performance and if the difference is significant when supervised algorithms are used. Another venue worth exploring is the use of different walk length distributions, which is not explored in this paper. In fields such as medicine, explainability of machine learning might not be enough for practical use. Because of this we want to further explore causal implications, as defined in <ref type="bibr" target="#b10">(Holzinger et al. (2019)</ref>). We also want to research which metrics work best on different datasets (also known as metric learning). Lastly, SNoRe's behavior in the inductive and dynamic setting could be explored to further show the algorithms' usefulness. <ref type="figure" target="#fig_0">Figure 17</ref>: Micro F1 plots comparing digitized and non-digitized embedding. optimized for sparse matrices. Here we would like to highlight that the HPI distance metric we implemented even less time to execute than the implementation of sparse euclidean distance and cosine similarity from the highly optimized Python library scikit learn. <ref type="figure" target="#fig_1">Fig. 24</ref> shows how the parameter number of pivot nodes affects execution time. From the figure, we see that the difference between different values of the parameter is not significant and that the impact of the number of nodes is far greater. A small impact of the different  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNoRe</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Number of pivot nodes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Maximum walk length</head><p>The execution time is also affected by the maximum walk length parameter. The execution time between different values of the parameter can be seen in <ref type="figure" target="#fig_1">Fig. 25</ref>. The execution time rises linearly on all datasets. Linear rise is expected since we sample walks from the uniform distribution, making s = walk length + 1 2 . As seen in Section 3.8.1 s affects time linearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Number of walks per node</head><p>The last parameter we show is the number of random walks per node. The effects of different values of this parameter on execution time can be seen in <ref type="figure" target="#fig_1">Fig. 26</ref>. We see that execution time grows linearly with the number of walks per node. Linear growth is expected and further backs the claim made in Section 3.8.1. <ref type="figure" target="#fig_1">Figure 20</ref>: Macro F1 plots for different number of features (pivot nodes). <ref type="figure" target="#fig_1">Fig. 27</ref> shows how we can interpret the classification of an instance using SHAP. In the figure, we see two examples, one where class (label) 0 is assigned, and one where class (label) 1 is not assigned. The classification for class 0 starts at the expected value of 0.018. Then the value of features that represent nodes 13820, 11141, and some others lower this value for around 0.39. Values of features that represent nodes 13668, 9707, ..., 15178, and 18655 raise the value by 1.47 to the final value 1.107. Since the final score is high the class (label) 0 is assigned to the instance. We see that the value of feature that represents node 18655 has the biggest impact and that class (label) 0 is mostly assigned because of the high <ref type="figure" target="#fig_0">Figure 21</ref>: Macro F1 plots showing the effect of maximum walk length parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNoRe</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Classification explanation with SHAP</head><p>(or less likely low) similarity between the neighborhood of node 18655 and the observed node (instance).</p><p>Class (label) 1 is not assigned to this instance. We can see that the classification starts at the expected value of 0.684 for class 1 and is only lowered. All features lower the score of the classification from 0.501 to the final 0.183. Since this score is low, the class (label) 1 is not assigned to this node. We can see that the prediction is lowered the most by features that represent nodes 4149, 11449, and 11894.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>SNoRe key idea overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of Cora and Pubmed networks, colored based on node labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Micro F1 plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Micro F1 average rank diagram where best performing percentage is chosen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Macro F1 average rank diagram where best performing percentage is chosen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Micro F1 plots for different number of features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Micro F1 results for different distance metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Macro F1 results for different distance metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Visualization of execution time across the considered datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>SHAP summary on Pubmed dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>UMAP-based clustering of SNoRe (SDF) embedding on Cora, Citeseer, Pubmed and Coauthor PHY datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 :</head><label>18</label><figDesc>Micro F1 average rank diagram where average performance across all training percentages is chosen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 19 :</head><label>19</label><figDesc>Macro F1 average rank diagram where average performance across all training percentages is chosen. number of pivot nodes on execution time gives us further reason to use SNoRe (SDF) since execution time stays similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 22 :</head><label>22</label><figDesc>Macro F1 plots showing the effect of different number of random walks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 23 :</head><label>23</label><figDesc>Time plot for different metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 24 :</head><label>24</label><figDesc>Time plot of different number of features (pivot nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 25 :</head><label>25</label><figDesc>Time plot of different values of maximum walk length parameter. SNoRe Figure 26: Time plot for different number of random walks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 27 :</head><label>27</label><figDesc>Waterfall explanation for classification of a node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the networks used for testing.</figDesc><table><row><cell>Name</cell><cell>Nodes</cell><cell>Edges</cell><cell>Connected Components</cell><cell>Classes</cell></row><row><cell>Ions</cell><cell>1969</cell><cell>16092</cell><cell>326</cell><cell>12</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5278</cell><cell>78</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>3327</cell><cell>4676</cell><cell>438</cell><cell>6</cell></row><row><cell>Bitcoin Alpha</cell><cell>3783</cell><cell>14124</cell><cell>5</cell><cell>20</cell></row><row><cell>Homo sapiens (PPI)</cell><cell>3890</cell><cell>38739</cell><cell>35</cell><cell>50</cell></row><row><cell>Wikipedia</cell><cell>4777</cell><cell>92517</cell><cell>1</cell><cell>40</cell></row><row><cell>Bitcoin</cell><cell>5881</cell><cell>21492</cell><cell>4</cell><cell>20</cell></row><row><cell>BlogCatalog</cell><cell cols="2">10312 333983</cell><cell>1</cell><cell>39</cell></row><row><cell>Coauthor-CS</cell><cell cols="2">18333 100227</cell><cell>1</cell><cell>15</cell></row><row><cell>Pubmed</cell><cell>19717</cell><cell>64041</cell><cell>1</cell><cell>3</cell></row><row><cell>Coauthor-PHY</cell><cell cols="2">34493 282455</cell><cell>1</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Mean aggregated micro F1 scores.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Figure 4: Macro F1 plots.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>setting</cell><cell>Random</cell><cell>LP</cell><cell>VGAE</cell><cell>PPRS</cell><cell>LINE</cell><cell>Node2Vec</cell><cell>Deepwalk</cell><cell>NetMF (SCD)</cell><cell>SNoRe</cell><cell>SNoRe SDF</cell></row><row><cell>dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bitcoin</cell><cell cols="2">0.269 (?0.004) 0.287 (?0.002)</cell><cell cols="6">0.304 (?0.006) 0.277 (?0.006) 0.293 (?0.008) 0.315 (?0.012) 0.318 (?0.012) 0.312 (?0.009)</cell><cell>0.314 (?0.013)</cell><cell>0.293 (? 0.009)</cell></row><row><cell>Bitcoin Alpha</cell><cell cols="2">0.270 (?0.006) 0.277 (?0.004)</cell><cell cols="5">0.288 (?0.005) 0.282 (?0.007) 0.282 (?0.006) 0.296 (?0.009) 0.299 (?0.011)</cell><cell>0.299 (?0.009)</cell><cell cols="2">0.303 (?0.009) 0.283 (? 0.005)</cell></row><row><cell>BlogCatalog</cell><cell cols="2">0.037 (?0.004) 0.068 (?0.000)</cell><cell>OOM</cell><cell cols="4">0.027 (?0.001) 0.169 (?0.022) 0.206 (?0.017) 0.243 (?0.025)</cell><cell cols="2">0.271 (?0.022) 0.067 (?0.012)</cell><cell>0.065 (? 0.011)</cell></row><row><cell>Citeseer</cell><cell cols="2">0.157 (?0.006) 0.620 (?0.060)</cell><cell cols="5">0.344 (?0.011) 0.270 (?0.038) 0.255 (?0.018) 0.532 (?0.023) 0.532 (?0.025)</cell><cell>0.540 (?0.019)</cell><cell>0.621 (?0.066)</cell><cell>0.623 (? 0.067)</cell></row><row><cell>Coauthor CS</cell><cell cols="2">0.032 (?0.009) 0.120 (?0.000)</cell><cell cols="6">0.662 (?0.021) 0.026 (?0.002) 0.622 (?0.036) 0.849 (?0.011) 0.855 (?0.013) 0.853 (?0.008)</cell><cell>0.451 (?0.155)</cell><cell>0.803 (? 0.109)</cell></row><row><cell>Coauthor PHY</cell><cell cols="2">0.134 (?0.000) 0.309 (?0.000)</cell><cell>OOM</cell><cell cols="4">0.134 (?0.000) 0.675 (?0.013) 0.908 (?0.003) 0.912 (?0.004)</cell><cell cols="2">0.914 (?0.003) 0.381 (?0.141)</cell><cell>0.853 (? 0.118)</cell></row><row><cell>Cora</cell><cell cols="7">0.117 (?0.014) 0.825 (?0.038) 0.616 (?0.029) 0.351 (?0.073) 0.366 (?0.043) 0.799 (?0.021) 0.808 (?0.028)</cell><cell>0.812 (?0.024)</cell><cell>0.811 (?0.054)</cell><cell>0.815 (? 0.054)</cell></row><row><cell cols="3">Homo sapiens (PPI) 0.046 (?0.002) 0.066 (?0.000)</cell><cell cols="5">0.104 (?0.010) 0.065 (?0.015) 0.121 (?0.015) 0.173 (?0.019) 0.174 (?0.022)</cell><cell cols="2">0.189 (?0.022) 0.142 (?0.037)</cell><cell>0.156 (? 0.037)</cell></row><row><cell>Ions</cell><cell cols="7">0.076 (?0.005) 0.333 (?0.031) 0.163 (?0.013) 0.176 (?0.036) 0.288 (?0.031) 0.299 (?0.030) 0.321 (? 0.026)</cell><cell>0.309 (?0.029)</cell><cell>0.319 (?0.053)</cell><cell>0.312 (? 0.052)</cell></row><row><cell>Pubmed</cell><cell cols="2">0.295 (?0.004) 0.190 (?0.000)</cell><cell cols="5">0.641 (?0.018) 0.190 (?0.000) 0.567 (?0.011) 0.790 (?0.005) 0.792 (?0.006)</cell><cell>0.800 (?0.006)</cell><cell>0.742 (?0.059)</cell><cell>0.805 (? 0.032)</cell></row><row><cell>Wikipedia</cell><cell cols="2">0.041 (?0.003) 0.059 (?0.000)</cell><cell>OOM</cell><cell cols="4">0.080 (?0.011) 0.058 (?0.004) 0.099 (?0.010) 0.087 (?0.008)</cell><cell cols="2">0.103 (?0.008) 0.050 (?0.009)</cell><cell>0.034 (? 0.001)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Mean aggregated macro F1 scores.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. We use scikit-learn implementation<ref type="bibr" target="#b26">(Pedregosa et al. (2011)</ref>) for efficient cosine similarity calculation between sparse vectors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. Stored using 16-bit NumPy (Van Der Walt et al. (2011); Oliphant (2015)) type float16.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability</head><p>The proposed methodology is available as a Python library at: https://github.com/ smeznar/SNoRe. Appendix A. Comparison between digitized and non-digitized embedding <ref type="figure">Fig. 16</ref> and 17 show how results performance is affected if embeddings are digitized as described in Section 3.6. We can see that digitized embeddings usually perform similarly and even outperform non-digitized embeddings on both Co-authorship datasets, Homo sapiens (PPI) dataset, and Pubmed dataset. Appendix B. Average rank diagrams of mean classification results <ref type="figure">Fig. 18</ref> and 19 show the average rank diagrams when we average classification results of every training set size. Here we see that SNoRe (SDF) and SNoRe achieve similar ranks in both micro and macro F1. We can see that here SNoRe (SDF) achieves worse results than on <ref type="figure">Fig. 5</ref> and 6. The reason behind this can be seen on Coauthor CS and Coauthor PHY datasets in <ref type="figure">Fig. 3</ref>, where the algorithm performs poorly compared to others when a small amount of training data is used.</p><p>Appendix C. Parameter study macro plots <ref type="figure">Fig. 20, 21, and 22</ref> show the macro F1 plots for different parameter settings. We can see that the overall parameter number of features (pivot nodes) impacts classification the most. On the other hand, the number of walks parameter only affects the macro F1 score on datasets Coauthor CS and Coauthor PHY. Datasets Bitcoin, Bitcoin Alpha, BlogCatalog, and Wikipedia have the same score no matter which parameters we use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Additional execution time plots</head><p>In this section, we show how different distance metrics and parameters affect execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Distance metrics</head><p>We present the effect of different distance metrics in <ref type="figure">Fig. 23</ref>. We use sparse matrices for storing random walk hashes, which makes the implementation of similarity calculation crucial to obtain good performance. This can be seen in the figure, where Euclidean distance, HPI, and cosine similarity need significantly less time than other distance metrics that are not</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time for a Change: A Tutorial for Comparing Multiple Classifiers through Bayesian Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Dem?ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Zaffalon</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2653" to="2688" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Node classification in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smriti</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is PageRank All You Need for Scalable Graph Neural Networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD, MLG Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: Problems, techniques, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analyzing and modeling real-world phenomena with complex networks: a survey of applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fontoura</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvaldo</forename><forename type="middle">N</forename><surname>Oliveira</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Travieso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">Aparecido</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulino Ribeiro Villas</forename><surname>Boas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Antiqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palhares</forename><surname>Matheus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis Enrique Correa</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Physics</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="412" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Dem?ar</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Antisocial and Prosocial Correlates of Classroom Social Positions: The Social Network Centrality Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rodkin</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9507.1996.tb00079.x</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9507.1996.tb00079.x" />
	</analytic>
	<monogr>
		<title level="j">Social Development</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="188" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><forename type="middle">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Python in Science Conference</title>
		<editor>Ga?l Varoquaux, Travis Vaught, and Jarrod Millman</editor>
		<meeting>the 7th Python in Science Conference<address><addrLine>Pasadena, CA USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
	<note>ISBN 9781510860964</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Causability and explainability of artificial intelligence in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heimo</forename><surname>M?ller</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312" />
	</analytic>
	<monogr>
		<title level="j">WIREs Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1312</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HINMINE: heterogeneous information network mining with information retrieval heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kralj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Robnik-?ikonja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Lavra?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Numba: A LLVM-Based Python JIT Compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Siu Kwan Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Pitrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seibert</surname></persName>
		</author>
		<idno type="DOI">10.1145/2833157.2833162</idno>
		<ptr target="https://doi.org/10.1145/2833157.2833162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC, LLVM &apos;15</title>
		<meeting>the Second Workshop on the LLVM Compiler Infrastructure in HPC, LLVM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Propositionalization and embeddings: two sides of the same coin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Lavra?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Bla??krlj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robnik-?ikonja</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-020-05890-8</idno>
		<ptr target="https://doi.org/10.1007/s10994-020-05890-8" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="1465" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Link prediction in complex networks: A survey. Physica A: statistical mechanics and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Link-based Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003-01" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From local explanations to global understanding with explainable AI for trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Erion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">M</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bala</forename><surname>Prutkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronit</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisha</forename><surname>Himmelfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<ptr target="http://www.mattmahoney.net/text/text.html" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2522" to="5839" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Matt Mahoney. Large text compression benchmark</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UMAP: Uniform Manifold Approximation and Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Grossberger</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00861</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Guide to NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CreateSpace Independent Publishing Platform</publisher>
			<biblScope unit="volume">151730007</biblScope>
			<pubPlace>North Charleston, SC, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2623330.2623732</idno>
		<ptr target="http://doi.acm.org/10.1145/2623330.2623732" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review of feature selection techniques in bioinformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvan</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?aki</forename><surname>Inza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Larra?aga</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btm344</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btm344" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2507" to="2517" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pitfalls of Graph Neural Network Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Embedding-based Silhouette community detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Bla??krlj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Kralj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavra?</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-020-05882-8</idno>
		<ptr target="https://doi.org/10.1007/s10994-020-05882-8" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Erik?trumbelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kononenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="647" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PTE: Predictive Text Embedding through Large-Scale Heterogeneous Text Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783307</idno>
		<ptr target="https://doi.org/10.1145/2783258.2783307" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The NumPy array: a structure for efficient numerical computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gael</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unifying Graph Convolutional Neural Networks and Label Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghahramani</forename><surname>Zoubin</surname></persName>
		</author>
		<idno>CMU-CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/xu18c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>Machine Learning Research<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Social computing data repository at ASU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Learning on Graphs: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Predicting missing links via local information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1140/epjb/e2009-00335-8</idno>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B -Condensed Matter and Complex Systems</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Insights from Ion Binding Site Network Analysis into Evolution and Functions of Proteins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bla??krlj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Kunej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konc</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1002/minf.201700144</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/minf.201700144" />
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page">1700144</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Node Ranking: Structural Network Embedding and End-to-End Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Bla??krlj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Kralj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Konc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Robnik-?ikonja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavra?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Symbolic Graph Embedding Using Frequent Pattern Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Bla??krlj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Lavra?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kralj</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-33778-0_21</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-33778-0_21" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="261" to="275" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
