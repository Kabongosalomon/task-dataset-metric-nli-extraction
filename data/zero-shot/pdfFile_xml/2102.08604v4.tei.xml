<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SWAD: Domain Generalization by Seeking Flat Minima</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Chung-Ang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Cho</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">NAVER Clova</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">NAVER Clova</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Upstage AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kakao</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">SWAD: Domain Generalization by Seeking Flat Minima</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain generalization (DG) methods aim to achieve generalizability to an unseen target domain by using only training data from the source domains. Although a variety of DG methods have been proposed, a recent study shows that under a fair evaluation protocol, called DomainBed, the simple empirical risk minimization (ERM) approach works comparable to or even outperforms previous methods. Unfortunately, simply solving ERM on a complex, non-convex loss function can easily lead to sub-optimal generalizability by seeking sharp minima. In this paper, we theoretically show that finding flat minima results in a smaller domain generalization gap. We also propose a simple yet effective method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima. SWAD finds flatter minima and suffers less from overfitting than does the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large margins of +1.6% averagely on outof-domain accuracy. We also compare SWAD with conventional generalization methods, such as data augmentation and consistency regularization methods, to verify that the remarkable performance improvements are originated from by seeking flat minima, not from better in-domain generalizability. Last but not least, SWAD is readily adaptable to existing DG methods without modification; the combination of SWAD and an existing DG method further improves DG performances. Source code is available at https://github.com/khanrc/swad.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Independent and identically distributed (i.i.d.) condition is the underlying assumption of machine learning experiments. However, this assumption may not hold in real-world scenarios, i.e., the training and the test data distribution may differ significantly by distribution shifts. For example, a self-driving car should adapt to adverse weather or day-to-night shifts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Even in a simple image recognition scenario, systems rely on wrong cues for their prediction, e.g., geographic distribution <ref type="bibr" target="#b2">[3]</ref>, demographic statistics <ref type="bibr" target="#b3">[4]</ref>, texture <ref type="bibr" target="#b4">[5]</ref>, or backgrounds <ref type="bibr" target="#b5">[6]</ref>. Consequently, a practical system should require generalizability to distribution shift, which is yet often failed by traditional approaches.</p><p>Domain generalization (DG) aims to address domain shift simulated by training and evaluating on different domains. DG tasks assume that both task labels and domain labels are accessible. For example, PACS dataset <ref type="bibr" target="#b6">[7]</ref> has seven task labels (e.g., "dog", "horse") and four domain labels (e.g., "photo", "sketch"). Previous approaches explicitly reduced domain gaps in the latent space [8- <ref type="table" target="#tab_10">Table 1</ref>: Comparisons with SOTA. The proposed SWAD outperforms other state-of-the-art DG methods on five different DG benchmarks with significant gaps (+1.6pp in the average).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PACS</head><p>VLCS OfficeHome TerraInc DomainNet Avg.</p><p>ERM <ref type="bibr" target="#b28">[29]</ref> 85.5 77.5 66. <ref type="bibr" target="#b4">5</ref> 46.1 40.9 63.3 Best SOTA competitor 86.6 <ref type="bibr" target="#b29">[30]</ref> 78.8 <ref type="bibr" target="#b30">[31]</ref> 68.7 <ref type="bibr" target="#b30">[31]</ref> 48.6 <ref type="bibr" target="#b31">[32]</ref> 43.6 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>   <ref type="bibr" target="#b11">12]</ref>, obtained well-transferable model parameters by the meta-learning framework <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, data augmentation <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, or capturing causal relation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Despite numerous previous attempts for a decade, Gulrajani and Lopez-Paz <ref type="bibr" target="#b21">[22]</ref> showed that a simple empirical risk minimization (ERM) approach works comparably or even outperforms the previous attempts on diverse DG benchmarks under a fair evaluation protocol, called "DomainBed".</p><p>Unfortunately, although ERM showed surprising empirical success on DomainBed, simply minimizing the empirical loss on a complex and non-convex loss landscape is typically not sufficient to arrive at a good generalization <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. In particular, the connection between the generalization gap and the flatness of loss landscapes has been actively discussed under the i.i.d. condition <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. Izmailov et al. <ref type="bibr" target="#b24">[25]</ref> argued that seeking flat minima will lead to robustness against the loss landscape shift between training and test datasets, while a simple ERM converges to the boundary of a wide flat minimum and achieves insufficient generalization. In the DG scenario, because training and test loss landscapes differ more drastically due to the domain shift, we conjecture that the generalization gap between flat and sharp minima is larger than expected in the i.i.d. scenario.</p><p>To show that flatter minima generalize better to unseen domains, we formulate a robust risk minimization (RRM) problem defined by the worst-case empirical risks within neighborhoods in parameter space <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. We theoretically show that the generalization gap of DG, i.e., the error on the target domain, is upper bounded by RRM, i.e., a flat optimal solution. Based on our theoretical observation, we modify stochastic weight averaging (SWA) <ref type="bibr" target="#b24">[25]</ref>, one of the popular existing flatness-aware solvers, by introducing a dense and overfit-aware stochastic weight sampling strategy. First, we suggest to sample weights densely, i.e., for every iteration. Also, we search the start and end iterations for averaging by considering the validation loss to avoid overfitting. We empirically show that the proposed Stochastic Weight Averaging Densely (SWAD) finds flatter minima than the vanilla SWA does, resulting in better generalization to unseen domains.</p><p>Contribution. Our main contribution is introducing flatness into DG, and showing remarkably outperforming performances against existing DG methods. As shown in <ref type="table" target="#tab_10">Table 1</ref>, our SWAD improves the average DG performances by 3.6pp against the ERM baseline and 1.6pp against the existing best methods. Furthermore, by combining SWAD and previous SOTA <ref type="bibr" target="#b30">[31]</ref>, we even achieve 0.4pp improvements against the vanilla SWAD results. We also empirically show that while popular indomain generalization methods without considering flatness, e.g., Mixup <ref type="bibr" target="#b34">[35]</ref> or CutMix <ref type="bibr" target="#b35">[36]</ref>, are not effective to out-of-domain generalization <ref type="table" target="#tab_5">(Table 3)</ref>, flatness-aware methods, e.g., SWA <ref type="bibr" target="#b24">[25]</ref> or SAM <ref type="bibr" target="#b25">[26]</ref>, are only effective methods to both in-domain and out-of-domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Theoretical Relationship between Flatness and Domain Generalization</head><p>Let D := {D i } I i be a set of training domains, where D i is a distribution over input space X , and I is the total number of domains. From each domain, we observe n training data points which consist of input x and target label y, (x i j , y i j ) n j=1 ? D i . We also define a set of target domain T := {T i } T i similarly, where the number of target domains T is usually set to one. For the sake of simplicity, unlike Ben-David et al. <ref type="bibr" target="#b36">[37]</ref>, we assume that there exists a global labeling function h(x) that generates target label for multiple domains, i.e., y i j = h(x i j ) for all i and j. Domain generalization (DG) aims to find a model parameter ? ? ? which generalizes well over both multiple training domains D and unseen target domain T . More specifically, let us consider a bounded instance loss function : Y ? Y ? [0, c], such that (y 1 , y 2 ) = 0 holds if and only if y 1 = y 2 where Y is a set of labels. For simplicity, we set c to one in our proofs, but we note that (?, ?) can be generalized for any bounded loss function. Then, we can define a population loss over multiple domains by In practice, ERM, i.e., arg min ??D (?), can have multiple solutions that provide similar values of the training losses but significantly different generalizability on E D (?) and E T (?). Unfortunately, the typical optimization methods, such as SGD and Adam <ref type="bibr" target="#b37">[38]</ref>, often lead sub-optimal generalizability as finding sharp and narrow minima even under the i.i.d. assumption <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. In the DG scenario, the generalization gap between empirical loss and target domain loss becomes even worse due to domain shift. Here, we provide a theoretical interpretation of the relationship between finding a flat minimum and minimizing the domain generalization gap, inspired by previous studies <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>.  We consider a robust empirical loss function defined by the worst-case loss within neighborhoods in the parameter space as? ? D (?) := max ? ???D (? + ?), where ? denotes the L2 norm and ? is a radius which defines neighborhoods of ?. Intuitively, if ? is sufficiently larger than the "radius" of a sharp optimum ? s of? D (?), ? s is no longer an optimum of? ? D (?) as well as its neighborhoods within the ?-ball. On the other hand, if an optimum ? f has larger "radius" than ?, there exists a local optimum within ?-ball -See <ref type="figure" target="#fig_2">Figure 1</ref>. Hence, solving the robust risk minimization (RRM), i.e., arg min ?? ? D (?), will find a near solution of a flat optimum showing better generalizability <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. However, as domain shift worsen the generalization gap by breaking the i.i.d. assumption, it is not trivial that RRM will find an optimum with better DG performance. To answer the question, we first show the generalization bound between? ? D and E T as follows:</p><formula xml:id="formula_0">E D (?) = 1 I I i=1 E x i ?Di [ (f (x i ; ?), y i ))], where f (?; ?)</formula><formula xml:id="formula_1">Theorem 1. Consider a set of N covers {? k } N k=1 such that the parameter space ? ? ? N k ? k where diam(?) := sup ?,? ?? ? ? ? 2 , N := (diam(?)/?) d and d is dimension of ?.</formula><p>Let v k be a VC dimension of each ? k . Then, for any ? ? ?, the following bound holds with probability at least 1 ? ?,</p><formula xml:id="formula_2">E T (?) &lt;? ? D (?) + 1 2I I i=1 Div(D i , T ) + max k?[1,N ] v k ln (m/v k ) + ln(N/?) m ,<label>(1)</label></formula><p>where m = nI is the number of the training samples and Div(D i , T ) := 2 sup A |P Di (A) ? P T (A)| is a divergence between two distributions.</p><p>Proof can be done similarly as <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b33">[34]</ref>. In Theorem 1, the test loss E T (?) is bounded by three terms: (1) the robust empirical loss? ? D (?), (2) the discrepancy between training distribution and test distribution, i.e., the quantity of domain shift, and (3) a confidence bound related to the radius ? and the number of the training samples m. Our theorem is similar to Ben-David et al. <ref type="bibr" target="#b36">[37]</ref>, while our theorem does not have the term related to the difference in labeling functions across the domains. It is because we simply assume there is no difference between labeling functions for each domain for simplicity. If one assumes a different labeling function, the dissimilarity term can be derived easily because it is independent and compatible with our main proof. More details of Theorem 1, including proof and discussions on the confidence bound, are in Appendix C.1 and C.2.</p><p>From Theorem 1, one can conjure that minimizing the robust empirical loss is directly related to the generalization performances on the target distribution. We show that the domain generalization gap on the target domain T by the optimal solution of RRM,? ? , is upper bounded as follows:</p><p>Theorem 2. Let? ? denote the optimal solution of the RRM, i.e.,? ? := arg min ?? ? D (?), and let v be a VC dimension of the parameter space ?. Then, the gap between the optimal test loss, min ? E T (? ), and the test loss of? ? , E T (? ? ), has the following bound with probability at least 1 ? ?. Our SWAD collects stochastic weights densely, i.e., for every iteration, to obtain sufficiently many weights. SWAD collects the weights from the start iteration ts to the end iteration te, where ts and te are obtained by monitoring the validation loss (overfit-aware scheduling).</p><formula xml:id="formula_3">E T (? ? ) ? min ? E T (? ) ?? ? D (? ? ) ? min ? ? D (? ) + 1 I I i=1 Div(D i , T ) + max k?[1,N ] v k ln (m/v k ) + ln (2N/?) m + v ln (m/v) + ln (2/?) m<label>(2)</label></formula><p>Proof is in Appendix C. <ref type="bibr" target="#b2">3</ref>. It implies that if we find the optimal solution of the RRM (i.e.,? ? ), then the generalization gap in the test domain (i.e., E T (? ? ) ? min ? E T (? )) is upper bounded by the gap between the RRM and ERM (i.e.,? ? D (? ? ) ? min ? ? D (? )). Other terms in Theorem 2 are the discrepancy between the train domains D and the target domain T , and the confidence bounds caused by sample means. We remark that if we choose a proper ?, the optimal solution of the RRM will find a point near a flat optimum of ERM as shown in <ref type="figure" target="#fig_2">Figure 1</ref>. Hence, Theorem 2 and the intuition from <ref type="figure" target="#fig_2">Figure 1</ref> imply that seeking a flat minimum of ERM will lead to a better domain generalization gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SWAD: Domain Generalization by Seeking Flat Minima</head><p>We have shown that flat minima will bring a better domain generalization. In this section, we propose Stochastic Weight Averaging Densely (SWAD) algorithm, and provide empirical quantitative and qualitative analyses on SWAD and flatness to understand why SWAD works better than ERM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A baseline method: stochastic weight averaging</head><p>Since the importance of flatness in loss landscapes has emerged <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>, several methods have been proposed to find flat minima <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>. We select stochastic weight averaging (SWA) <ref type="bibr" target="#b24">[25]</ref> as a baseline, which finds flat minima by a weight ensemble approach. More specifically, SWA updates a pretrained model (namely, a model trained with sufficiently enough training epochs, K 0 ) with a cyclical <ref type="bibr" target="#b39">[40]</ref> or high constant learning rate scheduling. SWA gathers model parameters for every K epochs during the update and averages them for the model ensemble. SWA finds an ensembled solution of different local optima found by a sufficiently large learning rate to escape a local minimum. Izmailov et al. <ref type="bibr" target="#b24">[25]</ref> empirically showed that SWA finds flatter minima than ERM. We also considered sharpness-aware minimization (SAM) <ref type="bibr" target="#b25">[26]</ref>, which is another popular flatness-aware solver, but SWA finds flatter minima than SAM (See <ref type="figure" target="#fig_5">Figure 3</ref>). We illustrate an overview of SWA in <ref type="figure" target="#fig_3">Figure 2a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dense and overfit-aware stochastic weight sampling strategy</head><p>Despite its advantages, directly applying SWA to DG task has two problems. First, SWA averages a few weights (usually less than ten) by sampling weights for every K epochs, results in an inaccurate approximation of flat minima on a high-dimensional parameter space (e.g., 23M for ResNet-50 <ref type="bibr" target="#b40">[41]</ref>). Furthermore, a common DG benchmark protocol uses relatively small training epochs (e.g., Gulrajani and Lopez-Paz <ref type="bibr" target="#b21">[22]</ref> trained with less than two epochs for DomainNet benchmark), resulting in insufficient stochastic weights for SWA. From this motivation, we propose a "dense" sampling strategy for gathering sufficiently enough stochastic weights.</p><p>In addition, widely used DG datasets, such as PACS (? 10K images, 7 classes) and VLCS (? 11K images, 5 classes), are relatively smaller than large-scale datasets, such as ImageNet <ref type="bibr" target="#b41">[42]</ref> (? 1.2M images, 1K classes). In this case, we observe that a simple ERM approach is rapidly reached to a local optimum only within a few epochs, and easily suffers from the overfitting issue, i.e., the validation loss is increased after a few training epochs. It implies that directly applying the vanilla SWA will suffer from the overfitting issue by averaging sub-optimal solutions (i.e., overfitted parameters). Hence, we need an "overfit-aware" sampling scheduling to omit the sub-optimal solutions for SWA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Art painting (test)</head><p>Cartoon The main idea of Stochastic Weight Averaging Densely (SWAD) is a dense and overfit-aware stochastic weight gathering strategy. First, instead of collecting weights for every K epochs, SWAD collects weights for every iteration. This dense sampling strategy easily collects sufficiently many weights than the sparse one. We also employ overfit-aware sampling scheduling by considering traces of the validation loss. Instead of sampling weights from K 0 pretraining epochs to the final epoch, we search the start iteration (when the validation loss achieves a local optimum for the first time) and the end iteration (when the validation loss is no longer decreased, but keep increasing). More specifically, we introduce three parameters: an optimum patient parameter N s , an overfitting patient parameter N e , and the tolerance rate r for searching the start iteration t s and the end iteration t e . First, we search t s which satisfies min i?[0,...,</p><formula xml:id="formula_4">(test) Photo (test) Sketch (test) (c) Flatness for each target domain</formula><formula xml:id="formula_5">Ns?1] E (ts+i) val = E (ts) val , where E (i)</formula><p>val denotes the validation loss at iteration i. Simply, t s is the first iteration where the loss value is no longer decreased during N s iterations. Then, we find t e satisfying min i?[0,1,...,</p><formula xml:id="formula_6">Ne?1] E (te+i) val &gt; rE (ts)</formula><p>val . In other words, t e is the first iteration where the validation loss values exceed the tolerance r during N e iterations.</p><p>We illustrate the overview of SWAD and the comparison of SWAD to SWA in <ref type="figure" target="#fig_3">Figure 2</ref>. Detailed pseudo code is provided in Appendix B.4. We compare SWAD with other possible SWA strategies in ?4.3 and show that our design choice works better for DG tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Empirical analysis of SWAD and flatness</head><p>Here, we analyze solutions found by SWAD in terms of flatness. We first verify that the SWAD solution is flatter than those of ERM, SWA, and SAM. Our loss surface visualization shows that the SWAD solution is located on the center of the flat region, while ERM finds a boundary solution. Finally, we show that the sharp boundary solutions by ERM are not generalized well, resulting in sensitivity to the model selection. All following empirical analyses are conducted on PACS dataset, validating by all four domains (art painting, cartoon, photo, and sketch).</p><p>Local flatness anaylsis. To begin with, we quantify the local flatness of a model parameter ? by assuming that flat minima will have smaller changes of loss value within its neighborhoods than sharp minima. For the given model parameter ?, we compute the expected loss value changes between ? and parameters on the sphere surrounding ? with radius ?, i.e.,    practice, F ? (?) is approximated by Monte-Carlo sampling with 100 samples. Note that the proposed local flatness F ? (?) is computationally efficient than measuring curvature using the Hessian-based quantities. Also, F ? (?) has an unbiased finite sample estimator, while the worst-case loss value, i.e., max ? = ? +? [E(? ) ? E(?)] has no unbiased finite sample estimator.</p><formula xml:id="formula_7">F ? (?) = E ? = ? +? [E(? ) ? E(?)].</formula><p>In <ref type="figure" target="#fig_5">Figure 3</ref>, we compare F ? (?) of ERM, SAM, SWA with cyclic learning rate, SWA with constant learning rate, and SWAD by varying radius ?. SAM and SWA find the solutions with lower local flatness than ERM on average. SWAD finds the most flat minimum in every experiment.</p><p>Loss surface visualization. We visualize the loss landscapes by choosing three model weights on the optimization trajectory (? 1 , ? 2 , ? 3 ) 2 , and computing the loss values by linear combinations of ? 1 , ? 2 , ? 3 MASF <ref type="bibr" target="#b13">[14]</ref> 82.7 -----DMG <ref type="bibr" target="#b32">[33]</ref> 83.4 ---43.6 -MetaReg <ref type="bibr" target="#b14">[15]</ref> 83.6 ---43.6 -ER <ref type="bibr" target="#b11">[12]</ref> 85.3 -----pAdaIN <ref type="bibr" target="#b46">[47]</ref> 85.4 -----EISNet <ref type="bibr" target="#b47">[48]</ref> 85  <ref type="bibr" target="#b41">[42]</ref> trained ResNet-50 <ref type="bibr" target="#b40">[41]</ref> is employed as the initial weight, and optimized by Adam <ref type="bibr" target="#b37">[38]</ref> optimizer with a learning rate of 5e-5.</p><formula xml:id="formula_8">.8 - - - - - DSON [30] 86.6 - - - - - ERM ? [</formula><p>We construct a mini-batch containing all domains where each domain has 32 images. We set SWAD HPs N s to 3, N e to 6, and r to 1.2 for VLCS and 1.3 for the others by HP search on the validation sets. Additional implementation details, such as other HPs, are given in Appendix B.</p><p>Evaluation metrics. We report out-of-domain accuracies for each domain and their average, i.e., a model is trained and validated on training domains and evaluated on the unseen target domain. Each out-of-domain performance is an average of three different runs with different train-validation splits. Comparison with domain generalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>We report the full out-of-domain performances on five DG benchmarks in <ref type="table" target="#tab_3">Table 2</ref>. The full tables including outof-domain accuracies for each domain are in Appendix E. In all experiments, our SWAD achieves significant performance gain against ERM as well as the previous best results: +2.6pp in PACS, +0.3pp in VLCS, +1.4pp in TerraIncognita, +1.9pp in OfficeHome, and +2.9pp in DomainNet comparing to the previous best results. We observe that SWAD provides two practical advantages comparing to previous methods. First, SWAD does not need any modification on training objectives or model architecture, i.e., it is universally applicable to any other methods. As an example, we show that SWAD actually improves the performances of other DG methods, such as CORAL <ref type="bibr" target="#b30">[31]</ref> in <ref type="table" target="#tab_6">Table 4</ref>. Moreover, as we discussed before, SWAD is free to the model selection, resulting in stable performances (i.e., small standard errors) on various benchmarks. Note that we only compare results with ResNet-50 backbone for a fair comparison. We describe the implementation details of each comparison method and the hyperparameter search protocol in Appendix B.</p><p>Comparison with conventional generalization methods. We also compare SWAD with other conventional generalization methods to show that the remarkable domain generalization gaps by SWAD is not achieved by better generalization, but by seeking flat minima. The comparison methods include flatness-aware optimization methods, such as SAM <ref type="bibr" target="#b25">[26]</ref>, ensemble methods, such as EMA <ref type="bibr" target="#b55">[56]</ref>, data augmentation methods, such as Mixup <ref type="bibr" target="#b34">[35]</ref> and CutMix <ref type="bibr" target="#b35">[36]</ref>, and consistency regularization methods, such as VAT <ref type="bibr" target="#b56">[57]</ref> and ?-model <ref type="bibr" target="#b57">[58]</ref>. We also split in-domain datasets into training (60%), validation (20%), and test (20%) splits, while no in-domain test set used for <ref type="table" target="#tab_3">Table 2</ref>. Every experiment is repeated three times.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. We observe that all conventional methods helps in-domain generalization, i.e., performing better than ERM on in-domain test set. However, their out-of-domain performances are similar to or even worse than ERM. For example, CutMix and ?-model improve in-domain performances by 1.0pp and 0.2pp but degrade out-of-domain performances by 1.5pp and 1.8pp. SAM, another method for seeking flat minima, slightly increases both in-domain and out-of-domain performances but the out-of-domain performance is not statistically significant. We will discuss performances of SAM in other benchmarks later. In contrast, the vanilla SWA and our SWAD significantly improve both in-domain and out-of-domain performances. SWAD improves the performances by SWA with statistically significantly gaps: 1.2pp on the out-of-domain and 0.6pp on the in-domain. Further comparison between SWA and SWAD is provided in ?4.3. Combinations with other methods. Since SWAD does not require any modification on training procedures and model architectures, SWAD is universally applicable to any other methods. Here, we combine SWAD with ERM, CORAL <ref type="bibr" target="#b30">[31]</ref>, and SAM <ref type="bibr" target="#b25">[26]</ref>. Results are shown in <ref type="table" target="#tab_6">Table 4</ref>. Both CORAL and SAM solely show better performances than ERM with +1.2pp average out-of-domain accuracy gap. Note that SAM is not a DG method but a sharpness-aware optimization method to find flat minima. It supports our theoretical motivation: DG can be achieved by seeking flat minima.</p><p>By applying SWAD on the baselines, the performances are consistently improved by 3.6pp on ERM, 2.8pp on CORAL, and 1.0pp on SAM. Interestingly, CORAL + SWAD show the best performances with both incorporating different advantages of utilizing domain labels and seeking flat minima. We also observe that SAM + SWAD shows worse performance than ERM + SWAD, while SAM performs better than ERM. We conjecture that it is because the objective control by SAM restricts the model parameter diversity durinig training, reducing the diversity for SWA ensemble. However, applying SWAD on SAM still leads to better performances than the sole SAM. The results demonstrate that the application of SWAD on other baselines is a simple yet effective method for DG. <ref type="table">Table 5</ref>: Ablation studies of the stochastic weights selection strategies on PACS and VLCS. In the configuration, "ts", "te", "lr", and "interval" indicate start and end iterations of sampling, a learning rate schedule, and a stochastic weight sampling interval, respectively. "Opt" and "Overfit" indicate the start and end iterations identified by our overfit-aware sampling strategy, and "Val" means the start and end iterations whose averaging shows the best accuracy on the validation set. "Cyclic" and "Const" represent cyclic and constant learning rate schedules. All experiments are repeated three times.  <ref type="table">Table 5</ref> provides ablative studies on the starting and ending iterations for averaging, the learning rate schedule, and the sampling interval. SWA w/ cyclic (SWA in <ref type="table" target="#tab_5">Table 3</ref>) and SWA w/ constant are vanilla SWAs with fixed sampling positions. We also report SWAD by eliminating three factors: the dense sampling strategy, and searching the start iteration, searching the end iteration. The dense sampling strategy lets SWAD estimate a more accurate approximation of flat minima: showing 0.8pp degeneration in the average out-of-domain accuracy (SWAD w/o Dense ). When we take an average from t s to the final iteration, the out-of-domain performance degrades by 0.6pp (SWAD w/o Overfit ). Similarly, a fixed scheduling without the overfit-aware scheduling only shows very marginal improvements from the vanilla SWA (SWAD w/o Opt-Overfit ). We also evaluate SWAD fit-on-val that uses the range achieving the best performances on the validation set, but it becomes overfitted to the validation, results in lower performances than SWAD. The results demonstrate the benefits of combining "dense" and "overfit-aware" sampling strategies of SWAD. Since SWAD does not rely on domain labels, it can be applied to other robustness tasks not containing domain labels. <ref type="table" target="#tab_8">Table 6</ref> show the generalizability of SWAD on ImageNet <ref type="bibr" target="#b41">[42]</ref> and its shifted benchmarks, namely, ImageNet-C <ref type="bibr" target="#b58">[59]</ref>, ImageNet-R <ref type="bibr" target="#b59">[60]</ref>, and background challenge (BGC) <ref type="bibr" target="#b60">[61]</ref>. SWAD consistently improves robustness performances against the ERM baseline and the SWA baseline. These results support that our method is robustly and widely applicable to improve both in-domain and out-of-domain generalizability. The detailed setup is provided in Appendix B.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Exploring the other applications: ImageNet robustness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Limitations</head><p>Despite many benefits from SWAD, such as the significant performance improvements, model selection-free property, working plug-and-play manner for various methods, there are some potential limitations. Here, we discuss the limitations of SWAD for further improvements.</p><p>Confidence error in Theorem 1. While the confidence error in Theorem 1 tells the effect of ? on generalization error bound, there exists a limitation in that the confidence error term shows improper behavior with respect to ? if ? is close to zero. The behavior we expect is that the confidence error of RRM converges to the confidence error of ERM as ? decreases to zero, however, the current theorem does not show such tendency since the confidence bound diverges to infinity when ? goes to zero. However, we would like to note that this limitation is not a drawback of RRM, but it is caused by the looseness of the union bound which is a mathematical technique used to derive the confidence error of RRM. Our RRM formulation has a similarity to previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> and we note that the counter-intuitive behavior of the confidence bound and ? also appears in Foret et al. <ref type="bibr" target="#b25">[26]</ref>.</p><p>SWAD is not a perfect flatness-aware optimization method. Note that SWAD is not a perfect and theoretically guaranteed solver for flat minima, but a heuristic approximation with empirical benefits. However, even if a better flatness-aware optimization method is proposed, our theoretical contribution still holds: showing the relationship between flat minima and DG.</p><p>SWAD does not strongly utilize domain-specific information. In Theorem 2, the domain generalization gap is bounded by three factors: flat minima, domain discrepancy, and confidence bound. Most of the existing approaches focus on domain discrepancy, reducing the difference between the source domains and the target domain by domain invariant learning <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. SWAD focuses on the first factor, the flat minima. While the domain labels are used to construct a mini-batch, SWAD does not strongly utilize domain-specific information. It implies that if one can consider both flatness and domain discrepancy, better domain generalization can be achievable. <ref type="table" target="#tab_6">Table 4</ref> gives us a clue: the combination of CORAL (utilizing domain-specific information) and SWAD (seeking flat minima) shows the best performance among all comparison methods. As a future research direction, we encourage studying a method that can achieve both flat optima and small domain discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>In this paper, we theoretically and empirically demonstrate that domain generalization (DG) is achievable by seeking flat minima. We propose SWAD that captures flatter minima than the vanilla SWA does. The extensive experiments on five DG benchmarks show superior performances of SWAD compared with existing DG methods. In addition, combinations of SWAD and existing DG methods even show better performances than the vanilla SWAD. We theoretically and empirically observe that seeking flat minima can achieve better generalizability to both in-domain and out-of-domain, while strong in-domain generalization methods without consideration of flatness, e.g., Mixup or CutMix, cannot guarantee to achieve out-of-domain generalizability in both theory and practice. This study first brings the concept of flatness into DG tasks, and shows strong empirical performances not only in DG but also in ImageNet benchmarks. We hope that this study promotes a new research direction of seeking flat minima for domain generalization and other robustness tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Potential Societal Impacts</head><p>In this study, we theoretically and empirically demonstrate that domain generalization (DG) is achievable by seeking flat minima, and propose SWAD to find flat minima. With SWAD, researchers and developers can make a model robust to domain shift in a real deployment environment, without relying on a task-dependent prior, a modified objective function, or a specific model architecture. Accordingly, SWAD has potential positive impacts by developing machines less biased towards ethical aspects, as well as potential negative impacts, e.g., improving weapon or surveillance systems under unexpected environment changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hyperparameters of SWAD</head><p>The evaluation protocol by Gulrajani and Lopez-Paz <ref type="bibr" target="#b21">[22]</ref> is computationally too expensive; it requires about 4,142 models for every DG algorithm. Hence, we reduce the search space of SWAD for computational efficiency; batch size and learning rate are set to 32 for each domain and 5e-5, respectively. We set dropout probability and weight decay to zero. We only search N s , N e and r. N s and N e are searched in PACS dataset, and the searched values are used for all experiments, while r is searched in [1.2, 1.3] depending on dataset. As a result, we use N s = 3, N e = 6, and r = 1.2 for VLCS and r = 1.3 for the others. We initialize our model by ImageNet-pretrained ResNet-50 and batch normalization statistics are frozen during training. The number of total iterations is 15, 000 for DomainNet and 5, 000 for others, which are sufficient numbers to be converged. Finally, we slightly modify the evaluation frequency because it should be set to small enough to detect the moments that the model is optimized and overfitted. However, too small frequency brings large evaluation overhead, thus we compromise between exactness and efficiency: 50 for VLCS, 500 for DomainNet, and 100 for others. We evaluate recently proposed methods, SAM <ref type="bibr" target="#b25">[26]</ref> and Mixstyle <ref type="bibr" target="#b16">[17]</ref>, and compare them with previous results. For a fair comparison, we follow the hyperparameter (HP) search protocol proposed by Gulrajani and Lopez-Paz <ref type="bibr" target="#b21">[22]</ref>, with a modification to reduce computational resources. They searched HP by training a total of 58,000 models, corresponding to about 4,142 runs for each algorithm. It is too much computational burden to train 4,142 models whenever evaluate a new algorithm. Therefore, we re-design the HP search protocol efficiently and effectively. In the HP search protocol of DomainBed <ref type="bibr" target="#b21">[22]</ref>, training domains and algorithm-specific parameters are included in the HP search space, and HP is found for every data split independently by random search. Instead, we do not sample training domains, use HP found in the first data split to the other splits, search algorithm-specific HP independently, and conduct grid search on the more effectively designed HP space as shown in <ref type="table" target="#tab_9">Table 7</ref>. Through the proposed protocol, we find HP for an algorithm under only 396 runs. Although the number of total runs is reduced to about 10% (4, 142 ? 396), the results of reproduced ERM is improved 0.9pp in average (63.3% ? 64.2%). It demonstrates both the effectiveness and the efficiency of our search protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Hyperparameter search protocol for reproduced results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Algorithm-specific hyperparameters</head><p>We search the algorithm-specific hyperparameters independently in PACS dataset, based on the values suggested from each paper. For Mixstyle <ref type="bibr" target="#b16">[17]</ref>, we insert Mixstyle block with domain label after the 1st, 2nd, and 3rd residual blocks with ? = 0.1 and p = 0.5. We train SAM <ref type="bibr" target="#b25">[26]</ref> with ? = 0.05, and VAT [57] with = 1.0 and ? = 1.0. In ?-model <ref type="bibr" target="#b57">[58]</ref>, w max = 1 is chosen among various w max values such as 1, 10, 100, and 300. We use EMA <ref type="bibr" target="#b55">[56]</ref> with decay = 0.99, Mixup <ref type="bibr" target="#b34">[35]</ref> with ? = 0.2, and CutMix <ref type="bibr" target="#b35">[36]</ref> with ? = 1.0 and p = 0.5. Output: averaged weight ? SWAD from t s to t e 1 t s ? 0 // start iteration for averaging 2 t e ? T // end iteration for averaging </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Pseudo code</head><formula xml:id="formula_9">3 l ? None // loss threshold 4 for i ? 1 to T do 5 ? i ? ? i?1 ? ??E (i?1) train 6 if l = None then 7 if E (i?Ns+1) val = min 0?i &lt;Ns E (i?i ) val then 8 t s ? i ? N s + 1 9 l ? r Ns Ns?1 i =0 E (i?i ) val 10 else if l &lt; min 0?i &lt;Ne E (i?i ) val</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Loss surface visualization</head><p>Following Garipov et al. <ref type="bibr" target="#b23">[24]</ref>, we choose three model weights ? 1 , ? 2 , ? 3 and define two dimensional weight plane from the weights:</p><formula xml:id="formula_10">u = ? 2 ? ? 1 , v = (? 3 ? ? 1 ) ? ? 3 ? ? 1 , ? 2 ? ? 1 ? 2 ? ? 1 2 ? (? 2 ? ? 1 ) ,<label>(3)</label></formula><p>where? = u/ u andv = v/ v are orthonormal bases of the weight plane. Then, we build Cartesian grid near the weights on the plane. For each grid point, we calculate the weight corresponding to the point and compute loss from the weight. The results are visualized as a contour plot, as shown in <ref type="figure" target="#fig_6">Figure 4</ref> in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 ImageNet robustness experiments</head><p>We investigate the extensibility of SWAD via three robustness benchmarks (Section 4.4 in the main text), namely ImageNet-C <ref type="bibr" target="#b58">[59]</ref>, ImageNet-R <ref type="bibr" target="#b59">[60]</ref>, and background challenge (BGC) <ref type="bibr" target="#b60">[61]</ref>. ImageNet-C measures the robustness against common corruptions such as Gaussian noise, blur, or weather changes. We follow Hendrycks and Dietterich <ref type="bibr" target="#b58">[59]</ref> for measuring mean corruption error (mCE). The lower ImageNet-C implies that the model is robust against corruption noises. BGC evaluates the robustness against background manipulations as well as the adversarial robustness. The BGC dataset has two groups, foreground and background. BGC manipulates images by combining the foregrounds and backgrounds, and measures whether the model predicts a consistent prediction with any manipulated image. ImageNet-R tests the robustness against different domains. ImageNet-R collects very different domain images of ImageNet, such as art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions. Showing better performances in ImageNet-R leads to the same conclusion as other domain generalization benchmarks.</p><p>Experiment details. We use ResNet-50 architecture and mostly follow standard training recipes. We use SGD optimizer with momentum of 0.9, base learning rate of 0.1 with linear scaling rule <ref type="bibr" target="#b62">[63]</ref> and polynomial decay, 5 epochs gradual warmup, batch size of 2048, and total epochs of 90. For SWA, the learning rate is decayed to 1/20 until 80% of training (72 epochs), and the cyclic learning rate with 3 epochs cycle length is used for the left 20% of training. SWAD follows the same learning rate decay until 80% of training, but averages every weight from every iteration after 80% of training with constant learning rate. Note that if we set h as a true label function which generates the label of inputs, y = h(x), then, it becomes a population loss E P (?) = E P (f (?; ?), h). Given two distributions, P and Q, the following lemma shows that the difference between the error with P and the error with Q is bounded by the divergence between P and Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorems</head><formula xml:id="formula_11">Lemma 1. |E P (h 1 , h 2 ) ? E Q (h 1 , h 2 )| ? 1 2 Div(P, Q)</formula><p>Proof. We employ the same technique in Zhao et al. <ref type="bibr" target="#b63">[64]</ref> for our loss function . From the Fubini's theorem, we have,</p><formula xml:id="formula_12">E x?P [ (h 1 (x), h 2 (x))] = ? 0 P P ( (h 1 (x), h 2 (x)) &gt; t) dt<label>(4)</label></formula><p>By using this fact,</p><formula xml:id="formula_13">|E x?P [ (h 1 (x), h 2 (x))] ? E x?Q [ (h 1 (x), h 2 (x))]| (5) = ? 0 P P ( (h 1 (x), h 2 (x)) &gt; t) dt ? ? 0 P Q ( (h 1 (x), h 2 (x)) &gt; t) dt (6) ? ? 0 |P P ( (h 1 (x), h 2 (x)) &gt; t) ? P Q ( (h 1 (x), h 2 (x)) &gt; t)| dt (7) ? M sup t?[0,M ] |P P ( (h 1 (x), h 2 (x)) &gt; t) ? P Q ( (h 1 (x), h 2 (x)) &gt; t)| (8) ? M sup h1,h2 sup t?[0,M ] |P P ( (h 1 (x), h 2 (x)) &gt; t) ? P Q ( (h 1 (x), h 2 (x)) &gt; t)| (9) ? M sup h?H P P h (x) = 1 ? P Q h (x) = 1 (10) ? M sup A |P P (A) ? P Q (A)|<label>(11)</label></formula><p>whereH </p><formula xml:id="formula_14">:= {I[ (h(x), h (x)) &gt; t]|h, h ? H, t ? [0, M ]}.</formula><formula xml:id="formula_15">E S (?) ?? ? S (?) ? max k (v k [ln (n/v k ) + 1] + ln (N/?)) 2n<label>(12)</label></formula><p>where? ? S (? k ) is an empirical robust risk with n samples.</p><p>Proof. We first show that the following inequality holds for the local maximum of N covers,</p><formula xml:id="formula_16">P max k E S (? k ) ?? S (? k ) &gt; ? N k=1 P E S (? k ) ?? S (? k ) &gt; (13) ? N k=1 P sup ??? k E S (?) ?? S (?) &gt;<label>(14)</label></formula><formula xml:id="formula_17">? N k=1 en v k v k e ?2n 2 .<label>(15)</label></formula><p>Now, we introduce a confidence error bound k :</p><formula xml:id="formula_18">= (v k [ln(n/v k )+1]+ln(N/?)) 2n</formula><p>. Then, we set := max k k . Then, we get,</p><formula xml:id="formula_19">P max k E S (? k ) ?L S (? k ) &gt; ? N k=1 en v k v k e ?2n 2<label>(16)</label></formula><formula xml:id="formula_20">? N k=1 en v k v k e ?2n 2 k (17) = N k=1 ? N = ?,<label>(18)</label></formula><formula xml:id="formula_21">since &gt; (v k [ln(n/v k )+1]+ln(N/?)) 2n</formula><p>for all k. Hence, the inequality holds with probability at least 1 ? ?.</p><p>Based on this fact, let us consider the set of events such that max k E S (? k ) ?? S (? k ) ? . Then, for any ?, there exists k such that ? ? ? k . Then, we get</p><formula xml:id="formula_22">E S (?) ?? ? S (?) ? E S (?) ?? S (? k ) (19) ? E S (?) ? E S (? k ) + (20) ? E S (? k ) ? E S (? k ) + = ,<label>(21)</label></formula><p>where the second inequality holds since E S (? k ) ?? S (? k ) ? max k E S (? k ) ?? S (? k ) ? and the final inequality holds since ? k is the local maximum in ? k . In this regards, we know that</p><formula xml:id="formula_23">max k E S (? k ) ?? S (? k ) ? implies E S (?) ?? ? S (?) ? . Consequently, E S (?) ?? ? S (?) ? holds with probability at least 1 ? ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Theorem 1</head><p>Proof. The proof consists of two parts. First, we show that the following inequality holds with high probability.</p><formula xml:id="formula_24">E T (?) ?? ? S (?) + 1 2 Div (S, T ) + max k (v k [ln (n/v k ) + 1] + ln (N/?)) 2n .</formula><p>Then, secondly, we apply the inequality for multiple source domains.</p><p>The first part can be proven by simply combining Lemma 1 and Lemma 2. Then, we get,</p><formula xml:id="formula_25">E T (?) ? E S (?) + 1 2 Div (S, T )<label>(22)</label></formula><p>?? ? S (?) +</p><formula xml:id="formula_26">1 2 Div (S, T ) + max k (v k [ln (n/v k ) + 1] + ln (N/?)) 2n<label>(23)</label></formula><p>where Div (S, T ) is a divergence between S and T .</p><p>For the second part, we set D := I i=1 D i /I which is a mixture of source distributions. Then, by applying D to the first part, we obtain the following inequality,</p><formula xml:id="formula_27">E T (?) ?? ? D (?) + 1 2 Div (D, T ) + max k (v k [ln (In/v k ) + 1] + ln (N/?)) 2In (24) ?? ? D (?) + 1 2I I i=1 Div (D i , T ) + max k (v k [ln (In/v k ) + 1] + ln (N/?)) 2In<label>(25)</label></formula><p>where the total number of training data set is In and, for the second inequality, we use the fact that</p><formula xml:id="formula_28">1 2 Div (D, T ) ? 1 2I I i=1 Div (D i , T )</formula><p>, which has been proven in <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Theorem 2</head><p>Proof. First, let? ? arg max ??? E T (?). Then, from generalization error bound of E D (?), the following inequality holds with probability at most ? 2 ,</p><formula xml:id="formula_29">E D (?) ? E D (?) &gt; v ln (In/v) + ln (2/?) In ,<label>(26)</label></formula><p>where v is a VC dimension of ?. Furthermore, from Theorem 1, we have the following inequality with probability at most ? 2 ,</p><formula xml:id="formula_30">E T (? ? ) &gt; E ? D (? ? ) + 1 2 Div(D, T ) + max k?[1,N ] v k ln (In/v k ) + ln(2N/?) In .<label>(27)</label></formula><p>Finally, let us consider the set of event such that?</p><formula xml:id="formula_31">D (?)?E D (?) ? v ln(In/v)+ln(2/?) In and E T (? ? ) ? E ? D (? ? ) + 1 2 Div(D, T ) + max k?[1,N ] v k ln(In/v k )+ln(2N/?) In</formula><p>whose probability is at least greater than 1 ? ?. Then, under this set of event, we have, </p><p>Consequently, we have,  followed by SWAs, SAM, and ERM. It is another evidence of our claim that domain generalization is achievable by seeking flat minima.</p><formula xml:id="formula_33">E T (? ? ) ? min ? E T (? ) ? E ? D (? ? ) ? min ? ? D (? ) + Div(D, T ) + max k?[1,N ] v k ln (In/v k ) + ln(2N/?) In + v ln (In/v) + ln (2/?) In (31) ? E ? D (? ? ) ? min ? ? D (? ) + 1 I I i=1 Div(D i , T ) + max k?[1,N ] v k ln (In/v k ) + ln(2N/?) In</formula><p>On the other hand, comparing SWAs and SWAD demonstrates the effectiveness of the proposed dense and overfit-aware sampling strategy. SWAD improves average performance up to 1.4pp, and surpasses both SWAs on every benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Full Results</head><p>In this section, we show detailed results of <ref type="table" target="#tab_3">Table 2</ref>       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Datasets</head><p>While we use public datasets only, we track how the datasets were built to discuss licenses, copyrights, and potential ethical issues. For DomainNet <ref type="bibr" target="#b45">[46]</ref> and OfficeHome <ref type="bibr" target="#b43">[44]</ref>, we use the datasets for non-profit academic research only following their fair use notice. TerraIncognita <ref type="bibr" target="#b44">[45]</ref> is a subset of Caltech Camera Traps (CCT) dataset, distributed under the Community Data License Agreement (CDLA) license. PACS <ref type="bibr" target="#b6">[7]</ref> and VLCS <ref type="bibr" target="#b42">[43]</ref> datasets have images collected from the web and we could not find any statements about licenses, copyrights, or whether consent was obtained. Considering that both datasets contain person class and images of people, there may be potential ethical issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Reproducibility</head><p>To provide details of our algorithm and guarantee reproducibility, we provide the source code 5 publicly. The code also specifies detailed environments, dependencies, how to download datasets, and instructions to reproduce the main results ( <ref type="table" target="#tab_10">Table 1</ref> and 2 in the main text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Infrastructures</head><p>Every experiment is conducted on a single NVIDIA Tesla P40 or V100, Python 3.8.6, PyTorch 1.7.0, Torchvision 0.8.1, and CUDA 9.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Runtime Analysis</head><p>The total runtime varies depending on datasets and the moment detected to overfit. It takes about 4 hours for PACS and VLCS, 8 hours for OfficeHome, 8.5 hours for TerraIncognita, and 56 hours for DomainNet on average, when using a single NVIDIA Tesla P40 GPU. Each experiment includes the leave-one-out cross-validations for all domains in each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Complexity Analysis</head><p>The only additional time overhead incurs from stochastic weights selection, which requires further evaluations. To analyze the overhead, let the forward time t f , backward time t b , training and validation split ratio r = |X train |/|X valid |, total in-domain samples n, and evaluation frequency v that indicates how many evaluations are conducted for each epoch. For conciseness, we assume t = t f = t b and do not consider early stopping.</p><p>For one epoch, training time is 2tnr/(r + 1), and evaluation time is vtn/(r + 1). The total runtime for one epoch is tn(2r + v)/(r + 1). Final overhead ratio is (2r + v)/(2r + v b ) where v b is the evaluation frequency of a baseline. In our main experiments, we use r = 4. Compared to the default parameters of DomainBed <ref type="bibr" target="#b21">[22]</ref>, we use v = 2v b for DomainNet, v = 6v b for VLCS, and v = 3v b for the others. Then, the total runtime of our algorithm takes from 1.07 (PACS) to 1.27 (DomainNet) times more than the ERM baseline. In practice, it can be improved by conducting approximated evaluations using sub-sampled validation set.</p><p>In terms of memory complexity, our method does not require additional GPU memory. Instead, we leverage CPU memory to minimize training time overhead, which takes up to max(N, M ) times more than the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is a model parameterized by ?. Formally, the goal of DG is to find a model which minimizes both E D (?) and E T (?) by only minimizing an empirical risk? D (?) := 1 In I i=1 n j=1 (f (x i ; ?), y i )) over training domains D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Robust risk minimization (RRM) and flat minima. With proper ?, RRM will find flat minima.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between SWA and SWAD. (a) SWA collects stochastic weights for every K epochs from the pre-defined K0 epochs to the final epoch. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Local flatness comparisons. We plot the local flatness via loss gap, i.e., F?(?) = E ? = ? +? [E(? ) ? E(?)], of ERM, SAM, SWA, and SWAD by varying radius ? on different domains of PACS dataset. For each figure, Y-axis indicates the flatness F?(?) and X-axis indicates the radius ?. We measure the train flatness F D ? (?) on seen domains and the test flatness F T ? (?) on unseen domain. Each point is computed by Monte-Carlo approximation with 100 random samples. This comparisons show SWAD finds flatter minima than not only ERM but also SAM and SWA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Loss surfaces on model parameters in PACS dataset for each target domain. The three triangles indicate model weights chosen at the end of training phase with equal intervals. Each plane is defined by the three weights and losses upon the plane are visualized with contours. The center cross mark is averaged point of the three weights. The first and second rows show the averaged training loss and the test loss surfaces, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Validation accuracies for in-domains. The X-and Y-axis indicate the training iterations and accuracy, respectively, about the validation domains (legend) and the test domain (caption). The vertical dot lines represent start and end iterations, ts and te, identified by the overfit-aware sampling strategy of SWAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>C. 1</head><label>1</label><figDesc>Technical Lemmas Consider an instance loss function (y 1 , y 2 ) such that : Y ? Y ? [0, 1] and (y 1 , y 2 ) = 0 if and only if y 1 = y 2 . Then, we can define a functional error as E P (f (?; ?), h) := E P [ (f (x; ?), h(x))].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma 2 .</head><label>2</label><figDesc>Consider a distribution S on input space and global label function f : X ? Y. Let ? k ? R d , k = 1, ? ? ? , N be a finite cover of a parameter space ? which consists of closed balls with radius ?/2 where N := (diam(?)/?) d . Let ? k ? arg max ? k ?? E S (?) be a local maximum in the k-th ball. Let a VC dimension of ? k be v k . Then, for any ? ? ?, the following bound holds with probability at least 1 ? ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>?</head><label></label><figDesc>D (? ) ?? D (?) ? E D (?) + v ln (In/v) + ln (2/?) , T ) + v ln (In/v) + ln (2/?) In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>1</head><label>1</label><figDesc>Comparison of flatness-aware solversInterestingly, the average performance ranking of flatness-aware solvers is the same as the results of the local flatness test (SeeFigure 3in the main text). In both experiments, SWAD performs best,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with domain generalization methods and SWAD. Out-of-domain accuracies on five domain generalization benchmarks are shown. We highlight the best results and the second best results. Note that ERM (reproduced), Mixstyle are reproduced numbers, and other numbers are from the original literature and Gulrajani and Lopez-Paz<ref type="bibr" target="#b21">[22]</ref> (denoted with ?). Our experiments are repeated three times.</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS OfficeHome TerraInc DomainNet Avg.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Following Gulrajani and Lopez-Paz [22], we exhaustively evaluate our method and comparison methods on various benchmarks: PACS [7] (9,991 images, 7 classes, and 4 domains), VLCS [43] (10,729 images, 5 classes, and 4 domains), OfficeHome [44] (15,588 images, 65 classes, and 4 domains), TerraIncognita [45] (24,788 images, 10 classes, and 4 domains), and DomainNet [46] (586,575 images, 345 classes, and 6 domains).For a fair comparison, we follow training and evaluation protocol by Gulrajani and Lopez-Paz<ref type="bibr" target="#b21">[22]</ref>, including the dataset splits, hyperparameter (HP) search and model selection (while SWAD does not need it) on the validation set, and optimizer HP, except the HP search space and the number of iterations for DomainNet. We use a reduced HP search space to reduce the computational costs. We also tripled the number of iterations for DomainNet from 5,000 to 15,000 because we observe that 5,000 is not sufficient to convergence. We re-evaluate ERM with 15,000 iterations, and observe 3.1pp average performance improvement (40.9% ? 44.0%) in DomainNet. For training, we choose a domain as the target domain and use the remaining domains as the training domain where 20% samples are used for validation and model selection. ImageNet</figDesc><table><row><cell>29]</cell><cell>85.5</cell><cell>77.5</cell><cell>66.5</cell><cell>46.1</cell><cell>40.9</cell><cell>63.3</cell></row><row><cell>ERM (reproduced)</cell><cell>84.2</cell><cell>77.3</cell><cell>67.6</cell><cell>47.8</cell><cell>44.0</cell><cell>64.2</cell></row><row><cell>IRM  ? [20]</cell><cell>83.5</cell><cell>78.6</cell><cell>64.3</cell><cell>47.6</cell><cell>33.9</cell><cell>61.6</cell></row><row><cell>GroupDRO  ? [49]</cell><cell>84.4</cell><cell>76.7</cell><cell>66.0</cell><cell>43.2</cell><cell>33.3</cell><cell>60.7</cell></row><row><cell>I-Mixup  ? [50-52]</cell><cell>84.6</cell><cell>77.4</cell><cell>68.1</cell><cell>47.9</cell><cell>39.2</cell><cell>63.4</cell></row><row><cell>MLDG  ? [13]</cell><cell>84.9</cell><cell>77.2</cell><cell>66.8</cell><cell>47.8</cell><cell>41.2</cell><cell>63.6</cell></row><row><cell>CORAL  ? [31]</cell><cell>86.2</cell><cell>78.8</cell><cell>68.7</cell><cell>47.7</cell><cell>41.5</cell><cell>64.5</cell></row><row><cell>MMD  ? [53]</cell><cell>84.7</cell><cell>77.5</cell><cell>66.4</cell><cell>42.2</cell><cell>23.4</cell><cell>58.8</cell></row><row><cell>DANN  ? [9]</cell><cell>83.7</cell><cell>78.6</cell><cell>65.9</cell><cell>46.7</cell><cell>38.3</cell><cell>62.6</cell></row><row><cell>CDANN  ? [10]</cell><cell>82.6</cell><cell>77.5</cell><cell>65.7</cell><cell>45.8</cell><cell>38.3</cell><cell>62.0</cell></row><row><cell>MTL  ? [54]</cell><cell>84.6</cell><cell>77.2</cell><cell>66.4</cell><cell>45.6</cell><cell>40.6</cell><cell>62.9</cell></row><row><cell>SagNet  ? [32]</cell><cell>86.3</cell><cell>77.8</cell><cell>68.1</cell><cell>48.6</cell><cell>40.3</cell><cell>64.2</cell></row><row><cell>ARM  ? [16]</cell><cell>85.1</cell><cell>77.6</cell><cell>64.8</cell><cell>45.5</cell><cell>35.5</cell><cell>61.7</cell></row><row><cell>VREx  ? [21]</cell><cell>84.9</cell><cell>78.3</cell><cell>66.4</cell><cell>46.4</cell><cell>33.6</cell><cell>61.9</cell></row><row><cell>RSC  ? [55]</cell><cell>85.2</cell><cell>77.1</cell><cell>65.5</cell><cell>46.6</cell><cell>38.9</cell><cell>62.7</cell></row><row><cell>Mixstyle [17]</cell><cell>85.2</cell><cell>77.9</cell><cell>60.4</cell><cell>44.0</cell><cell>34.0</cell><cell>60.3</cell></row><row><cell>SWAD (ours)</cell><cell>88.1 (?0.1)</cell><cell>79.1 (?0.1)</cell><cell>70.6 (?0.2)</cell><cell>50.0 (?0.3)</cell><cell>46.5 (?0.1)</cell><cell>66.9</cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Evaluation protocols</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Dataset and optimization protocol.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison between generalization methods on PACS. The scores are averaged over all settings using different target domains. (?) and (?) indicate statistically significant improvement and degradation from ERM.</figDesc><table><row><cell></cell><cell cols="2">Out-of-domain In-domain</cell></row><row><cell>ERM</cell><cell>85.3?0.4</cell><cell>96.6?0.0</cell></row><row><cell>EMA</cell><cell cols="2">85.5?0.4(-) 97.0?0.1(?)</cell></row><row><cell>SAM</cell><cell cols="2">85.5?0.1(-) 97.4?0.1(?)</cell></row><row><cell>Mixup</cell><cell cols="2">84.8?0.3(-) 97.3?0.1(?)</cell></row><row><cell>CutMix</cell><cell cols="2">83.8?0.4(?) 97.6?0.1(?)</cell></row><row><cell>VAT</cell><cell cols="2">85.4?0.6(-) 96.9?0.2(?)</cell></row><row><cell cols="3">?-model 83.5?0.5(?) 96.8?0.2(?)</cell></row><row><cell>SWA</cell><cell cols="2">85.9?0.1(?) 97.1?0.1(?)</cell></row><row><cell>SWAD</cell><cell cols="2">87.1?0.2(?) 97.7?0.1(?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Combination of SWAD and other methods. The scores are averaged over every target domain case.The performances of ERM, CORAL, and SAM are optimized by HP searches of DomainBed. In contrast, for the SWAD combination cases, CORAL and SAM use default HPs without additional HP search. We additionally compare SWAD to SWAw/ const. Note that ERM + SWAD is same as "SWAD" inTable 2.</figDesc><table><row><cell></cell><cell>PACS</cell><cell>VLCS</cell><cell cols="3">OfficeHome TerraInc DomainNet</cell><cell>Avg. (?)</cell></row><row><cell>ERM</cell><cell cols="2">85.5 ?0.2 77.5 ?0.4</cell><cell>66.5 ?0.3</cell><cell>46.1 ?1.8</cell><cell>40.9 ?0.1</cell><cell>63.3</cell></row><row><cell cols="3">ERM + SWAw/ const 86.9 ?0.2 76.6 ?0.1</cell><cell>69.3 ?0.3</cell><cell>49.2 ?1.2</cell><cell>45.9 ?0.0</cell><cell>65.6 (+2.3)</cell></row><row><cell>ERM + SWAD</cell><cell cols="2">88.1 ?0.1 79.1 ?0.1</cell><cell>70.6 ?0.2</cell><cell>50.0 ?0.3</cell><cell>46.5 ?0.1</cell><cell>66.9 (+3.6)</cell></row><row><cell>CORAL</cell><cell cols="2">86.2 ?0.3 78.8 ?0.6</cell><cell>68.7 ?0.3</cell><cell>47.6 ?1.0</cell><cell>41.5 ?0.1</cell><cell>64.5</cell></row><row><cell>CORAL + SWAD</cell><cell>88.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>3 ?0.1 78.9 ?0.1 71.3 ?0.1 51.0 ?0.1 46.8 ?0.0 67.3 (+2.8) SAM 85.8 ?0.2 79.4 ?0.1 69.6 ?0.1 43.3 ?0.7 44.3 ?0.0 64.5 SAM + SWAD 87.1 ?0.2 78.5 ?0.2 69.9 ?0.1 45.3 ?0.9 46.5 ?0.1 65.5 (+1.0)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>?0.1 76.6 ?0.1 81.2 97.1 ?0.1 85.0 ?0.2 91.0 ?0.3 76.7 ?0.2 81.6 97.3 ?0.1 85.0 ?0.2 91.1 ?0.2 78.9 ?0.2 83.0 97.7 ?0.1 86.1 ?0.5 91.9</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Configuration</cell><cell></cell><cell cols="2">Out-of-domain</cell><cell></cell><cell></cell><cell>In-domain</cell></row><row><cell></cell><cell>ts</cell><cell>te</cell><cell>lr</cell><cell>interval</cell><cell>PACS</cell><cell>VLCS</cell><cell>Avg.</cell><cell>PACS</cell><cell>VLCS</cell><cell>Avg.</cell></row><row><cell cols="11">SWAw/ cyclic 4000 5000 Cyclic 100 85.9 SWAw/ const 4000 5000 Const 100 86.5 SWADw/o Dense Opt Overfit Const 100 86.5 ?0.4 78.0 ?0.7 82.2 97.6 ?0.1 85.8 ?0.4 91.7</cell></row><row><cell cols="4">SWADw/o Opt-Overfit 4000 5000 Const</cell><cell>1</cell><cell cols="6">86.6 ?0.6 76.9 ?0.3 81.7 97.5 ?0.1 85.2 ?0.1 91.3</cell></row><row><cell>SWADw/o Overfit</cell><cell cols="3">Opt 5000 Const</cell><cell>1</cell><cell cols="6">87.1 ?0.3 77.6 ?0.1 82.4 97.7 ?0.1 85.8 ?0.3 91.8</cell></row><row><cell>SWADfit-on-val</cell><cell>Val</cell><cell>Val</cell><cell>Const</cell><cell>1</cell><cell cols="6">86.2 ?0.2 78.6 ?0.1 82.4 97.5 ?0.2 85.8 ?0.3 91.7</cell></row><row><cell cols="4">SWAD (proposed) Opt Overfit Const</cell><cell>1</cell><cell>87.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>ImageNet robustness benchmarks. We show the ImageNet generalization performances on ImageNet-C, background challenge (BGC), and ImageNet-R.</figDesc><table><row><cell>ERM</cell><cell>76.5</cell><cell>57.6</cell><cell>8.7</cell><cell>36.7</cell></row><row><cell>SWA</cell><cell>76.9</cell><cell>56.8</cell><cell>10.9</cell><cell>37.5</cell></row><row><cell>SWAD (ours)</cell><cell>77.0</cell><cell>55.7</cell><cell>11.8</cell><cell>38.8</cell></row></table><note>Method ImageNet (%) ? ImageNet-C (mCE) ? BGC (%) ? ImageNet-R (%) ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter search space comparison. U and list indicate Uniform distribution and random choice, respectively.</figDesc><table><row><cell>Parameter</cell><cell cols="2">Default value DomainBed</cell><cell>Ours</cell></row><row><cell>batch size</cell><cell>32</cell><cell>2 U(3,5.5)</cell><cell>32</cell></row><row><cell>learning rate</cell><cell>5e-5</cell><cell>10 U(-5,-3.5)</cell><cell>[1e-5, 3e-5, 5e-5]</cell></row><row><cell cols="2">ResNet dropout 0</cell><cell cols="2">[0.0, 0.1, 0.5] [0.0, 0.1, 0.5]</cell></row><row><cell>weight decay</cell><cell>0</cell><cell>10 U(-6,-2)</cell><cell>[1e-4, 1e-6]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Algorithm 1 :</head><label>1</label><figDesc>Stochastic Weight Averaging Densely Input: initial weight ? 0 , constant learning rate ?, tolerance rate r, optimum patience N s , overfit patience N e , total number of iterations T , training loss E</figDesc><table><row><cell>(i) train , validation loss E val (i)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Flatness-aware solvers comparison. SWAs collect 10 weights from the last 20% of training. ?0.2 77.5 ?0.4 66.5 ?0.3 46.1 ?1.8 40.9 ?0.1 63.3 SAM 85.8 ?0.2 79.4 ?0.1 69.6 ?0.1 43.3 ?0.7 44.3 ?0.0</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell cols="4">OfficeHome TerraInc DomainNet Avg.</cell></row><row><cell cols="7">ERM (baseline) 85.5 64.5</cell></row><row><cell>SWAw/ cyclic</cell><cell cols="2">87.1 ?0.1 76.5 ?0.2</cell><cell>68.5 ?0.2</cell><cell>49.6 ?1.0</cell><cell>45.6 ?0.0</cell><cell>65.5</cell></row><row><cell>SWAw/ const</cell><cell cols="2">86.9 ?0.2 76.6 ?0.1</cell><cell>69.3 ?0.3</cell><cell>49.2 ?1.2</cell><cell>45.9 ?0.0</cell><cell>65.6</cell></row><row><cell>SWAD</cell><cell cols="2">88.1 ?0.1 79.1 ?0.1</cell><cell>70.6 ?0.2</cell><cell>50.0 ?0.3</cell><cell>46.5 ?0.1</cell><cell>66.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>in the main text. ? and ? indicate results from DomainBed's and our HP search protocols, respectively. Standard errors are reported from three trials, if available.</figDesc><table /><note>E.1 PACS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Out-of-domain accuracies (%) on PACS. ?1.8 75.5 ?0.9 96.8 ?0.3 73.5 ?0.6 82.?1.3 76.4 ?1.1 96.7 ?0.6 76.1 ?1.0 83.?0.8 77.4 ?0.8 97.3 ?0.4 73.5 ?2.3 83.7 ERM ? 85.7 ?0.6 77.1 ?0.8 97.4 ?0.4 76.6 ?0.7 84.2 GroupDRO ? 83.5 ?0.9 79.1 ?0.6 96.7 ?0.3 78.3 ?2.0 84.4 MTL ? 87.5 ?0.8 77.1 ?0.5 96.4 ?0.8 77.3 ?1.8 84.6 I-Mixup 86.1 ?0.5 78.9 ?0.8 97.6 ?0.1 75.8 ?1.8 84.6 MMD ? 86.1 ?1.4 79.4 ?0.9 96.6 ?0.2 76.5 ?0.5 84.7 VREx ? 86.0 ?1.6 79.1 ?0.6 96.9 ?0.5 77.7 ?1.7 84.9 MLDG ? 85.5 ?1.4 80.1 ?1.7 97.4 ?0.3 76.6 ?1.1 84.9 ARM ? 86.8 ?0.6 76.8 ?0.5 97.4 ?0.3 79.3 ?1.2 85.1 RSC ? 85.4 ?0.8 79.7 ?1.8 97.6 ?0.3 78.2 ?1.2 85.2 Mixstyle ? 86.8 ?0.5 79.0 ?1.4 96.6 ?0.1 78.5 ?2.3 85.?0.4 80.8 ?0.6 97.2 ?0.3 79.3 ?1.0 85.?0.2 80.0 ?0.5 97.5 ?0.3 78.8 ?1.3 86.2 SagNet ? 87.4 ?1.0 80.7 ?0.6 97.1 ?0.1 80.0 ?0.4 86.?0.2 83.4 ?0.6 97.3 ?0.3 82.5 ?0.5 88.1</figDesc><table><row><cell>Algorithm</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>S</cell><cell>Avg</cell></row><row><cell>CDANN  ?</cell><cell cols="5">84.6 6</cell></row><row><cell>MASF</cell><cell>82.9</cell><cell>80.5</cell><cell>95.0</cell><cell>72.3</cell><cell>82.7</cell></row><row><cell>DMG</cell><cell>82.6</cell><cell>78.1</cell><cell>94.5</cell><cell>78.3</cell><cell>83.4</cell></row><row><cell>IRM  ?</cell><cell cols="5">84.8 5</cell></row><row><cell>MetaReg</cell><cell>87.2</cell><cell>79.2</cell><cell>97.6</cell><cell>70.3</cell><cell>83.6</cell></row><row><cell>DANN  ?</cell><cell cols="5">86.4 2</cell></row><row><cell>ER</cell><cell>87.5</cell><cell>79.3</cell><cell>98.3</cell><cell>76.3</cell><cell>85.3</cell></row><row><cell>pAdaIN</cell><cell>85.8</cell><cell>81.1</cell><cell>97.2</cell><cell>77.4</cell><cell>85.4</cell></row><row><cell>ERM  ?</cell><cell cols="5">84.7 5</cell></row><row><cell>EISNet</cell><cell>86.6</cell><cell>81.5</cell><cell>97.1</cell><cell>78.1</cell><cell>85.8</cell></row><row><cell>CORAL  ?</cell><cell cols="5">88.3 3</cell></row><row><cell>DSON</cell><cell>87.0</cell><cell>80.6</cell><cell>96.0</cell><cell>82.9</cell><cell>86.6</cell></row><row><cell>Ours</cell><cell>89.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>E.2 VLCS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Out-of-domain accuracies (%) on VLCS. ?0.3 63.4 ?0.9 69.5 ?0.8 76.7 ?0.7 76.7 RSC ? 97.9 ?0.1 62.5 ?0.7 72.3 ?1.2 75.6 ?0.8 77.1 MLDG ? 97.4 ?0.2 65.2 ?0.7 71.0 ?1.4 75.3 ?1.0 77.2 MTL ? 97.8 ?0.4 64.3 ?0.3 71.5 ?0.7 75.3 ?1.7 77.2 ERM ? 98.0 ?0.3 64.7 ?1.2 71.4 ?1.2 75.2 ?1.6 77.3 I-Mixup 98.3 ?0.6 64.8 ?1.0 72.1 ?0.5 74.3 ?0.8 77.4 ERM ? 97.7 ?0.4 64.3 ?0.9 73.4 ?0.5 74.6 ?1.3 77.5 MMD ? 97.7 ?0.1 64.0 ?1.1 72.8 ?0.2 75.3 ?3.3 77.5 CDANN ? 97.1 ?0.3 65.1 ?1.2 70.7 ?0.8 77.1 ?1.5 77.5 ARM ? 98.7 ?0.2 63.6 ?0.7 71.3 ?1.2 76.7 ?0.6 77.6 SagNet ? 97.9 ?0.4 64.5 ?0.5 71.4 ?1.3 77.5 ?0.5 77.8 Mixstyle ? 98.6 ?0.3 64.5 ?1.1 72.6 ?0.5 75.7 ?1.7 77.9 VREx ? 98.4 ?0.3 64.4 ?1.4 74.1 ?0.4 76.2 ?1.3 78.3 IRM ? 98.6 ?0.1 64.9 ?0.9 73.4 ?0.6 77.3 ?0.9 78.6 DANN ? 99.0 ?0.3 65.1 ?1.4 73.1 ?0.3 77.2 ?0.6 78.6 CORAL ? 98.3 ?0.1 66.1 ?1.2 73.4 ?0.3 77.5 ?1.2 78.8 Ours 98.8 ?0.1 63.3 ?0.3 75.3 ?0.5 79.2 ?0.6 79.1</figDesc><table><row><cell>Algorithm</cell><cell>C</cell><cell>L</cell><cell>S</cell><cell>V</cell><cell>Avg</cell></row><row><cell cols="2">GroupDRO  ? 97.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>E.3 OfficeHome</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Out-of-domain accuracies (%) on OfficeHome.</figDesc><table><row><cell>Algorithm</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>Avg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Out-of-domain accuracies (%) on TerraIncognita. ?3.0 34.8 ?1.0 57.0 ?1.9 35.2 ?1.8 42.2 GroupDRO ? 41.2 ?0.7 38.6 ?2.1 56.7 ?0.9 36.4 ?2.1 43.2 Mixstyle ? 54.3 ?1.1 34.1 ?1.1 55.9 ?1.1 31.7 ?2.1 44.0 ARM ? 49.3 ?0.7 38.3 ?2.4 55.8 ?0.8 38.7 ?1.3 45.5 MTL ? 49.3 ?1.2 39.6 ?6.3 55.6 ?1.1 37.8 ?0.8 45.6 CDANN ? 47.0 ?1.9 41.3 ?4.8 54.9 ?1.7 39.8 ?2.3 45.8 ERM ? 49.8 ?4.4 42.1 ?1.4 56.9 ?1.8 35.7 ?3.9 46.1 VREx ? 48.2 ?4.3 41.7 ?1.3 56.8 ?0.8 38.7 ?3.1 46.4 RSC ? 50.2 ?2.2 39.2 ?1.4 56.3 ?1.4 40.8 ?0.6 46.6 DANN ? 51.1 ?3.5 40.6 ?0.6 57.4 ?0.5 37.7 ?1.8 46.7 IRM ? 54.6 ?1.3 39.8 ?1.9 56.2 ?1.8 39.6 ?0.8 47.6 CORAL ? 51.6 ?2.4 42.2 ?1.0 57.0 ?1.0 39.8 ?2.9 47.7 MLDG ? 54.2 ?3.0 44.3 ?1.1 55.6 ?0.3 36.9 ?2.2 47.8 I-Mixup 59.6 ?2.0 42.2 ?1.4 55.9 ?0.8 33.9 ?1.4 47.9 SagNet ? 53.0 ?2.9 43.0 ?2.5 57.9 ?0.6 40.4 ?1.3 48.6 ERM ? 54.3 ?0.4 42.5 ?0.7 55.6 ?0.3 38.8 ?2.5 47.8 Ours 55.4 ?0.0 44.9 ?1.1 59.7 ?0.4 39.9 ?0.2 50.0</figDesc><table><row><cell>Algorithm</cell><cell>L100</cell><cell>L38</cell><cell>L43</cell><cell>L46</cell><cell>Avg</cell></row><row><cell>MMD  ?</cell><cell>41.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>E.5 DomainNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Out-of-domain accuracies (%) on DomainNet. Code Our work is built upon DomainBed [22] 4 , which is released under the MIT license.</figDesc><table><row><cell>Algorithm</cell><cell>clip</cell><cell>info</cell><cell>paint</cell><cell>quick</cell><cell>real</cell><cell>sketch</cell><cell>Avg</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">as<ref type="bibr" target="#b24">[25]</ref>. More details are in Appendix B.5. InFigure 4, we observe that for all cases, ERM solutions are located at the boundary of a flat minimum of training loss, resulting in poor generalizability in test domains, that is aligned with our theoretical analysis and empirical flatness analysis. Since ERM solutions are located on the boundary of a flat loss surface, we observe that ERM solutions are very sensitive to model selection. InFigure 5, we illustrate the validation accuracies for each train-test domain combination of PACS by ERM, over training iterations (one epoch is equivalent to 83 iterations). We first observe that ERM rapidly reaches the best accuracy within only a few training epochs, namely less than 6 epochs. Furthermore, the ERM validation accuracies fluctuate a lot, and the final performance is very sensitive to the model selection criterion.On the other hand, we observe that SWA solutions are located on the center of the training loss surfaces as well as of the test loss surfaces(Figure 4). Also, our overfit-aware stochastic weight gathering strategy (denoted as the vertical dot lines inFigure 5) prevents the ensembled weight from overfitting and makes SWAD model selection-free.<ref type="bibr" target="#b1">2</ref> We choose weights at iteration 2500, 3500, 4500 during the training.<ref type="bibr" target="#b2">3</ref> Each point is defined by two axes u and v computed by u = ?2 ? ?1 and v = (? 3 ?? 1 )? ? 3 ?? 1 ,? 2 ?? 1 ? 2 ?? 1 2 ?(? 2 ?? 1 ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/facebookresearch/DomainBed 5 https://github.com/khanrc/swad</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3819" to="3824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Benchmarking robustness in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07484</idno>
	</analytic>
	<monogr>
		<title level="m">Autonomous driving when winter is coming</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Does object recognition work for everyone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="547" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain generalization via conditional invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning de-biased representations with biased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization via entropy regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing System</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptive risk minimization: A meta-learning approach for tackling group shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02931</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain generalization with mixstyle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00688</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In search of lost domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel M</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fantastic generalization measures and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>NY: Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to optimize domain specific normalization for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reducing domain gap by reducing style bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8690" to="8699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to balance specificity and invariance for in and out of domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="301" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Norton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">O</forename><surname>Royset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10844</idno>
		<title level="m">Diametrical risk minimization: Theory and computations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124018</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Permuted adain: Reducing the bias towards global statistics in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Nuriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from extrinsic and intrinsic supervisions for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caizi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="159" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distributionally robust neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improve unsupervised domain adaptation with mixup training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lincan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Heterogeneous domain generalization via domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3622" to="3626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Domain generalization by marginal transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Anand Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urun</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-challenging improves crossdomain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16241</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Yuanqing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">this section, we discuss about licenses, copyrights, and ethical issues of our assets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note>Adversarial multiple source domain adaptation. such as code and datasets</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
