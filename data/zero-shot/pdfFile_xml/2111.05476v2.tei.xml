<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Disentangle Scenes for Person Re-identification ? A R T I C L E I N F O</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghao</forename><surname>Zang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Shu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518034</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Disentangle Scenes for Person Re-identification ? A R T I C L E I N F O</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>person re-identification divide-and-conquer multi-branch network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>There are many challenging problems in the person re-identification (ReID) task, such as the occlusion and scale variation. Existing works usually tried to solve them by employing a one-branch network. This one-branch network needs to be robust to various challenging problems, which makes this network overburdened. This paper proposes to divide-and-conquer the ReID task. For this purpose, we employ several self-supervision operations to simulate different challenging problems and handle each challenging problem using different networks. Concretely, we use the random erasing operation and propose a novel random scaling operation to generate new images with controllable characteristics. A general multi-branch network, including one master branch and two servant branches, is introduced to handle different scenes. These branches learn collaboratively and achieve different perceptive abilities. In this way, the complex scenes in the ReID task are effectively disentangled, and the burden of each branch is relieved. The results from extensive experiments demonstrate that the proposed method achieves state-of-the-art performances on three ReID benchmarks and two occluded ReID benchmarks. Ablation study also shows that the proposed scheme and operations significantly improve the performance in various scenes. The code is available at https://git.openi.org.cn/zangxh/LDS.git. Recently, multi-branch networks have shown their potentials in many fields of deep learning, such as image classification [73] [65], knowledge distillation [51] [1], cross-?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (ReID) has drawn increasing attention in computer vision society. Given a person image from the query, ReID aims to find all images of the same person from the gallery. In practice, the ReID task has widespread applications in social security and surveillance systems. For example, with the help of surveillance cameras, it can help find out the suspect criminals, look for a lost child in a large mall, etc <ref type="bibr" target="#b69">[70]</ref>.</p><p>Despite achieving much progress <ref type="bibr" target="#b37">[38]</ref>  <ref type="bibr" target="#b73">[74]</ref>, it is still challenging to handle various complex scenes in the ReID task, such as scale variation, occlusion, false detection, and a similar appearance. Most existing approaches can be categorized as the one-branch network. And the challenging problems overburden these one-branch networks. To achieve good performance, these one-branch networks have to utilize sophisticated designs or employ additional information (semantics, pose information, etc. <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b39">[40]</ref> [55] <ref type="bibr" target="#b48">[49]</ref>). These elaborate designs make the one-branch network complicated and over-engineered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORCID(s):</head><p>domain/modality learning <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b34">[35]</ref>. For the ReID task, many multi-branch networks were also proposed. These networks fall into two categories. The first category borrows thoughts from knowledge distillation <ref type="bibr" target="#b44">[45]</ref>  <ref type="bibr" target="#b79">[80]</ref>. They usually have two networks, teacher and student, and use a two-stage training process. The teacher is trained for complex tasks in the first stage. In the second stage, the teacher network is fixed, and its knowledge is transferred to the student. Although this method has two branches, only one branch learns in each training stage. Therefore, it is still under the paradigm of a one-branch network. The second category employs two equal networks and let them co-teach <ref type="bibr" target="#b60">[61]</ref> [60] <ref type="bibr" target="#b15">[16]</ref>. Their branches share the same responsibilities and support each other, which produces better performance than their one-branch counterpart. However, each branch still needs to deal with various challenging scenes, and its responsibility has not reduced, resulting in limited performance improvement.</p><p>To effectively disentangle the complex scenes, we propose a divide-and-conquer strategy for the ReID task. We mainly analyze two challenging scenes, i.e., occlusion and scale variation, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We conquer them one by one and improve the overall performance for the ReID task. To this end, we apply two self-supervision operations to the input image to obtain new images with the characteristics of challenging scenes. Concretely, we employ the random erasing to generate the occluded scenes and propose random scaling to generate the scale variation scenes. In this way, the ReID task is divided into simpler ones. The original image is also kept as the general scenes to provide the missing information for other generated images. We also introduce a new input manner, i.e., homologous input. This manner solves the input image misalignment problem and further improves the performance. To conquer each chal-Learning to Disentangle Scenes for Person Re-identification lenging scene, we propose a multi-branch network, as illustrated in <ref type="figure">Fig. 2</ref>. There are one master branch and two servant branches in this framework. Each servant branch is assigned to deal with one specific challenge. And the master branch is designed to handle the general scenes. The traditional one-branch network needs to deal with occlusion, scale variation, etc. In our framework, each servant branch only needs to deal with occlusion or scale variation. Therefore, the burden of each servant branch is relieved. To train the multi-branch network, we employ mutual learning to transfer the knowledge between different branches. These branches learn collaboratively and promote each other. For the master branch, we use the original input image without any artificial change. The knowledge from the servant branches benefits the master branch and decreases its overfitting possibility for general scenes. For the servant branch, the artificial image loses some information due to the selfsupervision operations. The knowledge from the master branch makes the servant branch implicitly learn the missing information and obtain robustness for a specific scene. In the testing process, features from multiple branches are concatenated as the whole feature representation. Since each branch has a different perceptive ability, the concatenated feature is robust for various scenes. We evaluate the proposed scheme on three ReID benchmarks, including Market1501 <ref type="bibr" target="#b68">[69]</ref>, DukeMTMC-reID <ref type="bibr" target="#b46">[47]</ref>, MSMT17 <ref type="bibr" target="#b56">[57]</ref>, and two large-scale occluded ReID benchmark, P-DukeMTMC-reID <ref type="bibr" target="#b78">[79]</ref>, and Occluded-DukeMTMC <ref type="bibr" target="#b39">[40]</ref>. The experiments demonstrate our method achieves state-of-the-art performances. An extensive ablation study also shows that the proposed scheme improves the robustness in various scenes.</p><p>The main contributions of this paper can be summarized as follows:</p><p>? For the challenging ReID task, we introduce a divideand-conquer strategy to deal with it, which effectively reduces the network learning burden.</p><p>? Unlike the traditional multi-branch networks, knowledge communication from different scenes improves the overall performance, which capitalizes on the potentials of multi-branch networks.</p><p>? Experiments on three ReID benchmarks and two occluded ReID benchmarks show that our scheme achieves state-of-the-art performances. The ablation study also demonstrates the effectiveness of the proposed scheme in various scenes.</p><p>The rest of this paper is organized as follows. The related works are reviewed and discussed in Section 2, and then we elaborate on the proposed method in Section 3. Experimental results and analysis are presented in Section 4, and finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Person Re-identification</head><p>For the ReID task, the problem of misalignment introduced by scale variation and occlusion has aroused great interest in the computer vision community. Many works explored this problem <ref type="bibr" target="#b17">[18]</ref> [19] <ref type="bibr" target="#b25">[26]</ref> [37] <ref type="bibr" target="#b12">[13]</ref>. Luo et.al <ref type="bibr" target="#b36">[37]</ref> proposed a Dynamically Matching Local Information (DMLI) to align the local information dynamically. The DMLI calculates the distances between different parts of possible image pairs through a dynamic programming strategy. Miao et.al <ref type="bibr" target="#b39">[40]</ref> employed the pose estimator to generate landmarks. These landmarks are utilized to indicate the model to focus on the non-occluded regions to overcome the noise introduced by the various obstacles. In this way, they obtained aligned feature representations for the ReID task.</p><p>The methods above explicitly achieved the feature alignment with the help of various supporting information. Others deal with this problem in a implicit manner. Zhou et.al <ref type="bibr" target="#b75">[76]</ref> proposed Omni-Scale Networks (OSNet), which introduced an aggregating gate to fuse feature from different scales to achieve an omni-scale feature representation. The OSNet handles the misalignment by aggregating the multiscale features and achieves good performance. Jin et.al <ref type="bibr" target="#b27">[28]</ref> proposed Semantics Aligning Network (SAN), which employed a decoder to reconstruct a dense semantics aligned full texture image. They supervise the ReID task and the semantic texture generating process simultaneously to learn a semantics-aligned feature representation. Quan et.al <ref type="bibr" target="#b45">[46]</ref> employed the Neural Architecture Search (NAS) to find a part-aware network in a retrieval-based search space automatically.</p><p>These methods above can be categorized as a one-branch network. These one-branch networks need to be robust to various challenging problems in the ReID task, which makes these networks overburdened. Although these methods utilized sophisticated designs or additional information to improve the performance, these endeavors make the one-branch network complicated and bring a limited improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Branch Networks</head><p>There are many multi-branch networks for the ReID task <ref type="bibr" target="#b79">[80]</ref>  <ref type="bibr" target="#b40">[41]</ref> [60] <ref type="bibr" target="#b26">[27]</ref> [16] <ref type="bibr" target="#b65">[66]</ref>. The first category uses a teacher network to teach a student network. Porrello et.al <ref type="bibr" target="#b44">[45]</ref> introduced multiple Views Knowledge Distillation (VKD), which trains the teacher network using multiple views and only gives the student a small set of input views. After the knowledge distillation process, the student outperforms his teacher in the image-to-video setting. Zhuo et.al <ref type="bibr" target="#b79">[80]</ref> employed a teacher-student framework for occluded ReID. They train the teacher network with a co-saliency network to simulate the occluded ReID, which enables the teacher to perceive the occlusion. Then they use the teacher network to generate the occluded mask to supervise the student network. These methods above employed a two-stage training process where only one network is trained in each stage. Therefore, they still under the paradigm of a one-branch network.</p><p>The second category trains each branch simultaneously and makes them co-teach. Yang et.al <ref type="bibr" target="#b59">[60]</ref> proposed asymmetric co-teaching for the cross-domain ReID. They employed two branches and fed them with samples as pure as possible and as miscellaneous as possible, respectively. To achieve this goal, they encouraged their two networks to promote each other. Ge et.al <ref type="bibr" target="#b15">[16]</ref> proposed Mutual Mean-Teaching (MMT) for the cross-domain ReID. They employed mean network, soft classification loss, and soft triplet loss to let two networks mutual-teach. The mean network is updated using the running average mean weight of each network. Zhang et.al <ref type="bibr" target="#b64">[65]</ref> proposed Deep Mutual Learning (DML) and gave two branches the same optimization objective. Although using one branch can complete this task, the DML scheme can find a much wider minimum for its loss function and provide a better generalization performance. However, each branch in these methods still needs to deal with various challenges, resulting in a limited performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Person Re-identification in A Specific Scene</head><p>There are various challenging scenes in the ReID task. However, there are no benchmarks designed for the scale variation scene. On the other side, two large-scale benchmarks, P-DukeMTMC-reID and Occluded-DukeMTMC, are proposed for the occluded scenes recently. The occluded ReID has raised increasing attention from the computer vision community <ref type="bibr">[</ref>  <ref type="bibr" target="#b13">[14]</ref>. In this field, Miao et.al <ref type="bibr" target="#b39">[40]</ref> introduced Pose-Guided Feature Alignment (PGFA) and exploited pose landmarks to disentangle the useful information from the occlusion noise. However, this method largely depends on an accurate human pose estimator to detect human landmarks. Sun et.al <ref type="bibr" target="#b48">[49]</ref> introduced Visibility-aware Part Model (VPM) and employed selfsupervision learning to enable the model visibility-aware. Due to the limited self-supervision, the VPM learns a coarse division strategy, which limits its performance.</p><p>These methods above merely focused on a specific scene and may fail to handle the ReID task in general scenes. On the contrary, our scheme is effective in various scenes. We divide the challenging scenes in the ReID task into multiple simpler ones and conquer them individually. Each branch achieves the perceptive ability for a particular scene in the training process. Concatenating features from each branch aggregate these different perceptive abilities and produce significant performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning to Disentangle Scenes</head><p>This section elaborates on the proposed method for ReID, i.e., Learning to Disentangle Scenes (LDS). The framework of the proposed LDS method is illustrated in <ref type="figure">Fig. 2</ref>. In this framework, we adopt the design philosophy of divide-andconquer to deal with the ReID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">"Divide" the Complex Scenes</head><p>This paper identifies the occlusion and scale variation scenes from the complex scenes in the ReID task. We adopt a self-supervision operation to generate new images with the controlled characteristics. First, we apply a random data augmentation strategy to each image. The random data augmentation introduces more samples for the network training. Then we make three copies from the new samples.</p><p>Occlusion Scenes. To generate an image with occlusion, we apply the random erasing to the first copy. The probability of random erasing is set to 1 to ensure occlusion exists in this image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale Variation Scenes.</head><p>To generate an image with scale variation, we propose the random scaling and apply it to the second copy. The random scaling is described in detail below. We first generate a baseboard with the mean value of three channels (R, G, B) of all the images in ImageNet. Then we scale the second copy to 0.8 ? 1.1 times its original size. The zoom value is randomly generated. If the zoom value is less than 0.9, the scaled image is pasted in the baseboard center. For the ReID task, the center of input images is often informative, and the marginal part usually contains background and noise information. Putting it at the center of the baseboard makes the servant branch focus on the center of input images. If the zoom value is between 0.9 and 1.0, the scaled image is pasted anywhere on the baseboard. If the zoom value is more than 1.0, the scaled image is pasted at the baseboard center. All marginal parts beyond the baseboard boundary are discarded. The probability of the random scaling operation is also set to 1. We set the minimum zoom value to 0.8 because a smaller margin around the image can improve the performance. Meanwhile, a much larger zoomed image introduces more information loss. Through a co-teach strategy in Section 3.3, the master and servant branches focus on the image center, making each branch neglect the marginal part and improve the overall performance.</p><p>General Scenes. We keep the third copy without any artificial change. The reason is explained below. The first and second copies are manipulated by the random erasing and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>The framework of the proposed LDS method. We propose a divide-and-conquer strategy to deal with the ReID task. This framework contains two parts, "divide" and "conquer" parts. They are used to "divide" the complex scenes and "conquer" the specific scene. In the "conquer" part, we use mutual learning to promote each branch. the random scaling, respectively. Thus some useful information in them is lost. We keep the third copy as the original one to provide the missing information to the first and second ones. On the other side, the third copy represents the images that happened in general scenes.</p><p>Image Alignment for Different Scenes. There are misalignment problems for the traditional multi-branch networks, as illustrate in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. This misalignment problem is mainly derived from the different input images, denoted as the heterologous input. In general, the random data augmentation operation usually includes random flipping and random cropping. These operations make the input images change their orientations and center positions, which makes the different branches receive misaligned images. Therefore, the heterologous input utilized by most multi-branch networks often results in a misalignment problem. We propose homologous input to solve this problem, as illustrated in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>. The self-supervision operation is after the ran-dom data augmentation. This fashion ensures that the different branches have the same source image. This homologous input is simple but effective, and the latter extensive ablation studies demonstrate its effectiveness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">"Conquer" the Specific Scene</head><p>We propose a multi-branch network to "conquer" each specific scene. This network consists of three branches, including the master branch and two servant branches. The master branch deals with the ReID problem in general scenes, while servant branches handle the occluded scenes and scale variation scenes. We formulate this optimization process for each branch. Given</p><formula xml:id="formula_0">image samples ? = { } =1 , there are corresponding person IDs as ? = { } =1 where ? {1, 2, ? , }.</formula><p>is the total number of person identities. After the process of homologous input, the input image becomes three copes with different characteristics denoted as , , and , which are fed to the master branch, the servant branch for the occluded scenes, and the servant branch for the scale variation scenes, respectively. We employ ? to represent each network and use ? to represent the generated image, , , and</p><p>, for each branch. The feature map extracted from one specific branch is denoted as ? . Then, each feature map is processed by average pooling and BN Neck layer to generate a normalized feature ? .</p><p>The first loss function for optimizing each branch is the triplet loss. We employ existing soft margin triplet loss with batch hard mining <ref type="bibr" target="#b22">[23]</ref>, which is calculated as follows:</p><formula xml:id="formula_1">? ? = 1 ? =1 ? ? ln {1 + exp [ + max ??( ) ( ? , ? ) ? min ?? ( ) ( ? , ? )]},<label>(1)</label></formula><p>where is the ? batch, is the number of batches in each benchmark, , , are anchor, positive, and negative samples, respectively, ?( ) and ? ( ) are the positive and negative sample sets corresponding to the given anchor in this batch, is the distance margin threshold, and function (?) calculates the Euclidean distance between two extracted features. In Eq. 1, we follow the previous research <ref type="bibr" target="#b8">[9]</ref> [64] <ref type="bibr" target="#b22">[23]</ref> and replace the hinge function [ + ?] + with the soft-plus function ln (1 + exp (?)). The soft-plus function is a smooth approximation of the hinge function and decays exponentially without having a hard cut-off, resulting in a numerically stable implementation.</p><p>The second loss function for optimizing each branch is the classification loss. We use the additive margin softmax (AM-softmax) <ref type="bibr" target="#b51">[52]</ref> to calculate this loss as follows:</p><formula xml:id="formula_2">? ? = ? 1 ? =1 log [ ( ? ? )] [ ( ? ? )] + ? =1, ? ( ? ) ,<label>(2)</label></formula><p>where and are the weight vectors associated with class and in the final classification layer, is the scaling factor, and is the margin to distinguish the similarity distance. In Eq. 2, we follow the <ref type="bibr" target="#b30">[31]</ref> [32] <ref type="bibr" target="#b52">[53]</ref> [52] to calculate the inner product using normalized weights vectors, and , and normalized feature . The scaling factor is set to a constant instead of a learned weight to accelerate the training process.</p><p>For each specific scene, the loss function ? is formulated as follows:</p><formula xml:id="formula_3">? ? = ? ? + ? ? .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mutual Learning</head><p>We employ mutual learning to make each branch communicate its knowledge to others. Since all the branches have the same source image, their logits should have a similar data distribution. The difference between their logits is mainly derived from the missing information introduced by the self-supervision operations. Through mutual learning, the servant branch becomes "sensitive" to the specific scene and can "guess" the missing information under the guidance of knowledge from the master branch. In this way, the servant branch gets the perceptive ability for the specific scene. Meanwhile, the master branch receives knowledge from more scenes, which reduces its overfitting probability and improves its robustness.</p><p>We use the Kullback Leibler (KL) Divergence to quantify the similarity of logits from different branches. We first calculate the probability ? of class for pedestrian image sample as follows:</p><formula xml:id="formula_4">? ( ) = exp ( ? ) ? =1 exp ( ? ) ,<label>(4)</label></formula><p>where ? is the logit fed to the "softmax" layer in the branch ?. We optimize each branch by employing a KL loss which is calculated as follows:</p><formula xml:id="formula_5">? ? = 1 ? 1 ? =1, ?? ? =1 ? =1 ? ( ) log ? ( ) ( ) ,<label>(5)</label></formula><p>where is the branch number. The KL loss ? ? makes the logits from different branches as similar as possible. For each branch, the loss function ? is formulated as follows:</p><formula xml:id="formula_6">? ? = ? ? + ? ? .<label>(6)</label></formula><p>In this paper, we train the proposed LDS in an end-toend manner. There are three losses for all branches, i.e., , and . The overall optimization function for our scheme is calculated as follows,</p><formula xml:id="formula_7">? = ? + ? + ? .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Advantages of Proposed LDS Method</head><p>Existing works usually use a one-branch network for the challenging ReID task. There are many challenging problems that need to be dealt with, making the one-branch network overburdened. Many sophisticated designs and additional information are utilized to strengthen the one-branch network. However, these designs make the one-branch network complicated, and the performance improvement is limited.</p><p>The Knowledge Distillation (KD) <ref type="bibr" target="#b23">[24]</ref> is proposed to enable a small network to become strong through learning the knowledge from a large one. The difficult learning task is assigned to the teacher network. Then the teacher is fixed, and the knowledge is distilled to the student. Although having fewer parameters, the student becomes as strong as the teacher. However, only one network learns in each training stage. Thus these KD methods are still under the paradigm of a one-branch network.</p><p>Mutual learning or co-teaching methods, such as DML <ref type="bibr" target="#b64">[65]</ref>, MMT <ref type="bibr" target="#b15">[16]</ref>, make two same networks share the responsibility and promote each other. The performance improvement is mainly due to a different network weight initialization. This operation makes each branch learn in different directions, and the KD loss makes them have an intermediate and better optimization direction. The multi-branch network often achieves better performance than a one-branch network. However, the number of challenging problems is not reduced, and each branch in these methods has the same responsibility, leading to difficulty in improving performance.</p><p>The proposed LDS introduces a divide-and-conquer strategy for the ReID task. Through self-supervision operations, the challenges are divided into simpler ones. Each servant branch only needs to deal with a specific challenge, which reduces the burden of each branch. By employing mutual learning, the servant branches can receive the knowledge from the master branch and obtain the ability to recover the missing information. The knowledge from the servant branches contains incomplete information, which reduces the overfitting possibility for the master branch. Therefore, the performance improvement of the proposed LDS is from the knowledge communication of different scenes. Correspondingly, the performance improvement of the traditional co-teaching methods is from the different initial conditions, i.e., random initialization of network weights.</p><p>On the other side, the proposed LDS is a general and flexible framework. More image transformation operations can be introduced to deal with other challenging issues, i.e., lighting variation, similar appearance, etc. This paper focuses on the divide-and-conquer strategy, and two scenes can illustrate the effects of this strategy. Therefore, we only investigate two typical scenes, occlusion and scale variation. Experiment results in the latter section demonstrate that the proposed LDS is more effective than the existing one-branch and multi-branch networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks and Evaluation Metrics</head><p>We evaluate the proposed LDS on three image-based ReID bechmarks, including Market1501 <ref type="bibr" target="#b68">[69]</ref>, DukeMTMC-reID <ref type="bibr" target="#b70">[71]</ref>, MSMT17 <ref type="bibr" target="#b56">[57]</ref>, and two occluded ReID benchmarks, P-DukeMTMC-reID <ref type="bibr" target="#b78">[79]</ref> and Occluded-DukeMTMC <ref type="bibr" target="#b39">[40]</ref>.</p><p>The Market1501 contains 1501 person identities captured by six different cameras on campus. In the training set, 12936 images for 751 persons are used. There are 19732 and 3368 images of the rest 750 person identities for gallery and query in the testing set.</p><p>The DukeMTMC-reID benchmark consists of 1812 person identities collected by eight synchronized cameras from campus. There are 16522 images of 702 identities in the training set. There are 17661 and 2228 images of the other 702 identities for gallery and query in the test set.</p><p>The MSMT17 contains 4101 person identities captured by a 15-camera network, including 12 outdoor and 3 indoor cameras on campus. The training set includes 32621 images of 1041 identities. There are 82161 and 11659 images of the other 3060 identities for the gallery and query in the test set.</p><p>In P-DukeMTMC-reID, there are 12927 images of 665 person identities for training, including 2647 images with occlusion and 10280 images without occlusion. There are 11216 images from 634 person identities for test, including 2163 images with occlusion for query and 9053 images without occlusion forming the gallery.</p><p>The Occluded-DukeMTMC contains 15618 images of 702 person identities in the training set, 17661 images of 1110 person identities in the gallery, and 2210 images of 519 person identities in the query.</p><p>The Cumulative Matching Characteristics (CMC) <ref type="bibr" target="#b16">[17]</ref> and mean Average Precision (mAP) <ref type="bibr" target="#b68">[69]</ref> are reported. We use the Rank-scores to represent the CMC curve. All the experiments are performed in a single query setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the PyTorch toolbox, FastReID <ref type="bibr" target="#b21">[22]</ref>, to achieve the proposed LDS. Additionally, we use the ResNet-ibn <ref type="bibr" target="#b19">[20]</ref> [42] as our backbone and initialize it by the ImageNet <ref type="bibr" target="#b10">[11]</ref> pre-trained model. The non-local layer <ref type="bibr" target="#b55">[56]</ref> is also employed in our backbone. Each person image is resized to 384?128. We set the batch size to 64 and use Adam <ref type="bibr" target="#b28">[29]</ref> with initialized learning rate 3.5 ? 10 ?4 to train each benchmark for 60 epochs. We use the cosine annealing part of the SGDR <ref type="bibr" target="#b33">[34]</ref> to adjust the learning rate. We also freeze the backbone in the first 2000 iterations for each benchmark to train the network. Then we train the whole multi-branch network for the rest iterations. <ref type="table" target="#tab_2">Table 1</ref> represents the performance comparisons between the proposed LDS and other state-of-the-art methods on three popular benchmarks in terms of CMC accuracy and mAP scores. These methods are within two yeas and include nine attention-based methods, six semantics-based methods, four stripe/part-related methods, three multi-branch networks, and nine other kinds of methods. They are all trained on the standard training sets without depending on additional images or labels. Early literature often used the re-rank <ref type="bibr" target="#b71">[72]</ref> technique. This technique can effectively adjust the order of image candidates and improve the mAP scores. In <ref type="table" target="#tab_2">Table 1</ref>, we also present the performance of the proposed LDS with the re-rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performances on Market1501. In table 1, compared to</head><p>the other state-of-the-art methods, the proposed LDS achieves competitive results. There are other three multi-branch methods, PTL <ref type="bibr" target="#b62">[63]</ref>, CAMA <ref type="bibr" target="#b61">[62]</ref>, and HBFP-Net <ref type="bibr" target="#b32">[33]</ref>. These methods employed the feature map from different layers to form a rich feature representation. Meanwhile, our method gives each branch a different perceptive ability by feeding them images with different characteristics. Thus, the proposed LDS is more easily implemented. And LDS also achieves a better performance than them. The proposed LDS with re-rank also achieves better performance. These  <ref type="bibr" target="#b0">1</ref> The best results are in bold. <ref type="bibr" target="#b1">2</ref> The metric 'R1' is the abbreviation of 'Rank-1'.</p><p>extensive comparisons demonstrate the effectiveness of our scheme.</p><p>Performances on DukeMTMC-reID. <ref type="table" target="#tab_2">Table 1</ref> shows that the proposed LDS achieves state-of-the-art performance. Compared to the best competitor, DSFL <ref type="bibr" target="#b43">[44]</ref>, our method achieves performance improvement of 1.3% and 1.4% on the metric of Rank-1 and mAP, respectively. For the re-rank counterparts, we conduct performance improvement of 4.4% and 4.7% on the metric of Rank-1 and mAP compared to the best competitor, DCDS <ref type="bibr" target="#b1">[2]</ref>. These comparisons demonstrate our scheme achieves considerable improvement compared to other state-of-the-art methods.</p><p>Performances on MSMT17. <ref type="table" target="#tab_2">Table 1</ref> also represents the proposed LDS achieves the state-of-the-art performance. For the metric of Rank-1, we achieve a performance improvement of 2.3% compared to the best competitor, DSFL <ref type="bibr" target="#b43">[44]</ref>. For the metric of mAP, we achieve a performance improvement of 6.4% compared to the best competitor, ABD-Net <ref type="bibr" target="#b6">[7]</ref>. For the re-rank version, our scheme achieves significant improvement in the metric of mAP, i.e., 18.29%, compared to the best competitor, SFT <ref type="bibr" target="#b35">[36]</ref>. These comparisons demonstrate the effectiveness of our method. 1 ' ?': reimplemented by us. <ref type="bibr" target="#b1">2</ref> The metric 'R1' is the abbreviation of 'Rank-1'. <ref type="table" target="#tab_3">Table 2</ref> shows the ablation study. The baseline is a onebranch network and trained using classification loss and triplet loss, following the same backbone and training parameters with the proposed LDS. We also use DML <ref type="bibr" target="#b64">[65]</ref> as the baseline of the multi-branch network. The evaluation of DML also uses the concatenated features from different branches. In table 2, the LDS-( ) denotes the ? configuration of the LDS with branches, and the DML-denotes the DML with branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, the proposed homologous input ensures the different branches have the same source image. The LDS with two branches has one master branch and one servant branch, and the LDS with three branches has one master branch and two servant branches. These servant branches are utilized to deal with occluded and scale variation scenes.</p><p>Effectiveness of Proposed Scheme. We make three comparisons to demonstrate the effectiveness of the proposed scheme. These comparisons are between the baseline and the LDS-2 <ref type="bibr" target="#b4">(5)</ref> , between the DML-2 and the LDS-2 <ref type="bibr" target="#b4">(5)</ref> , between the DML-3 and the LDS-3 <ref type="bibr" target="#b2">(3)</ref> . Compared to the baseline, the LDS-2 <ref type="bibr" target="#b4">(5)</ref> configuration achieves noticeable performance improvement, i.e., an average Rank-1 improvement of 2.5% and an average mAP improvement of 7% for the three benchmarks. Compared to the DML-2 <ref type="bibr" target="#b64">[65]</ref>, the LDS-2 <ref type="bibr" target="#b4">(5)</ref> has an average Rank-1 score of 1.2% and an average mAP score of 4.4% superiority. Compared to DML-3, the LDS-3 <ref type="bibr" target="#b2">(3)</ref> achieves an average Rank-1 score of 1.5% and an average mAP score of 4.5% performance improvement. These comparisons demonstrate the effectiveness of the proposed scheme over the one-branch and multi-branch networks.</p><p>Effectiveness of Proposed Random Scaling. We make another three comparisons to demonstrate the effectiveness of the proposed random scaling. These comparisons are between the baseline and the LDS-2 <ref type="bibr" target="#b2">(3)</ref> , between the DML-2 and the LDS-2 <ref type="bibr" target="#b2">(3)</ref> , between the LDS-2 <ref type="bibr" target="#b4">(5)</ref> and the LDS-3 <ref type="bibr" target="#b2">(3)</ref> . Compared to the baseline, the LDS-2 (3) configuration achieves an average Rank-1 improvement of 1.6% and average mAP improvement of 3.4% for the three benchmarks. Compared to the DML-2, the LDS-2 <ref type="bibr" target="#b2">(3)</ref> improves an average Rank-1 score of 0.3% and an average mAP score of 0.9%. Compared to the LDS-2 <ref type="bibr" target="#b4">(5)</ref> , LDS-3 <ref type="bibr" target="#b2">(3)</ref> achieves an average Rank-1 score of 0.3% and an average mAP score of 0.2% performance improvement. These comparisons demonstrate the proposed random scaling can effectively improve the performance. We also make another two comparisons, which are between the LDS-2 <ref type="bibr" target="#b1">(2)</ref> and the DML-2, between the LDS-2 <ref type="bibr" target="#b2">(3)</ref> and the DML-2. The LDS-2 <ref type="bibr" target="#b1">(2)</ref> achieves inferior performance compared to the DML-2. Although introducing more scale variations, random scaling introduces an additional misalignment problem when zoom value is between 0.9 and 1.0. In this situation, the misalignment problem gets severer, which results in a worse performance. After applying the homologous input, the LDS-2 <ref type="bibr" target="#b2">(3)</ref> achieves better performance than the DML-2.</p><p>Effectiveness of Proposed Homologous Input. We make two comparisons to demonstrate the effectiveness of the proposed homologous input. These comparisons are between the DML-2 and the LDS-2 <ref type="bibr" target="#b0">(1)</ref> , between the DML-3 and the LDS-3 <ref type="bibr" target="#b0">(1)</ref> . Compared to the DML-2, the LDS-2 (1) employs the homologous input and achieves an average Rank-1 score of 0.5% and an average mAP score of 1.0% performance improvement on the three benchmarks. The comparisons between the DML-3 and LDS-3 <ref type="bibr" target="#b0">(1)</ref> also have similar performance. We explain these performance improvements below. In the DML, the different optimization processes of each branch are mainly derived from the random initialization of the non-local layer and the random data augmentation. The proposed homologous input makes each branch have the same source images and avoids the misalignment problem introduced by the random data augmentation. The random initialization of the non-local layer ensures the branches have a different learning process. Therefore, employing the homologous input can help to achieve a  <ref type="bibr" target="#b4">(5)</ref> achieves better performance than scale variation, i.e., LDS-2 <ref type="bibr" target="#b2">(3)</ref> . This phenomenon may be caused by the fact that the occlusion scenes are more common than the scale variation scenes in the three benchmarks. Or the random erasing can increase the diversity of input images more effectively than the proposed random scaling.</p><p>Effectiveness in Occluded Scenes. To verify effectiveness of the proposed scheme in the occluded scenes, we train LDS-2 (5) on the P-DukeMTMC-reID and the Occluded-DukeMTMC benchmarks. These two large-scale benchmarks include training sets for model learning. The results are illustrated in <ref type="table" target="#tab_4">table 3 and table 4</ref>. The baseline method and the DML with two branches are also trained on these two benchmarks. The performances on these two largescale benchmarks demonstrate the effectiveness of the proposed scheme in the occluded scenes. In table 3, the proposed LDS 2 <ref type="bibr" target="#b4">(5)</ref> achieves state-of-the-art performance on P-DukeMTMC-reID benchmark under a supervised setting. In table 3, PCB [50] is a stripe/part-related method, and PVPM <ref type="bibr" target="#b13">[14]</ref> utilized the pose-guided attention to mine the part visibility for the occluded ReID task. Based on the strong baseline method, the proposed LDS 2 <ref type="bibr" target="#b4">(5)</ref> improves the performance further. In table 4, the proposed LDS 2 <ref type="bibr" target="#b4">(5)</ref> achieves state-of-the-art performance on Occluded-DukeMTMC benchmark under a supervised setting. In table 4, PGFA <ref type="bibr" target="#b39">[40]</ref> exploited the pose landmarks to disentangle the visible region from the occlusion noise. HOReID <ref type="bibr" target="#b54">[55]</ref> utilized the keypoint information to obtain a local-feature graph to learn the high-order relation and topology knowledge. Compared with them, our method employs a simple idea and also achieves a better performance. Effectiveness of Mutual Learning. Although multiple branches in our scheme employ mutual learning to transfer knowledge to each branch, we also propose master-servant learning (MS Learning). In MS learning, the knowledge communication is only between the master branch and servant branch. And there is no knowledge communication between the different servant branches. In table 5, the performance comparisons between the proposed LDS using mutual learning and MS learning are listed. The LDS with MS learning achieves better performance than the DML-3. However, the LDS with MS learning is inferior to the LDS with mutual learning. We explain these performances below. For master-servant learning, the knowledge from different servant branches is indirectly communicated through the intermediate master branch. For mutual learning, the servant branches have direct communication, and the experiment results indicate this direct communication is more effective. Therefore, we apply mutual learning in our scheme to promote each branch. Qualitative Analysis. We show the visual comparisons of retrieval images in <ref type="figure" target="#fig_4">Fig. 5</ref>. We select a query image with occlusion from the MSMT17 benchmark and compare the retrievals of different methods, including baseline, DML, and  <ref type="figure">Figure 6</ref>: Activation maps of images in <ref type="figure" target="#fig_4">Fig. 5</ref>. In each row, the first one is the original input image. The second, third, and fourth ones are the grad-CAM++ <ref type="bibr" target="#b2">[3]</ref> results from the models of baseline method, DML, and the proposed LDS, respectively.</p><p>the proposed LDS. In <ref type="figure" target="#fig_4">Fig. 5</ref>, the baseline method neglects many obvious correct results. The DML method identifies those easy image samples. However, it fails in the scale variation scenes, e.g., the last result in 5(d). These results demonstrate the effectiveness of our scheme in occluded scenes and scale variation scenes. To further analyze the learning ability of different models, we show the activation maps of different methods for the same input image, as illustrated in <ref type="figure">Fig. 6</ref>. Since the DML and the proposed LDS have three branches, we only show the activation map of the first branch. For each example, the activation map of the proposed LDS shows better responses than the others. These comparisons explain why the proposed LDS performs better and demonstrate its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose to learn to disentangle scenes for the ReID task. This scheme employs a divide-and-conquer strategy for the ReID task. Concretely, we use two selfsupervision operations to generate new image with the characteristics of occluded and scale variation scenes. Then we utilize two servant branches to deal with them. In this way, the burden of each branch is relieved. We also use a master branch to handle the general scenes. Mutual learning is employed to promote each branch. Through collaborative learning, the servant branch learns the missing information through guidance from the master branch. Moreover, the knowledge from the servant branches makes the master branch more robust. Extensive experimental results show that our method outperforms the existing one-branch and multi-branch networks and achieves state-of-the-art performances on three ReID benchmarks and two large-scale occluded ReID benchmarks. Additionally, the ablation study also validates our scheme can significantly improve performance in various scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two typical challenging scenes in ReID. The top and bottom rows show normal and difficult scenes. (a) Occluded scenes. The occluded areas are within the yellow dotted lines. (b) Scale variation scenes. The sizes of pedestrians in the yellow boxes change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 3 :</head><label>13</label><figDesc>(a) Input (b) 0.8 ? 0.9 (c) 0.9 ? 1.0 (d) 1.0 ? 1.Random scaling. (a) Original input image. (b), (c), (d) Examples when the zoom scale in different ranges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustrations of heterologous input and the proposed homologous input. The homologous input is applied to our scheme inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Retrieval image examples. The query image is from the MSMT17 benchmark. The query and retrievals contain obvious occlusion and scale variation. The correct and incorrect ones are in a green and red box, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 90.37 91.56 82.50 86.54 67.21</head><label>1</label><figDesc>Performance comparisons with state-of-the-art methods on Market1501, DukeMTMC-reID, and MSMT17.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Publication</cell><cell>Market1501 R1 mAP</cell><cell cols="2">DukeMTMC-reID R1 mAP</cell><cell>MSMT17 R1 mAP</cell></row><row><cell></cell><cell>BAT [12]</cell><cell>ICCV19</cell><cell cols="2">95.10 87.40 87.70</cell><cell>77.30</cell><cell>79.50 56.80</cell></row><row><cell></cell><cell>ABD-Net [7]</cell><cell>ICCV19</cell><cell cols="2">95.60 88.28 89.00</cell><cell>78.59</cell><cell>82.30 60.80</cell></row><row><cell></cell><cell>CAR [77]</cell><cell>ICCV19</cell><cell cols="2">96.10 84.70 86.30</cell><cell>73.10</cell></row><row><cell>Attention-based</cell><cell>SCAL (spatial) [5] SONA 2+3 -Net [58] MHN-6 (PCB) [4]</cell><cell>ICCV19 ICCV19 ICCV19</cell><cell cols="2">95.40 88.90 89.00 95.58 88.83 89.38 95.10 85.00 89.10</cell><cell>79.60 78.23 77.20</cell></row><row><cell></cell><cell>IANet [25]</cell><cell>CVPR19</cell><cell cols="2">94.40 83.10 87.10</cell><cell>73.40</cell><cell>75.50 46.80</cell></row><row><cell></cell><cell>SCSN (3 stage) [8]</cell><cell>CVPR20</cell><cell cols="2">95.70 88.50 90.10</cell><cell>79.00</cell><cell>83.00 58.00</cell></row><row><cell></cell><cell>RGA-SC [68]</cell><cell>CVPR20</cell><cell>96.10 88.40</cell><cell></cell><cell></cell><cell>80.30 57.50</cell></row><row><cell></cell><cell>2 -Net (+triplet loss) [18]</cell><cell>ICCV19</cell><cell cols="2">95.20 85.60 86.50</cell><cell>73.10</cell></row><row><cell></cell><cell>DSA-reID [67]</cell><cell>CVPR19</cell><cell cols="2">95.70 87.60 86.20</cell><cell>74.30</cell></row><row><cell>Semantics-</cell><cell>SAN [28]</cell><cell>AAAI20</cell><cell cols="2">96.10 88.00 87.90</cell><cell>75.50</cell><cell>79.20 55.70</cell></row><row><cell>based</cell><cell>DLBC [6]</cell><cell cols="3">ACM MM20 94.60 87.40 88.70</cell><cell>78.50</cell><cell>78.20 55.60</cell></row><row><cell></cell><cell>ISP [78]</cell><cell>ECCV20</cell><cell cols="2">95.30 88.60 89.60</cell><cell>80.00</cell></row><row><cell></cell><cell>Auto-ReID [46]</cell><cell>ICCV19</cell><cell>94.50 85.10</cell><cell></cell><cell></cell><cell>78.20 52.50</cell></row><row><cell>Stripe/Part-</cell><cell>BDB + Cut [10]</cell><cell>ICCV19</cell><cell cols="2">95.30 86.70 89.00</cell><cell>76.00</cell></row><row><cell>related</cell><cell>RRID [43]</cell><cell>AAAI20</cell><cell cols="2">95.20 88.90 89.70</cell><cell>78.60</cell></row><row><cell></cell><cell>HAA [59]</cell><cell cols="3">ACM MM20 95.80 89.50 89.00</cell><cell>80.40</cell></row><row><cell></cell><cell>SFT [36]</cell><cell>ICCV19</cell><cell cols="2">93.40 82.70 86.90</cell><cell>73.20</cell><cell>73.60 47.60</cell></row><row><cell></cell><cell>DCDS [2]</cell><cell>ICCV19</cell><cell cols="2">94.81 85.80 87.50</cell><cell>75.50</cell></row><row><cell></cell><cell>VCFL [30]</cell><cell>ICCV19</cell><cell>89.25 74.48</cell><cell></cell><cell></cell></row><row><cell>Others</cell><cell>MVP Loss [48] OSNet [76]</cell><cell>ICCV19 ICCV19</cell><cell cols="2">91.40 80.50 83.40 94.80 84.90 88.60</cell><cell>70.00 73.50</cell><cell>71.30 46.30 78.70 52.90</cell></row><row><cell></cell><cell>DSFL [44]</cell><cell cols="3">ACM MM20 96.20 89.90 90.20</cell><cell>81.10</cell><cell>84.20 60.70</cell></row><row><cell></cell><cell>NEWTH [39]</cell><cell>NeurIPS20</cell><cell>95.60 89.40</cell><cell></cell><cell></cell><cell>71.50 53.10</cell></row><row><cell></cell><cell>M 3 + HA-CNN [75]</cell><cell>CVPR20</cell><cell cols="2">96.50 85.20 87.10</cell><cell>72.20</cell><cell>74.30 43.80</cell></row><row><cell></cell><cell>CtF [54]</cell><cell>ECCV20</cell><cell cols="2">93.70 84.90 87.60</cell><cell>74.80</cell></row><row><cell></cell><cell>DML [65]</cell><cell>CVPR18</cell><cell>89.34 70.51</cell><cell></cell><cell></cell></row><row><cell></cell><cell>PTL + MGN [63]</cell><cell>IJCAI19</cell><cell cols="2">94.83 87.34 89.36</cell><cell>79.16</cell><cell>73.12 41.38</cell></row><row><cell>multi-branch</cell><cell>CAMA (N=3) [62]</cell><cell>CVPR19</cell><cell cols="2">94.70 84.50 85.80</cell><cell>72.90</cell></row><row><cell></cell><cell>HBFP-Net [33]</cell><cell cols="3">ACM MM20 95.80 89.80 89.50</cell><cell>80.20</cell></row><row><cell cols="5">Proposed LDS VCFL [30] + Re-rank DCDS [2] + Re-rank Auto-ReID [46] + Re-rank 95.84 + Re-rank ICCV19 90.91 86.67 ICCV19 95.40 93.30 88.50 ICCV19 95.40 94.20 SFT [36] + Re-rank ICCV19 93.50 90.60 88.30</cell><cell>86.10 83.30</cell><cell>76.10 60.80</cell></row><row><cell></cell><cell>MVP Loss [48] + Re-rank</cell><cell>ICCV19</cell><cell cols="2">93.30 90.90 86.30</cell><cell>83.90</cell></row><row><cell></cell><cell>Proposed LDS + Re-rank</cell><cell></cell><cell cols="2">96.17 94.89 92.91</cell><cell>91.00</cell><cell>88.35 79.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Performance comparisons with the baseline and DML.</figDesc><table><row><cell cols="2"># Branch Method</cell><cell>Random Erasing</cell><cell>Random Scaling</cell><cell>Homologous Input</cell><cell>Market1501 R1 mAP</cell><cell cols="2">DukeMTMC-reID R1 mAP</cell><cell>MSMT17 R1 mAP</cell></row><row><cell cols="2">1-Branch Baseline</cell><cell></cell><cell></cell><cell></cell><cell cols="2">95.01 86.57 88.78</cell><cell>76.29</cell><cell>81.65 55.53</cell></row><row><cell></cell><cell>DML-2[65] ?</cell><cell></cell><cell></cell><cell></cell><cell cols="2">95.34 87.76 89.59</cell><cell>77.91</cell><cell>84.30 60.30</cell></row><row><cell></cell><cell>LDS-2 (1)</cell><cell></cell><cell></cell><cell>?</cell><cell cols="2">95.55 88.28 90.44</cell><cell>79.21</cell><cell>84.92 61.62</cell></row><row><cell>2-Branch</cell><cell>LDS-2 (2) LDS-2 (3)</cell><cell></cell><cell>? ?</cell><cell>?</cell><cell cols="2">95.37 87.75 89.00 95.61 88.19 89.99</cell><cell>77.80 79.31</cell><cell>83.94 59.58 84.73 61.35</cell></row><row><cell></cell><cell>LDS-2 (4)</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">95.75 89.94 90.93</cell><cell>81.74</cell><cell>86.10 66.24</cell></row><row><cell></cell><cell>LDS-2 (5)</cell><cell>?</cell><cell></cell><cell>?</cell><cell cols="2">95.55 90.24 90.98</cell><cell>82.17</cell><cell>86.49 67.05</cell></row><row><cell></cell><cell>DML-3[65] ?</cell><cell></cell><cell></cell><cell></cell><cell cols="2">95.58 87.95 89.18</cell><cell>78.01</cell><cell>84.39 60.55</cell></row><row><cell>3-Branch</cell><cell>LDS-3 (1) LDS-3 (2)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">95.16 88.25 89.95 95.72 89.76 90.31</cell><cell>79.55 80.88</cell><cell>85.16 62.46 85.89 65.25</cell></row><row><cell></cell><cell>LDS-3 (3)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>95.84 90.37</cell><cell>91.56</cell><cell>82.50</cell><cell>86.54 67.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Performance comparisons on the P-DukeMTMC-reID benchmark under a supervised setting.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell cols="4">Rank-1 Rank-5 Rank-10 mAP</cell></row><row><cell>PCB [50]</cell><cell>ECCV2018</cell><cell>79.4</cell><cell>87.1</cell><cell>90.0</cell><cell>63.9</cell></row><row><cell cols="2">PVPM [14] CVPR2020</cell><cell>85.1</cell><cell>91.3</cell><cell>93.3</cell><cell>69.9</cell></row><row><cell>Baseline</cell><cell></cell><cell>88.2</cell><cell>93.1</cell><cell>94.3</cell><cell>76.4</cell></row><row><cell>DML-2</cell><cell></cell><cell>90.5</cell><cell>94.1</cell><cell>95.1</cell><cell>78.8</cell></row><row><cell>LDS-2 (5)</cell><cell></cell><cell>91.9</cell><cell>95.2</cell><cell>96.3</cell><cell>82.9</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Performance comparisons on the Occluded-DukeMTMC</cell></row><row><cell cols="4">benchmark under a supervised setting.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Venue</cell><cell cols="4">Rank-1 Rank-5 Rank-10 mAP</cell></row><row><cell>PGFA [40]</cell><cell>ICCV2019</cell><cell>51.4</cell><cell>68.6</cell><cell>74.9</cell><cell>37.3</cell></row><row><cell cols="2">HOReID [55] CVPR2020</cell><cell>55.1</cell><cell></cell><cell></cell><cell>43.8</cell></row><row><cell>Baseline</cell><cell></cell><cell>62.6</cell><cell>75.1</cell><cell>80.6</cell><cell>50.2</cell></row><row><cell>DML-2</cell><cell></cell><cell>63.6</cell><cell>76.4</cell><cell>80.5</cell><cell>51.9</cell></row><row><cell>LDS-2 (5)</cell><cell></cell><cell>64.3</cell><cell>77.1</cell><cell>82.6</cell><cell>55.7</cell></row><row><cell cols="4">noticeable performance improvement.</cell><cell></cell><cell></cell></row></table><note>Random Erasing vs. Proposed Random Scaling. Ap- plying different servant branches brings different performance improvements. For the two-branch version in the table 2, ap- plying servant branch for occlusion, i.e., LDS-2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Performance comparisons between the proposed LDS using mutual learning and master-servant learning on different benchmarks.</figDesc><table><row><cell></cell><cell cols="2">Market1501</cell><cell cols="2">DukeMTMC-reID</cell><cell cols="2">MSMT17</cell></row><row><cell></cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>DML-3 [65]</cell><cell cols="3">95.58 87.95 89.18</cell><cell>78.01</cell><cell cols="2">84.39 60.55</cell></row><row><cell>w/ MS Learning</cell><cell cols="3">95.64 89.89 91.34</cell><cell>81.97</cell><cell cols="2">86.27 66.25</cell></row><row><cell cols="4">w/ Mutual Learning 95.84 90.37 91.56</cell><cell>82.50</cell><cell cols="2">86.54 67.21</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9163" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep constrained dominant sets for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Alemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9855" to="9864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixed high-order attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep local binary coding for person re-identification by delving into the details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abd-net: Attentive but diverse person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilinear attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8030" to="8039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Texture semantically aligned with visibility-aware for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3771" to="3779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pose-guided visible part matching for occluded person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11744" to="11752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep spatial pyramid features collaborative reconstruction for partial person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1879" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01526</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS), Citeseer</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS), Citeseer</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3642" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual-alignment feature embedding for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7073" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fastreid: A pytorch toolbox for general instance re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02631</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Eanet: Enhancing alignment for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11369</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Uncertainty-aware multishot knowledge distillation for image-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05197</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semanticsaligned representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13143</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">View confusion feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03189</idno>
		<title level="m">Deep hyperspherical learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical bi-directional feature perception network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4289" to="4298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification with shared-specific feature transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13379" to="13389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spectral feature transformation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4976" to="4985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Alignedreid++: Dynamically matching local information for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Person reidentification with expanded neighborhoods distance re-ranking. Image and Vision Computing 95</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">103875</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The dilemma of trihard loss and an element-weighted trihard loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xinggao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Knowledge distillation for end-to-end person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Munjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01058</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Relation network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09318</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discriminative spatial feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Robust reidentification by multiple views knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04174</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mvp matching: A maximum-value perfect matching for mining hard samples, with application to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6737" to="6747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Faster person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06826</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">High-order information matters: Learning relation and topology for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Black reid: A head-shoulder descriptor for the challenging problem of person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08528</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Asymmetric co-teaching for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12597" to="12604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Willis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12978</idno>
		<title level="m">Mutualnet: Adaptive convnet via mutual learning from network width and resolution</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards rich feature discovery with class activation maps augmentation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1389" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Progressive transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02492</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Rgbir cross-modality person reid based on teacher-student gan model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Da Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07452</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Relationaware global attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3186" to="3195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bbn: Bilateralbranch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Cross-correlated attention networks for person re-identification. Image and Vision Computing 100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">103931</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Online joint multi-metric adaptation from frequent sharing-subset mining for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2909" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Discriminative feature learning with consistent attention regularization for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8040" to="8049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Identity-guided human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13467</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Occluded person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">A novel teacher-student learning framework for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03253</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
