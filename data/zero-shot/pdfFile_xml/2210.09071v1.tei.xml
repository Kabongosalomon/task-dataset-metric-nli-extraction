<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Attention Everywhere: Monocular Depth Prediction with Skip Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Agarwal</surname></persName>
							<email>ashutosh.agarwal@cse.iitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">IIT Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Arora</surname></persName>
							<email>chetan@cse.iitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">IIT Delhi</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention Attention Everywhere: Monocular Depth Prediction with Skip Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular</head><p>Depth Estimation (MDE) aims to predict pixel-wise depth given a single RGB image. For both, the convolutional as well as the recent attention-based models, encoder-decoder-based architectures have been found to be useful due to the simultaneous requirement of global context and pixel-level resolution. Typically, a skip connection module is used to fuse the encoder and decoder features, which comprises of feature map concatenation followed by a convolution operation. Inspired by the demonstrated benefits of attention in a multitude of computer vision problems, we propose an attention-based fusion of encoder and decoder features. We pose MDE as a pixel query refinement problem, where coarsest-level encoder features are used to initialize pixel-level queries, which are then refined to higher resolutions by the proposed Skip Attention Module (SAM). We formulate the prediction problem as ordinal regression over the bin centers that discretize the continuous depth range and introduce a Bin Center Predictor (BCP) module that predicts bins at the coarsest level using pixel queries. Apart from the benefit of image adaptive depth binning, the proposed design helps learn improved depth embedding in initial pixel queries via direct supervision from the ground truth. Extensive experiments on the two canonical datasets, NYUV2 and KITTI, show that our architecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively, along with an improved generalization performance by 9.4% on the SUNRGBD dataset. Code is available at https://github.com/ ashutosh1807/PixelFormer.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular Depth Estimation (MDE) is a well-studied topic in computer vision. State-of-the-art (SOTA) techniques for MDE are based on encoder-decoder style Convolutional Neural Network (CNN) architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>. Due to the inherently local nature of a convolution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>NeWCRFs <ref type="bibr" target="#b33">[34]</ref> Ours GT <ref type="figure" target="#fig_0">Figure 1</ref>: We observe that depth boundaries in state-of-theart <ref type="bibr" target="#b33">[34]</ref> align well with object boundaries, but the depth label is often incorrect. Note the confusion for the middle pillow in the first row and the bed in the second row. We propose skip attention module for fusing long range context from the decoder into the encoder features, which successfully mitigates the discrepancy.</p><p>kernel, early-stage feature maps have higher resolution but lack a global receptive field. The feature pyramidal-based decoder mitigates the issue by fusing low-resolution, semantically rich decoder features with the higher resolution but semantically weaker encoder features via a top-down path-way and lateral connections called skip connections <ref type="bibr" target="#b17">[18]</ref>. Inline with the recent success of transformers, many latest works have used a self-attention based architectures for MDE <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. Self-attention increases the receptive field and allows to capture long-range dependencies in feature maps. Practically, it is challenging to use self-attention for high-resolution feature maps due to memory and computational constraints. Hence, the current SOTA <ref type="bibr" target="#b33">[34]</ref> uses window based attention using Swin transformer-based encoder backbone <ref type="bibr" target="#b18">[19]</ref> to improve efficiency. We observe that SOTA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref> techniques are highly accurate in aligning depth edges with the object boundaries. However, there exists a confusion in giving the depth label to a pixel (c.f. <ref type="figure" target="#fig_0">Fig. 1</ref>). We posit this due to the inability of current techniques to effectively fuse highresolution local features from the encoder and global contextual features from the decoder. Typically such a fusion is achieved through a skip connection module implementing feature concatenation followed by a convolution operation. Weights of the convolution kernels are highly localized, which restricts the flow of semantic information from long ranges affecting the ability of the model to predict the correct depth label for a pixel. To mitigate the constraint, we introduce a skip-attention module (SAM) that helps integrate information using window-based cross-attention. SAM calculates self-similarity between pixel queries based on decoder features and their corresponding neighbors from the encoder features in a predefined window to attend to and aggregate information at a longer range. We implement the overall architecture as a pixel query refinement problem. We use the coarsest feature map from the encoder with maximum global information to initialize pixel queries using a Pixel Query Initialiser Module. The pixel queries are then refined with the help of a SAM module to finer scales. <ref type="bibr" target="#b0">[1]</ref> formulate the problem as a classification-regression one, in which the depth is predicted by a linear combination of bin centers discretized over the depth range. The bin centers are predicted adaptively per image, allowing the network to concentrate on the depth range regions that are more likely to occur in the scene of the input image. A vision transformer that aggregates global information from the output of another encoder-decoder-based transformer model is typically used to generate the bin centers. Since we pose MDE as a pixel query refinement problem starting from the coarsest resolution, we propose a lightweight Bin Center Module (BCP) that predicts bin centers based on the initial pixel queries. This is more efficient than decoding features and then attending again in current SOTA <ref type="bibr" target="#b0">[1]</ref>. The proposed design also helps embed the depth information into the initial pixel queries via direct ground truth supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent MDE techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>The specific contributions of this work are as follows: (1) We propose a novel strategy for predicting depth using a single image by viewing it as a pixel query refinement problem. (2) We introduce a Skip Attention Module (SAM) that uses a window-based cross-attention module to refine pixel queries from the decoder feature maps for cross-attending to higher resolution encoder features.</p><p>(3) We present a Bin Center Predictor (BCP) Module that estimates bin centers adaptively per image using the global information from the coarsest-level feature maps. This helps to provide direct supervision to initial pixel queries from ground truth depth, leading to better query embedding. (4) We combine the novel design elements in an encoder-decoder framework comprised of a vision transformer backbone. The proposed architecture called Pix-elFormer achieves state-of-the-art (SOTA) performance on indoor NYUV2 and outdoor KITTI datasets, improving the current SOTA by 5.3% and 3.9%, in terms of absolute relative error and square relative error, respectively. Additionally, PixelFormer improves the generalization performance by 9.4% over SOTA on the SUNRGBD dataset in terms of absolute relative error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>CNN based MDE Techniques: Eigen et al. <ref type="bibr" target="#b2">[3]</ref> first utilized CNN to predict depth from a single image by integrating global and local information. Song et al. <ref type="bibr" target="#b27">[28]</ref> have proposed a Laplacian pyramid-based model, and CLIFFNet <ref type="bibr" target="#b29">[30]</ref> a multi-scale convolutional fusion architecture to generate high-quality depth prediction. Yin et al. <ref type="bibr" target="#b32">[33]</ref> introduced a geometric constraint named virtual normal, and Naderi et al. <ref type="bibr" target="#b19">[20]</ref> proposed similarity between the RGB image and the corresponding depth map at the geometric edges to regularize predicted depth. Lee et al. <ref type="bibr" target="#b16">[17]</ref> enforce a model to learn structural information about the scene by learning the relationship between image patches close to each other. Whereas, Patil et al. <ref type="bibr" target="#b22">[23]</ref> exploit coplanar pixels to improve the predicted depth.</p><p>Transformer based MDE Models: Recent works have used Vision Transformer (ViT) architecture to improve the receptive field of a CNN in lower layers. Ranftl et al. <ref type="bibr" target="#b23">[24]</ref> uses a CNN to extract feature maps at 1 16 th resolution, which are passed to a vision transformer for global information aggregation. Bhat et al. <ref type="bibr" target="#b0">[1]</ref> uses a CNN-based encoderdecoder backbone and a ViT model to predict adaptive bins and pixel-level depths. NeWCRFs <ref type="bibr" target="#b33">[34]</ref> uses Swin Transformer backbone <ref type="bibr" target="#b18">[19]</ref> with CRFs at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDE as Classification Vs Regression Task:</head><p>Modeling MDE as a regression problem leads to suboptimal solutions and faces convergence issues. Huan et al. <ref type="bibr" target="#b3">[4]</ref> first introduced the depth prediction task as a classificationregression problem solved by a CNN-based classification network in which the depth is predicted as a linear combination of bin centers discretized over the depth range. Recently, Bhat et al. <ref type="bibr" target="#b0">[1]</ref> proposed the prediction of bin centers adaptively per image using a ViT transformer on top of a transformer-based encoder-decoder backbone. In this work, we propose a single encoder-decoder backbone with a lightweight BCP module to predict bin centers using the coarsest resolution encoder feature maps.</p><p>Skip Connections: Skip connections were introduced by UNet <ref type="bibr" target="#b24">[25]</ref> to forward high-resolution information from the encoder to the decoder via feature fusion. However, a naive fusion of early encoder and late decoder information is hindered by their semantic gap <ref type="bibr" target="#b34">[35]</ref>. MultiResUnet <ref type="bibr" target="#b11">[12]</ref> replaces simplistic skip connection with a series of residual blocks to alleviate the semantic gap. Attention U-Net <ref type="bibr" target="#b20">[21]</ref> suppresses irrelevant regions in an input image while highlighting salient features useful for a specific task before the feature fusion. SANet <ref type="bibr" target="#b30">[31]</ref> uses attention to complete the point cloud at the decoder stage by injecting information <ref type="figure">Figure 2</ref>: Detailed Architecture of our proposed approach PixelFormer : Given an input image, a vision transformerbased encoder first extracts the multiscale feature maps. The feature map with the coarsest resolution (E 4 ) is given as input to the PQI module. The PQI module produces initial pixel queries that are given as input to the BCP module that produces the bin widths. The initial pixel queries are then refined to higher resolutions using the SAM modules. Finally, a convolution operation followed by a softmax is applied to get probability distribution per pixel over the bin centers.</p><p>using the encoder. In this work, we use skip attention to retrieve high-resolution details from encoder features using global contextual queries based on the decoder features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methodology</head><p>Problem Definition: Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, we model MDE as a classification-regression task. Given an input image I, the network predicts bin widths, b, that discretize continuous depth range into an n bins number of intervals. The bins are predicted adaptively for each image. The final n bins dimensional probability vector is treated as the weight vector, and the depth, d i , at a pixel i, is computed as a linear combination of the probability scores at the pixel with the predicted per-image bin-centers.</p><p>Architecture Overview: The input image I is first fed to a Swin Transformer <ref type="bibr" target="#b18">[19]</ref> that uses multiple layers of windowbased self-attention to extract the feature maps representing the image at resolution scale { 1 4 , 1 8 , 1 16 , 1 32 } w.r.t. I. The feature maps have a global receptive field due to the inherent nature of a ViT backbone. The feature map at 1 32 th scale is then fed to the proposed Pixel Query Initialiser (PQI) module. The PQI module aggregates the entire scene information using multi-scale global average pooling to initialize pixel queries. The pixel queries are hierarchically refined with the encoder feature maps using the proposed Skip Attention Module (SAM) deployed at various stages to predict per-pixel probability distribution over the bin centers. The initialized pixel queries are also sent to the Bin Center Predictor (BCP) proposed in this work. BCP predicts bin centers adaptively per image using global average pooling followed by MLP layers. <ref type="figure">Fig. 2</ref> gives a pictorial description.</p><p>Pixel Query Initialiser (PQI): The Pixel Query Initialiser (PQI) module aggregates the global information of the scene into each pixel-level embedding. The image feature map with the coarsest resolution, which contains the most essential details in the scene, is fed as an input to the PQI module. Given an input feature map of size H 32 ? W 32 ? C 4 , the PQI module uses pyramid spatial pooling (PSP) <ref type="bibr" target="#b9">[10]</ref> with an adaptive global pooling at scales 1, 2, 3, and 6. The feature maps are then upsampled to 1 32 th scale and concatenated. A convolution operation is then performed to integrate the global information effectively, as in <ref type="bibr" target="#b33">[34]</ref>, to get initial pixel queries</p><formula xml:id="formula_0">Q I of size H 32 ? W 32 ? C, where C = 512.</formula><p>Bin Center Predictor (BCP): Previous works <ref type="bibr" target="#b0">[1]</ref> have used a vision transformer (ViT) to predict bin centers that discretize the image depth into a fixed number of intervals. ViT divides the image feature map into 16 ? 16 patches and uses self-attention layers to exchange information among the patches. The first embedding is passed through an MLP head to predict the bin centers. Instead of decoding the feature map to high resolution and then using ViT, we propose to use the initial pixel queries to predict the bin centers. Apart from being more efficient, the proposed design helps in embedding the depth information into the pixel queries via direct ground truth supervision. Our BCP module consists of a simple Global Average Pooling followed by an MLP layer to predict the bin widths b of dimension n bins . Here, n bins denotes the number of adaptive bins per image. We use n bins = 256 for our model as suggested in <ref type="bibr" target="#b0">[1]</ref>. Given the pixel queries</p><formula xml:id="formula_1">Q I of size H 32 ? W 32 ? C, we predict: b = MLP(GAP(Q))<label>(1)</label></formula><p>Finally, the bin centers for the input image are computed as:</p><formula xml:id="formula_2">c (b i ) = d min + (d max ? d min ) b i 2 + i?1 j=1 b j ,<label>(2)</label></formula><p>i ? {1, . . . , n bins } Skip Attention Module (SAM) Overview: For a dense estimation task, coarse-level semantic features and fine-level details are both critical for an accurate estimation. Hence, similar to previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>, we also use a bottomdown approach that starts with the lowest resolution feature map, upsample it, and injects the fine-level details from the encoder feature map at a particular scale using skip connections. Typically skip connections use convolution operation after concatenating the encoder-decoder features. Unlike convolution operation to fuse the encoder-decoder features where the kernel weights are not adaptive as per the pixel location, we use the skip attention module (SAM), which uses self-similarity between the pixel queries and the corresponding encoder feature map, to effectively fuse the global-local features.</p><p>SAM Implementation: Given a pixel query mapQ i and the corresponding encoder features E i for a particular scale i, we first perform a 3 ? 3 convolution E i with D i channels on both E i andQ i so that the number of channels of the pixel queries generated from the decoder features is the same as the number of channels in the encoder feature maps. Post the convolutional operation, a query matrix Q is obtained fromQ i , and the key K and value V matrices are obtained from E i using the weights W q , W k and W v implemented using MLP layers. Since it's not computationally feasible for a query q i corresponding to the pixel query at location i to attend to all the keys in the matrix K, we restrict the attention to a window as suggested for Swin Transformer <ref type="bibr" target="#b18">[19]</ref>. Q, K, and V matrices are first divided into windows of size W ? W . Similar to <ref type="bibr" target="#b18">[19]</ref>, we use W = 7.</p><p>Let Q w , K w , and V w be the query, key, and the value corresponding to the pixels in window w. We compute the output as follows:</p><formula xml:id="formula_3">Attention(Q, K, V ) = Rearrange(Softmax(Q w K T w +B)V w ).</formula><p>Here, B denotes relative position bias. B is a learnable matrix of size w 2 ? w 2 , representing the relative position embedding corresponding to each query and key pair. The attention is computed for each window w after which the rearrange operation places the windows as per their respective spatial location in Q.</p><p>To embed the information corresponding to the various depth ranges, each pixel query is divided into H i heads, and the attention operation is applied for each head. Post attention, per pixel depth embeddings are aggregated using MLP layers. The residual connections post attention and MLP layers are added for smooth gradient flows. To summarise, givenQ i and E i for pixel query and encoder at level i: Decoder Architecture: As shown in <ref type="figure">Fig. 2</ref>, we start with an initial pixel queryQ I outputted from the PQI module. Q I is upsampled to twice the resolution size using Pixel Shuffle <ref type="bibr" target="#b25">[26]</ref> and sent as input to the SAM module along with the corresponding encoder feature E 4 . The initial pixel queries are refined to finer resolutions by attending to multiscale encoder feature maps at various resolutions through our proposed SAM module. For the given pixel query at levelQ i and the corresponding encoder feature E i ,</p><formula xml:id="formula_4">Q i = LayerNorm(Q i ) E i = LayerNorm(E i ) Q = W QQi , K = W K?i , V = W V?? Q i?1 = MultiheadAttention(Q, K, V ) +Q ? Q i?1 = MLP(Q i?1 ) +Q i?1 Q i?1 = MLP(Q i?1 ) +Q i + E i</formula><formula xml:id="formula_5">Q i = SAM(Upsample(Q i+1 , E i+1 ) i ? {0, 1, 2, 3}.</formula><p>Here,Q 4 is same asQ I . A convolution operation is performed onQ 0 to produce the final depth embedding F of size H 32 ? W 32 ? n bins . Finally, a pixel-wise softmax operation is applied to obtain the per bin probability distribution p bins :</p><formula xml:id="formula_6">p bins = Softmax(Conv(Q 0 ))<label>(3)</label></formula><p>The final depth is predicted by the linear combination of the bin centers weighted by the probability values:</p><formula xml:id="formula_7">d i = nbins k=1 c (b k ) p ik ,<label>(4)</label></formula><p>where d i is the predicted depth at pixel i, c (b k ) is the k th bin center, n bins are the number of bins, and p ik is the probability for bin center k for a pixel i.</p><p>Training loss: Following previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b0">1]</ref>, we use a scaled version of scaled version of the Scale-Invariant loss (SILog) <ref type="bibr" target="#b2">[3]</ref> to supervise our network. Given, the ground truth depth (d * i ) and the predicted depth (d i ) at a pixel location i, first the logarithmic distance between d i and d * i is calculated as:</p><formula xml:id="formula_8">g i = log(d i ) ? log(d * i ).</formula><p>The SIlog loss is then calculated as follows:</p><formula xml:id="formula_9">L SILog = ? 1 n i g 2 i ? ? n 2 i g i 2 .<label>(5)</label></formula><p>Here, n denotes the number of pixels in an image that have the ground truth values available. Following <ref type="bibr" target="#b0">[1]</ref>, we use ? = 0.85 and ? = 10 for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets and Evaluation</head><p>NYU Depth V2: NYUV2 <ref type="bibr" target="#b26">[27]</ref> is an indoor dataset containing 120K RGB and depth pairs with a size of 480 ? 640 acquired as video sequences from 464 indoor scenes using a Microsoft Kinect. We follow the official training/testing split to evaluate our method, where 249 scenes comprising 50K images are used for training, and 654 images from 215 scenes are used for testing. We use the center cropping proposed by Eigen et al. <ref type="bibr" target="#b2">[3]</ref>, with the depth maps having an upper bound of 10 meters. Our network outputs depth prediction with a resolution of 120 ? 160, which we upsample by 4? to match the ground truth resolution during training and testing.</p><p>KITTI Dataset: KITTI <ref type="bibr" target="#b6">[7]</ref> is an outdoor dataset that consists of stereo images and 3D scans from 61 scenes captured by multiple sensors mounted on top of a moving vehicle. The dataset contains an input RGB image with a 1241?375 pixels resolution, and the LIDAR scans correspond to it. We use the training/testing split defined by <ref type="bibr" target="#b2">[3]</ref> that consists of a subset of 26K left view images from the official Kitti dataset for the training and 697 test set images. To evaluate the test set, we use the crop defined by Garg et al. <ref type="bibr" target="#b5">[6]</ref> with the depth maps having an upper bound of 80 meters. We use bilinear interpolation to upsample the prediction to match the ground truth image resolution.</p><p>SUNRGB-D: SUNRGB-D <ref type="bibr" target="#b28">[29]</ref> is an indoor dataset collected using various sensors. It comprises 10335 real RGB-D images of room scenes. The training and testing sets contain 5285 and 5050 images, respectively. We use the official test set for evaluation purposes with an upper bound on the depth of 8 meters.</p><p>Evaluation Metrics: We use the standard metrics Average relative error (Abs Rel), Root mean squared error (RMSE), Average Log error (log 10 ), Threshold Accuracy (? i ) at thresholds ? i s = 1.25, 1.25 2 , 1.25 3 used in earlier works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34]</ref> to compare our method against state-ofthe-art. For KITTI evaluation, we additionally use Square relative error (Sq Rel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Implementation Details: The proposed method is implemented in Pytorch <ref type="bibr" target="#b21">[22]</ref>. We use Adam optimizer <ref type="bibr" target="#b13">[14]</ref> (? s= 0.9, 0.999), with a batch size of 8 and a weight decay of 10 ?2 . We use 20 epochs for both KITTI and NYUV2 datasets, with an initial learning rate of 4 ? 10 ?5 , which is decreased linearly to 4 ? 10 ?6 across the training iterations. Our model takes 30 minutes per epoch using 4 NVIDIA A100 GPUs. We use various data augmentation techniques like random rotation, horizontal flipping, changing the image brightness, and Cut Depth <ref type="bibr" target="#b12">[13]</ref>. We use the pre-trained weights of Swin-L <ref type="bibr" target="#b18">[19]</ref> to initialize our encoder backbone.</p><p>We follow a similar test protocol as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>, and output final depth values by averaging the predicted depth for the original image and its mirror image.</p><p>Results on NYUV2: <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Fig. 3</ref> show the quantitative and qualitative results, respectively, on the indoor dataset NYUV2 using our approach, named PixelFormer.</p><p>Following the test protocol of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>, and without additional training data, our method improves the Absolute relative error by 5.3% over SOTA. The performance gain is significant considering the saturated performance of the dataset in recent years. We see an improvement of 9.6% and 3.5% over the recently proposed methods <ref type="bibr" target="#b22">[23]</ref> and NeWCRFs, respectively, in terms of the RMSE error. We see in <ref type="figure">Fig. 3</ref> that PixelFormer produces more accurate depth maps than Adabins and NeWCRFs, which can be attributed to the proposed SAM module, which allows capturing long-range dependencies. In contrast to other approaches, PixelFormer can estimate depth maps corresponding to missing objects, as shown in the third row of <ref type="figure">Fig. 3</ref>. <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure">Fig. 4</ref> show the quantitative and qualitative results, respectively, on the outdoor dataset KITTI. We see an overall improvement of 3.9% and 2.3% in terms of Sq. Rel and RMSE respectively against the SOTA NeWCRFs [34] on the KITTI Eigen Split. We also compare our method against the previous SOTA approaches on the official KITTI test set. Currently, we rank 1 st on the official    <ref type="table" target="#tab_2">Table 3</ref>, Pix-elFormer outperforms Adabins by 9.4% and 7.4% in terms of Abs Rel and RMSE, respectively. Thus demonstrating the effectiveness of pixel-adaptive global local fusion for out-of-distribution input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on KITTI:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>Efficacy of Skip Attention Module:   <ref type="table">Table 5</ref>: Ablation experiment to demonstrate the usefulness of embedding the depth information into the initial pixel queries on KITTI Eigen Split using Swin-T as the encoder. ? means higher the better and ? means lower the better. The best results are in bold and second best are underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>module.</head><p>Effectiveness of embedding the depth information into the pixel queries: We experiment to showcase the usefulness of using the initial pixel queries to predict the bin centers. We compare our proposed design that predicts the bin centers using the initial pixel queries against using a vision transformer to predict the bin centers as in <ref type="bibr" target="#b0">[1]</ref>. mViT-Last converts the feature map of the highest resolution (F shown in <ref type="figure">Fig. 2)</ref> into 16?16 patches and integrates the information in the first patch using multiple self-attention layers (L = 4). The first patch embedding is passed through MLP layers to predict the bin centers. Similarly, mViT-First predicts bin centers by passing the initial pixel queries to a ViT. We use patch size = 1 for mViT-First for a fair comparison. <ref type="table">Table 5</ref> shows that both mVit-First and our approach outperform mVit-Last by 2.0% and 3.0%, respectively, in terms of Abs Rel, indicating that embedding depth information into the initial pixel queries via direct loss supervision helps predict better depth estimates. mVit-First does not give any further benefit to predicting bin centers since global information is already aggregated into the initial pixel queries via the PQI module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This work presents PixelFormer, a novel encoderdecoder strategy for Monocular Depth Estimation that poses the problem as a pixel query refinement problem. The global initial pixel queries predicted by the Pixel Query Initialiser module are refined to a higher resolution by querying the multiscale encoder features at various resolutions through the proposed Skip Attention Module. Unlike convolution-based skip connections, the module can fuse decoder features with long-range dependency, leading to more accurate depth labels. Our proposed Bin Center Prediction module helps constrain the network with depth information embedded into the initial pixel queries through direct loss supervision. Through extensive experiments, we showcase that PixelFormer improves state-of-the-art performance on the indoor dataset NYUV2 and outdoor dataset KITTI by 5.3% and 3.9%, respectively, along with an improved generalization performance by 9.4% on the indoor SUNRGBD dataset. In the future, we will try to apply our content adaptive fusion using SAM to other dense estimation tasks like semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>We have used D 1 ,</head><label>1</label><figDesc>D 2 , D 3 , D 4 = {128, 256, 512, 1024} where D i corresponds to the number of channels in the convolutional kernel that is applied before the attention-based fusion of encoder and decoder features at stage i. The number of heads H 1 , H 2 , H 3 , H 4 = {4, 8, 16, 32} where H i represents the number of attention heads used in the SAM module at level i. More details can be seen in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on NYUV2<ref type="bibr" target="#b26">[27]</ref> Dataset. The best results are in bold and second best are underlined. "*" means using additional data for training. ? means higher the better and ? means lower the better. An upper bound of 10 meters on the ground truth depth map is used for evaluation. All the numbers have been taken from the corresponding papers. We see an overall improvement against the SOTA in terms of almost all the metrics used for evaluation.</figDesc><table><row><cell>Input</cell><cell>Adabins[1]</cell><cell cols="2">NeWCRFs [34]</cell><cell>Ours</cell><cell></cell><cell>GT</cell><cell></cell></row><row><cell cols="8">Figure 3: Qualitative comparison of our proposed method PixelFormer on the indoor dataset NYUV2 against Adabins and</cell></row><row><cell>NeWCRFs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Venue</cell><cell>Abs Rel?</cell><cell>RMSE?</cell><cell>log 10 ?</cell><cell>? 1 ?</cell><cell>? 2 ?</cell><cell>? 3 ?</cell></row><row><cell>Eigen et al. [3]</cell><cell>NIPS'14</cell><cell>0.158</cell><cell>0.641</cell><cell>-</cell><cell>0.769</cell><cell>0.950</cell><cell>0.988</cell></row><row><cell>DORN [4]</cell><cell>CVPR'18</cell><cell>0.115</cell><cell>0.509</cell><cell>0.051</cell><cell>0.828</cell><cell>0.965</cell><cell>0.992</cell></row><row><cell>Yin et al. [33]</cell><cell>ICCV'19</cell><cell>0.108</cell><cell>0.416</cell><cell>0.048</cell><cell>0.872</cell><cell>0.976</cell><cell>0.994</cell></row><row><cell>BTS [16]</cell><cell>Arxiv'19</cell><cell>0.110</cell><cell>0.392</cell><cell>0.047</cell><cell>0.885</cell><cell>0.978</cell><cell>0.994</cell></row><row><cell>DAV [11]</cell><cell>ECCV'20</cell><cell>0.108</cell><cell>0.412</cell><cell>-</cell><cell>0.882</cell><cell>0.980</cell><cell>0.996</cell></row><row><cell>TransDepth [32]</cell><cell>ICCV'21</cell><cell>0.106</cell><cell>0.365</cell><cell>0.045</cell><cell>0.900</cell><cell>0.983</cell><cell>0.996</cell></row><row><cell>DPT* [24]</cell><cell>ICCV'21</cell><cell>0.110</cell><cell>0.367</cell><cell>0.045</cell><cell>0.904</cell><cell>0.988</cell><cell>0.998</cell></row><row><cell>PackNet-SAN* [9]</cell><cell>CVPR'21</cell><cell>0.106</cell><cell>0.393</cell><cell>-</cell><cell>0.892</cell><cell>0.979</cell><cell>0.995</cell></row><row><cell>Adabins [1]</cell><cell>CVPR'21</cell><cell>0.103</cell><cell>0.364</cell><cell>0.044</cell><cell>0.903</cell><cell>0.984</cell><cell>0.997</cell></row><row><cell>Naderi et al. [20]</cell><cell>WACV'22</cell><cell>0.097</cell><cell>0.444</cell><cell>0.042</cell><cell>0.897</cell><cell>0.982</cell><cell>0.996</cell></row><row><cell>Lee et al. [17]</cell><cell>WACV'22</cell><cell>0.107</cell><cell>0.373</cell><cell>0.046</cell><cell>0.893</cell><cell>0.985</cell><cell>0.997</cell></row><row><cell>P3Depth [23]</cell><cell>CVPR'22</cell><cell>0.104</cell><cell>0.356</cell><cell>0.043</cell><cell>0.898</cell><cell>0.981</cell><cell>0.996</cell></row><row><cell>NeWCRFs [34]</cell><cell>CVPR'22</cell><cell>0.095</cell><cell>0.334</cell><cell>0.041</cell><cell>0.922</cell><cell>0.992</cell><cell>0.998</cell></row><row><cell>PixelFormer (ours)</cell><cell></cell><cell>0.090</cell><cell>0.322</cell><cell>0.039</cell><cell>0.929</cell><cell>0.991</cell><cell>0.998</cell></row><row><cell cols="3">benchmark 1 against the previous peer-reviewed approaches</cell><cell cols="5">with an improvement of 2.5% in terms of Abs Rel and 1.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">in terms SILog against NeWCRFs. As shown in the top row</cell></row><row><cell cols="3">1 http://www.cvlibs.net/datasets/kitti/eval_</cell><cell cols="5">of Fig. 4, PixelFormer can precisely estimate depth maps for</cell></row><row><cell cols="2">depth.php?benchmark=depth_prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on KITTI Eigen Split test set<ref type="bibr" target="#b2">[3]</ref>. The best results are in bold and second best are underlined. "*" means using additional data for training. ? means higher the better and ? means lower the better. An upper bound of 80 meters on the ground truth depth map is used for evaluation. All the numbers have been taken from the corresponding papers.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>Sq Rel?</cell><cell>Abs Rel?</cell><cell>RMSE?</cell><cell>log 10 ?</cell><cell>? 1 ?</cell><cell>? 2 ?</cell><cell>? 3 ?</cell></row><row><cell>Chen et al. [2]</cell><cell>IJCAI'19</cell><cell>-</cell><cell>0.166</cell><cell>0.494</cell><cell>0.071</cell><cell>0.757</cell><cell>0.943</cell><cell>0.984</cell></row><row><cell>Yin et al. [33]</cell><cell>ICCV'19</cell><cell>-</cell><cell>0.183</cell><cell>0.541</cell><cell>0.082</cell><cell>0.696</cell><cell>0.912</cell><cell>0.973</cell></row><row><cell>BTS [16]</cell><cell>Arxiv'19</cell><cell>-</cell><cell>0.172</cell><cell>0.515</cell><cell>0.075</cell><cell>0.740</cell><cell>0.933</cell><cell>0.980</cell></row><row><cell>Adabins [1]</cell><cell>CVPR'21</cell><cell>-</cell><cell>0.159</cell><cell>0.476</cell><cell>0.068</cell><cell>0.771</cell><cell>0.944</cell><cell>0.983</cell></row><row><cell>PixelFormer (ours)</cell><cell></cell><cell>0.0915</cell><cell>0.144</cell><cell>0.441</cell><cell>0.062</cell><cell>0.802</cell><cell>0.962</cell><cell>0.990</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on SUNRGB-D test set without fine-tuning the models trained on NYUV2. The best results are in bold and second best are underlined. ? means higher the better and ? means lower the better. An upper bound of 8 meters on the ground truth depth map is used for evaluation. The numbers have been taken from the<ref type="bibr" target="#b0">[1]</ref>. far-sighted road objects.</figDesc><table><row><cell>Results on SUNRGB-D: Following [1], we analyze our</cell></row><row><cell>network's generalization performance by evaluating the</cell></row><row><cell>model performance on test SUNRGB-D without finetuning</cell></row><row><cell>the model on the NYUV2 dataset. As shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Abs Rel?</cell><cell>Sq Rel?</cell><cell>? 1 ?</cell></row><row><cell>Add-Conv</cell><cell>0.0602</cell><cell>0.190</cell><cell>0.964</cell></row><row><cell>Cat-Conv</cell><cell>0.0613</cell><cell>0.192</cell><cell>0.964</cell></row><row><cell>Decoder-Ours (SAM)</cell><cell>0.0578</cell><cell>0.182</cell><cell>0.967</cell></row><row><cell>demonstrates</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the effectiveness of our proposed SAM module against</cell><cell></cell><cell></cell><cell></cell></row><row><cell>other baseline convolution-based alternatives Add-Conv and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cat-Conv to combine the encoder and decoder features at a</cell><cell></cell><cell></cell><cell></cell></row><row><cell>particular scale. Add-Conv fuses the encoder-decoder fea-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tures by pixel-wise addition followed by a convolution oper-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ation. Cat-Conv concatenates the encoder and decoder fea-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiment to demonstrate the efficacy of SAM module on KITTI Eigen Split using Swin-T as the encoder. ? means higher the better and ? means lower the better. The best results are in bold and second best are underlined.tures w.r.t. the channel dimension, followed by a convolution operation. We see that the addition-based approach outperforms the concatenation-based approach by a small margin. However, using our SAM module outperforms Add-Conv by 4.0% in terms of Abs Rel and 4.2% in terms of Sq. Rel. This validates the contribution of the proposed SAMFigure 4: Qualitative comparison of previous state-of-the-art method NeWCRFs on the outdoor dataset KITTI.</figDesc><table><row><cell>Method</cell><cell>Abs Rel?</cell><cell>Sq Rel?</cell><cell>? 1 ?</cell></row><row><cell>mViT-Last</cell><cell>0.0596</cell><cell>0.190</cell><cell>0.964</cell></row><row><cell>mViT-First</cell><cell>0.0584</cell><cell>0.185</cell><cell>0.966</cell></row><row><cell>Ours (BCP)</cell><cell>0.0578</cell><cell>0.183</cell><cell>0.967</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work has been partly supported by the funding received from DST through the IMPRINT program (IMP/2019/000250). We acknowledge National Supercomputing Mission (NSM) for providing computing resources of 'PARAM Siddhi-AI', under National PARAM Supercomputing Facility, CDAC Pune, and supported by the Ministry of Electronics and Information Technology and Department of Science and Technology, Government of India.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4009" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structureaware residual pyramid network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI&apos;19</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence, IJCAI&apos;19</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="694" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="232" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse auxiliary networks for unified monocular depth prediction and completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2014</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiresunet : Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabil</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cutdepth: Edgeaware data augmentation in depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayoshi</forename><surname>Yamashita</surname></persName>
		</author>
		<idno>abs/2107.07684, 2021. 5</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Edgeconv with attention module for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaewon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2858" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with adaptive geometric attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taher</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hayward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="944" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><forename type="middle">Le</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention U-Net: Learning Where to Look for the Pancreas. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">P3depth: Monocular depth estimation with a piecewise planarity prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishakh</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<editor>Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using laplacian pyramid-based depth residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4381" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cliffnet for monocular depth estimation with hierarchical embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="316" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Point cloud completion by skip-attention network with hierarchical folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformer-based attention networks for continuous pixel-wise prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural window fully-connected crfs for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hybrid skip: A biologically inspired skip connection for the UNet architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Zioulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Albanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Drakoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53928" to="53939" />
		</imprint>
	</monogr>
	<note>Federico Alvarez, Dimitrios Zarpalas, and Petros Daras</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
