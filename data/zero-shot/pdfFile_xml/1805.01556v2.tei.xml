<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel-wise Attentional Gating for Scene Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
							<email>skong2@ics.uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
							<email>fowlkes@ics.uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pixel-wise Attentional Gating for Scene Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>[Project Page] [Github], [Slides]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To achieve dynamic inference in pixel labeling tasks, we propose Pixel-wise Attentional Gating (PAG), which learns to selectively process a subset of spatial locations at each layer of a deep convolutional network. PAG is a generic, architecture-independent, problem-agnostic mechanism that can be readily "plugged in" to an existing model with fine-tuning. We utilize PAG in two ways: 1) learning spatially varying pooling fields that improve model performance without the extra computation cost associated with multi-scale pooling, and 2) learning a dynamic computation policy for each pixel to decrease total computation (FLOPs) while maintaining accuracy.</p><p>We extensively evaluate PAG on a variety of per-pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation. We demonstrate that PAG allows competitive or state-ofthe-art performance on these tasks. Our experiments show that PAG learns dynamic spatial allocation of computation over the input image which provides better performance trade-offs compared to related approaches (e.g., truncating deep models or dynamically skipping whole layers). Generally, we observe PAG can reduce computation by 10% without noticeable loss in accuracy and performance degrades gracefully when imposing stronger computational constraints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The development of deep convolutional neural networks (CNN) has allowed remarkable progress in wide range of image pixel-labeling tasks such as boundary detection <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b28">29]</ref>, semantic segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6]</ref>, monocular depth estimation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b12">13]</ref>, and surface normal estimation <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>. Architectures that enable training of increasingly deeper networks have resulted in cor-responding improvements in prediction accuracy <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b21">22]</ref>. However, with great depth comes great computational burden. This hinders deployment of such deep models in edge and mobile computing applications which have significant power/memory constraints.</p><p>To make deep models more practically applicable, a flurry of recent work has focused on reducing these storage and computational costs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30]</ref>. Static offline techniques like network distillation <ref type="bibr" target="#b22">[23]</ref>, pruning <ref type="bibr" target="#b42">[42]</ref>, and model compression <ref type="bibr" target="#b4">[5]</ref> take a trained network as input and synthesize a new network that approximates the same functionality with reduced memory footprint and testtime execution cost. Our approach is inspired by a complementary family of techniques that learn to vary the network computation depth adaptively, depending on the input data <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>In this paper, we study the problem of achieving dynamic inference for per-pixel labeling tasks with a deep CNN model under limited computational budget. For image classification, dynamic allocation of computational "attention" can be interpreted as expending more computation on ambiguous images (e.g., <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b54">54]</ref>) or limiting processing to informative image regions <ref type="bibr" target="#b14">[15]</ref>. However, understanding the role of dynamic computation in pixel labeling tasks has not been explored. Pixel-level labeling requires analyzing fine-grained image details and making predictions at every spatial location, so it is not obvious that dynamically allocating computation to different image regions is useful. Unlike classification, labeling locally uninformative regions would seem to demand more computation rather than less (e.g., to incorporate long-range context).</p><p>To explore these questions, we introduce a Pixel-wise Attentional Gating (PAG) unit that selects a sparse subset of spatial locations to process based on the input feature map. We utilize the Gumbel sampling trick <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39]</ref> to allow differentiable, end-to-end latent training of PAG units inserted across multiple computational blocks of a given taskspecific architecture. We exploit this generic PAG unit in <ref type="figure">Figure 1</ref>: Pixel-wise Attentional Gating units (PAG) achieve dynamic inference by learning a dynamic computation path for each pixel under limited computation budget. The "ponder maps" shown in the last row provide a visualization of the amount of computation allocated to each location (generated by accumulating binary masks from PAG units across all layers); whereas the "MultiPool" adaptively chooses the proper pooling size for each pixel to aggregate information for inference. We apply PAG to a variety of per-pixel labeling tasks (boundary detection, semantic segmentation, monocular geometry) and evaluate over diverse image datasets (indoor/outdoor scenes, narrow/wide field-of-view). two ways: bypassing sequential (residual) processing layers and dynamically selecting between multiple parallel network branches. Dynamic computation depth: Inserting PAG at multiple layers of a Residual Network enables learning a dynamic, feedforward computation path for each pixel that is conditional on the input image (see the second last row in <ref type="figure">Fig. 1</ref>). We introduce a sparsity hyperparameter that provides control over the average total and per-layer computation. For a fixed computational budget, we show this dynamic, per-pixel gating outperforms architectures that meet the budget by either using a smaller number of fixed layers or learning to dynamically bypass whole layers (Section 3.3). Dynamic spatial pooling: We exploit PAG to dynamically select the extent of pooling regions at each spatial image location (see the last row in <ref type="figure">Fig. 1</ref>). Previous work has demonstrated the benefits of averaging features from multiple pooling scales using either learned weights <ref type="bibr" target="#b5">[6]</ref>, or spatially varying weights based on attention <ref type="bibr" target="#b6">[7]</ref> or scene depth <ref type="bibr" target="#b31">[32]</ref>. However, such multi-scale pooling requires substantially more computation. We show the proposed PAG unit can learn to select appropriate spatially-varying pooling, outperforming the recent work of <ref type="bibr" target="#b31">[32]</ref> without the computational burden of multiple parallel branches (Section 3.4).</p><p>We carry out an extensive evaluation of pixel-wise attentional gating over diverse datasets for a variety of perpixel labeling tasks including boundary detection, semantic segmentation, monocular depth estimation and surface normal estimation (see <ref type="figure">Fig. 1</ref>). We demonstrate that PAG helps deliver state-of-the-art performance on these tasks by dynamically allocating computation. In general, we observe that the introduction of PAG units can reduce total computa-tion by 10% without noticeable drop in accuracy and shows graceful degradation in performance even with substantial budget constraints (e.g., a 30% budget cut).</p><p>To summarize our primary contribution: (1) we introduce a pixel-wise attentional gating unit which is problemagnostic, architecture-independent and provides a simple method to allow user-specified control computational parsimony with standard training techniques; (2) we investigate the role of dynamic computation in pixel-labeling tasks and demonstrate improved prediction performance while maintaining or reducing overall compute cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep CNN models with residual or "skip" connections have yielded substantial performance improvements with increased depth <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>, but also introduced redundant parameters and computation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">42]</ref>. In interpreting the success of residual networks (ResNet) <ref type="bibr" target="#b21">[22]</ref>, it has been suggested that ResNet can be seen as an ensemble of many small networks <ref type="bibr" target="#b52">[52]</ref>, each defined by a path through the network topology. This is supported by the observation that ResNet still performs well even when some layers are removed after training <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15]</ref>. This indicates it may be possible to reduce test-time computation by dynamically choosing only a subset of these paths to evaluate <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b14">15]</ref>. This can be achieved by learning a halting policy that stops computation after evaluation of a particular layer <ref type="bibr" target="#b14">[15]</ref>, or a more flexible routing policy trained through reinforcement learning <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b54">54]</ref>. Our method is most closely related to <ref type="bibr" target="#b51">[51]</ref>, which utilizes the "Gumbel sampling trick" <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39]</ref> to learn binary gating that determines whether each layer is computed. The Gumbel sampling technique allows one to perform gradient descent on models that include a discrete argmax operation without resorting to approximation by softmax or reinforcement learning techniques.</p><p>The PerforatedCNN <ref type="bibr" target="#b15">[16]</ref> demonstrated that convolution operations could be accelerated by learning static masks that skip computation at a subset of spatial positions. This was used in <ref type="bibr" target="#b14">[15]</ref> to achieve spatially varying dynamic depth. Our approach is simpler (it uses a simple sparsity regularization to directly control amount per-pixel or per-layer computation rather than ponder cost) and more flexible (allowing more flexible routing policies than early halting 1 ).</p><p>Finally, our use of dynamic computation to choose between branches is related to <ref type="bibr" target="#b31">[32]</ref>, which improves semantic segmentation by fusing features from multiple branches with various pooling sizes using a spatially varying weighted average. Unlike <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> which require computing the outputs of parallel pooling branches, our PAGbased learns to select a pooling size for each spatial location and only computes the necessary pooled features. This is similar in spirit to the work of <ref type="bibr" target="#b46">[46]</ref>, which demonstrated that sparsely-gated mixture-of-experts can dramatically increase model capacity using multi-branch configuration with only minor losses in computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pixel-wise Attentional Gating</head><p>We first describe our design of Pixel-wise Attentional Gating (PAG) unit and its relation to the ResNet architecture <ref type="bibr" target="#b21">[22]</ref>. Then, we elaborate how we exploit the Gumbel sampling technique to learning PAG differentiable even when generating binary masks. Finally we describe how the PAG unit can be used to perform dynamic inference by <ref type="bibr" target="#b0">(1)</ref> selecting the subset of layers in the computational path for each spatial location, and (2) selecting the correct pooling size at each spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Plug-in PAG inside a Residual Block</head><p>Consider a block that computes output O using a residual update Z = F(I) to some input I. To reduce computation, one can learn a gating function G(I) that selects a subset of spatial locations (pixels) to process conditional on the input. We represent the output of G as a binary spatial mask G which is replicated along feature channel dimension as needed to match dimension of O and I. The spatially gated residual update can be written as:</p><formula xml:id="formula_0">G =G(I) O =? I + G (F G (I) + I) =I + G F G (I)<label>(1)</label></formula><p>where is element-wise product,? = 1 ? G, and the notation F G indicates that we only evaluate F at the locations <ref type="bibr" target="#b0">1</ref> Results in <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b54">54]</ref> suggest general routing offers better performance than truncating computation at a particular depth. aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa aaaaaa Boxes/arrows denote activations/computations. G is a sparse, binary map that modulates what processing applied to each spatial location. " " means the perforated convolution <ref type="bibr" target="#b15">[16]</ref>, which assembles only active pixels for computation. specified by G. An alternative to spatially varying computation is for the gating function to predict a single binary value that determines whether or not the residual is calculated at this layer <ref type="bibr" target="#b51">[51]</ref> in which case F G is only computed if G = 1.</p><p>Both pixel-wise and layer-wise gating have the intrinsic limitation that the gating function G must be evaluated prior to F, inducing computational delays. To overcome this limitation we integrate the gating function more carefully within the ResNet block. We demonstrate ours in the equations below comparing a standard residual block (left) and the one with PAG (right), respectively with corresponding illustrations in <ref type="figure" target="#fig_0">Fig. 2</ref>:</p><formula xml:id="formula_1">X =F 1 (I) Y =F 2 (X) Z =F 3 (Y) O =I + Z X =F 1 (I), G = G(I) Y =F 2 G (X) Z =F 3 G (? X + G Y) O =I + Z (2)</formula><p>The transformation functions F's consist of convolution, batch normalization <ref type="bibr" target="#b26">[27]</ref> and ReLU <ref type="bibr" target="#b43">[43]</ref> layers. As seen from the right set of equations, our design advocates computing the gating mask on the input I to the current building block in parallel with X = F X (I). ResNet adopts bottleneck structure so the first transformation F 1 performs dimensionality reduction with a set of 1?1 kernels, F 2 utilizes 3?3 kernels, and F 3 is another transform with 1?1 kernels that restores dimensionality. As a result, the most costly computation is in the second transformation F 2 which is mitigated by gating the computation. We show in our ablation study (Section 5.2) that for per-pixel labeling tasks, this design outperforms layer-wise gating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Discrete Attention Maps</head><p>The key to the proposed PAG is the gating function G that produces a discrete (binary) mask which allows for reduced computation. However, producing the binary mask using hard thresholding is non-differentiable, and thus cannot be simply incorporated in CNN where gradient descent is used for training. To bridge the gap, we exploit the Gumbel-Max trick <ref type="bibr" target="#b18">[19]</ref> and its recent continuous relaxation <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>A random variable m follows a Gumbel distribution if m ? ? log(? log(u)), where u is a sample from the uniform distribution u ? U[0, 1]. Let g be a discrete ran-dom variable with probabilities P (g = k) ? a k , and let {m k } k=1,...,K be a sequence of i.i.d. Gumbel random variables. Then we can sample from the discrete variable with:</p><formula xml:id="formula_2">g = argmax k=1,...,K (log ? k + m k )<label>(3)</label></formula><p>The drawback of this approach is that the argmax operation is not continuous when mapping the Gumbel samples to the realizations of discrete distribution. To address this issue, a continuous relaxation the Gumbel Sampling Trick, proposed in <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b27">28]</ref>, replaces the argmax operation with a softmax. Using a one-hot vector g = [g 1 , . . . , g K ] to encode g, a sample from the Gumbel softmax relaxation can be expressed by the vector:</p><formula xml:id="formula_3">g =sof tmax((log(?) + m)/? )<label>(4)</label></formula><p>where ? = [? 1 , . . . , ? K ], m = [m 1 , . . . , m K ], and ? is the "temperature" parameter. In the limit as ? ? 0, the softmax function approaches the argmax function and Eq. (4) becomes equivalent to the discrete sampler. Since the softmax function is differentiable and m contains i.i.d Gumbel random variables which are independent to input activation ?, we can easily propagate gradients to the probability vector ?, which is treated as the gating mask for a single pixel in the per-pixel labeling tasks. As suggested in <ref type="bibr" target="#b51">[51]</ref>, we employ the straight-through version <ref type="bibr" target="#b39">[39]</ref> of Eq. (4) during training. In particular, for the forward pass, we use discrete samples from Eq. (3), but during the backwards pass, we compute the gradient of the softmax relaxation in Eq. (4). Based on our empirical observation as well as that reported in <ref type="bibr" target="#b39">[39]</ref>, such greedy straight-through estimator performs slightly better than strictly following Eq. (4), even though there is a mismatch between forward and backward pass. In our work, we initialize ? = 1 and decrease it to 0.1 gradually during training. We find this works even better than training with a constant small ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Per-Pixel Computation Routing</head><p>By stacking multiple PAG residual blocks, we can construct a model in which the subset of layers used to compute an output varies for each spatial location based the collection of binary masks. We allow the user to specify the computational budget in terms of a target sparsity ?. For a binary mask G ? {0, 1} H?W , we compute the empirical sparsity g = 1 H * W H,W h,w G h,w (smaller values indicate sparser computation) and measure how well it matches the target ? using the KL divergence:</p><formula xml:id="formula_4">KL(? g) ? ? log( ? g ) + (1 ? ?) log( 1 ? ? 1 ? g )<label>(5)</label></formula><p>To train the model, we jointly minimize the sum of a task-specific loss task and the per-layer sparsity loss ing size for each spatial location so that contextual information can be better aggregated. This can be implemented efficiently using perforated convolution <ref type="bibr" target="#b15">[16]</ref>, denoted by , which assembles only active pixels for computation in each pooling branch and thus avoids computing all pooled versions.</p><p>summed over all layers of interest:</p><formula xml:id="formula_5">= task + ? L l=1 KL(? g l )<label>(6)</label></formula><p>where l indexes one of L layers which have PAG inserted for dynamic computation and ? controls the weight for the constraints. In our experiments we set ? = 10 ?4 but found performance is stable over a wide range of penalties (? ? [10 ?5 , 10 ?2 ]). To visualize the spatial distribution of computation, we accumulate the binary gating masks from all to produce a "ponder map". This reveals that trained models do not allocate computation uniformly, but instead responds to image content (e.g. focusing computation on boundaries between objects where semantic labels, depths or surface normals undergo sharp changes). An alternative to per-layer sparsity is to compute the total sparsity g = 1 L L l=1 g l and penalize g with KL(? g). However, training in this way does not effectively learn dynamic computational paths and results in trivial, nondynamic solutions, e.g. completely skipping a subset of layers and always using the remaining ones. Similar phenomenon is reported in <ref type="bibr" target="#b51">[51]</ref>. In training models we typically start from a pre-trained model and insert sparsity constraints progressively. We found this incremental construction produces better diversity in the PAG computation paths. We also observe that when targeting reduced computation budget, fine-tuning a model which has already been trained with larger ? consistently brings better performance than fine-tuning a pre-trained model directly with a small ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic Spatial Pooling</head><p>In pixel-labeling tasks, the ideal spatial support for analyzing a pixel can vary over the visual field in order to simultaneously maintain fine-grained details and capture context. This suggests an adaptive pooling mechanism at pixel level, or multi-scale pooling module (MultiPool) that chooses the appropriate pooling size for each pixel (see e.g., <ref type="bibr" target="#b31">[32]</ref>). Given a collection of P pooled feature maps {M i } i=1,...,P which are computed in convolution with different dilate rates ( <ref type="figure" target="#fig_1">Fig. 3)</ref>, we can generate a MultiPool fea-</p><formula xml:id="formula_6">ture map O = i W i M i , where {W i } i=1,.</formula><p>..,P are spatial selection masks, and indicates element-wise product between W i and each channel of M i . We utilize the PAG to generate binary weight W i that selects the "correct" pooling region at each spatial location by applying Eq. (4). This MultiPool module, illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, can be inserted in place of regular pooling with little computational overhead and learned in a latent manner using the task-specific loss (no additional sparsity loss).</p><p>We implement pooling using a set of 3?3-kernels applied at a set of user-specified dilation rates ([0, 1, 2, 4, 6, 8, 10]) <ref type="bibr" target="#b57">[57]</ref>. A dilation rate of 0 means the input feature is simply copied into the output feature map. In our experiments, we observe that only a small portion of pixels are exactly copied for the final representation without being fed into any multi-pooling branches. Note that in <ref type="figure" target="#fig_1">Fig. 3</ref>, a multiplicative gating operation is shown for clarity, but an efficient implementation would utilize masking to directly select pixels in a matrix multiplication implementation of the convolutional layers in GPU or FPGA kernel <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">45]</ref>. Our MultiPool module is thus distinct from <ref type="bibr" target="#b31">[32]</ref> which use weighted averages over all intermediate feature activations from all branches for the final feature representation. Our approach selects a single pooling size for each pixel and hence does not require overhead of computing all branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation and Training</head><p>While our PAG unit is agnostic to network architectures, in all our experiments we utilize ResNet <ref type="bibr" target="#b21">[22]</ref> pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref> as the base model. We following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref> and increase the output resolution of ResNet by removing the top global 7?7 pooling layer and the last two 2?2 pooling layers, replacing them with atrous convolution with dilation rate 2 and 4, respectively, to maintain a spatial sampling rate. Such a modification thus outputs predictions at 1/8 the input resolution. Rather than up-sampling the output (or downsampling the ground-truth) 8? for benchmarking <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>, we find it better to apply a deconvolution layer followed by two or more convolutional layers before the final output.</p><p>We augment the training sets with random left-right flips and crops with 20-pixel margin and of size divisible by 8. When training the model, we fix the batch normalization, using the same constant global moments in both training and testing. This modification does not impact the performance and allows a batch size of one during training (a single input image per batch). We use the "poly" learning rate policy <ref type="bibr" target="#b5">[6]</ref> with a base learning rate of 0.0002 scaled as a function of iteration by (1 ? iter maxiter ) 0.9 . We adopt a stage-wise training strategy over all tasks, i.e. training a base model, adding PAG-based MultiPool, inserting PAG for dynamic computation progressively, and finally decreasing ? to achieve target computational budget. Since our goal is to explore computational parsimony in per-pixel labeling tasks, we implement our models without "bells and whis-tles", e.g. no utilization of ensembles, no CRF as postprocessing, and no external training data. We implement our approach using the toolbox MatConvNet <ref type="bibr" target="#b50">[50]</ref>, and train using SGD on a single Titan X GPU 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Per-Pixel Labeling Vision Tasks</head><p>Boundary Detection We train a base model using (binary) logistic loss. Following <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b30">31]</ref>, we include four prediction branches at macro residual blocks (denoted by Res 2, 3, 4, 5) and a fusion branch for training. To handle class imbalance, we utilize a weighted loss accumulated over the prediction losses given by:</p><formula xml:id="formula_7">boundary = ? b?B j?Y ?y j log(P (yj|? b ))<label>(7)</label></formula><p>where b indexes the branches, ? Semantic Segmentation For semantic segmentation, we train a model using K-way cross-entropy loss as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>:</p><formula xml:id="formula_8">+ = |Y ? |/|Y ? ? Y + |, ? ? = 1 ? ? + ; Y + and Y ? denote</formula><formula xml:id="formula_9">semantic = ? i K c=1 1 [y i =c] ? log(Ci)<label>(8)</label></formula><p>where C i is the class prediction (from a softmax transform) at pixel i, and y i is the ground-truth class label. Monocular Depth Estimation For monocular depth estimation, we use combined L 2 and L 1 losses to compare the predicted and ground-truth depth maps D andD which are on a log scale:</p><formula xml:id="formula_10">depth = i=1 Di ?Di 2 2 + ? Di ?Di 1<label>(9)</label></formula><p>where ? = 2 controls the relative importance of the two losses. This mixed loss penalizes large errors quadratically (the L 2 term) while still assuring a non-vanishing gradient that continues to drive down small errors (the L 1 term). The idea behind our loss is similar to the reverse Huber loss as used in <ref type="bibr" target="#b35">[35]</ref>, which can be understood as concatenation of truncated L 2 and L 1 loss. However, the reverse Huber loss requires specifying a hyper-parameter for the boundary between L 2 and L 1 ; we find our mixed loss is robust and performs well with ? ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. Surface Normal Estimation To predict surface normals, we insert a final L 2 normalization layer so that predicted normals have unit Euclidean length. In the literature, cosine distance is often used in the loss function to train the model, while performance metrics for normal estimation measure the angular difference between prediction n and the target normaln <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13]</ref>. We address this discrepancy by incorporating inverse cosine distance along with cosine distance as our objective function:</p><formula xml:id="formula_11">normal = i ?n T ini + ? cos ?1 (n T ini )<label>(10)</label></formula><p>where ? controls the importance of the two part and we set ? = 4 throughout our experiments. <ref type="figure" target="#fig_3">Fig. 4</ref> compares the curves of the two losses, and we can clearly see that the inverse cosine loss always produce meaningful gradients, whereas the popular cosine loss has "vanishing gradient" issue when prediction errors become small (analogous to the mixed L 1 /L 2 loss for depth).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To evaluate our method based on PAG, we choose datasets that span a variety of per-pixel labeling tasks, including boundary detection, semantic segmentation, depth and surface normal estimation. We first describe the datasets, and then carry out experiments to determine the best architectural configurations to exploit PAG and measure compute-performance trade-offs. We then evaluate our best models on standard benchmarks and show our approach achieves state-of-the-art or competitive performance for pixel-labeling. Finally, we visualize the attentional maps from MultiPool and ponder maps, and demonstrate qualitatively that our models pay more "attention" to specific regions/pixels, especially on boundaries between regions, e.g. semantic segments, and regions with sharp change of depth and normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We utilize the following benchmark datasets. BSDS500 <ref type="bibr" target="#b0">[1]</ref> is the most popular dataset for boundary detection. It provides a standard split <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b56">56]</ref> of 300 train-val images and 200 test images. NYUv2 <ref type="bibr" target="#b48">[48]</ref> consists of 1,449 RGB-D indoor scene images of the resolution 640 ? 480 which include color and pixel-wise depth obtained by a Kinect sensor. We use the ground-truth segmentation into 40 classes provided in <ref type="bibr" target="#b19">[20]</ref> and a standard train/test split into 795 and 654 images, respectively. For surface normal estimation, we compute the normal as target from depth using the method in <ref type="bibr" target="#b48">[48]</ref> by fitting least-squares planes to neighboring sets of points in the point cloud.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of Pixel-wise Attentional Gating</head><p>We evaluate different configurations of PAG on the BSDS500 and NYUv2 datasets for boundary detection and semantic segmentation (similar observations hold on other tasks, see supplementary materials). The goal of these experiments is to establish:</p><p>1. whether our base model is comparable to state-of-theart methods;</p><p>2. where to insert the PAG-based MultiPool module for the best performance;</p><p>3. how our PAG-based method for computational parsimony impacts performance, and how it performs compared with other related methods, e.g. truncated ResNet and methods learning to skip/drop layers.</p><p>Base models: We train our base models as described in Section 4 without PAG units. The performance of our base model is on-par with state-of-the-art systems, achieving IoU=42.05% on NYUv2 for semantic segmentation (Re-fineNet <ref type="bibr" target="#b37">[37]</ref> achieves IoU=44.5 with multi-resolution input), and F = 0.79 on BSDS500 for boundary detection (HED <ref type="bibr" target="#b56">[56]</ref> achieves F = 0.78). More comprehensive comparisons with other related methods are shown later in Section 5.3.</p><p>MultiPool: <ref type="table" target="#tab_0">Table 1</ref> explores the effect of inserting the Mul-tiPool operation at different layers in the base model. In Table 1, Res6 means that we insert MultiPool module in the additional convolutional layers above the ResNet50 base.</p><p>For boundary detection, we do not initialize more convolutional layers above the backbone, so there is no Res6. For both tasks, we observe that including a PAG-based Mul-tiPool module improves performance, but including more than one MultiPool module does not offer further improvements. We find inserting MultiPool module at second last macro residual block (Res4 or Res5 depending on task) yields the largest gain. For semantic segmentation, our MultiPool also outperforms the weighted pooling in <ref type="bibr" target="#b31">[32]</ref>, which uses the same ResNet50 base. We conjecture this is due to three reasons. First, we apply the deconvolutional layer way before the last convolutional layer for softmax input as explained in Section 4.1. This increases resolution that enables the model to see better the fine details. Additionally, our set of pooling regions includes finer scales (rather than using powers of 2). Finally, the results in <ref type="table" target="#tab_3">Table 4</ref> show that PAG with binary masks performs slightly better (IoU=46.5 vs. IoU=46.3) than the (softmax) weighted average operation used in <ref type="bibr" target="#b31">[32]</ref>. Computation-Performance Tradeoffs: Lastly, we evaluate how our dynamic parsimonious computation setup impacts performance and performs compared with other baselines. We show results of semantic segmentation on NYUv2 dataset in <ref type="table" target="#tab_1">Table 2</ref>, comparing different baselines and our models with MultiPool at macro block Res5, MP@Res5 (PAG) for short, which are trained with different target computational budgets (specified by ?). The "truncated" baseline means we simple remove top layers of ResNet to save computation, while "layer-skipping" is an implementation of <ref type="bibr" target="#b51">[51]</ref> that learns to dynamically skip a subset of layer. "PerforatedCNN" is our implementation of <ref type="bibr" target="#b15">[16]</ref> that matches the computational budget using a learned constant gating function (not dependent on input image). For fair comparison, we insert MultiPool module at the top of all  the compared methods. These results clearly suggest that the PAG approach outperforms all these methods, demonstrating that learning dynamic computation path at the pixel level is helpful for per-pixel labeling tasks. It is also worth noting that PerforatedCNN does not support fully convolutional computation requiring that the input image have a fixed size in order to learn fixed computation paths over the image. In contrast, our method is fully convolutional that is able to take as input images of arbitrary size and perform computing with input-dependent dynamic paths. <ref type="figure" target="#fig_4">Fig. 5</ref> shows that, as we decrease the computation budget, the performance of the PAG-based method degrades gracefully even as the amount of computation is scaled back to 70%, merely inducing 2.4% and 5.6% performance degradation on boundary detection and semantic segmentation compared to their full model, respectively, i.e., F =0.773 vs. F =0.792 and IoU =0.409 vs. IoU =0.465. <ref type="table" target="#tab_1">Table 2</ref> highlights the comparison to truncation and layerskipping models adjusted to match the same computational budget as PAG. For these approaches, performance decays much more sharply with decreasing budget. These results also highlight that the target sparsity parameter ? provides tight control over the actual average computation of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comprehensive Benchmark Comparison</head><p>We now compare our models under different degrees of computational parsimony (?={0.5, 0.7, 0.9, 1.0}) with other state-of-the-art systems for pixel labeling.</p><p>Taking boundary detection as the first task, we quantitatively compare our model to COB <ref type="bibr" target="#b41">[41]</ref>, HED <ref type="bibr" target="#b56">[56]</ref>, gPb-UCM <ref type="bibr" target="#b0">[1]</ref>, LEP <ref type="bibr" target="#b44">[44]</ref>, UCM <ref type="bibr" target="#b0">[1]</ref>, NCuts <ref type="bibr" target="#b47">[47]</ref>, EGB <ref type="bibr" target="#b13">[14]</ref>, MCG <ref type="bibr" target="#b1">[2]</ref> and the mean shift (MShift) algorithm <ref type="bibr" target="#b8">[9]</ref>. <ref type="table" target="#tab_2">Table 3</ref>   shows comparison to all the methods (PR curves in supplementary material), demonstrating our model achieves stateof-the-art performance. Note that our model has the same backbone architecture of HED <ref type="bibr" target="#b56">[56]</ref>, but outperforms it with our MultiPool module which increases receptive fields at higher levels. Our model performs on par with COB <ref type="bibr" target="#b41">[41]</ref>, which uses auxiliary losses for oriented boundary detection. Note that it is possible to surpass human performance with sophisticated techniques <ref type="bibr" target="#b28">[29]</ref>, but we don't pursue this as it is out the scope of this paper. <ref type="table" target="#tab_3">Table 4</ref>, 5 and 6 show the comprehensive comparisons on the tasks of semantic segmentation, monocular depth and surface normal estimation, respectively. In addition to comparing with state-of-the-art methods, we also show the result of MultiPool module with softmax weighted average operation, termed by MP@Res5 (w-Avg.). Interestingly, MultiPool performs slightly better when equipped with PAG than the weighted average fusion. We attribute this to the facts that, longer training has been done in the stage-wise training strategy, and PAG unit also constrains the information flow to train specific branches.</p><p>Our baseline model achieves performance on par with recent methods for all tasks. When inserting the Multi-Pool module, we improve even further and surpass the compared methods for most tasks and datasets. In particular, on datasets with large perspective images, i.e. Stanford-2D-3D and Cityscapes, the MultiPool module shows greater improvement. Reducing the computation 20-30% only yields a performance drop of 3-5% generally. For depth and surface normal estimation tasks, our baseline models also perform very well. This is notable since we don't leverage multi-task learning (unlike Eigen <ref type="bibr" target="#b11">[12]</ref>) and do not use extra images to augment training set (unlike most methods for depth estimation using ?100k extra images to augment the training set as shown in <ref type="table" target="#tab_4">Table 5</ref>). We attribute this to the combination of the proposed PAG Mul-tiPool and carefully designed losses for depth and surface normal estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Visualization</head><p>We visualize the prediction and attention maps in <ref type="figure">Fig.  1</ref> for the four datasets, respectively. We find that the binary attention maps are qualitatively similar across layers and hence summarize them with a "ponder map" by summing maps across layers (per-layer maps can be found in the supplementary material). We can see our models allocate more computation on the regions/pixels which are likely sharp transitions, e.g. boundaries between semantic segments, depth discontinuties and normal discontinuities (e.g. between wall and ceiling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we have studied the problem of dynamic inference for pixel labeling tasks under limited computation budget with a deep CNN network. To achieve this, we propose a Pixel-wise Attentional Gating unit (PAG) that learns to generate sparse binary masks that control computation at each layer on a per-pixel basis. Our approach differs from previous methods in demonstrating improved performance on pixel labeling tasks using spatially varying computation trained with simple task-specific loss. This makes our approach a good candidate for general use as it is agnostic to tasks and architectures, and avoids more complicated reinforcement learning-style approaches, instead relying on a simple, easy-to-set sparsity target that correlates closely with empirical computational cost. As our PAG is based on a generic attention mechanism, we anticipate future work might explore task-driven constraints for further improvements and savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel-wise Attentional Gating for Scene</head><p>Parsing (Appendix)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In the supplementary material, we first present in detail how to transform the (unpredictable) global surface normals into (predictable) local normals in panoramic images. We then show more ablation studies on the loss functions introduced in the main paper and MultiPool module with/without PAG unit. Finally, we provide more qualitative visualization of the results for various pixel-labeling tasks, as well as the attentional ponder maps and MultiPool maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Local Surface Normal in Panoramas</head><p>Stanford-2D-3D <ref type="bibr" target="#b2">[3]</ref> provides cylindrical panoramas with global surface normals, which are in a global Earth-Centered-Earth-Fixed coordinate system (ECEF). For example, the normals for a wall have the same direction pointing to the true north. However, such global coordinate system is impossible to determine from a single image, and thus the global normals are unpredictable purely based on panoramic image data alone. For this reason, we propose to transform the global normals into "local normals" which are relative to the camera viewing direction. We note that such a predictability makes relative normals more useful in scene understanding and reconstruction.</p><p>For a cylindrical panorama, we assume the vertical axis of the panorama is aligned with the global coordinate frame. Given a global normal at a pixel n = [x, y, z] T , we can apply a rotation matrix R in the horizontal plane (x and y) to obtain its local normal [x , y , z] T in the "camera viewing" coordinate system:</p><formula xml:id="formula_12">? ? ? x y z ? ? ? = R, 0 0, 1 ? ? ? x y z ? ? ?<label>(11)</label></formula><p>where z is the variable for vertical direction. We would like to determine the appropriate rotation matrices for all pixels where each pixel has its own rotation matrix which is controlled by a single signed angle parameter ?. For a cylindrical panorama, the relative difference in viewing direction between two image locations is completely specified by their horizontal separation in image coordinates. Therefore, to determine the set of rotations, we simply need to specify an origin for which the rotation is ? = 0, i.e., a canonical point whose surface normal points exactly to the camera. Given the rotation matrix for the canonical point, the rotation angle for remaining points can be calculated as 2? W W , where W is the width of panorama and W is the offset from the target pixel to the canonical pixel (with sign). <ref type="figure">Fig. 6</ref> illustrates the principle behind our methodology.</p><p>We note that it is straightforward to identify canonical points manually by choosing a flat vertical surface (e.g., a wall) and selecting the point on it which is nearest to the camera (e.g. shape of panoramic topology). An automated method can be built with the same rule based on semantic annotation and depth map. However, the automated method may suffer from cluttered scene (e.g. boards and bookcase on the wall), yet such manual annotation enables us to visualize what the local normals would look like if we clicked the "wrong" points 3 . From the three random panoramic images as shown in <ref type="figure">Fig. 7</ref>, we can see such canonical points lie in the color bands (manually drawn for demonstration) noted in the figure. They are easy to detect by eye based on the warping effect due to panoramic topology. We made an easy-to-use tool to click a canonical point for each panoramic image. <ref type="figure">Fig. 7</ref> demonstrates the resulting transformed normals after an annotator has clicked on some point in the color band. We note that annotating each panoramic image costs less than 5 seconds, and it required less than three hours to carefully annotate all 1,559 panoramas in the dataset. We also compare the annotation when clicking on different canonical points (at different color bands) for the same image, and the maximum difference of normals for all spatial locations is only less than 8 ? degree. This means the annotation is easy and robust. We will release to public all the transformed local surface normals as an extension to Stanford-2D-3D dataset, as well as our interactive tool for annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Further Analysis of Loss Functions and MultiPool Module</head><p>In this section, we describe further analysis on the architectural choices of where to insert the MultiPool module, as well as new loss functions introduced in our main paper. We conduct experiments on BSDS500 <ref type="bibr" target="#b0">[1]</ref> and NYUv2 <ref type="bibr" target="#b11">[12]</ref> datasets for boundary detection, depth and surface normal estimation, to complement the analysis in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Boundary Detection on BSDS500 Dataset</head><p>In <ref type="figure" target="#fig_5">Fig. 8</ref>, we show the precision-recall curves for boundary detection on BSDS500 dataset <ref type="bibr" target="#b0">[1]</ref>. First, <ref type="figure" target="#fig_5">Fig. 8 (a)</ref> summarizes our ablation study on where to insert the Multi-Pool module to obtain the largest performance gain. We observe that the best performance is achieved when the Multi-Pool module is inserted at the fourth macro building block (Res4), i.e. the second last macro block or the Resnet50 <ref type="figure">Figure 6</ref>: An illustration of how to transform global normals into local normals specified relative to the camera viewing direction. Global normals in Stanford-2D-3D dataset <ref type="bibr" target="#b2">[3]</ref> are in Earth-Centered-Earth-Fixed (ECEF) coordinate system. <ref type="figure">Figure 7</ref>: We draw the color bands on global normal maps to indicate points of the wall within these bands can be treated as canonical points, whose normals exactly face towards the camera. For a human annotator, these points can be easily detected by looking at the shape of the room. Our interface allows an annotator to click a point supposedly within any one of the bands, and through the coordinate transform, such global normals can be transformed into predictable, relative normals. The rightmost pie chart provides reference on the local surface normal relative to the camera viewing direction.</p><p>architecture. This supports our conclusion that MultiPool inserted at the second last macro building block leads to the largest performance gain.</p><p>Moreover, based on the "MultiPool@Res4" model, we gradually insert PAG units for parsimonious inference, and decrease the hyper-parameter ? which controls the sparsity degree of binary masks (the sparser the masks are, the more parsimonious constraint is imposed). In particular, note that ? = 0.5 means ?30% computation has been saved, at which point our model achieves F = 0.773, only degrading by 2.4% performance. This shows that the ResNet50 model has sufficient capacity for boundary detection, and more parsimonious constraint does not harm the performance too much. Perhaps due to this reason, the MultiPool module does not improve performance remarkably for boundary detection. Finally, by comparing to state-of-the-art methods as shown in <ref type="figure" target="#fig_5">Fig. 8 (b)</ref>, we note our "MultiPool@Res4" model outperforms HED <ref type="bibr" target="#b56">[56]</ref> which shares the same architecture but without MultiPool module, and performs similarly with COB <ref type="bibr" target="#b41">[41]</ref> which further exploits auxiliary loss for oriented boundary detection. This validates that the PAG-based Mul-tiPool module improves performance by providing each pixel the "correct" size of pooling field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Monocular Depth Estimation on NYUv2 Dataset</head><p>We provide complementary ablation study on the task of monocular depth estimation on NYUv2 dataset.</p><p>First, we study where to insert the MultiPool module to obtain the largest performance gain. We train our base model using L2 loss function only, and insert the Multi-Pool module (without PAG but the softmax weighted average operation) at each macro building block one by one. We list the performance of these models in <ref type="table" target="#tab_6">Table 7</ref>. From the table, we observe that no matter where to insert the Mul-tiPool module, it consistently improves the performance; while when MultiPool module is inserted at Res5, which is the second last macro building block, we obtain the largest performance gain. These observations, along with what reported in the main paper, support our conclusion that one is able to get the best performance when inserting the Mul-tiPool module at the second last macro building block of a ResNet model.</p><p>Then, we study the loss function mixing L1 and L2 as introduced in the main paper. We train the models with Multi-Pool module inserted at the fifth macro block (MP@Res5), using different loss functions, and report the results in <ref type="table" target="#tab_7">Table 8</ref>. It's clear to see the mixed L1 and L2 loss leads to the best performance, especially on the metric of &lt; 1.25, focusing on the range of small prediction errors where L2 loss alone is unable to provide a meaningful gradient. Moreover, we compare the model between PAG-based MultiPool and that based on softmax weighted average operation in <ref type="table" target="#tab_7">Table 8</ref>. Again, our PAG-based MultiPool not only outperforms the weighted-average MultiPool, but also maintains computation as well as memory storage because of the perforated convolution (selecting spatial locations to compute).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Surface Normal Estimation on NYUv2 Dataset</head><p>Similar to the ablation study for monocular depth estimation task, we study firstly how the proposed loss function improves performance, then where to insert the Mul-tiPool module for the best performance, and lastly performance comparison between PAG-based and weightedaverage MultiPool.</p><p>In <ref type="table" target="#tab_8">Table 9</ref>, we compare the results from models trained with different loss functions. We can see the combination of cosine distance loss and the inverse cosine loss achieves the best performance. From the table, we clearly see that the improvement on metric 11.25 ? is more remarkable, which focuses on small prediction errors. This is because, as an-  alyzed in the main paper, the combined loss function provides meaningful gradient "everywhere", whereas the cosine distance loss alone has "vanishing gradient" issue when the prediction errors become small. We then study where to insert the MultiPool module to get the best performance in <ref type="table" target="#tab_0">Table 10</ref>. Note that, in this ablation study, we didn't use PAG for binary masks, but instead using weighted average based on softmax operator. Consistent to previous discovery, when inserting MultiPool at the second last macro block, we achieve the best performance. In <ref type="table" target="#tab_0">Table 11</ref>, we compare the results with PAG-based MultiPool and weighted-average MultiPool. We conclude with consistent observation that PAG unit does not harm the performance compared to the softmax weighted average fusion, but instead achieves better performance with the same computation overhead, thanks to the perforated convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">More Qualitative Visualization</head><p>In this section, we visualize more results of boundary detection, semantic segmentation, monocular depth estimation and surface normal estimation, over the four datasets used in the paper, BSDS500 <ref type="figure">(Fig. 9</ref>), NYUv2 <ref type="figure">(Fig. 10)</ref>, Stanford-2D-3D ( <ref type="figure">Fig. 11)</ref> and Cityscapes <ref type="figure" target="#fig_0">(Fig. 12</ref>). In the figures, we show the ponder map for each macro building block, as well as the overall ponder map. From these ponder maps, we can see our model learns to dynamically allocate computation at different spatial location, primarily expending more computation on the regions/pixels which are regions with sharp changes, e.g. boundary between semantic segments, regions between two depth layer, loca-tions around normal changes like between wall and ceiling. We also show all the binary maps produced by PAG units in <ref type="figure" target="#fig_1">Fig. 13</ref> over a random image from Stanford2D3D dataset for semantic segmentation, surface normal estimation and depth estimation.</p><p>In <ref type="figure" target="#fig_3">Fig. 14,</ref> we visualize the learned binary masks by Per-foratedCNN <ref type="bibr" target="#b15">[16]</ref> on NYUv2 dataset for semantic segmentation. We also accumulate all the binary masks towards the ponder map, from which we can see that the active pixels largely follow uniform distribution. This is different from what reported in <ref type="bibr" target="#b15">[16]</ref> that the masks mainly highlight central region in image classification, which is due to the fact that images for the classification task mainly contain object in the central region; whereas for scene images, it is hard for PerforatedCNN to focus on any specific location of the image. Note again that PerforatedCNN does not support either dynamic pixel routing or fully convolutional computation, requiring that the input image have a fixed size in order to learn fixed computation paths over the image. In contrast, our method is fully convolutional that is able to take as input images of arbitrary size and perform computing with input-dependent dynamic paths.   : Visualization on BSDS500 dataset <ref type="bibr" target="#b0">[1]</ref> of sparse binary attention maps at each layer for boundary detection, together with the output and ponder map accumulating all binary maps. PAG-based MultiPool module is inserted at layer Res4-2, which is not included in the ponder map. <ref type="figure">Figure 10</ref>: Visualization on NYUv2 dataset <ref type="bibr" target="#b11">[12]</ref> for semantic segmentation, depth estimation and surface normal estimation. Besides the overall ponder map, we also show the partial ponder map for each macro residual block by summing the sparse binary attentional maps. The MultiPool binary masks are not included in the ponder maps. <ref type="figure">Figure 11</ref>: Visualization on Stanford2D3D <ref type="bibr" target="#b2">[3]</ref> for semantic segmentation, depth estimation and surface normal estimation. Besides the overall ponder map, we also show the partial ponder map for each macro residual block by summing the sparse binary attentional maps. The MultiPool binary masks are not included in the ponder maps. <ref type="figure" target="#fig_0">Figure 12</ref>: Visualization on Cityscapes dataset <ref type="bibr" target="#b9">[10]</ref> for semantic segmentation and depth estimation. Besides the overall ponder map, we also show the partial ponder map for each macro residual block by summing the sparse binary attentional maps. The MultiPool binary masks are not included in the ponder maps. <ref type="figure" target="#fig_1">Figure 13</ref>: Visualization on Stanford2D3D <ref type="bibr" target="#b2">[3]</ref> for semantic segmentation, surface normal estimation and depth estimation. Besides the overall ponder map (accumulated computation), we show all the binary maps produced by PAG, as well as the one in the MultiPool module at layer 5-2. <ref type="figure" target="#fig_3">Figure 14</ref>: Visualization on binary masks trained by PerforatedCNN <ref type="bibr" target="#b15">[16]</ref> on NYUv2 dataset for semantic segmentation. Note that we also insert a MultiPool module at Res5-2 block. This makes it fair to compare between our method and PerforatedCNN. We also accumulate all the binary masks towards the ponder map, from which we can see that the active pixels largely follow uniform distribution. This is different from what reported in <ref type="bibr" target="#b15">[16]</ref> that the masks mainly highlight central region in image classification, which is due to the fact that images for the classification task mainly contain object in the central region; whereas for scene images, it is hard for PerforatedCNN to focus on any specific location of the image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) A standard residual block. (b) Pixel-wise Attentional Gating unit (PAG) integrated into a residual block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>PAG-based MultiPool module learns to select the pool-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the set of boundary and nonboundary annotations, respectively. Y = Y ? ? Y + contains the indices of all pixels. This base model is modeled after HED [56] and performs similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Inverse cosine loss has a constant gradient, while the gradient of the widely used cosine loss decreases as the prediction errors become small, preventing further model refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Performance vs. computation budget (controlled by ?) for boundary detection and semantic segmentation, with saved computation (%) compared to full model as indicated by percentage number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Precision-Recall curves on boundary detection on BSDS500 dataset. (a) Ablation study. (b) Comparison with state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study for where to insert the PAG-based Mul-tiPool module. Experiments are from boundary detection and semantic segmentation on BSDS500 and NYUv2 dataset, measured by F -score (F bnd. ) and IoU (IoUseg.), respectively. Numbers are in % (higher is better). IoUseg. 42.05 44.13 45.67 46.52 45.99 45.48 44.83 44.97 46.44</figDesc><table><row><cell cols="5">metrics base. Res3 Res4 Res5 Res6 Res4-5 Res3-5 Res4-6 Res5-6</cell></row><row><cell>F bnd.</cell><cell>79.00 79.19 79.19 79.14 -</cell><cell>79.18 79.07</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison w.r.t computational parsimony controlled by hyper-parameter ? on NYUv2 dataset for semantic segmentation.</figDesc><table><row><cell cols="10">param.&amp;FLOPs truncated layer-skipping perforatedCNN MP@Res5 (PAG)</cell></row><row><cell>?</cell><cell>?10 10</cell><cell cols="3">IoU acc. IoU</cell><cell>acc.</cell><cell>IoU</cell><cell>acc.</cell><cell>IoU</cell><cell>acc.</cell></row><row><cell>0.5</cell><cell>6.29</cell><cell cols="7">36.30 67.36 37.78 67.31 37.37 66.76 40.89</cell><cell>69.44</cell></row><row><cell>0.7</cell><cell>8.27</cell><cell cols="7">37.69 67.44 39.84 69.00 40.09 68.78 43.61</cell><cell>71.41</cell></row><row><cell>0.9</cell><cell>8.95</cell><cell cols="7">40.29 69.66 41.27 70.01 42.94 70.94 45.75</cell><cell>72.93</cell></row><row><cell>1.0</cell><cell>9.63</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.52</cell><cell>73.50</cell></row></table><note>Stanford-2D-3D [3] contains 1,559 RGB panoramic im- ages with depths, surface normal and semantic annotations covering six large-scale indoor areas from three different buildings. We use area 3 and 4 as a validation set (489 panoramas) and the remaining four areas for training (1,070 panoramas). The panoramas are very large (2048?4096) and contain black void regions at top and bottom due to the spherical panoramic topology. We rescale them by 0.5 and crop out the central two-thirds (y ? [160, 863]) resulting in final images of size 704?2048-pixels. We randomly crop out sub-images of 704?704 resolution for training. Note that the surface normals in panoramic images are relative to the global coordinate system which cannot be determined from the image alone. Thus we transformed this global nor- mal into local normal specified relative to the camera view- ing direction (details in supplementary material). Note that such relative normals are also useful in scene understanding and reconstruction. Cityscapes [10] contains high-quality pixel-level annota- tions of images collected in street scenes from 50 different cities. We use the standard split of training set (2,975 im- ages) and validation set (500 images) for testing, respec- tively, labeled for 19 semantic classes as well as depth obtained by disparity. The images are of high resolution (1024 ? 2048), and we randomly crop out sub-images of 800?800 resolution during training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art methods for boundary detection on BSDS500 dataset.</figDesc><table><row><cell cols="6">method MP@Res4 HED COB LEP MCG MShift gPb-UCM NCut EGB</cell></row><row><cell></cell><cell></cell><cell>[56] [41] [44] [2]</cell><cell>[9]</cell><cell>[1]</cell><cell>[47] [14]</cell></row><row><cell>odsF</cell><cell>0.792</cell><cell cols="2">0.780 0.793 0.757 0.747 0.601</cell><cell>0.726</cell><cell>0.641 0.636</cell></row><row><cell>oisF</cell><cell>0.806</cell><cell cols="2">0.796 0.820 0.793 0.779 0.644</cell><cell>0.760</cell><cell>0.674 0.674</cell></row><row><cell>AP</cell><cell>0.832</cell><cell cols="2">0.834 0.859 0.828 0.759 0.493</cell><cell>0.727</cell><cell>0.447 0.581</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Semantic segmentation is measured by Intersection over Union (IoU), pixel accuracy (acc), and iIoU that leverages the size of segments w.r.t categories. Results marked by ? are from our trained models with the released code.</figDesc><table><row><cell></cell><cell cols="6">NYUv2 [48] Stanford-2D-3D [3] Cityscapes [48]</cell></row><row><cell>methods/metrics</cell><cell cols="3">IoU pixel acc. IoU</cell><cell cols="2">pixel acc. IoU</cell><cell>iIoU</cell></row><row><cell>baseline</cell><cell>42.1</cell><cell>71.1</cell><cell>79.5</cell><cell>92.1</cell><cell>73.8</cell><cell>54.7</cell></row><row><cell>MP@Res5 (w-Avg.)</cell><cell>46.3</cell><cell>73.4</cell><cell>83.7</cell><cell>93.6</cell><cell>75.8</cell><cell>56.9</cell></row><row><cell>MP@Res5 (PAG)</cell><cell>46.5</cell><cell>73.5</cell><cell>83.7</cell><cell>93.7</cell><cell>75.7</cell><cell>55.8</cell></row><row><cell>MP@Res5 (?=0.9)</cell><cell>45.8</cell><cell>72.9</cell><cell>82.8</cell><cell>93.3</cell><cell>75.0</cell><cell>55.4</cell></row><row><cell>MP@Res5 (?=0.7)</cell><cell>43.6</cell><cell>71.4</cell><cell>82.4</cell><cell>93.2</cell><cell>72.6</cell><cell>55.1</cell></row><row><cell>MP@Res5 (?=0.5)</cell><cell>40.9</cell><cell>69.4</cell><cell>81.8</cell><cell>92.9</cell><cell>70.8</cell><cell>53.2</cell></row><row><cell cols="2">PerspectiveParsing [32] 44.5</cell><cell>72.1</cell><cell>76.5</cell><cell>91.0</cell><cell>75.4</cell><cell>56.8</cell></row><row><cell>DeepLab [6]</cell><cell>-</cell><cell>-</cell><cell>69.8  ?</cell><cell>88.0  ?</cell><cell>71.4</cell><cell>51.6</cell></row><row><cell>LRR [18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.0</cell><cell>48.0</cell></row><row><cell>PSPNet [58]</cell><cell>-</cell><cell>-</cell><cell>67.4  ?</cell><cell>87.6  ?</cell><cell>78.7</cell><cell>60.4</cell></row><row><cell>RefineNet-Res50 [37]</cell><cell>43.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RefineNet-Res152 [37]</cell><cell>46.5</cell><cell>73.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Depth estimation is measured by standard threshold accuracy, i.e. the percentage (%) of predicted pixel depths d i s.t.</figDesc><table><row><cell>? =</cell></row></table><note>i ) &lt; ? , where ? = {1.25, 1.25 2 , 1.25 3 }. Methods with* use ?100k extra images to train.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Surface normal estimation is measured by mean angular error and the percentage of prediction error within t ? degree where t = {11.25, 22.50, 30.00}. Smaller ang. err. means better performance as marked by ?. .? 11.25 ? 22.50 ? 30.00 ? ang. err.? 11.25 ? 22.50 ? 30.00 ?</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NYUv2 [48]</cell><cell></cell><cell></cell><cell cols="2">Stanford-2D-3D [3]</cell><cell></cell></row><row><cell cols="2">methods/metrics ang. errbaseline 22.3</cell><cell>34.4</cell><cell>62.5</cell><cell>74.4</cell><cell>19.0</cell><cell>51.5</cell><cell>68.6</cell><cell>76.3</cell></row><row><cell>MP@Res5 (w-Avg.)</cell><cell>21.9</cell><cell>35.9</cell><cell>63.8</cell><cell>75.3</cell><cell>16.5</cell><cell>58.2</cell><cell>74.2</cell><cell>80.4</cell></row><row><cell>MP@Res5 (PAG)</cell><cell>21.7</cell><cell>36.1</cell><cell>64.2</cell><cell>75.5</cell><cell>16.5</cell><cell>58.3</cell><cell>74.2</cell><cell>80.4</cell></row><row><cell>MP@Res5 (?=0.9)</cell><cell>21.9</cell><cell>35.9</cell><cell>63.9</cell><cell>75.4</cell><cell>16.7</cell><cell>57.5</cell><cell>73.7</cell><cell>80.1</cell></row><row><cell>MP@Res5 (?=0.7)</cell><cell>22.5</cell><cell>34.7</cell><cell>62.5</cell><cell>74.1</cell><cell>17.0</cell><cell>56.5</cell><cell>73.1</cell><cell>79.7</cell></row><row><cell>MP@Res5 (?=0.5)</cell><cell>23.6</cell><cell>31.9</cell><cell>59.7</cell><cell>71.8</cell><cell>17.7</cell><cell>54.7</cell><cell>71.4</cell><cell>78.5</cell></row><row><cell>Fouhey [17]</cell><cell>35.3</cell><cell>16.4</cell><cell>36.6</cell><cell>48.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ladicky [34]</cell><cell>35.5</cell><cell>24.0</cell><cell>45.6</cell><cell>55.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang [53]</cell><cell>28.8</cell><cell>35.2</cell><cell>57.1</cell><cell>65.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Eigen [12]</cell><cell>22.2</cell><cell>38.6</cell><cell>64.0</cell><cell>73.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of where to insert the MultiPool module to obtain the largest performance gain for monocular depth estimation on NYUv2 dataset. All models are evaluated over the standard split of NYUv2 dataset, with L2 loss only, and with softmax weighted average in the MultiPool module. The performance is measured by standard threshold accuracy, i.e. the percentage of predicted pixel depths d i s.t. ? = max( d? di ,d i di ) &lt; ? , where ? = {1.25, 1.25 2 , 1.25 3 }.</figDesc><table><row><cell cols="8">metrics base. MP@Res3 MP@Res4 MP@Res5 MP@Res6 MP@Res3-4 MP@Res5-6</cell></row><row><cell>1.25</cell><cell>0.711</cell><cell>0.721</cell><cell>0.726</cell><cell>0.737</cell><cell>0.725</cell><cell>0.726</cell><cell>0.726</cell></row><row><cell>1.25 2</cell><cell>0.932</cell><cell>0.935</cell><cell>0.939</cell><cell>0.939</cell><cell>0.936</cell><cell>0.938</cell><cell>0.939</cell></row><row><cell>1.25 3</cell><cell>0.985</cell><cell>0.986</cell><cell>0.986</cell><cell>0.986</cell><cell>0.985</cell><cell>0.986</cell><cell>0.985</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of loss functions and PAG unit for monocular depth estimation on NYUv2 dataset. Our MP@Res5 model is the base model, unless specified, all the models are trained with softmax weighted average in the MultiPool module. The performance is measured by standard threshold accuracy, i.e. the percentage of predicted pixel depths d i s.t. ? = max( d? di ,d i di ) &lt; ? , where ? = {1.25, 1.25 2 , 1.25 3 }.</figDesc><table><row><cell>metrics</cell><cell>L2 loss</cell><cell>L1 loss</cell><cell>L1+L2 loss</cell><cell>L1+L2 loss (PAG)</cell></row><row><cell>1.25</cell><cell>0.737</cell><cell>0.743</cell><cell>0.745</cell><cell>0.751</cell></row><row><cell>1.25 2</cell><cell>0.939</cell><cell>0.942</cell><cell>0.944</cell><cell>0.944</cell></row><row><cell>1.25 3</cell><cell>0.986</cell><cell>0.987</cell><cell>0.988</cell><cell>0.988</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Ablation study of the loss functions for surface normal estimation over NYUv2 dataset. Performance is measured by mean angular error (ang. err.) and the portion of prediction error within t ? degree where t = {11.25, 22.50, 30.00}. Smaller ang. err. means better performance as marked by ?. metrics cosine distance (?n Tn ) inverse cosine (cos ?1 n Tn ) cosine and inverse cosine ang. err.?</figDesc><table><row><cell></cell><cell>23.3462</cell><cell>23.1191</cell><cell>22.7170</cell></row><row><cell>11.25 ?</cell><cell>0.3163</cell><cell>0.3279</cell><cell>0.3382</cell></row><row><cell>22.50 ?</cell><cell>0.5995</cell><cell>0.6093</cell><cell>0.6195</cell></row><row><cell>30.00 ?</cell><cell>0.7240</cell><cell>0.7302</cell><cell>0.7383</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Ablation study of at which layer to insert MultiPool module (with softmax weighted average, w-Avg.) for surface normal estimation on NYUv2 dataset. Performance is measured by mean angular error (ang. err.) and the portion of prediction error within t ? degree where t = {11.25, 22.50, 30.00}. Smaller ang. err. means better performance as marked by ?.</figDesc><table><row><cell>metrics</cell><cell>base.</cell><cell cols="6">MP@Res3 MP@Res4 MP@Res5 MP@Res6 MP@Res3-4 MP@Res3-5</cell></row><row><cell cols="2">ang. err.? 22.7170</cell><cell>21.9951</cell><cell>22.5506</cell><cell>21.9556</cell><cell>22.1661</cell><cell>22.5183</cell><cell>22.5051</cell></row><row><cell>11.25 ?</cell><cell>0.3382</cell><cell>0.3560</cell><cell>0.3366</cell><cell>0.3567</cell><cell>0.3514</cell><cell>0.3375</cell><cell>0.3389</cell></row><row><cell>22.50 ?</cell><cell>0.6195</cell><cell>0.6362</cell><cell>0.6188</cell><cell>0.6374</cell><cell>0.6323</cell><cell>0.6198</cell><cell>0.6209</cell></row><row><cell>30.00 ?</cell><cell>0.7383</cell><cell>0.7514</cell><cell>0.7386</cell><cell>0.7526</cell><cell>0.7482</cell><cell>0.7392</cell><cell>0.7394</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Comparison of MultiPool module with PAG and softmax weighted average (w-Avg.) over surface normal estimation on NYUv2 dataset. Performance is measured by mean angular error (ang. err.) and the portion of prediction error within t ? degree where t = {11.25, 22.50, 30.00}. Smaller ang. err. means better performance as marked by ?.</figDesc><table><row><cell>metrics</cell><cell cols="4">MP@Res3 (w-Avg.) MP@Res3 (PAG) MP@Res5 (w-Avg.) MP@Res5 (PAG)</cell></row><row><cell>ang. err.?</cell><cell>21.9951</cell><cell>21.9793</cell><cell>21.9556</cell><cell>21.9226</cell></row><row><cell>11.25 ?</cell><cell>0.3560</cell><cell>0.3591</cell><cell>0.3567</cell><cell>0.3587</cell></row><row><cell>22.50 ?</cell><cell>0.6362</cell><cell>0.6396</cell><cell>0.6374</cell><cell>0.6384</cell></row><row><cell>30.00 ?</cell><cell>0.7514</cell><cell>0.7523</cell><cell>0.7526</cell><cell>0.7532</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As MatConvNet itself does not provide perforated convolution, we release the code and models implemented with multiplicative gating at https://github.com/aimerykong/Pixel-Attentional-Gating.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Clicking on the "wrong" points will leads to some normals pointing outwards the camera.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This project is supported by NSF grants IIS-1618806, IIS-1253538, a hardware donation from NVIDIA and Google Graduate Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5965" to="5974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model compression as constrained optimization, with application to neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpin?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Idelbayev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Part III: Pruning. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perforatedcnns: Acceleration through elimination of redundant convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ibraimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3392" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Statistics of extremes. Courier Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Gumbel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workship in Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Refinenet: Multipath refinement networks with identity mappings for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Geodesic saliency of watershed contours and hierarchical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1163" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accelerating deep convolutional neural networks using specialized hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microsoft Research Whitepaper</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
