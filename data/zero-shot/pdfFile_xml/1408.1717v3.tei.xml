<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matrix Completion on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-11-27">27 Nov 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
							<email>vassilis.kalofolias@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique F?d?rale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
							<email>xavier.bresson@epfl.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Ecole Polytechnique F?d?rale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
							<email>michael.bronstein@usi.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">Universit? della Svizzera Italiana</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
							<email>pierre.vandergheynst@epfl.ch</email>
							<affiliation key="aff3">
								<orgName type="department">Ecole Polytechnique F?d?rale de Lausanne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Matrix Completion on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-11-27">27 Nov 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of finding the missing values of a matrix given a few of its entries, called matrix completion, has gathered a lot of attention in the recent years. Although the problem under the standard low rank assumption is NP-hard, Cand?s and Recht showed that it can be exactly relaxed if the number of observed entries is sufficiently large. In this work, we introduce a novel matrix completion model that makes use of proximity information about rows and columns by assuming they form communities. This assumption makes sense in several real-world problems like in recommender systems, where there are communities of people sharing preferences, while products form clusters that receive similar ratings. Our main goal is thus to find a low-rank solution that is structured by the proximities of rows and columns encoded by graphs. We borrow ideas from manifold learning to constrain our solution to be smooth on these graphs, in order to implicitly force row and column proximities. Our matrix recovery model is formulated as a convex non-smooth optimization problem, for which a well-posed iterative scheme is provided. We study and evaluate the proposed matrix completion on synthetic and real data, showing that the proposed structured low-rank recovery model outperforms the standard matrix completion model in many situations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>How to reconstruct signals exactly from very few measurements? This central question in signal processing has been extensively studied in the last few years and has triggered a fast emerging field of research, namely compressed sensing. Exact recovery from few measurements is actually possible if the signal is sparse in some representation domain. A related essential question has been recently considered for matrices: is it possible to reconstruct matrices exactly from very few observations? It appears that exact recovery is also possible in this setting if the matrix is low-rank. The problem of low-rank recovery from sparse observations is referred as the matrix completion problem. Several important real-world problems can be cast as a matrix completion problem, including remote sensing <ref type="bibr" target="#b25">[26]</ref>, system identification <ref type="bibr" target="#b16">[17]</ref> and recommendation systems <ref type="bibr" target="#b26">[27]</ref>. Throughout the paper, we will consider the recommendation system problem as an illustration of the matrix completion problem. Such systems have indeed become very common in many applications such as movie or product recommendation (e.g. Netflix, Amazon, Facebook, and Apple). The Netflix recommendation system tries to predict ratings of movies never seen by users. Collaborative filtering is widely used today to solve this problem <ref type="bibr" target="#b5">[6]</ref>, inferring recommendations by finding similar rating patterns and using them to complete missing values. This is typically a matrix completion problem where the unknown values of the matrix are computed by finding a low-rank matrix that fits the given entries.</p><p>How much information is needed for the exact recovery of low-rank matrices? In the case of random uniformly sampled entries without noise, Cand?s and Recht showed in <ref type="bibr" target="#b8">[9]</ref> that, to guarantee perfect recovery, the number of observed entries must be larger than cn 1.2 r log n for n ? n matrices of rank r (this bound has been refined more recently, see <ref type="bibr" target="#b23">[24]</ref> and references therein). The case of noisy observations was studied in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, while a non-uniform sampling setting was considered in <ref type="bibr" target="#b24">[25]</ref>. In this work, we propose to use additional information about rows and columns of the matrix to further improve the matrix completion solution.</p><p>In the standard matrix completion problem, rows and columns are assumed to be completely unorganized. However, in many real-world problems like the Netflix problem, there exist relationships between users (such as their age, gender, hobbies, education, etc) and movies (such as their genre, release year, actors, origin country, etc). This information can be taken advantage of, since people sharing the same tastes for a class of movies are likely to rate them similarly. We make use of graphs to encode relationships between users and movies and we introduce a new reconstruction model called matrix completion on graphs. Our main goal is to find a low-rank matrix that is structured by the proximities between users and movies. Introducing structure in sparse recovery problems is not new in the literature of compressed sensing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16]</ref>, while similar structure inducing regularization has been proposed for factorized models for matrix completion <ref type="bibr" target="#b17">[18]</ref>. Yet, introducing structures via graphs in the convex low-rank matrix recovery setting is novel to the best of our knowledge. We note that a large class of recommendation systems, called content-based filtering, use graphs and clustering techniques to make predictions <ref type="bibr" target="#b14">[15]</ref>. Along this line, our proposed methodology can be seen as a hybrid recommendation system that combines collaborative filtering (low-rank property) and content-based filtering (graphs of users and movies).</p><p>We borrow ideas from the field of manifold learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and force the solution to be smooth on the manifolds of users and movies. We make the standard assumption that the graphs of users and movies are (non-uniform) discretizations of the corresponding manifolds. Forcing smoothness can be achieved through different regularizations. We use the popular Dirichlet/Laplacian energy, whose minimizing flow is the well-known linear heat diffusion on manifold/graph, and show that the proposed model leads to a convex non-smooth optimization problem. Convexity is a desired property for uniqueness of the solution. Non-smoothness can be highly challenging, however, our problem belongs to the class of ? 1 -type optimization problems, for which several recently proposed efficient solvers exist <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. The corresponding algorithm is derived in Section 4. It is tested on synthetic and real data <ref type="bibr" target="#b19">[20]</ref> in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Original matrix completion problem</head><p>The problem of matrix completion is to find the values of an m ? n matrix M given a sparse set ? of observations M ij : (i, j) ? ? ? {1, . . . , m} ? {1, . . . , n}. Problems of this kind are often encountered in collaborative filtering or recommender system applications, the most famous of which is the Netflix problem, in which one tries to predict the rating that n users (columns of M ) would give to m films (rows of M ), given only a few ratings provided by each user. A particularly popular model is to assume that the ratings are affected by a few factors, resulting in a low-rank matrix. This leads to the rank minimization problem min</p><formula xml:id="formula_0">X?R m?n rank(X) s.t. A ? (X) = A ? (M ),<label>(1)</label></formula><p>where A ? (M ) = (M ij?? ) denotes the observed elements of M . Problem (1) is NP-hard. However, replacing rank(X) with its convex surrogate known as the nuclear or trace norm <ref type="bibr" target="#b26">[27]</ref> X * = tr((XX ? ) 1/2 ) = k ? k , where ? k are singular values of X, one obtains a semidefinite program min</p><formula xml:id="formula_1">X?R m?n X * s.t. A ? (X) = A ? (M ).<label>(2)</label></formula><p>Under the assumption that M is sufficiently incoherent, if the indices ? are uniformly distributed and |?| is sufficiently large, the minimizer of (2) is unique and coincides with the minimizer of (1) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. If in addition the observations are contaminated by noise, one can reformulate problem <ref type="bibr" target="#b1">(2)</ref> as min</p><formula xml:id="formula_2">X?R m?n ? n X * + ? (A ? (X), A ? (M )) ,<label>(3)</label></formula><p>where the data term ? in general depends on the type of noise assumed. If ? is the squared Frobenius norm A ? ? (X ? M ) 2 F (A ? here is the observations mask matrix, ? the Hadamard product), the distance between the solution of (3) and M can be bounded by the norm of the noise <ref type="bibr" target="#b7">[8]</ref>.</p><p>One notable disadvantage of problems (2-3) is the assumption of a "good" distribution of the observed elements ?, which implies, in the movie rating example, that on average each user rates an equal number of movies, and each movie is rated by an equal number of users. In practice, this uniformity assumption is far from being realistic: for instance, in the Netflix dataset, the number of movie ratings of different users varies from 5 to 10 4 . When the sampling is nonuniform, the quality of the lower bound on |?| deteriorates dramatically <ref type="bibr" target="#b24">[25]</ref>, from approximately constant number of observations per row in the former case, to an order of n 1/3 ? n 1/2 in the latter. In such settings, Salakhutdinov and Srebro <ref type="bibr" target="#b24">[25]</ref> suggest using the weighted nuclear norm X * (p,q) = diag( ? p)Xdiag( ? q) * , where p and q are mand ndimensional row-and columnmarginals of the distribution of observations, showing a significant performance improvement over the unweighted nuclear norm. Pathologically non-uniform sampling patterns, such as an entire row or column of M missing, cannot be handled. Furthermore, in many situations the number of observations might be significantly smaller than the lower bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Matrix completion on graphs</head><p>Low rank implies the linear dependence of rows/columns of M . However, this dependence is unstructured. In many situations, the rows/columns of matrix M possess additional structure that can be incorporated into the completion problem in the form of a regularization. In this paper, we assume that rows/columns of M are given on vertices of graphs. In the Netflix example, the users (columns of M ) are the vertices of a "social graph" whose edges represent e.g. friendship or similar tastes relations. Thus, it is reasonable to assume that connected users would give similar movie ratings, i.e., interpreting the ratings as an m-dimensional vector-valued function on the n vertices of the social graph, such a function would be smooth.</p><p>More formally, let us be given the undirected weighted row graph G r = (V r , E r , W r ) with vertices V r = {1, . . . , m} and edges E r ? V r ? V r weighted with non-negative weights represented by the m ? m matrix W r ; and respectively the column graph G c = (V c , E c , W c ) defined in the same way. Let X ? R m?n be a matrix, which we will regard as a collection of m-dimensional column vectors denoted with subscripts X = (x 1 , . . . , x n ), or of n-dimensional row vectors denoted with superscripts X = ((x 1 ) ? , . . . , (x m ) ? ) ? . Regarding the columns x 1 , . . . , x n as a vector-valued function defined on the vertices V c , the smoothness assumption implies that</p><formula xml:id="formula_3">x j ? x j ? if (j, j ? ) ? E c . Stated differently, we want j,j ? w c jj ? x j ? x j ? 2 2 = tr(XL c X ? ) = X 2 D,c<label>(4)</label></formula><p>to be small, where</p><formula xml:id="formula_4">D c = Diag( n j ? =1 w c jj ? ), L c = D c ? W c</formula><p>is the Laplacian of the column graph G c , and ? D,c is the graph Dirichlet semi-norm for columns. Similarly, for the rows we get a corresponding expression tr(X ? L r X) = X 2 D,r with the Laplacian L r of the row graph G r . These smoothness terms are added to the matrix completion problem as regularization terms (in the sequel, we treat the case where ? is the squared Frobenius norm),</p><formula xml:id="formula_5">min X ? n X * + ? (A ? (X), A ? (M )) + ? r 2 X 2 D,r + ? c 2 X 2 D,c .<label>(5)</label></formula><p>Relation to simultaneous sparsity models. Low rank promoted by the nuclear norm implies sparsity in the space of outer products of the singular vectors, i.e., in the singular value decomposition X = k ? k u k v ? k only a few coefficients ? i are non-zero. Recent works <ref type="bibr" target="#b22">[23]</ref> proposed imposing additional structure constraints, considering matrices that are simultaneously low-rank (i.e., sparse in the space of singular vectors outer products) and sparse (in the original representation). Our regularization can also be considered as a kind of simultaneously structured model. The column smoothness prior (4) makes the rows of X be close to the eigenvectors of the column graph Laplacian L c , i.e., each row of X can be expressed as a linear combination of a few eigenvectors of L c . This can be interpreted as row-wise sparsity of X in the column graph Laplacian eigenbasis. Similarly, the row smoothness prior results in column-wise sparsity of X in the row graph Laplacian eigenbasis. Overall, the whole model (5) promotes simultaneous sparsity of X in the singular vectors outer product space, and row/column-wise sparsity in the respective Laplacian eigenspaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimization</head><p>Algorithm. Problems like (5) containing non-differential terms cannot be tackled efficiently with a direct approach, while proximal based methods can be applied. We use the Alternating Direction Method of Multipliers (ADMM) that has seen great success recently <ref type="bibr" target="#b4">[5]</ref> (other choices could include f.e. fast iterative soft thresholding <ref type="bibr" target="#b1">[2]</ref>) by first introducing the equivalent splitting version of (5)</p><formula xml:id="formula_6">min X,Y ?R m?n ? n X * F (X) + 1 2 A ? ? (Y ? M ) 2 F + ? r 2 Y 2 D,r + ? c 2 Y 2 D,c G(Y ) s.t. X = Y.<label>(6)</label></formula><p>This splitting step followed by an augmented Lagrangian method to handle the linear equality constraint is what constitutes ADMM. The success of ADMM for ? 1 problems is mainly due to the fact that it does not require an exact solution for the iterative sub-optimization problems, but rather an approximate solution. The augmented Lagrangian of <ref type="formula" target="#formula_6">(6)</ref> is</p><formula xml:id="formula_7">L(X, Y, Z) = F (X) + G(Y ) + Z, X ? Y + ? 2 X ? Y 2 F .</formula><p>Both F and G are closed, proper and convex, and since we have no inequality constraints Slater's conditions and therefore strong duality hold.</p><formula xml:id="formula_8">Then (X ? , Y ? ) and Z ? are primal-dual optimal if (X ? , Y ? , Z ? ) is a saddle point of the augmented Lagrangian L, i.e. sup Z inf X,Y L(X, Y, Z) = L(X ? , Y ? , Z ? ) = inf X,Y sup Z L(X, Y, Z), or L(X ? , Y ? , Z) ? L(X ? , Y ? , Z ? ) ? L(X, Y, Z ? ) ?X, Y, Z. ADMM finds a saddle point with the following iterative scheme X k+1 = argmin X L(X, Y k , Z k ),<label>(7)</label></formula><formula xml:id="formula_9">Y k+1 = argmin Y L(X k+1 , Y, Z k ),<label>(8)</label></formula><formula xml:id="formula_10">Z k+1 = Z k + ?(X k+1 ? Y k+1 ).<label>(9)</label></formula><p>Eventually the convergence of the proposed ADMM algorithm <ref type="formula" target="#formula_8">(7)</ref>- <ref type="formula" target="#formula_10">(9)</ref> can be studied (and likely proved) with different mathematical approaches, for example <ref type="bibr" target="#b6">[7]</ref>.</p><p>Solving sub-optimization problems. ADMM algorithms can be very fast as long as we can compute fast approximate solutions to the sub-optimization problems, here <ref type="formula" target="#formula_8">(7)</ref> and <ref type="formula" target="#formula_9">(8)</ref>. Problem <ref type="formula" target="#formula_8">(7)</ref> requires finding X k+1 that minimizes</p><formula xml:id="formula_11">L(X, Y k , Z k ), i.e. X k+1 = argmin X ? n X * + ?/2 X ?H 2 F , where H = Y k ?? ?1 Z k = prox F/? (H),</formula><p>where prox E is the proximal operator of E defined as prox E (H) = argmin X E(X) + 1/2 X ? H 2 F . In the case of the nuclear norm, there exists a closed-form solution: X k+1 = U soft ?n/? (?)V ? , where U, V, ? are respectively the singular value decomposition (SVD) of H, i.e. H = U ?V ? , and soft ? (?) = max(0, ? ? ?) ? |?| is the soft-thresholding operator defined for ? ? R <ref type="bibr" target="#b6">[7]</ref>. <ref type="formula" target="#formula_8">(7)</ref>, there is no closed-form solution to compute the proximal of G as the solution is given by solving a linear system of equations. Precisely, the optimality condition of <ref type="formula" target="#formula_9">(8)</ref> </p><formula xml:id="formula_12">Problem (8) requires finding Y k+1 that minimizes L(X k+1 , Y, Z k ), i.e. Y k+1 = argmin Y 1/2 A ? ? (Y ? M ) 2 F + ? r /2 Y 2 D,r + ? c /2 Y 2 D,c + ?/2 Y ? H 2 F = prox G/? (H), where H = X k+1 + ? ?1 Z k . Unlike Problem</formula><formula xml:id="formula_13">is A ? ? (Y ? M ) + ? r Y L r + ? c L c Y + ?(Y ? H) = 0,</formula><p>which can be re-written as Ay = b as follows: (? ? + ? r L r ? I n + ? c I m ? L c + ?I mn )vec(Y ) = vec(M + ?H), where vec(.) is the column-stack vectorization operator, ? is the Kronecker product, A ? = Diag(vec(A ? )) and we use the formula vec(ABC) = (C ? ? A)vec(B). Also note that A is symmetric positive semidefinite (s.p.s.d) as the Kronecker product of two s.p.s.d matrices, thus the conjugate gradient (CG) algorithm can be applied to compute a fast approximate solution of (8).</p><p>Computational complexity. The overall complexity of the algorithm is dominated by the computation of the nuclear proximal solutions by SVD, whose complexity is O(mn 2 ) per iteration for m &gt; n <ref type="bibr" target="#b11">[12]</ref>. The computational complexity of the CG algorithm is O(kmn) for k-NN graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic 'Netflix' dataset</head><p>We start the evaluation of our matrix recovery model with a synthetic Netflix-like dataset, to study the behavior of model under controlled conditions. The artificial dataset M is generated such that if fulfills two assumptions: (1) M is low-rank and (2) its columns and rows are respectively smooth w.r.t. the column graph G c and the row graph G r . <ref type="figure" target="#fig_0">Figure 1a</ref> shows our synthetic dataset. It is inspired by the problem of movie recommendations as elements of M are chosen to be integers from {1 . . . 5} like in the Netflix prize problem. The matrix in <ref type="figure" target="#fig_0">Fig. 1a</ref> is noiseless, showing the ideal ratings for each pair of user and movie groups.  The row graph G r of the matrix M is constructed as follows. The rows of M are grouped into 10 communities of different sizes. We connect nodes within a community using a 3-nearest neighbors graph and then add different amounts of erroneous edges, that is, edges between vertices belonging to different communities. The erroneous edges form a standard Erd?s-R?nyi graph with variable probability. We follow the same construction process for the column graph G c that contains 12 communities. For both graphs, binary edge weights are used. The intuition behind this choice of graphs is that users form communities of people with similar taste. Likewise, movies can be grouped according to their type, so that movies of the same group obtain similar ratings. The users graph is depicted in <ref type="figure" target="#fig_0">Fig 1b,</ref> where nodes of the same community are clustered together. Note that matrix M in <ref type="figure" target="#fig_0">Fig. 1a</ref> has rank equal to the minimum of user communities and the movie communities, in this case 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Recovery quality versus number of observations</head><p>Two standard assumptions often made in the literature on matrix completion are that the observed elements of the matrix are sampled uniformly at random, and that the reconstructed matrix is perfectly low-rank (the case that we call noiseless). Noiseless case. We test the performance of our method in this setting, comparing it to the standard nuclear norm-based matrix completion (a particular case of our problem with ? c = ? r = 0) and to a method that uses only the graphs (? n = 0). We reconstruct the matrix M using different levels of observed values and report the reconstruction root mean squared error (RMSE) on a fixed set of 35% of the elements that was not observed. The result is depicted in <ref type="figure" target="#fig_1">Fig 2a.</ref> We use graphs with 10%, 20%, and 30% of erroneous edges. Noisy graphs alone (green lines) perform poorly compared to the nuclear norm reconstruction (blue line). However, when we use both graphs and nuclear norm (red lines), we obtain results that are better than any of the two alone.</p><p>Noisy case. We add noise to M using a discretized Laplacian distribution. This type of noise models the human tendency to impulsively over-or under-rate a movie. In this case, the matrix that  we try to reconstruct is close to low-rank, and the nuclear norm is still expected to perform well. As we see in <ref type="figure" target="#fig_1">Fig. 2b</ref> though, if we have high-quality graphs (green line with 10% erroneous edges), we can expect the same reconstruction quality of the nuclear norm regularization by using just half of the number of observations and only with the graph smoothness terms (green dashed line), that computationally is much cheaper to run. In this figure, the dashed black line designates the level of added noise in the data. Note also that even if we use connectivity information of relatively bad quality (green dashed line with 30% wrong edges), we can still benefit by combining the smoothness and the low-rank regularization terms (solid red line). Therefore the combination of nuclear and graph is robust to graph construction errors for low levels of observation. However, when the observation level is high enough (&gt; 50% for this specific size of matrices -note that this number may vary significantly depending on the matrix size), this benefit is lost (solid red line) and the nuclear norm regularizer (blue line) works better without the graph smoothness terms.</p><p>Non-uniform sampling. As noted in <ref type="bibr" target="#b24">[25]</ref>, the pattern of the observed values in real datasets does not usually follow a uniform distribution. In fact, the observations are such that the rating frequencies of users and movies closely follow power law distributions. In our experiment, we assume a simple generative process where users and movies are independently sampled from a power law, that is pr (sample{i, j}) = 1/ij. This is a very sparse distribution with fixed expected number of observations, so we repeat this process identically s times in order to control the overall density. Our final sampling is the logical OR operator of all these s 'epochs', that follows the distribution p ({i, j} ? ?) = 1 ? (1 ? 1/ij) s . We find that this simple sampling scheme gives results close to the actual ratings of real datasets such as the MovieLens 10M that we use in the following. The results of our experiments for this setting are summarized in <ref type="figure" target="#fig_3">Fig. 3a</ref>. Not surprisingly, all methods suffer from the non-uniformity of the sampling distribution. Still, the nuclear norm (blue line) crosses the line of a high-quality graph (10% green line) only after 35% observations, while in the uniform case, <ref type="figure" target="#fig_1">Fig. 2a</ref>, this happened for less than 20% observed values. A similar behavior is exhibited for the noisy case, <ref type="figure" target="#fig_3">Fig. 3b</ref>. There, the nuclear norm regularization quality is better than the medium-quality graph (20% green line) only for more than 45% observations, while in the uniform case, <ref type="figure" target="#fig_1">Fig. 2b</ref>, the corresponding percentage was 25%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Movielens dataset</head><p>In this section, we report experiments on real data, which appear consistent with the results on the aforementioned artificial data. We work with the widely used MovieLens 10M dataset <ref type="bibr" target="#b19">[20]</ref>, containing ratings ('stars') from 0.5 to 5.0 (increments of 0.5) given by 71,567 users for 10,677 movies. The density of the observations is 1.31%. In our experiments, we use a 500 ? 500 subset of the original matrix for the reconstruction evaluation. This serves two purposes: firstly, we can choose an arbitrary density of the submatrix, and secondly, we can use ratings outside of it as features for the construction of the column and row graphs, as detailed below (see <ref type="figure" target="#fig_5">Figure 4a</ref>). Furthermore, the effect of non-uniformity is weaker. The density of the observations is selected as follows. We sort the rows (users) and columns (movies)  by order of increasing sampling frequency <ref type="figure" target="#fig_5">(Figure 4b</ref>). Then, users and movies are chosen to be close to the 99-th and 95-th percentile of their corresponding distributions. <ref type="bibr" target="#b0">1</ref> The resulting 500 ? 500 matrix has 39.4% observed values that correspond to the ratings that a user has given to a movie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>After a row and column permutation, the original MovieLens 10M matrix A is partitioned in blocks</head><formula xml:id="formula_14">A = [M, F u ; F m , R],</formula><p>where M is the 500 ? 500 matrix that we use for our experiments <ref type="figure" target="#fig_5">(Figure 4a</ref>). We treat F u as the users feature matrix, F m as the movies feature matrix and discard the remaining matrix R.</p><p>Graph construction. Quality of graphs obviously plays an important role to our matrix recovery algorithm. Since a detailed analysis of how to construct good graphs is beyond the scope of this paper, we will resort to a simple, yet natural way of constructing the graphs for our setting, using the feature matrices F u and F m . We adapt the basic algorithm of <ref type="bibr" target="#b3">[4]</ref> to our setting that contains missing values. The distance we use between two users is the RMS distance between their commonly rated movies</p><formula xml:id="formula_15">d uij = F ui ? F uj ?u ij ?2 / |? uij |, ? uij = ? ui ? ? uj ,</formula><p>where ? ui is the set of observed movie ratings for user (row) i in F u and |? uij | is the number of movies in F u that both users i and j have rated. We do the same to construct the movie distances from F m , that is, for each pair of movies we only take into account the ratings from users that have rated both. Note that distances between movies or between users, that take values from [0, 4.5] stars, share the same scale with the ratings and with the reconstruction error. Since the distances are all Euclidean, choosing the parameters of the graphs becomes more natural. The first choice we make is to use an ?-neighborhood graph instead of a k-NN graph. To give weights to the edges, we use a Gaussian kernel, that is, w uij = exp ? d uij ? d min-u 2 /? if d uij &lt; ?, 0 otherwise. In the latter, d min-u denotes the minimum distance among all pairs of users and ? controls how fast the weights decay as distances increase. The transfer function used for the movies graph is plotted in <ref type="figure" target="#fig_8">Fig. 5a</ref>, while the one for users is nearly identical.</p><p>We give weight values equal to 1 for distances close to the minimum one (around 0.6 stars), while the weights decay fast as the distance increases. We choose ? = 1.1 star, while ? is chosen so that the transfer function is already very close to 0 for d uij ? ?. This means that our model is equivalent to an inf-NN graph with the same exponential kernel. Note that the final reconstruction error is better than 1.1 star in RMS, which justifies that distances that are smaller than that are trusted. We found that the results are indeed much better when a k-NN graph is not used. A possible explanation for this is that in that case a user that deviates a lot from the habits of other users would still have k connections. These connections would not contribute positively in the recommendations quality regarding this user. All this being said, it is here essential to emphasize that the graphs constructed for these experiments are not optimal. We foresee that the results presented in this paper can be further improved if one has access to detailed profile information about users and movies/products. This information is available to typical business companies that sell products to users.   Results. We apply a standard cross-validation technique to evaluate the quality of our completion algorithm. For this purpose, the 39.4% observations of the 500 ? 500 matrix are split into a fixed test set (7.4%) and a varying size training set (from 1% to 32%). We perform 5-fold cross validation to select the parameters ? n , ? r and ? c of our model <ref type="bibr" target="#b4">(5)</ref> and only use the test set to evaluate the performance of the final models. The recommendation error results are plotted in <ref type="figure" target="#fig_8">Fig. 5b</ref>. The behavior of the algorithms is similar to the one exhibited by the noisy artificial data above (medium quality of graphs). For most observation levels our method combining nuclear norm and graph regularization (red line) clearly outperforms the rest. There are however two boundary phases that are noteworthy. When very few observations are available (1%) there seems to be no benefit in adding the expensive nuclear norm term in the optimization problem, as the graph regularization alone (green line) performs best. On the other hand, for very dense observation levels (32%) the nuclear norm (blue line) reaches the performance of the combined model. In general our combined model is very robust to observation sparsity, while the standard nuclear norm model performs worse even than the much cheaper graphs-only model for up to 8% observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The main message of this work is that the standard low-rank matrix recovery problem can be further improved using similarity information about rows and columns. We solve an optimization problem seeking a low-rank solution that is structured by the proximity between rows and columns that form communities. As an application, our matrix completion model offers a new recommendation algorithm that combines the traditional collaborative filtering and content-based filtering tasks into one unified model. The associated convex non-smooth optimization problem is solved with a well-posed iterative ADMM scheme, which alternates between nuclear proximal operators and approximate solutions of linear systems. Artificial and real data experiments are conducted to study and validate the proposed matrix recovery model, suggesting that in real-life applications where the number of available matrix entries (ratings) is usually low and information about products and people taste is available, our model would outperform the standard matrix completion approaches. Specifically, our model is robust to graph construction and to non-uniformly sampling of observations. Furthermore, it significantly outperforms the standard matrix completion when the number of observations is small. The proposed matrix recovery algorithm can be improved in several ways. The effect of the nonuniformity of sampling matrix entries, as discussed in Sections 3 and 5.2, can be partially alleviated using a special weighting of the nuclear norm <ref type="bibr" target="#b24">[25]</ref>. The non-uniform sampling of user data points and movie data points from the corresponding manifolds, which influences the quality of graph Laplacians, can also be corrected using special graph normalizations <ref type="bibr" target="#b9">[10]</ref>. Furthermore, the optimization algorithm can be improved, firstly in terms of speed by using enhanced iterative schemes like <ref type="bibr" target="#b21">[22]</ref>. Secondly in terms of scalability, either by using distributed schemes like <ref type="bibr" target="#b18">[19]</ref> or by carrying out techniques from the recent work <ref type="bibr" target="#b12">[13]</ref>, which deals with nuclear norm for matrices with sizes much bigger than the Netflix dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Rank-10 matrix M of ideal ratings (b) Graph of users with 25% of wrong edges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Matrix recovery error on synthetic 'Netflix' dataset (uniform sampling). Percentage of erroneous edges in graphs is shown on top of green and red lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Matrix recovery error on synthetic 'Netflix' dataset (non-uniform sampling). Percentage of erroneous edges in graphs is shown on top of green and red lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Submatrix M ? A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Movielens 10M dataset. The submatrix M of A is used for training and testing. The blocks F m and F u are used to construct the movie and user graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Transfer function used for movies graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Experiments on a part of the Movielens-10M dataset</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since the number of movies in the full matrix is much smaller than the number of users, we keep more frequently rating users in order to have a dense features matrix when we create the users graph.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model-based Compressive Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Empirical analysis of predictive algorithms for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncertainty in Artificial Intelligence</title>
		<meeting>Conf. Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matrix completion with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Plan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="925" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FCM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Diffusion Maps. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="5" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proximal splitting methods in signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fixedpoint algorithms for inverse problems in science and engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="185" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Matrix computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JHU Press</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nuclear norm minimization via active subspace selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A graph-based recommender system for digital library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE-CS joint conference on Digital libraries</title>
		<meeting>ACM/IEEE-CS joint conference on Digital libraries</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured Variable Selection with Sparsity-Inducing Norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2777" to="2824" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interior-Point Method for Nuclear Norm Approximation with Application to System Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1235" to="1256" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed nuclear norm minimization for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPAWC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="354" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MovieLens unplugged: Experiences with an occasionally connected recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Intelligent User Interfaces</title>
		<meeting>Conf. Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Restricted strong convexity and weighted matrix completion: Optimal bounds with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Negahban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1665" to="1697" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On first-order algorithms for l 1/nuclear norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="509" to="575" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Simultaneously structured models with application to sparse and low-rank matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.3753</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simpler approach to matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3413" to="3430" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative filtering in a non-uniform world: Learning with the weighted trace norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple emitter location and signal parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Antennas and Propagation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="276" to="280" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum-margin matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1329" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
