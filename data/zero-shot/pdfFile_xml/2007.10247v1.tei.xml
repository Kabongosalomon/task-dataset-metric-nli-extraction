<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Joint Spatial-Temporal Transformations for Video Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Joint Spatial-Temporal Transformations for Video Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Video Inpainting; Generative Adversarial Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-quality video inpainting that completes missing regions in video frames is a promising yet challenging task. State-of-the-art approaches adopt attention models to complete a frame by searching missing contents from reference frames, and further complete whole videos frame by frame. However, these approaches can suffer from inconsistent attention results along spatial and temporal dimensions, which often leads to blurriness and temporal artifacts in videos. In this paper, we propose to learn a joint Spatial-Temporal Transformer Network (STTN) for video inpainting. Specifically, we simultaneously fill missing regions in all input frames by self-attention, and propose to optimize STTN by a spatial-temporal adversarial loss. To show the superiority of the proposed model, we conduct both quantitative and qualitative evaluations by using standard stationary masks and more realistic moving object masks. Demo videos are available at https://github.com/researchmm/STTN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video inpainting is a task that aims at filling missing regions in video frames with plausible contents <ref type="bibr" target="#b1">[2]</ref>. An effective video inpainting algorithm has a wide range of practical applications, such as corrupted video restoration <ref type="bibr" target="#b9">[10]</ref>, unwanted object removal <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>, video retargeting <ref type="bibr" target="#b15">[16]</ref> and under/over-exposed image restoration <ref type="bibr" target="#b17">[18]</ref>. Despite of the huge benefits of this technology, high-quality video inpainting still meets grand challenges, such as the lack of high-level understanding of videos <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref> and high computational complexity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Significant progress has been made by using 3D convolutions and recurrent networks for video inpainting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>. These approaches usually fill missing regions by aggregating information from nearby frames. However, they suffer from temporal artifacts due to limited temporal receptive fields. To solve the above challenge, state-of-the-art methods apply attention modules to capture long-range correspondences, so that visible contents from distant frames can be used to fill missing regions in a target frame <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. One of these approaches synthesizes missing contents by a weighting sum over the aligned frames with frame-wise attention <ref type="bibr" target="#b17">[18]</ref>. The other approach proposes a step-by-step fashion, which gradually fills missing regions with similar pixels from boundary towards the inside by pixel-wise attention <ref type="bibr" target="#b24">[25]</ref>. Although promising results have been shown, these methods have two major limitations due to the significant appearance changes caused by complex motions in videos. One limitation is that these methods usually assume global affine transformations or homogeneous motions, which makes them hard to model complex motions and often leads to inconsistent matching in each frame or in each step. Another limitation is that all videos are processed frame by frame without specially-designed optimizations for temporal coherence. Although post-processing is usually used to stabilize generated videos, it is usually time-costing. Moreover, the post-processing may fail in cases with heavy artifacts.</p><p>To relieve the above limitations, we propose to learn a joint Spatial-Temporal Transformer Network (STTN) for video inpainting. We formulate video inpainting as a "multi-to-multi" problem, which takes both neighboring and distant frames as input and simultaneously fills missing regions in all input frames. To fill missing regions in each frame, the transformer searches coherent contents from all the frames along both spatial and temporal dimensions by a proposed multi-scale patch-based attention module. Specifically, patches of different scales are extracted from all the frames to cover different appearance changes caused by complex motions. Different heads of the transformer calculate similarities on spatial patches across different scales. Through such a design, the most relevant patches can be detected and transformed for the missing regions by aggregating attention results from different heads. Moreover, the spatial-temporal transformers can be fully exploited by stacking multiple layers, so that attention results for missing regions can be improved based on updated region features. Last but not least, we further leverage a spatial-temporal adversarial loss for joint opti-mization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Such a loss design can optimize STTN to learn both perceptually pleasing and coherent visual contents for video inpainting.</p><p>In summary, our main contribution is to learn joint spatial and temporal transformations for video inpainting, by a deep generative model with adversarial training along spatial-temporal dimensions. Furthermore, the proposed multi-scale patch-based video frame representations can enable fast training and inference, which is important to video understanding tasks. We conduct both quantitative and qualitative evaluations using both stationary masks and moving object masks for simulating real-world applications (e.g., watermark removal and object removal). Experiments show that our model outperforms the stateof-the-arts by a significant margin in terms of PSNR and VFID with relative improvements of 2.4% and 19.7%, respectively. We also show extensive ablation studies to verify the effectiveness of the proposed spatial-temporal transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To develop high-quality video inpainting technology, many efforts have been made on filling missing regions with spatially and temporally coherent contents in videos <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>. We discuss representative patch-based methods and deep generative models for video inpainting as below.</p><p>Patch-based methods: Early video inpainting methods mainly formulate the inpainting process as a patch-based optimization problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. Specifically, these methods synthesize missing contents by sampling similar spatial or spatial-temporal patches from known regions based on a global optimization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. Some approaches try to improve performance by providing foreground and background segments <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. Other works focus on joint estimations for both appearance and optical-flow <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. Although promising results can be achieved, patch-based optimization algorithms typically assume a homogeneous motion field in holes and they are often limited by complex motion in general situations. Moreover, optimization-based inpainting methods often suffer from high computational complexity, which is infeasible for real-time applications <ref type="bibr" target="#b32">[33]</ref>.</p><p>Deep generative models: With the development of deep generative models, significant progress has been made by deep video inpainting models. Wang et al. are the first to propose to combine 3D and 2D fully convolution networks for learning temporal information and spatial details for video inpainting <ref type="bibr" target="#b28">[29]</ref>. However, the results are blurry in complex scenes. Xu et al. improve the performance by jointly estimating both appearance and optical-flow <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref>. Kim et al. adopt recurrent networks for ensuring temporal coherence <ref type="bibr" target="#b15">[16]</ref>. Chang et al. develop Temporal SN-PatchGAN <ref type="bibr" target="#b34">[35]</ref> and temporal shift modules <ref type="bibr" target="#b18">[19]</ref> for freeform video inpainting <ref type="bibr" target="#b4">[5]</ref>. Although these methods can aggregate information from nearby frames, they fail to capture visible contents from distant frames.</p><p>To effectively model long-range correspondences, recent models have adopted attention modules and show promising results in image and video synthesis <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically, Lee et al. propose to synthesize missing contents by weighted summing aligned frames with frame-wise attention <ref type="bibr" target="#b17">[18]</ref>. However, the frame-wise attention relies on global affine transformations between frames, which is hard to handle complex motions. Oh et al. gradually fill holes step by step with pixel-wise attention <ref type="bibr" target="#b24">[25]</ref>. Despite promising results, it is hard to ensure consistent attention result in each recursion. Moreover, existing deep video inpainting models that adopt attention modules process videos frame by frame without specially-designed optimization for ensuring temporal coherence.</p><p>3 Spatial-Temporal Transformer Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall design</head><p>Problem formulation: Let X T 1 := {X 1 , X 2 , ..., X T } be a corrupted video sequence of height H, width W and frames length T . M T 1 := {M 1 , M 2 , ..., M T } denotes the corresponding frame-wise masks. For each mask M i , value "0" indicates known pixels, and value "1" indicates missing regions. We formulate deep video inpainting as a self-supervised task that randomly creates (X T 1 , M T 1 ) pairs as input and reconstruct the original video frames</p><formula xml:id="formula_0">Y T 1 = {Y 1 , Y 2 , ..., Y T }.</formula><p>Specifically, we propose to learn a mapping function from masked video X T 1 to the output? T 1 := {? 1 ,? 2 , ...,? T }, such that the conditional distribution of the real data p(Y T 1 |X T 1 ) can be approximated by the one of generated data p(? T 1 |X T 1 ).</p><p>The intuition is that an occluded region in a current frame would probably be revealed in a region from a distant frame, especially when a mask is large or moving slowly. To fill missing regions in a target frame, it is more effective to borrow useful contents from the whole video by taking both neighboring frames and distant frames as conditions. To simultaneously complete all the input frames in a single feed-forward process, we formulate the video inpainting task as a "multi-to-multi" problem. Based on the Markov assumption <ref type="bibr" target="#b10">[11]</ref>, we simplify the "multi-to-multi" problem and denote it as:</p><formula xml:id="formula_1">p(? T 1 |X T 1 ) = T t=1 p(? t+n t?n |X t+n t?n , X T 1,s ),<label>(1)</label></formula><p>where X t+n t?n denotes a short clip of neighboring frames with a center moment t and a temporal radius n. X T 1,s denotes distant frames that are uniformly sampled from the videos X T 1 in a sampling rate of s. Since X T 1,s can usually cover most key frames of the video, it is able to describe "the whole story" of the video. Under this formulation, video inpainting models are required to not only preserve temporal consistency in neighboring frames, but also make the completed frames to be coherent with "the whole story" of the video.</p><p>Network design: The overview of the proposed Spatial-Temporal Transformer Networks (STTN) is shown in <ref type="figure">Figure 2</ref>. As indicated in Eq. (1), STTN takes both neighboring frames X t+n t?n and distant frames X T 1,s as conditions, and complete all the input frames simultaneously. Specifically, STTN consists of three components, including a frame-level encoder, multi-layer multi-head spatialtemporal transformers, and a frame-level decoder. The frame-level encoder is built by stacking several 2D convolution layers with strides, which aims at encoding deep features from low-level pixels for each frame. Similarly, the frame-level decoder is designed to decode features back to frames. Spatial-temporal transformers are the core component, which aims at learning joint spatial-temporal transformations for all missing regions in the deep encoding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial-temporal transformer</head><p>To fill missing regions in each frame, spatial-temporal transformers are designed to search coherent contents from all the input frames. Specifically, we propose to search by a multi-head patch-based attention module along both spatial and temporal dimensions. Different heads of a transformer calculate attentions on spatial patches across different scales. Such a design allows us to handle appearance changes caused by complex motions. For example, on one hand, attentions for patches of large sizes (e.g., frame size H ? W ) aim at completing stationary backgrounds. On the other hand, attentions for patches of small sizes (e.g., A multi-head transformer runs multiple "Embedding-Matching-Attending" steps for different patch sizes in parallel. In the Embedding step, features of each frame are mapped into query and memory (i.e., key-value pair) for further retrieval. In the Matching step, region affinities are calculated by matching queries and keys among spatial patches that are extracted from all the frames. Finally, relevant regions are detected and transformed for missing regions in each frame in the Attending step. We introduce more details of each step as below.</p><p>Embedding:</p><formula xml:id="formula_2">We use f T 1 = {f 1 , f 2 , ..., f T },</formula><p>where f i ? R h?w?c to denote the features encoded from the frame-level encoder or former transformers, which is the input of transformers in <ref type="figure">Fig. 2</ref>. Similar to many sequence modeling models, mapping features into key and memory embeddings is an important step in transformers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>. Such a step enables modeling deep correspondences for each region in different semantic spaces:</p><formula xml:id="formula_3">q i , (k i , v i ) = M q (f i ), (M k (f i ), M v (f i )),<label>(2)</label></formula><formula xml:id="formula_4">where 1 ? i ? T , M q (?), M k (?)</formula><p>and M v (?) denote the 1 ? 1 2D convolutions that embed input features into query and memory (i.e., key-value pair) feature spaces while maintaining the spatial size of features.</p><p>Matching: We conduct patch-based matching in each head. In practice, we first extract spatial patches of shape r 1 ? r 2 ? c from the query feature of each frame, and we obtain N = T ? h/r 1 ? w/r 2 patches. Similar operations are conducted to extract patches in the memory (i.e., key-value pair in the transformer). Such an effective multi-scale patch-based video frame representation can avoid redundant patch matching and enable fast training and inference. Specifically, we reshape the query patches and key patches into 1-dimension vectors separately, so that patch-wise similarities can be calculated by matrix multiplication. The similarity between i-th patch and j-th patch is denoted as:</p><formula xml:id="formula_5">s i,j = p q i ? (p k j ) T ? r 1 ? r 2 ? c ,<label>(3)</label></formula><p>where 1 ? i, j ? N , p q i denotes the i-th query patch, p k j denotes the j-th key patch. The similarity value is normalized by the dimension of each vector to avoid a small gradient caused by subsequent softmax function <ref type="bibr" target="#b27">[28]</ref>. Corresponding attention weights for all patches are calculated by a softmax function:</p><formula xml:id="formula_6">? i,j = ? ? ? ? ? ? ? exp(s i,j )/ N n=1 exp(s i,n ), p j ? ?, 0, p j ??.<label>(4)</label></formula><p>where ? denotes visible regions outside masks, and? denotes missing regions. Naturally, we only borrow features from visible regions for filling holes. Attending: After modeling the deep correspondences for all spatial patches, the output for the query of each patch can be obtained by weighted summation of values from relevant patches: where p v j denotes the j-th value patch. After receiving the output for all patches, we piece all patches together and reshape them into T frames with original spatial size h ? w ? c. The resultant features from different heads are concatenated and further passed through a subsequent 2D residual block <ref type="bibr" target="#b11">[12]</ref>. This subsequent processing is used to enhance the attention results by looking at the context within the frame itself.</p><formula xml:id="formula_7">o i = N j=1 ? i,j p v j ,<label>(5)</label></formula><p>The power of the proposed transformer can be fully exploited by stacking multiple layers, so that attention results for missing regions can be improved based on updated region features in a single feed-forward process. Such a multilayer design promotes learning coherent spatial-temporal transformations for filling in missing regions. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we highlight the attention maps learned by STTN in the last layer in bright yellow. For the dog partially occluded by a random mask in a target frame, spatial-temporal transformers are able to "track" the moving dog over the video in both spatial and temporal dimensions and fill missing regions in the dog with coherent contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization objectives</head><p>As outlined in Section 3.1, we optimize the proposed STTN in an end-to-end manner by taking the original video frames as ground truths without any other labels. The principle of choosing optimization objectives is to ensure per-pixel reconstruction accuracy, perceptual rationality and spatial-temporal coherence in generated videos <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. To this end, we select a pixel-wise reconstruction loss and a spatial-temporal adversarial loss as our optimization objectives.</p><p>In particular, we include L 1 losses calculated between generated frames and original frames for ensuring per-pixel reconstruction accuracy in results. The L 1 losses for hole regions are denoted as:</p><formula xml:id="formula_8">L hole = M T 1 (Y T 1 ?? T 1 ) 1 M T 1 1 ,<label>(6)</label></formula><p>and corresponding L 1 losses for valid regions are denoted as:</p><formula xml:id="formula_9">L valid = (1 ? M T 1 ) (Y T 1 ?? T 1 ) 1 1 ? M T 1 1 ,<label>(7)</label></formula><p>where indicates element-wise multiplication, and the values are normalized by the size of corresponding regions. Inspired by the recent studies that adversarial training can help to ensure high-quality content generation results, we propose to use a Temporal Patch-GAN (T-PatchGAN) as our discriminator <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. Such an adversarial loss has shown promising results in enhancing both perceptual quality and spatialtemporal coherence in video inpainting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In particular, the T-PatchGAN is composed of six layers of 3D convolution layers. The T-PatchGAN learns to distinguish each spatial-temporal feature as real or fake, so that spatial-temporal coherence and local-global perceptual details of real data can be modeled by STTN. The detailed optimization function for the T-PatchGAN discriminator is shown as follows: <ref type="bibr" target="#b7">(8)</ref> and the adversarial loss for STTN is denoted as:</p><formula xml:id="formula_10">L D = E x?P Y T 1 (x) [ReLU (1 ? D(x))] + E z?P? T 1 (z) [ReLU (1 + D(z))],</formula><formula xml:id="formula_11">L adv = ?E z?P? T 1 (z) [D(z)].<label>(9)</label></formula><p>The overall optimization objectives are concluded as below:</p><formula xml:id="formula_12">L = ? hole ? L hole + ? valid ? L valid + ? adv ? L adv .<label>(10)</label></formula><p>We empirically set the weights for different losses as: ? hole = 1, L valid = 1, L adv = 0.01. Since our model simultaneously complete all the input frames in a single feed-forward process, our model runs at 24.3 fps on a single GPU NVIDIA V100. More details are provided in the Section D of our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To evaluate the proposed model and make fair comparisons with SOTA approaches, we adopt the two most commonly-used datasets in video inpainting, including Youtube-VOS <ref type="bibr" target="#b31">[32]</ref> and DAVIS <ref type="bibr" target="#b2">[3]</ref>. In particular, YouTube-VOS contains 4,453 videos with various scenes, including bedrooms, streets, and so on. The average video length in Youtube-VOS is about 150 frames. We follow the original train/validation/test split (i.e., 3,471/474/508) and report experimental results on the test set for Youtube-VOS. In addition, we also evaluate different approaches on DAVIS dataset <ref type="bibr" target="#b2">[3]</ref>, as this dataset is composed of 150 high-quality videos of challenging camera motions and foreground motions. We follow the setting in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>, and set the training/testing split as 60/90 videos. Since the training set of DAVIS is limited (60 videos with at most 90 frames for each), we initialize model weights by a pre-trained model on YouTube-VOS following the settings used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>To simulate real-world applications, we evaluate models by using two types of free-form masks, including stationary masks and moving masks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Because free-form masks are closer to real masks and have been proved to be effective for training and evaluating inpainting models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. Specifically, for testing stationary masks, we generate stationary random shapes as testing masks to simulate applications like watermark removal. More details of the generation algorithm are provided in the Section B of our supplementary material. Since this type of application targets at reconstructing original videos, we take original videos as ground truths and evaluate models from both quantitative and qualitative aspects. For testing moving masks, we use foreground object annotations as testing masks to simulate applications like object removal. Since the ground truths after foreground removal are unavailable, we evaluate the models through qualitative analysis following previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and evaluation metrics</head><p>Recent deep video inpainting approaches have shown state-of-the-art performance with fast computational time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. To evaluate our model and make fair comparisons, we select the most recent and the most competitive approaches for comparisons, which are listed as below:</p><p>-VINet <ref type="bibr" target="#b15">[16]</ref> adopts a recurrent network to aggregate temporal features from neighboring frames. -DFVI <ref type="bibr" target="#b32">[33]</ref> fills missing regions in videos by pixel propagation algorithm based on completed optical flows. -LGTSM <ref type="bibr" target="#b5">[6]</ref> proposes a learnable temporal shift module and a spatialtemporal adversarial loss for ensuring spatial and temporal coherence. -CAP <ref type="bibr" target="#b17">[18]</ref> synthesizes missing contents by a deep alignment network and a frame-based attention module. We fine-tune baselines multiple times on YouTube-VOS <ref type="bibr" target="#b31">[32]</ref> and DAVIS <ref type="bibr" target="#b2">[3]</ref> by their released models and codes and report their best results in this paper.</p><p>We report quantitative results by four numeric metrics, i.e., PSNR <ref type="bibr" target="#b32">[33]</ref>, SSIM <ref type="bibr" target="#b4">[5]</ref>, flow warping error <ref type="bibr" target="#b16">[17]</ref> and video-based Fr?chet Inception Distance (VFID) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, we use PSNR and SSIM as they are the most widely-used metrics for video quality assessment. Besides, the flow warping error is included to measure the temporal stability of generated videos. Moreover, FID has been proved to be an effective perceptual metric and it has been used by many inpainting models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38]</ref>. In practice, we use an I3D <ref type="bibr" target="#b3">[4]</ref> pre-trained video recognition model to calculate VFID following the settings in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with state-of-the-arts</head><p>Quantitative Evaluation: We report quantitative results for filling stationary masks on Youtube-VOS <ref type="bibr" target="#b31">[32]</ref> and DAVIS <ref type="bibr" target="#b2">[3]</ref> in <ref type="table">Table 1</ref>. As stationary masks often involve partially occluded foreground objects, it is challenging to reconstruct a video especially with complex appearances and object motions. <ref type="table">Table  1</ref> shows that, compared with SOTA models, our model performs better video reconstruction quality with both per-pixel and overall perceptual measurements. Specifically, our model outperforms the SOTA models by a significant margin, especially in terms of PSNR, flow warp error and VFID. The specific gains are 2.4%, 1.3% and 19.7% relative improvements on Youtube-VOS, respectively. The superior results show the effectiveness of the proposed spatial-temporal transformer and adversarial optimizations in STTN.  <ref type="table">Table 1</ref>. Quantitative comparisons with state-of-the-art models on Youtube-VOS <ref type="bibr" target="#b31">[32]</ref> and DAVIS <ref type="bibr" target="#b2">[3]</ref>. Our model outperforms baselines in terms of PSNR <ref type="bibr" target="#b32">[33]</ref>, SSIM <ref type="bibr" target="#b4">[5]</ref>, flow warping error (Ewarp) <ref type="bibr" target="#b16">[17]</ref> and VFID <ref type="bibr" target="#b29">[30]</ref>. Higher is better. ? Lower is better.</p><formula xml:id="formula_13">Models PSNR SSIM (%) Ewarp (%) ? VFID ? Youtube-</formula><p>Qualitative Evaluation: For each video from test sets, we take all frames for testing. To compare visual results from different models, we follow the setting used by most video inpainting works and randomly sample three frames from the video for case study <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. We select the most three competitive models, DFVI <ref type="bibr" target="#b32">[33]</ref>, LGTSM <ref type="bibr" target="#b5">[6]</ref> and CAP <ref type="bibr" target="#b17">[18]</ref> for comparing results for stationary masks in <ref type="figure">Fig. 4</ref>. We also show a case for filling in moving masks in <ref type="figure">Fig. 5</ref>. To conduct pair-wise comparisons and analysis in <ref type="figure">Fig. 5</ref>, we select the most competitive model, CAP <ref type="bibr" target="#b17">[18]</ref>, according to the quantitative comparison results. We can find from the visual results that our model is able to generate perceptually pleasing and coherent contents in results. More video cases are available online ? .</p><p>In addition to visual comparisons, we visualize the attention maps learned by STTN in <ref type="figure">Fig. 6</ref> are selected according to the attention weights calculated by Eq. (4). We can find in <ref type="figure">Fig. 6</ref> that STTN is able to precisely attend to the objects for filling partially occluded objects in the first and the third cases. For filling the backgrounds in the second and the fourth cases, STTN can correctly attend to the backgrounds.</p><p>User Study: We conduct a user study for a more comprehensive comparison. we choose LGTSM <ref type="bibr" target="#b5">[6]</ref> and CAP <ref type="bibr" target="#b17">[18]</ref> as two strong baselines, since we have observed their significantly better performance than other baselines from both quantitative and qualitative results. We randomly sampled 10 videos (5 from DAVIS and 5 from YouTube-VOS) for stationary masks filling, and 10 videos from DAVIS for moving masks filling. In practice, 28 volunteers are invited to the user study. In each trial, inpainting results from different models are shown to the volunteers, and the volunteers are required to rank the inpainting results. To ensure a reliable subjective evaluation, videos can be replayed multiple times by volunteers. Each participant is required to finish 20 groups of trials without time limit. Most participants can finish the task within 30 minutes. The results of the user study are concluded in <ref type="figure">Fig 7.</ref> We can find that our model performs better in most cases for these two types of masks. ours CAP input frame <ref type="figure">Fig. 5</ref>. Visual comparisons for filling moving masks. Comparing with CAP <ref type="bibr" target="#b17">[18]</ref>, one of the most competitive models for filling moving masks, our model is able to generate visually pleasing results even under complex scenes (e.g., clear faces for the first and the third frames, and better results than CAP for the second frame).  <ref type="figure">Fig. 7</ref>. User study. "Rank x" means the percentage of results from each model being chosen as the x-th best. Our model is ranked in first place in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To verify the effectiveness of the spatial-temporal transformers, this section presents ablation studies on DAVIS dataset <ref type="bibr" target="#b2">[3]</ref> with stationary masks. More ablation studies can be found in the Section E of our supplementary material. Effectiveness of multi-scale: To verify the effectiveness of using multiscale patches in multiple heads, we compare our model with several single-head STTNs with different patch sizes. In practice, we select patch sizes according to the spatial size of features, so that the features can be divided into patches without overlapping. The spatial size of features in our experiments is 108 ? 60. Results in <ref type="table" target="#tab_4">Table 2</ref> show that our full model with multi-scale patch-based video frame representation achieves the best performance under this setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of multi-layer:</head><p>The spatial-temporal transformers can be stacked by multiple layers to repeat the inpainting process based on updated region features. We verify the effectiveness of using multi-layer spatial-temporal transformers in <ref type="table" target="#tab_5">Table 3</ref>. We find that stacking more transformers can bring continuous improvements and the best results can be achieved by stacking eight layers. Therefore, we use eight layers in transformers as our full model. </p><formula xml:id="formula_14">Stack PSNR SSIM(%) Ewarp (%) ? VFID ?<label>?2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel joint spatial-temporal transformation learning for video inpainting. Extensive experiments have shown the effectiveness of multi-scale patch-based video frame representation in deep video inpainting models. Coupled with a spatial-temporal adversarial loss, our model can be optimized to simultaneously complete all the input frames in an efficient way. The results on YouTube-VOS <ref type="bibr" target="#b31">[32]</ref> and DAVIS <ref type="bibr" target="#b2">[3]</ref> with challenging free-form masks show the state-of-the-art performance by our model. We note that STTN may generate blurs in large missing masks if continuous quick motions occur. As shown in <ref type="figure">Fig. 8</ref>, STTN fails to generate continuous dancing motions and it generates blurs when reconstructing the dancing woman in the first frame. We infer that STTN only calculates attention among spatial patches, and the short-term temporal continuity of complex motions are hard to capture without 3D representations. In the future, we plan to extend the proposed transformer by using attention on 3D spatial-temporal patches to improve the short-term coherence. We also plan to investigate other types of temporal losses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref> for joint optimization in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material presents the details of complete video inpainting results in Section A and our stationary mask generation algorithm in Section B. We provide the details of our network architectures in Section C and the implementation details in Section D. Finally, extensive ablation studies and analysis for the proposed Spatial-Temporal Transformer Networks for video inpainting can be found in Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Video Inpainting Results</head><p>To compare visual results from different inpainting models in our main paper, we follow the setting used in most video inpainting works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>. Specifically, we sample several frames from video results and show them in <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 5</ref> in the main paper. However, sampled frames cannot truly reflect video results. Sometimes sampled static frames look less blurry but artifacts can be stronger in a dynamic video. Therefore, we provide 20 video cases for a more comprehensive comparison ? .</p><p>In practice, we test all the videos in the test sets of DAVIS dataset <ref type="bibr" target="#b2">[3]</ref> (90 cases) and Youtube-VOS dataset <ref type="bibr" target="#b31">[32]</ref> (508 cases), and we randomly show 20 cases for visual comparisons. Specifically, five cases from DAVIS and five cases from Youtube-VOS are used to test filling stationary masks. Since Youtube-VOS has no dense object annotations, we sample 10 videos with dense object annotations from DAVIS to test filling moving masks following the setting used in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>. To conduct side-by-side comparisons and analysis, we select the two most competitive video inpainting models, LGTSM <ref type="bibr" target="#b5">[6]</ref> and CAP <ref type="bibr" target="#b17">[18]</ref> in the videos. LGTSM and CAP are fine-tuned multiple times to achieve optimal video results by the codes and models publicly provided by their official Github homepage . We can find from the video results that our model outperforms the state-of-the-art models in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Stationary Mask Generation Algorithm</head><p>Inspired by Xu et al. <ref type="bibr" target="#b32">[33]</ref>, we use stationary masks and moving masks as testing masks to simulate real-world applications (e.g., watermark removal and object removal) in the main paper. As introduced in Section 4.1 in the main paper, on one hand, we use frame-wise foreground object annotations from DAVIS datasets <ref type="bibr" target="#b2">[3]</ref> as moving masks to simulate applications like object removal. On the other hand, we generate random shapes as stationary masks to simulate applications like watermark removal. Specifically, for the task of removing watermarks, a user often draw a mask along the outline of a watermark. Inspired by previous mask generation algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>, we propose a stationary mask generation algorithm to simulate such a behavior for drawing masks for watermarks. Specifically, the proposed algorithm randomly generates a set of control points around a unit circle, and then it smoothly connects these points into a closed cyclic contour by cubic Bezier curves. The details of the stationary mask generation algorithm are shown in Algorithm 1 as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Network Architecture</head><p>The Spatial-Temporal Transformer Network (STTN) is built upon a generative adversarial framework. Specifically, the proposed STTN plays a role as a generator in the framework, and we adopt a Temporal PatchGAN (T-PatchGAN) <ref type="bibr" target="#b4">[5]</ref> as our discriminator. The T-PatchGAN is composed of six layers of 3D convolution layers. Specifically, the T-PatchGAN learns to classify each spatial-temporal feature as real or fake, while STTN learns to fool the T-PatchGAN. Such an adversarial training allows STTN to model the local-global perceptual rationality and the spatial-temporal coherence of real videos <ref type="bibr" target="#b4">[5]</ref>. In addition to the introduction in Section 3 in the main paper, we provide the details of the architectures of STTN and the T-PatchGAN in <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref>, respectively. Specifically, features inside holes are computed by dilated 2D convolutions. We argue that STTN is able to leverages multi-scale contexts and updates holes' features multiple times to improve attention results.  <ref type="table">Tanh   Table 4</ref>. Details of the proposed Spatial-Temporal Transformer Networks (STTN). "2dConv" means 2D convolution layers. "Transformer ? 8" denotes stacking the proposed spatial-temporal transformers by eight layers. A transformer layer involves 1 ? 1 and 3 ? 3 convolutions (The overview of STTN is shown in <ref type="figure">Fig. 2</ref> in the main paper). We use bilinear interpolations for all upsample operations on feature maps <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>. We show whether and what nonlinearity layer is used in the nonlinearity column.  <ref type="table">Table 5</ref>. Details of the Temporal-PatchGAN (T-PatchGAN) discriminator <ref type="bibr" target="#b4">[5]</ref>. The T-PatchGAN is composed of six 3D convolution layers. "SN-3dConv" denotes a 3D convolution layer that adopts spectral normalization to stabilize GAN's training <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation details</head><p>Hyper-parameters: To maintain the aspect ratio of videos and take into account the memory limitations of modern GPUs, we resize all video frames into 432 ? 240 for both training and testing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>. During training, we set the batch size as 8, and the learning rate starts with 1e-4 and decays with factor 0.1 every 150k iterations. Specifically, for each iteration, we sample five frames from a video in a consecutive or discontinuous manner with equal probability for training following Lee et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Computation complexity: Our full model has a total of 12.6M trainable parameters. It costs about 3.9G GPU memory for completing a video from DAVIS dataset <ref type="bibr" target="#b2">[3]</ref> by STTN on average. The proposed multi-scale patch-based video frame representations can enable fast training and inference. Specifically, our model runs at about 24.3fps with an NVIDIA V100 GPU and it runs at about 10.43 fps with an NVIDIA P100 GPU on average. Its total training time was about 3 days on YouTube-VOS dataset <ref type="bibr" target="#b31">[32]</ref> and one day for fine-tuning on DAVIS dataset <ref type="bibr" target="#b2">[3]</ref> with 8 Tesla V100 GPUs. The computation complexity of the proposed spatial-temporal transformers are denoted as:</p><formula xml:id="formula_15">O( D l=1 2 ? (n ? HW p w p h ) 2 ? (p w p h C l ) + nk 2 l HW C l?1 C l ) ? O(n 2 ),<label>(11)</label></formula><p>where D is the number of transformer layers, n is the number of input frames, HW is the feature size, p w p h is the patch size, k l denotes for kernel size, and C is the channel number of features. In Eq. (11), we focus on the computation complexity caused by the spatial-temporal transformers and leave out other computation costs (e.g., encoding and decoding costs) for simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More ablation studies</head><p>To verify the effectiveness of the proposed Spatial-Temporal Transformer Networks (STTN) for video inpainting, this section presents extensive ablation studies on DAVIS dataset <ref type="bibr" target="#b2">[3]</ref> with stationary masks. Effectiveness of utilizing distant frames: we test our full model with different sample rates to prove the benefits of utilizing distant frames. Quantitative comparison results on DAVIS dataset <ref type="bibr" target="#b2">[3]</ref> with stationary masks can be found in <ref type="table" target="#tab_8">Table 6</ref>. The first row (s &gt; T ) means that the STTN takes only neighboring frames as input. Besides, the second row (s = 20) means that the STTN takes both neighboring frames and distant frames that are uniformly sampled from the videos in a sampling rate of 20 frames. <ref type="table" target="#tab_8">Table 6</ref> shows that leveraging visible contexts in distant frames helps in generating better results especially in terms of VFID with 5.70% relative improvements. Based on the observation that most videos in YouTube-VOS dataset <ref type="bibr" target="#b31">[32]</ref> and DAVIS dataset <ref type="bibr" target="#b2">[3]</ref> won't vary a lot within 10 frames on average, we set the sample rate as 10 in our full model to avoid sampling redundant frames and to save computation costs. Effectiveness of masked normalization: As shown in Eq. (3) and Eq. (4) in the main paper, we normalize the value of similarity by the dimension of vectors and filter out unknown regions for similarities calculating. In this part, we conduct comparisons between models with or without such a masked normalization in <ref type="table" target="#tab_9">Table 7</ref>. Results show that such an operation is necessary since it brings improvements with a significant margin comparing with the one without masked normalization. Effectiveness of the Temporal PatchGAN Loss: Recent state-of-theart deep video inpainting models that adopt attention modules often include a perceptual loss <ref type="bibr" target="#b13">[14]</ref> and a style loss <ref type="bibr" target="#b7">[8]</ref> as optimization objectives for perceptually pleasing results <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. However, they do not leverage specially-designed losses for ensuring temporal coherence. Chang et al. propose a novel Temporal Patch-GAN (T-PatchGAN) loss for ensuring both perceptual rationality and spatialtemporal coherence of videos <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. However, they only apply T-PatchGAN on consecutive frames while the attention-based deep video inpainting models take discontinuous frames as input for training. We are the first to introduce T-PatchGAN in video inpainting models that adopt attention modules and show that T-PatchGAN is also powerful in discontinuous frames. Such a joint optimization encourages STTN to learn both local-global perceptual rationality and coherent spatial-temporal transformations for video inpainting.</p><formula xml:id="formula_16">PSNR SSIM(%) Ewarp (%) ? VFID ? w/o</formula><p>We verify the effectiveness of the T-PatchGAN loss by quantitative comparisons in <ref type="table">Table 8</ref>. Compared with the STTN optimized by a style loss <ref type="bibr" target="#b7">[8]</ref> and a perceptual loss <ref type="bibr" target="#b13">[14]</ref> following previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>, the STTN optimized by a T-PatchGAN loss performs better by a significant margin, especially in terms of VFID with 6.9% relative improvements. We also provide a visual comparison in <ref type="figure">Fig. 9</ref>. The visual results show that the STTN optimized by a T-PatchGAN loss can generate more coherent results than the one optimized by a perceptual loss and a style loss. The superior results show the effectiveness of the joint spatial-temporal adversarial learning in STTN.</p><p>losses PSNR SSIM(%) Ewarp (%) ? VFID ? w/ style <ref type="bibr" target="#b7">[8]</ref>, w/ perceptual <ref type="bibr" target="#b13">[14]</ref>   <ref type="table">Table 8</ref>. Ablation study for different losses. Higher is better. ? Lower is better.</p><p>w/ T-PatchGAN loss w/ perceptual and style losses input frame <ref type="figure">Fig. 9</ref>. Visual comparisons between an STTN optimized by a perceptual loss <ref type="bibr" target="#b13">[14]</ref> and a style loss <ref type="bibr" target="#b7">[8]</ref> and an STTN optimized by a T-PatchGAN loss <ref type="bibr" target="#b4">[5]</ref>. These two models perform similarly in small missing regions, while in large missing regions, the model optimized by perceptual and style losses tends to generate artifacts in the missing regions. [Best viewed with zoom-in] Specifically, perceptual loss and style loss have shown great impacts in many image generation tasks since they were proposed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>. A perceptual loss computes L 1 distance between the activation maps of real frames and generated frames. A style loss is similar to the perceptual loss but aims at minimizing the L 1 distance between Gram matrices of the activation maps of real frames and generated frames. In practice, the activation maps are extracted from layers (e.g., pool1, pool2 and pool3) of a pre-trained classification network (more details see <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>). With the help of extracted low-level features, the perceptual loss and the style loss are helpful in generating high-frequency details.</p><p>Unfortunately, perceptual and style losses are calculated on the features of a single frame and they are unable to leverage temporal contexts. When filling in a large missing region in videos, the perceptual and style losses are hard to enforce the generator to synthesize rational contents due to limited contexts. As a result, they have to generate meaningless high-frequency textures to match ground truths' low-level features. For example, for filling the large missing regions in the second and the third frames in <ref type="figure">Fig. 9</ref>, the STTN optimized by perceptual and style losses tends to generate high-frequency artifacts in the large missing regions. Similar artifacts can be found in the failure cases of previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>. Since the T-PatchGAN is able to leverage temporal contexts to optimize the generator, there are fewer artifacts in the results by using the T-PatchGAN. For the above considerations, we use the T-PatchGAN loss instead of the perceptual and style losses in our final optimization objectives. In the future, we plan to design video-based perceptual and style losses which are computed on spatialtemporal features to leverage temporal contexts for optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>We propose Spatial-Temporal Transformer Networks for completing missing regions in videos in a spatially and temporally coherent manner. The top row shows sample frames with yellow masks denoting user-selected regions to be removed. The bottom row shows our completion results. [Best viewed with zoom-in]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>H 10 ? W 10 )</head><label>1010</label><figDesc>encourage capturing deep correspondences in any locations of videos for moving foregrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the attention maps for missing regions learned by STTN. For completing the dog corrupted by a random mask in a target frame (e.g., t=10), our model is able to "track" the moving dog over the video in both spatial and temporal dimensions. Attention regions are highlighted in bright yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Algorithm for stationary mask generation. maxPointNum, maxLength are hyper-parameters to control the statinary mask generation. mask = zeros(imgHeight, imgWidth) pointNum = random.uniform(maxPointNum) startX = origX = random.uniform(imgWidth) startY = origY = random.uniform(imgHeight) angles = linspace(0, 2*pi, pointNum) for i=0 to pointNum do length = random.uniform(maxLength) x = sin(angles[i]) * length y = cos(angles[i]) * length // comment: ensuring smoothness of contours Connect (startX, startY) to (x, y) by cubic Bezier curves. startX = x startY = y end for // comment: ensuring a closed cyclic contour Connect (startX, startY) to (origX, origY) by cubic Bezier curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Specifically, we highlight the top three relevant regions captured by the last transformer in STTN in bright yellow. The relevant regions</figDesc><table><row><cell>input frame</cell><cell>DFVI</cell><cell>LGTSM</cell><cell>CAP</cell><cell>ours</cell></row></table><note>Fig. 4. Visual results for stationary masks. The first column shows input frames from DAVIS [3] (top-3) and YouTube-VOS [32] (bottom-3), followed by results from DFVI [33], LGTSM [6], CAP [18], and our model. Comparing with the SOTAs, our model generates more coherent structures and details of the legs and boats in results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Fig. 6. Illustration of attention maps for missing regions learned by the proposed STTN. We highlight the most relevant patches in yellow according to attention weights. For filling partially occluded objects (the first and the third cases), STTN can precisely attend to the objects. For filling backgrounds (the second and the fourth cases), STTN can correctly attend to the backgrounds.</figDesc><table><row><cell cols="3">(a) stationary holes</cell><cell></cell><cell cols="3">(b) moving holes</cell></row><row><cell>60%</cell><cell></cell><cell></cell><cell>60%</cell><cell></cell><cell></cell></row><row><cell>40%</cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell></row><row><cell>20%</cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell></row><row><cell>0%</cell><cell></cell><cell></cell><cell>0%</cell><cell></cell><cell></cell></row><row><cell>rank 1</cell><cell cols="2">rank 2</cell><cell>rank 3</cell><cell>rank 1</cell><cell cols="2">rank 2</cell><cell>rank 3</cell></row><row><cell>Ours</cell><cell>CAP</cell><cell cols="2">LGTSM</cell><cell>Ours</cell><cell>CAP</cell><cell>LGTSM</cell></row><row><cell>input frame</cell><cell cols="2">output frame</cell><cell></cell><cell cols="2">attention map</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Ablation study by using different patch scales in attention layers. Ours combines the above four scales. Higher is better.</figDesc><table><row><cell cols="4">Patch size PSNR SSIM(%) Ewarp (%)  ? VFID  ?</cell></row><row><cell>108 ? 60 30.16</cell><cell>95.16</cell><cell>0.2243</cell><cell>0.168</cell></row><row><cell>36 ? 20 30.11</cell><cell>95.13</cell><cell>0.2051</cell><cell>0.160</cell></row><row><cell>18 ? 10 30.17</cell><cell>95.20</cell><cell>0.1961</cell><cell>0.159</cell></row><row><cell>9 ? 5 30.43</cell><cell>95.39</cell><cell>0.1808</cell><cell>0.163</cell></row><row><cell>Ours 30.67</cell><cell>95.60</cell><cell cols="2">0.1779 0.149</cell></row></table><note>? Lower is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Ablation study by using different stacking number of the proposed spatialtemporal transformers. Higher is better. ? Lower is better.</figDesc><table><row><cell>30.17</cell><cell>95.17</cell><cell>0.1843</cell><cell>0.162</cell></row><row><cell>?4 30.38</cell><cell>95.37</cell><cell>0.1802</cell><cell>0.159</cell></row><row><cell>?6 30.53</cell><cell>95.47</cell><cell>0.1797</cell><cell>0.155</cell></row><row><cell>?8 (ours) 30.67</cell><cell>95.60</cell><cell cols="2">0.1779 0.149</cell></row></table><note>input+mask our results Fig. 8. A failure case. The bottom row shows our results with enlarged patches in the bottom right corner. For reconstructing the dancing woman occluded by a large mask, STTN fails to generate continuous motions and it generates blurs inside the mask.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Sample Rate PSNR SSIM(%) Ewarp (%) ? VFID ? Ablation study by utilizing distant frames in different sampling rates. Our full model set s = 10. Higher is better.</figDesc><table><row><cell>s &gt; T 30.55</cell><cell>95.47</cell><cell>0.1802</cell><cell>0.158</cell></row><row><cell>s = 20 30.62</cell><cell>95.55</cell><cell>0.1790</cell><cell>0.152</cell></row><row><cell>s = 10 (ours) 30.67</cell><cell>95.60</cell><cell cols="2">0.1779 0.149</cell></row></table><note>? Lower is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Ablation study for the effectiveness of masked normalization operation on similarity calculation. Higher is better.</figDesc><table><row><cell>masked norm. 30.39</cell><cell>95.32</cell><cell>0.1849</cell><cell>0.162</cell></row><row><cell>w/ masked norm. 30.67</cell><cell>95.60</cell><cell cols="2">0.1779 0.149</cell></row></table><note>? Lower is better.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? video demo: https://github.com/researchmm/STTN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = 25 t = 25 t = 12 t = 14 t = 30 t = 43 t = 43 t = 40 t = 50 t = 60 t = 40 t = 40 t = 8 t = 16 t = 28 t = 30 t = 30 t = 34 t = 46 t = 52</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project was supported by NSF of China under Grant 61672548, U1611461.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-Temporal Transformer Networks for Video Inpainting</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<idno>24:1- 24:11</idno>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Navier-stokes, fluid dynamics, and image and video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<title level="m">The 2018 davis challenge on video object segmentation. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Free-form video inpainting with 3d gated convolution and temporal patchgan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9066" to="9075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learnable gated temporal shift module for deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplarbased image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How not to be seenobject removal from videos of crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Independence, invariance and the causal markov condition. The British journal for the philosophy of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="521" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep blind video decaptioning by temporal aggregation and recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4263" to="4272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5792" to="5801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Copy-and-paste networks for deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Da-gan: Instance-level image translation by deep attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5657" to="5666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Full-frame video stabilization with motion inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1150" to="1163" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Edgeconnect: Generative image inpainting with adversarial edge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video inpainting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1993" to="2019" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Onion-peel networks for deep video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4403" to="4412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video inpainting of occluding and occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP. pp</title>
		<imprint>
			<biblScope unit="page" from="11" to="69" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video inpainting under constrained camera motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalm?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="545" to="553" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Video inpainting by jointly learning temporal structure and spatial details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="5232" to="5239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuraIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="1152" to="1164" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Space-time completion of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="476" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Youtube-vos: A large-scale video object segmentation benchmark. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep flow-guided video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning pyramid-context encoder network for high-quality image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An internal learning approach to video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2720" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
