<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
							<email>hongwen.zhang@cripac</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
							<email>yatingtian@smail.</email>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
							<email>xinchi.zhou1@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
							<email>liuyebin@mail.tsinghua.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<email>lmwang@nju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
							<email>znsun@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">CRIPAC</orgName>
								<orgName type="institution" key="instit2">NLPR</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Regression-based methods have recently shown promising results in reconstructing human meshes from monocular images. By directly mapping raw pixels to model parameters, these methods can produce parametric models in a feed-forward manner via neural networks. However, minor deviation in parameters may lead to noticeable misalignment between the estimated meshes and image evidences. To address this issue, we propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted parameters explicitly based on the mesh-image alignment status in our deep regressor. In PyMAF, given the currently predicted parameters, mesh-aligned evidences will be extracted from finerresolution features accordingly and fed back for parameter rectification. To reduce noise and enhance the reliability of these evidences, an auxiliary pixel-wise supervision is imposed on the feature encoder, which provides mesh-image correspondence guidance for our network to preserve the most related information in spatial features. The efficacy of our approach is validated on several benchmarks, including Human3.6M, 3DPW, LSP, and COCO, where experimental results show that our approach consistently improves the mesh-image alignment of the reconstruction. The project page with code and video results can be found at https://hongwenzhang.github.io/pymaf.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aiming at the same goal of producing natural and wellaligned results, two different paradigms for human mesh recovery have been investigated in the research community. Optimization-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b64">65]</ref>  the models to 2D evidences, which can typically produce results with accurate mesh-image alignments but tend to be slow and sensitive to the initialization. Alternatively, regression-based ones <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref> suggest to directly predict model parameters from images, which have shown very promising results, and yet still suffer from the coarse alignment between predicted meshes and image evidences.</p><p>For parametric models like SMPL <ref type="bibr" target="#b34">[35]</ref>, the joint poses are represented as the relative rotations with respect to their parent joints, which means that minor rotation errors accumulated along the kinematic chain can result in notice-able drifts of joint positions. In order to generate wellaligned results, optimization-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> design data terms in the objective function so that the alignment between the projection of meshes and 2D evidences can be explicitly optimized. Similar strategies are also exploited in regression-based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref> to impose 2D supervisions upon the projection of estimated meshes in the training procedure. However, during testing, these deep regressors either are open-loop or simply include an Iterative Error Feedback (IEF) loop <ref type="bibr" target="#b21">[22]</ref> in their architectures. As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, IEF reuses the same global features in its feedback loop, making its regressors hardly perceive the mesh-image misalignment in the inference phase.</p><p>As suggested in previous works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref>, neural networks tend to retain high-level information and discard detailed local features when reducing the spatial size of feature maps. To leverage spatial information in the regression networks, it is essential to extract pixel-wise contexts for fine-grained perception. Several attempts have been made to leverage pixel-wise representations such as part segmentation <ref type="bibr" target="#b40">[41]</ref> or dense correspondences <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b67">68]</ref> in their regression networks. Though these regressors take pixel-level evidences into consideration, it is still challenging for them to learn structural priors and get hold of spatial details at the same time based merely on high-resolution contexts.</p><p>Motivated by above observations, we design a Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network to exploit multi-scale contexts for better meshimage alignment of the reconstruction. The central idea of our approach is to correct parametric deviations explicitly and progressively based on the alignment status. In Py-MAF, mesh-aligned evidences will be extracted from spatial features according to the 2D projection of the estimated mesh and then fed back to regressors for parameter updates. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the mesh alignment feedback loop can take advantage of more informative features for parameter correction in comparison with the commonly used iterative error feedback loop <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref>. To leverage multi-scale contexts, mesh-aligned evidences are extracted from a feature pyramid so that coarse-aligned meshes can be corrected with large step sizes based on the lower-resolution features. Moreover, to enhance the reliability of the spatial cues, an auxiliary task is imposed on the highest-resolution feature to infer pixel-wise dense correspondences, which provides guidance for the image encoder to preserve the mesh-image alignment information. The contributions of this work can be summarized as follows:</p><p>? A mesh alignment feedback loop is introduced for regression-based human mesh recovery, where meshaligned evidences are exploited to correct parametric errors explicitly so that the estimated meshes can be betteraligned with input images.</p><p>? A feature pyramid is further incorporated with the mesh alignment feedback loop so that the regression network can leverage multi-scale alignment contexts.</p><p>? An auxiliary pixel-wise supervision is imposed on the image encoder so that its spatial features can be more informative and the mesh-aligned evidences can be more relevant and reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Pose and Shape Recovery</head><p>Optimization-based Approaches. Pioneering work in this field mainly focus on the optimization process of fitting parametric models (e.g., SCAPE <ref type="bibr" target="#b2">[3]</ref> and SMPL <ref type="bibr" target="#b34">[35]</ref>) to 2D observations such as keypoints and silhouettes <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>. In their objective functions, prior terms are designed to penalize the unnatural shape and pose, while data terms measure the fitting errors between the re-projection of meshes and 2D evidences. Based on this paradigm, different updates have been investigated to incorporate information such as 2D/3D body joints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b69">70]</ref>, silhouettes <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16]</ref>, part segmentation <ref type="bibr" target="#b64">[65]</ref> in the fitting procedure. Despite the wellaligned results obtained by these optimization-based methods, their fitting process tends to be slow and sensitive to initialization. Recently, Song et al. <ref type="bibr" target="#b50">[51]</ref> exploit the learned gradient descent in the fitting process. Though this solution leverages rich 2D pose datasets and alleviates many issues in traditional optimization-based methods, it still relies on the accuracy of 2D poses and breaks the end-to-end learning. Alternatively, our solution supports the end-to-end learning but can also leverage rich 2D datasets thanks to the progress (e.g., SPIN <ref type="bibr" target="#b25">[26]</ref> and EFT <ref type="bibr" target="#b20">[21]</ref>) in the generation of more precise pseudo 3D ground-truth for 2D datasets.</p><p>Regression-based Approaches. Alternatively, taking advantage of the powerful nonlinear mapping capability of neural networks, recent regression-based approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8]</ref> have made significant advances in predicting human models directly from monocular images. These deep regressors take 2D evidences as input and learn model priors implicitly in a data-driven manner under different types of supervision signals <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b27">28]</ref> during the learning procedure. To mitigate the learning difficulty of the regressor, different network architectures have also been designed to leverage proxy representations such as silhouette <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b55">56]</ref>, 2D/3D joints <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b37">38]</ref>, segmentation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref> and dense correspondences <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b67">68]</ref>. Such strategies can benefit from synthetic data <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b48">49]</ref> and the progress in the estimation of proxy representations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref>. Despite the effectiveness of these modules, the quality of proxy representations becomes the bottleneck for the reconstruction task, which may also block the end-to-end learning of the deep regressor. Moreover, though supervision signals are imposed on the projection of the estimated models to penalize the fitting misalignment during the training of deep regressors, their architectures can hardly perceive the misalignment during the inference phase. In comparison, the proposed PyMAF is close-loop for both training and inference, which enables a feedback loop in our deep regressor to leverage spatial evidences for better mesh-image alignment of the estimated human models. Our work focuses on the design of regressor architectures and can also provides a better regressor for those approaches using post-processing <ref type="bibr" target="#b13">[14]</ref> or the work on pseudo ground-truth generation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Directly regressing model parameters from images is very challenging even for neural networks. Existing methods have also offered non-parametric solutions to reconstruct human body models in non-parametric representations. Among them, volumetric representation <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b71">72]</ref>, implicit function <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b70">71]</ref>, mesh vertices <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>, and position maps <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b65">66]</ref> have been adopted as regression targets. Using non-parametric representations as the regression targets is more readily to leverage high-resolution features but needs further processing to retrieve parametric models from the outputs. Besides, solely using high-resolution features makes the algorithms more sensitive to occlusions without additional structure priors. In our solution, the deep regressor makes use of spatial features at multiple scales for highlevel and fine-grained perception and produces parametric models directly with no further processing required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Iterative Fitting in Regression Tasks</head><p>Strategies of incorporating fitting processes along with the regression have also been investigated in the literature. For human model reconstruction, Kolotouros et al. <ref type="bibr" target="#b25">[26]</ref> combine an iterative fitting procedure to the training procedure in order to generate preciser ground truth for better supervision. To refine the estimated meshes during both training and inference phases, several attempts have been made to deform human meshes so that they can be aligned with the intermediate estimations such as depth maps <ref type="bibr" target="#b73">[74]</ref>, part segmentation <ref type="bibr" target="#b63">[64]</ref>, and dense correspondences <ref type="bibr" target="#b13">[14]</ref>. These approaches adopt intermediate estimations as the fitting goals and hence rely on their quality. In contrast, our approach uses the currently estimated meshes to extract deep features for refinement, which not only behaves symmetrically for both training and inference but also enables fully end-to-end learning of the deep regressor.</p><p>To put our approach in a broader view, there have been remarkable efforts made to involve iterative fitting strategies in other computer vision tasks, including facial landmark localization <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b53">54]</ref>, human/hand pose estimation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7]</ref>, etc. For generic objects, Pixel2Mesh <ref type="bibr" target="#b57">[58]</ref> progressively deforms an initial ellipsoid by leveraging perceptual features. Following the spirit of these works, we exploit new strategies to extract fine-grained evidences and contribute novel solutions in the context of human mesh recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we will present the technical details of our approach. As illustrated in <ref type="figure">Fig. 2</ref>, our network produces a feature pyramid for mesh recovery in a coarse-tofine fashion. Coarse-aligned predictions will be improved by utilizing the mesh-aligned evidences extracted from spatial feature maps. Moreover, an auxiliary prediction task is imposed on the image encoder, so that those spatial cues can be more reliable and relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Pyramid for Human Mesh Regression</head><p>The goal of our image encoder is to generate a pyramid of spatial features from coarse to fine granularities, which provide descriptions of the posed person in the image at different scale levels. The feature pyramid will be used in subsequent predictions of the SMPL model with the pose, shape, and camera parameters ? = {?, ?, ?}.</p><p>Formally, the encoder takes an image I as input and outputs a set of spatial</p><formula xml:id="formula_0">features {? t s ? R Cs?H t s ?W t s } T ?1 t=0</formula><p>at the end, where H t s and W t s are monotonically increasing. At level t, based on the feature map ? t s , a set of sampling points X t will be used to extract point-wise features. Specifically, for each 2D point x in X t , point-wise features ? t s (x) ? R Cs?1 will be extracted from ? t s accordingly using the bilinear sampling. These point-wise features will go through a MLP (multi-layer perceptron) for dimension reduction and be further concatenated together as a feature vector ? t p , i.e.,</p><formula xml:id="formula_1">? t p = F(? t s , X t ) = ? f ? t s (x) , for x in X t ,<label>(1)</label></formula><p>where F(?) denotes the feature sampling and processing operations, ? denotes the concatenation, and f (?) is the MLP. After that, a parameter regressor R t takes features ? t p and the current estimation of parameters ? t as inputs and outputs the parameter residual. Parameters are then updated as ? t+1 by adding the residual to ? t . For the level t = 0, ? 0 adopts the mean parameters calculated from training data.</p><p>Given the parameter predictions ? (the subscript t is omitted for simplicity) at each level, a mesh with vertices of M = M(?, ?) ? R N ?3 can be generated accordingly, where N = 6890 denotes the number of vertices in the SMPL model. These mesh vertices can be mapped to sparse 3D joints J ? R Nj ?3 by a pretrained linear regressor, and further projected on the image coordinate system as 2D keypoints K = ?(J) ? R Nj ?2 , where ?(?) denotes the projection function based on the camera parameters ?. Note that the pose parameters in ? are represented as relative rotations along kinematic chains and minor parameter errors can lead to large misalignment between the 2D projection and image evidences. To penalize such misalignment during the training of the regression network, we follow common practices <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref> to add 2D supervisions on the Encoder ? 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Pixel-wise Prediction Mesh Alignment Feedback Loop</head><p>Regressor .</p><p>Regressor .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh-aligned Features</head><p>Downsampling</p><formula xml:id="formula_2">Projection ? t ?t+1 Updated Mesh Mt+1 Spatial Feature Pyramid Spatial Feature Pyramid Spatial Feature Pyramid ? 0 Mesh M1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid Features Grid Features</head><p>Deconvolution Deconvolution Deconvolution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution Convolution Convolution</head><p>Image Image Image <ref type="figure">Figure 2</ref>: Overview of the proposed Pyramidal Mesh Alignment Feedback (PyMAF). PyMAF leverages a feature pyramid and enables an alignment feedback loop in our network. Given a coarse-aligned model prediction, mesh-aligned evidences are extracted from finer-resolution features accordingly and fed back a regressor for parameter rectification. To enhance the reliability of spatial evidences, an auxiliary pixel-wise prediction task is imposed on the final output of the image encoder.</p><p>2D keypoints projected from the estimated mesh. Meanwhile, additional 3D supervisions on 3D joints and model parameters are added when ground truth 3D labels are available. Overall, the loss function for the parameter regressor is written as</p><formula xml:id="formula_3">L reg = ? 2d ||K ?K||+? 3d ||J ??||+? para ||???||,<label>(2)</label></formula><p>where || ? || is the squared L2 norm,K,?, and? denote the ground truth 2D keypoints, 3D joints, and model parameters, respectively. One of the improvements over the commonly used parameter regressors is that our regressors can better leverage spatial information. Unlike the commonly used regressors taking the global features ? g ? R Cg?1 as input, our regressor uses the point-wise information obtained from spatial features ? t s . A straight-forward strategy to extract pointwise features would be using the points X t with a grid pattern and uniformly sampling features from ? t s . In the proposed approach, the sampling points X t adopt the grid pattern at the level t = 0 and will be updated according to the currently estimated mesh when t &gt; 0. We will show that, such a mesh conditioned sampling strategy helps the regressor to produce more plausible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mesh Alignment Feedback Loop</head><p>As mentioned in HMR <ref type="bibr" target="#b21">[22]</ref>, directly regressing mesh parameters in one go is challenging. To tackle this issue, HMR uses an Iterative Error Feedback (IEF) loop to iteratively update ? by taking the global features ? g and the current estimation of ? as input. Though the IEF strategy reduces parameter errors progressively, it uses the same global features each time for parameter update, which lacks fine-grained information and is not adaptive to new predictions. By contrast, we propose a Mesh Alignment Feedback (MAF) loop so that mesh-aligned evidences can be leveraged in our regressor to rectify current parameters and improve the meshimage alignment of the currently estimated model.</p><p>To this end, when t &gt; 0, we extract mesh-aligned features from ? t s based on the currently estimated mesh M t to obtain more fine-grained and position-sensitive evidences. Compared with the global features or the uniformly sampled grid features, mesh-aligned features can reflect the mesh-image alignment status of the current estimation, which is more informative for parameter rectification. Specifically, the sampling points X t are obtained by first down-sampling the mesh M t toM t and then projecting it on the 2D image plane, i.e., X t = ?(M t ). Based on X t , the mesh-aligned features ? t m will be extracted from ? t s using Eq. 1, i.e., ? t m = ? t p = F(? t s , ?(M t )). These mesh-aligned features will be fed into the regressor R t for parameter update. Overall, the proposed mesh alignment feedback loop can be formulated as</p><formula xml:id="formula_4">? t+1 = ? t + R t ? t , F(? t s , ?(M t )) , for t &gt; 0. (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Auxiliary Pixel-wise Supervision</head><p>As depicted in the second row of <ref type="figure" target="#fig_2">Fig. 3</ref>, spatial features tend to be affected by the noisy inputs, since input images may contain a large amount of unrelated information such as occlusions, appearance and illumination variations. To improve the reliability of the mesh-aligned cues extracted from spatial features, we impose an auxiliary pixelwise prediction task on the spatial features at the last level. Specifically, during training, the spatial feature maps ? T ?1 s will go through a convolutional layer for the prediction of dense correspondence maps with pixel-wise supervision applied. Dense correspondences encode the mapping relationship between foreground pixels on 2D image plane and mesh vertices in 3D space. In this way, the auxiliary supervision provides mesh-image correspondence guidance for the image encoder to preserve the most related information in the spatial feature maps.</p><p>In our implementation, we adopt the IUV maps defined in DensePose <ref type="bibr" target="#b0">[1]</ref> as the dense correspondence representation, which consists of part index and UV values of the mesh vertices. Note that we do not use DensePose annotations in the dataset but render IUV maps based on the ground-truth SMPL models <ref type="bibr" target="#b67">[68]</ref>. During training, classification and regression losses are applied on the part index P and U V channels of dense correspondence maps, respectively. Specifically, for the part index P channels, a crossentropy loss is applied to classify a pixel belonging to either background or one among body parts. For the U V channels, a smooth L1 loss is applied to regress the corresponding U V values of the foreground pixels. Only the foreground regions are taken into account in the U V regression loss, i.e., the estimated U V channels are firstly masked by the ground-truth part index channels before applying the regression loss. Overall, the loss function for the auxiliary pixel-wise supervision is written as</p><formula xml:id="formula_5">L aux =? pi CrossEntropy(P,P ) +? uv SmoothL1(P ? U,P ?? ) +? uv SmoothL1(P ? V,P ?V ),<label>(4)</label></formula><p>where ? denotes the mask operation. Note that the auxiliary prediction is required in the training phase only. <ref type="figure" target="#fig_2">Fig. 3</ref> visualizes the spatial features of the encoder trained with and without auxiliary supervision, where the feature maps are simply added along the channel dimension as grayscale images and visualized with colormap. We can see that the spatial features are more neat and robust to the input variations when the auxiliary supervision is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>The proposed PyMAF is validated on the ResNet-50 <ref type="bibr" target="#b14">[15]</ref> backbone pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>. The ResNet-50 backbone takes a 224 ? 224 image as input and produces image features with the size of 2048 ? 7 ? 7. For the classic regression network HMR <ref type="bibr" target="#b21">[22]</ref>, a 2048 ? 1 global feature vector will be obtained after average pooling. In our approach, the image features will go through deconvolution layers, resulting spatial feature maps with resolutions of {14 ? 14, 28 ? 28, 56 ? 56}, where C s = 256 for all resolutions. Here, the maximum number T is set to 3, which is equal to the iteration number used in HMR. When generating mesh-aligned features, the SMPL mesh is down-sampled using a pre-computed downsampling matrix provided in <ref type="bibr" target="#b26">[27]</ref>, after which the vertex number drops from  6890 to 431. The mesh-aligned features of each point will be processed by a three-layer MLP so that their dimension will be reduced from C s to 5. Hence, the mesh-aligned feature vector has a length of 2155 = 431 ? 5, which is similar to the length of global features. For the grid features used at t = 0, they are uniformly sampled from ? 0 s with a 21 ? 21 grid pattern, i.e., the point number is 441 = 21?21 which is approximate to the vertices number 431 after mesh downsampling. The regressors R t have the same architecture with the regressor in HMR except that they have slightly different input dimensions. Following the setting of SPIN <ref type="bibr" target="#b25">[26]</ref>, we train our network using the Adam <ref type="bibr" target="#b23">[24]</ref> optimizer with the learning rate set to 5e?5 and the batch size of 64 on a single 2080 Ti GPU. No learning rate decay is applied during training. More details of the implementation can be found in our code and the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Following the settings of previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>, our approach is trained on a mixture of data from several datasets with 3D and 2D annotations, including Hu-man3.6M <ref type="bibr" target="#b16">[17]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b36">[37]</ref>, LSP <ref type="bibr" target="#b18">[19]</ref>, LSP-Extended <ref type="bibr" target="#b19">[20]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref>, and COCO <ref type="bibr" target="#b32">[33]</ref>. For the last five datasets, we also utilize their pseudo ground-truth SMPL parameters <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref> for training. We do not use training data from 3DPW <ref type="bibr" target="#b56">[57]</ref> but only perform evaluations on its test set. Moreover, we do not use the DensePose annotations in COCO for auxiliary supervision, but render IUV maps based on the ground-truth SMPL meshes using the method described in <ref type="bibr" target="#b67">[68]</ref>. We evaluate our approach using a variety of metrics for quantitative comparisons with previous meth-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State of the Art</head><p>3D Human Pose and Shape Estimation. We first evaluate our approach on the 3D human pose and shape estimation task, and make comparisons with previous stateof-the-art regression-based methods. We present evaluation results for quantitative comparison on 3DPW and Hu-man3.6M datasets in <ref type="table" target="#tab_1">Table 1</ref>. Our PyMAF achieves competitive or superior results among previous approaches, including both frame-based and temporal approaches. Note that the approaches reported in <ref type="table" target="#tab_1">Table 1</ref> are not strictly comparable since they may use different training data, learning rate schedules, or training epochs, etc. For a fair comparison, we report results of our baseline in <ref type="table" target="#tab_1">Table 1</ref>, which is trained under the same setting with PyMAF. The baseline approach has the same network architecture with HMR <ref type="bibr" target="#b21">[22]</ref> and also adopts the 6D rotation representation <ref type="bibr" target="#b72">[73]</ref> for pose parameters. Compared with the baseline, PyMAF reduces the MPJPE by 5.7 mm and 7.1 mm on 3DPW and Hu-man3.6M datasets, respectively. The auxiliary supervision (AS) also helps PyMAF to have better reconstruction results as shown in the last two rows of <ref type="table" target="#tab_1">Table 1</ref>.</p><p>From <ref type="table" target="#tab_1">Table 1</ref>, we can see that PyMAF has more notable improvements on the metrics MPJPE and PVE. We would argue that the metric PA-MPJPE can not fully reveal the  2D Segmentation and Pose Estimation. To quantitatively measure the mesh-image alignment of the predictions, we also conduct evaluation on the 2D segmentation and pose estimation task, where the predicted meshes are projected on the image plane to obtain 2D part segmentation and keypoints. <ref type="table" target="#tab_3">Table 2</ref> reports the assessment of foreground-background and six-part segmentation performance on the LSP test set. As shown in <ref type="table" target="#tab_3">Table 2</ref>, optimization-based approaches remain very competitive in terms of 2D alignment metrics and tend to outperform most of the regression-based ones. The reason behind it is that optimization-based approaches is optimized for the mesh-image alignment explicitly. Though PyMAF is regression-based, it surpasses all other methods including the optimization-based ones.</p><p>Finally, we evaluate 2D human pose estimation performance on the COCO validation set to verify the effectiveness of our approach in real-world scenarios. During the evaluation, we project keypoints from the estimated mesh on the image plane, and compute the Average Precision (AP) based on the keypoint similarity with the ground truth 2D keypoints. The results of keypoint localization APs are reported in <ref type="table" target="#tab_5">Table 3</ref>. OpenPose <ref type="bibr" target="#b5">[6]</ref>, a widely-used 2D human pose estimation algorithm, are also included for reference. We can see that, the COCO dataset is very challenging for approaches to human mesh reconstruction as they typically have much worse performances in terms of 2D keypoint localization accuracy. In <ref type="table" target="#tab_5">Table 3</ref>, we also include the results of the optimization-based SMPLify <ref type="bibr" target="#b4">[5]</ref>    <ref type="bibr" target="#b4">[5]</ref> are evaluated based on the implementation in SPIN <ref type="bibr" target="#b25">[26]</ref> with 300 optimization iterations. Results of HMR <ref type="bibr" target="#b21">[22]</ref>, GraphCMR <ref type="bibr" target="#b26">[27]</ref>, and SPIN <ref type="bibr" target="#b25">[26]</ref> are evaluated based on their publicly released code and models.</p><p>model to the ground-truth 2D keypoints. As pointed out in previous work <ref type="bibr" target="#b25">[26]</ref>, SMPLify may produce well-aligned but unnatural results. Moreover, SMPLify is much more time-consuming than regression-based solutions. Among approaches recovering 3D human mesh, PyMAF outperforms previous regression-based methods by remarkable margins. Compared with our baseline, PyMAF significantly improves the AP and AP 50 by 7.8% and 10.7%, respectively. The auxiliary supervision (AS) also considerably contributes to more robust reconstruction in this challenging dataset and brings 3.9% performance gain on AP. Reconstruction results on COCO are depicted in <ref type="figure">Fig. 4</ref> for qualitative comparisons, where PyMAF convincingly performs better than SPIN <ref type="bibr" target="#b25">[26]</ref> and our baseline by producing betteraligned and natural results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this part, we will perform ablation studies under various settings on Human3.6M to validate the efficacy of the  key components proposed in our approach. All ablation approaches are trained and tested on Human3.6M, as it includes ground-truth 3D labels and is the most widely used benchmark for 3D human pose and shape estimation. Efficacy of Mesh-aligned Features. In PyMAF, meshaligned features provide the current mesh-image alignment information in the feedback loop, which is essential for better mesh recovery. To verify this, we alternatively replace mesh-aligned features with the global features or the grid features uniformly sampled from spatial features as the input for parameter regressors. <ref type="table" target="#tab_7">Table 4</ref> reports the performances of the approaches equipped with different types of features in the feedback loop. The results under the nonpyramidal setting are also included in <ref type="table" target="#tab_7">Table 4</ref>, where the grid and mesh-aligned features are extracted from the feature maps with the highest resolution (i.e., 56 ? 56) and the mesh-aligned features are extracted on the reprojected points of the mesh under the mean pose at t = 0. For fair comparisons with Baseline, the approaches with global, grid, and mesh-aligned feedback features under the nonpyramidal setting also use a single regressor but have individual supervision on the prediction of each iteration. Besides, all approaches in <ref type="table" target="#tab_7">Table 4</ref> do not use the auxiliary supervision.</p><p>Unsurprisingly, using mesh-aligned features yields the best performance under both non-pyramidal and pyramidal designs. The approach using the grid features sampled from spatial feature maps has better results than that using the global features but is worse than the mesh-aligned counterpart. When using pyramidal feature maps, the meshaligned solution achieves even more performance gain since multi-scale mesh-alignment evidences can be leveraged in the feedback loop. Though the grid features largely contain spatial cues on uniformly distributed pixel positions, they can not reflect the alignment status of the current estimation. This implies that mesh-aligned features are the most informative one for the regressor to rectify the current mesh parameters.</p><p>Benefit from Auxiliary Supervision. The auxiliary pixel-wise supervision helps to enhance the reliability of the mesh-aligned evidences extracted from spatial features. Using alternative pixel-wise supervision such part segmen-  <ref type="table">Table 5</ref>: Ablation study on using different auxiliary supervision settings and input types for regressors.</p><p>tation rather than dense correspondences is also possible in our framework. In our approach, these auxiliary predictions are solely needed for supervisions during training since the point-wise features are extracted from feature maps. For more in-depth analyses, we have also tried extracting point-wise features from the auxiliary predictions, i.e., the input type of regressors are intermediate representations such as part segmentation or dense correspondences. <ref type="table">Table 5</ref> shows the comparison of experiments with different auxiliary supervision settings and input types for regressors during training. Using part segmentation is slightly worse than our dense correspondence solution. Compared with the part segmentation, the dense correspondences preserve not only clean but also rich information in foreground regions. Moreover, using feature maps for point-wise feature extraction consistently performs better than using auxiliary predictions. This can be explained by the fact that using intermediate representations as input for regressors hampers the end-to-end learning of the whole network. Under the auxiliary supervision strategy, the spatial feature maps are learned with the signal backpropagated from both auxiliary prediction and parameter correction tasks. In this way, the background features can also contain information for mesh parameter correction since the deep features have larger receptive fields and are trained in an end-to-end manner. As shown in <ref type="table">Table 5</ref>, when the mesh-aligned features are masked with the foreground region of part segmentation predictions, the performance degrades from 75.5 mm to 77.6 mm on MPJPE.</p><p>Effect of the Initialization in the Feedback Loop. In our approach, the point-wise features are initially extracted on grid points for coarse mesh predictions before the extraction of mesh-aligned features. It is also feasible to extract mesh-aligned features based on a mean-pose mesh at t = 0 as the initial features. The performances of the approaches using different initial features are reported in <ref type="table">Table 6</ref>, where PyMAF can improve reconstruction results under both initialization settings. For the approach using the initial features extracted on the projection of the mean-pose mesh, we visualize its estimated meshes after each iteration in <ref type="figure" target="#fig_3">Fig 5.</ref> Though the mean-pose mesh is far away from the groundtruth, PyMAF can correct the drift of body parts progressively and result in better-aligned human models.  <ref type="table">Table 6</ref>: Ablation study on using different initial features across iterations in the feedback loop. </p><formula xml:id="formula_6">Initial Feat. Metric M 0 M 1 M 2 M 3</formula><formula xml:id="formula_7">Image M 0 M 1 M 2 M 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>In this paper, we present Pyramidal Mesh Alignment Feedback (PyMAF) for regression-based human mesh recovery. PyMAF is primarily motivated by the observation of the reprojection misalignment of parametric mesh results. Though PyMAF improves the mesh-image alignment on 2D image planes, it can still hardly address the depth ambiguity problem in 3D space. Moreover, PyMAF fails to handle extreme shapes due to the lack of training data, as shown in the Supplementary Material.</p><p>In the future, PyMAF can be extended and incorporated with the recent progress to improve the alignment in 3D space <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>. Moreover, combining PyMAF with Holo-Pose <ref type="bibr" target="#b13">[14]</ref>, SPIN <ref type="bibr" target="#b25">[26]</ref>, or EFT <ref type="bibr" target="#b20">[21]</ref> for the generation of more precise pseudo 3D ground-truth labels would also be interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. More Experimental Details</head><p>Our network is trained with the Adam <ref type="bibr" target="#b23">[24]</ref> optimizer and batch size of 64. The learning rate is set to 5e?5 without learning rate decay during training. Similar to SPIN <ref type="bibr" target="#b25">[26]</ref>, our network is first trained on Human3.6M for 60 epochs and then on the mixture of both 2D and 3D datasets for another 60 epochs.</p><p>The parameter regressors of PyMAF have the same design with that of HMR <ref type="bibr" target="#b21">[22]</ref> except for their slightly different input and output dimensions. Specifically, a regressor consists of two fully-connected layers each with 1024 hidden neurons and dropout added in between, followed by a final layer at the end with 157dimension output, corresponding to the residual of shape and pose parameters. The regressors in our network adopt the continuous representation <ref type="bibr" target="#b72">[73]</ref> for 3D rotations in the pose parameters ?. During the extraction of mesh-aligned features, the dimension of point-wise features is reduced from Cs (i.e., 256) to 5, where a three-layer MLP consisting of two hidden layers with neuron numbers of (128, 64) is used. The feature pyramid of PyMAF is generated by three deconvolution layers. The deconvolutions are not compulsory but help to produce better features maps. In our experiments, using the feature maps in the earlier layers is also feasible but inferior to our final solution.</p><p>Runtime. The PyTorch implementation of PyMAF takes about 30 ms to process one sample on the machine with a single 2080 Ti GPU. The proposed mesh alignment feedback loop takes about 6 ms for each iteration, including the time of generating new feature maps via deconvolution, projecting the mesh on image planes, the extraction of mesh-aligned features via bilinear sampling and MLPs, and the prediction of parameter updates by the regressor. For each iteration, compared to the feedback loop in HMR <ref type="bibr" target="#b21">[22]</ref> or SPIN <ref type="bibr" target="#b25">[26]</ref>, PyMAF introduces additional runtime in the generation of feature maps, the current SMPL meshes, and the mesh-aligned features, which accounts for 0.3 ms, 4 ms, and 1.2 ms respectively. We can see that the generation of the feature pyramid and meshaligned features is quite efficient, and the main runtime overhead comes from the SMPL mesh generation given the current parameters. In practice, we can speed up this process by using a downsampled version of SMPL to generate the mesh with 431 vertices directly. Note that the prediction of dense correspondences and the auxiliary supervision in the pipeline are needed for training only, which accounts for additional 15% runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Datasets</head><p>Following the protocols of previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>, we train our network on several datasets with 3D or 2D annotations, including Human3.6M <ref type="bibr" target="#b16">[17]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b36">[37]</ref>, LSP <ref type="bibr" target="#b18">[19]</ref>, LSP-Extended <ref type="bibr" target="#b19">[20]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref>, COCO <ref type="bibr" target="#b32">[33]</ref>. For the last five datasets, we also incorporate the SMPL parameters fitted in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref> as pseudo groud-truth annotations for training. Here, we provide more descriptions of the datasets to supplement the main manuscript.</p><p>3DPW <ref type="bibr" target="#b56">[57]</ref> is captured in challenging outdoor scenes with IMU-equipped actors under various activities. This dataset provides accurate shape and pose ground truth annotations. Following the protocol of previous work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>, we do not use its data for training but only perform evaluations on its test set.</p><p>Human3.6M <ref type="bibr" target="#b16">[17]</ref> is commonly used as the benchmark dataset for 3D human pose estimation, consisting of 3.6 million video frames captured in the controlled environment. The ground truth SMPL parameters in Human3.6M are generated by applying MoSh <ref type="bibr" target="#b33">[34]</ref> to the sparse 3D MoCap marker data, as done in Kanazawa et al. <ref type="bibr" target="#b21">[22]</ref>. It is common protocols <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22]</ref> to use five subjects (S1, S5, S6, S7, S8) for training and two subjects (S9, S11) for evaluation. The original videos are also down-sampled from 50 fps to 10 fps to remove redundant frames, resulting in 312,188 frames for training and 26,859 frames for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP</head><p>[37] is a recently introduced 3D human pose dataset covering more actor subjects and poses than Human3.6M. The images of this dataset were collected under both indoor and outdoor scenes, and the 3D annotations were captured by a multicamera marker-less MoCap system. Hence, there are some noise in the 3D ground truth annotations. <ref type="bibr" target="#b19">[20]</ref> is a 2D human pose benchmark dataset, containing person images with challenging poses. There are 14 visible 2D keypoint locations annotated for each image and 9,428 samples used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSP-Extended</head><p>LSP <ref type="bibr" target="#b18">[19]</ref> is a standard benchmark dataset for 2D human pose estimation. In our experiments, we will employ its test set for silhouette/parts segmentation evaluation, where the annotations come from Lassner et al. <ref type="bibr" target="#b28">[29]</ref>. There are 1,000 samples used for evaluation.</p><p>MPII <ref type="bibr" target="#b1">[2]</ref> is a standard benchmark for 2D human pose estimation. There are 25,000 images collected from YouTube videos covering a wide range of activities. We discard those images without complete keypoint annotations, producing 14,810 samples for training.</p><p>COCO <ref type="bibr" target="#b32">[33]</ref> contains a large scale of person images labeled with 17 keypoints. In our experiments, we only use those persons annotated with at least 12 keypoints, resulting in 28,344 samples for training. Since this dataset do not contain ground-truth meshes, we conduct quantitative evaluation on the 2D keypoint localization task using its validation set, which consists of 50,197 samples. Following <ref type="bibr" target="#b67">[68]</ref>, we crop input images using the ground-truth bounding boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Evaluation Metrics</head><p>In the main manuscript, we report results of our approach in a variety of evaluation metrics for quantitative comparisons with the state of the art, where all metrics are computed in the same way as previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b25">26]</ref> in the literature.</p><p>To quantitatively evaluate the 3D human reconstruction and pose estimation performance on 3DPW and Human3.6M, PVE, MPJPE, and PA-MPJPE are adopted as the evaluation metrics in <ref type="table" target="#tab_1">Table 1</ref> of the main manuscript. They are all reported in millimeters (mm) by default. Among these three metrics, PVE denotes the mean Per-vertex Error defined as the average point-to-point Euclidean distance between the predicted mesh vertices and the ground truth mesh vertices. MPJPE denotes the Mean Per Joint Position Error, and PA-MPJPE denotes MPJPE after rigid alignment of the prediction with ground truth using Procrustes Analysis. Note that the metric PA-MPJPE can not fully reveal the meshimage alignment performance since it is calculated as MPJPE after rigid alignment. As depicted in <ref type="figure">Fig. 6</ref>, a reconstruction result with a lower PA-MPJPE value can have a higher MPJPE value and worse alignment between the reprojected mesh and image.</p><p>In <ref type="table" target="#tab_3">Table 2</ref> of the main manuscript, segmentation accuracy metrics quantitatively measure the mesh-image alignment of different approaches on the LSP dataset. As originally done in Lassner et al. <ref type="bibr" target="#b28">[29]</ref>, silhouette (i.e., Foreground/Background, FB) and Part segmentation are considered in calculating the accuracy and f1 scores.</p><p>For 2D human pose estimation task on COCO 1 , the commonlyused Average Precision (AP) is adopted as the evaluation metric. AP is calculated based on the Object Keypoint Similarity (OKS), which plays a similar role as IoU in object detection. In <ref type="table" target="#tab_5">Table 3</ref> of the main manuscript, the results are reported using mean AP, and variants of AP including AP50 (AP at OKS = 0.50), AP75 (AP at OKS = 0.75), APM for persons with medium sizes, and APL for persons with large sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. More Experimental Results</head><p>More Quantitative Results. To evaluate the performances of PyMAF on human images with occlusions and different body shape styles, we conduct evaluation experiments on 3DOH50K <ref type="bibr" target="#b68">[69]</ref> and SSP-3D <ref type="bibr" target="#b48">[49]</ref> datasets. The test set of 3DOH50K includes 1,290 person images in occlusion scenarios, while SSP-3D consists of 311 images of sport persons with a variety of body shapes and poses. Note that we only perform testing on these two datasets and do not use their data for training. Experimental results on 3DOH50K and SSP-3D are reported in Tab. 7, and qualitative results are shown in <ref type="figure">Figures 7 and 8</ref>. PyMAF can improve the reconstruction under occlusions on 3DOH50K and help with more accurate shape estimation on SSP-3D. Despite the numerical performance gains, PyMAF fails to handle extreme shapes on the SSP-3D dataset, as shown in <ref type="figure">Figure 8</ref>.</p><p>More Qualitative Results. We provide more qualitative results and compare our PyMAF with the state-of-the-art approach SPIN <ref type="bibr" target="#b25">[26]</ref>. <ref type="figure">Figure 9</ref> shows the qualitative differences between each iterative loop in SPIN <ref type="bibr" target="#b25">[26]</ref> and PyMAF, which uses the global features and spatial features for the parameter update respectively. We can see that PyMAF convergences much faster and corrects the  mesh parameters more effectively. In <ref type="figure" target="#fig_0">Figure 10</ref>, we show more reconstruction results for qualitative comparisons with SPIN on both indoor and in-the-wild datasets, where PyMAF can produce natural results which are better-aligned with the images under challenging cases. Note that our approach is complementary to SPIN, since SPIN aims at providing better supervision for the regression network, while our work focuses on the architecture design of the regression network.</p><p>To demonstrate the efficacy of the parameter rectification in PyMAF, we further provide more examples on COCO and 3DPW in <ref type="figure" target="#fig_0">Figures 11 and 12</ref>, respectively. We can observe that PyMAF improves the mesh-image alignment progressively by correcting the predictions based on the current observations. We also visualize some erroneous results of our approach in <ref type="figure" target="#fig_0">Figure 13</ref>, where PyMAF may fail when the initial reconstructed results have severe deviations due to the heavy occlusions or ambiguous limb connections in complex scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Front View</head><p>Top View Side View Image Front View Top View Side View <ref type="figure">Figure 7</ref>: Reconstruction results of PyMAF on the 3DOH50K <ref type="bibr" target="#b68">[69]</ref> dataset. PyMAF helps to handle occlusions.  <ref type="figure" target="#fig_0">Figure 11</ref>: Successful results of PyMAF on the COCO dataset. For each example from left to right: image, the results after each iteration.</p><p>Image <ref type="figure" target="#fig_0">Figure 12</ref>: Successful results of PyMAF on the 3DPW dataset. Examples have the same layout with <ref type="figure" target="#fig_0">Figure 11</ref>. <ref type="figure" target="#fig_0">Figure 13</ref>: Erroneous reconstructions of our network. Though PyMAF can improve the alignment of some body parts, it remains challenging for PyMAF to correct those body parts with severe deviations, heavy occlusions, or ambiguous limb connections. Examples have the same layout with <ref type="figure" target="#fig_0">Figure 11</ref>.</p><formula xml:id="formula_8">M 1 M 2 M 3 Image M 1 M 2 M 3</formula><formula xml:id="formula_9">Image M 1 M 2 M 3 Image M 1 M 2 M 3</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>explicitly fit *: Equal contribution. : Corresponding authors. Illustration of our main idea. (a) The commonlyused iterative error feedback. (b) Our mesh alignment feedback. (c) Mesh-aligned evidences extracted from a feature pyramid. (d) Our approach PyMAF improves the meshimage alignment of the estimated mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the spatial feature maps and predicted dense correspondences. Top: Input images. Second / Third Row: Spatial feature maps learned without / with the Auxiliary Supervision (AS). Bottom: Predicted dense correspondence maps under the auxiliary supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of reconstruction results across different iterations in the feedback loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 Figure 6 :</head><label>46</label><figDesc>(a) PA-MPJPE: 26.9, MPJPE: 74.3 (b) PA-MPJPE: 27.7, MPJPE: 43.Examples of two reconstruction results. (a) A reconstruction result with a lower PA-MPJPE value but worse mesh-image alignment. (b) A reconstruction result with a higher PA-MPJPE value but better mesh-image alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Reconstruction errors on 3DPW and Human3.6M.</figDesc><table /><note>? denotes the numbers evaluated on non-parametric results.ods, i.e., PVE, MPJPE, and PA-MPJPE for the evaluation of 3D pose estimation, the segmentation accuracy, f1 scores, and AP for the measure of mesh-image alignment. Detailed descriptions about the datasets and evaluation metrics can be found in the Supplementary Material.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Foreground-background and six-part segmentation</cell></row><row><cell>accuracy and f1 scores on the LSP test set. SMPLify Or-</cell></row><row><cell>acle denotes the SMPLify using ground-truth keypoints as</cell></row><row><cell>inputs.</cell></row></table><note>mesh-image alignment performance since it is calculated as the MPJPE after rigid alignment. As depicted in the Sup- plementary Material, a reconstruction result with smaller PA-MPJPE value can have larger MPJPE value and worse alignment between the reprojected mesh and image.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Keypoint localization APs on the COCO validation set. There is a total of 50,197 samples used for evaluation.</figDesc><table /><note>Results of SMPLify</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on using different types of feedback features for refinement. No auxiliary supervision is applied.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>MPJPE 131.7 52.1 49.9 48.9</figDesc><table><row><cell>Mean-pose</cell><cell>MPJPE</cell><cell>274.0 81.4 78.0 77.3</cell></row><row><cell>Mesh</cell><cell cols="2">PA-MPJPE 131.7 54.1 51.1 50.3</cell></row><row><cell></cell><cell>MPJPE</cell><cell>274.0 80.3 76.6 75.1</cell></row><row><cell>Grid Points</cell><cell>PA-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>https://cocodataset.org/#keypoints-eval</figDesc><table><row><cell></cell><cell cols="2">3DOH50K</cell><cell>SSP-3D</cell></row><row><cell></cell><cell cols="3">PVE? MPJPE? mIOU?</cell></row><row><cell cols="2">SPIN [26] 113.4</cell><cell>102.3</cell><cell>70.2</cell></row><row><cell>Baseline</cell><cell>113.1</cell><cell>102.0</cell><cell>70.8</cell></row><row><cell>PyMAF</cell><cell>107.3</cell><cell>96.2</cell><cell>72.1</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Reconstruction performances on 3DOH50K and SSP-3D datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Figure 8 :</head><label>8</label><figDesc>Reconstruction results of PyMAF on the SSP-3D<ref type="bibr" target="#b48">[49]</ref> dataset. PyMAF fails to handle extreme shapes due to the lack of training data.Figure 9: Qualitative differences between each iterative loop of the SPIN [26] using global features vs. the PyMAF using spatial features. Qualitative comparison of the reconstruction results between SPIN [26] and our PyMAF approach. For each example, the upper / lower results correspond to the reconstructed meshes of SPIN (pink) / PyMAF (purple). Examples come from various datasets, including COCO (Rows 1-3), 3DPW (Row 4), and Human3.6M (Row 5).</figDesc><table><row><cell>SPIN [26]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PyMAF Figure 10: Image Image Image</cell><cell>M 1 Front View M 1</cell><cell>M 2 Side View M 2</cell><cell>Image</cell><cell>M 3 M 3</cell><cell>Front View</cell><cell>Image Side View Image</cell><cell>M 1 M 1</cell><cell>Image</cell><cell>M 2 Front View M 2</cell><cell>M 3 Side View M 3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the National Natural Science Foundation of China (Grant U1836217, 62125107, 62076119, 61921006), Australian Research Council (DP200103223), and Australian Medical Research Future Fund (MRFAI000085). Wanli Ouyang is also supported by SenseTime.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="10884" to="10894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human pose fitting towards in-the-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Appearance consensus driven self-supervised human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">Mysore</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="794" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3383" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pc-hmr: Pose calibration for 3d human mesh recovery from 2d images/videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized feedback loop for joint hand-object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1898" to="1912" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3d human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5340" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chained representation cycling: Learning to estimate 3d human pose and shape by cycling between representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><surname>Rueegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthetic training for accurate 3d human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1337" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph-based 3d multi-person pose estimation using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Size</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densebody</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10153</idno>
		<title level="m">Directly regressing dense 3d human pose and shape from a single color image</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural descent for visual 3d human pose and shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14484" to="14493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Danet: Decompose-and-aggregate network for 3d human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning 3d human shape and pose from dense body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Light-weight multi-person total capture using sparse multi-view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deepmulticap: Performance capture of multiple characters using sparse multiview cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7739" to="7749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
