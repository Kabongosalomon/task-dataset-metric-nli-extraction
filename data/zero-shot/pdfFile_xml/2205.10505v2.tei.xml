<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeper vs Wider: A Revisit of Transformer Configuration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianghai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxin</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeper vs Wider: A Revisit of Transformer Configuration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e., model width) to be 768 and the number of transformer layers (i.e., model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based language models have achieved promising results on natural language understanding tasks, e.g., Q&amp;A <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>, relation extraction <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref> and dialogue system <ref type="bibr" target="#b15">[16]</ref>. Recently, on vision tasks, transformers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> also outperform convolution-based models by a large margin. With sufficient training data, transformer-based models can be scaled to trillions of trainable parameters <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Through scaling along width (i.e., hidden dimension) and depth (i.e., number of transformer blocks), these huge transformers show effectiveness across various tasks and even areas.</p><p>When we use transformer, we typically follow existing work to set width and depth. For instance, we usually set the width of transformer-base model as 768 and the depth as 12. An interesting question here is: Why do we select these fixed hyper-parameters, even for problems in different areas? To answer this question, we revisit the conventional configurations from representative studies. For vision transformer <ref type="bibr" target="#b8">[9]</ref>, authors set the base ViT configuration according to those used in BERT <ref type="bibr" target="#b6">[7]</ref>. BERT selects such configuration following OpenAI GPT <ref type="bibr" target="#b17">[18]</ref>. OpenAI also follows the original transformer paper <ref type="bibr" target="#b22">[23]</ref>. In the original transformer paper, Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin <ref type="bibr" target="#b22">[23]</ref> conduct a set of ablation studies on machine translation task to find the optimal configurations. That is, for a good range of tasks, we have largely follow the transformer configuration based on an ablation study on machine translation task, i.e., a sequence-to-sequence task.</p><p>Nowadays, transformer-based models can be trained with various different training objectives or strategies. Taking vision transformer <ref type="bibr" target="#b8">[9]</ref> as an example, we can train transformer from scratch with a supervised learning setting for image classification. In this straightforward image classification task, each image is modeled as a sequence of multiple tokens, and each token corresponds to a patch in the image. We use the global information (from all tokens/patches of the image) to predict a single label, the category of the image. Here, as the training objective is to capture the global information of an image, the differences between token representations would not be considered directly. This image classification task is quite different from machine translation task, which requests for strong understanding of each input token and generating another sequence. Hence, it is natural to assume that different optimal transformer configurations exist for these two different tasks.</p><p>Previous work has tried to train a deeper transformer from scratch. However, as reported in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>, training by classification task (i.e., using the global signal of the input sequence) has the oversmoothing problem. That means, at the deeper transformer layers, all token representations tend to be identical. Such an issue harms the scalability of training vision transformer, especially scaling along depth. When we scale to a larger model, we only get a slight improvement or even worse accuracy. Recently, Gong, Wang, Li, Chandra, and Liu <ref type="bibr" target="#b11">[12]</ref> and Zhou, Kang, Jin, Yang, Lian, Jiang, Hou, and Feng <ref type="bibr" target="#b32">[33]</ref> show that, when we add special-designed regularization to avoid the "uniform tokens" (i.e., the over-smoothing problem), it is possible to train deeper transformer on the sequence (image) classification setting.</p><p>Different from training from scratch, the masked autoencoder is a two-stage training framework, including pre-training and fine-tuning. Given partially masked input sequence, the pre-training stage aims to recover the original unmasked sequence. The fine-tuning stage is similar to the aforementioned training from scratch. With masked autoencoder, recent studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> successfully train large scale transformers, even without using additional training data compared with supervised learning. This is counterintuitive because we usually assume more training data is the key in self-supervised pretraining to improve effectiveness. However, masked autoencoder is able to scale transformer to larger or deeper without the need of bigger dataset. This result motivates us to rethink the reason behind it.</p><p>We hypothesize that masked autoencoder training can alleviate the over-smoothing issue, which is an important reason why masked autoencoder can help to scale transformer up. The main reason behind this is the different training objectives. In masked autoencoder frameworks (e.g., BERT, BEiT), the target is to recover the masked tokens according to the unmasked tokens. Compared with training transformer from scratch under supervised learning, whose target is a simple classification task, masked autoencoder framework is using a sequence labeling target. The sequence labeling task requires the model to learn semantic information from neighboring unmasked tokens. Since different masked tokens have different unmasked neighboring tokens, the unmasked token representations must carry sufficient semantics for accurate prediction of the masked tokens, which in turn prevents the token representations to become identical. In a word, we can inference the masked autoencoder's training objective helps to alleviate over-smoothing problem by its regularization on token differences. To justify the reasoning above, we conduct both theoretical analysis and experimental verification. The results show that the over-smoothing issue is indeed alleviated in masked autoencoder. Compared with training under masked autoencder, training transformer by a simple classification task (e.g., training vision transformer from scratch) does not have such strength.</p><p>If the masked autoencoder well alleviates the over-smoothing issue, which is a challenge for scaling transformer along depth, does this mean the masked autoencoder prefers deeper models than wider ones for some tasks? We argue that we need to re-visit the configurations for different training objectives, especially for masked autoencoder. To this end, we conduct experiments to investigate the masked autoencoder configurations and propose our idea, Bamboo. Bamboo is, when we train transformer with masked autoencoder, we suggest to use deeper and narrower configurations to achieve better effectiveness with comparable computation cost as a typical setting. To evaluate our new configurations, we conduct comprehensive experiments on computer vision and natural language processing tasks. On vision tasks, we evaluate our configuration on large-scale vision transformer training. With Bamboo configurations, the masked autoencoders outperforms baselines by a large margin. For instance, on ImageNet, with comparable number of trainable parameters and computation cost, our narrower and deeper base-scale masked autoencoder, Bamboo-B, outperform MAE-B by 0.9% in terms of top-1 accuracy. On natural language processing tasks, we conduct experiments on BERT. Results show that our configurations can improve BERT-L by 1.1 points on GLUE datasets.</p><p>In summary, our main contributions are three folds:</p><p>? We propose the insight that masked autoencoder is better at scaling along depth. Training objective of masked autoencoder helps to handle transformer over-smoothing. We justify this reasoning by both theoretical analysis and experimental verification.</p><p>? We argue that the existing transformer configurations cannot fully use the strength of masked autoencoder. To this end, we propose Bamboo, an idea to scale transformer along depth with training with masked autoencoder. We show that narrower and deeper version show better performance than existing configurations, in a plug-and-play manner.</p><p>? We further verify our Bamboo configurations on larger scale vision transformer pre-training and language tasks. Results show that our Bamboo achieves state-of-the-art top-1 accuracy on image classification, and outperforms original BERT configurations by 1.1 points on GLUE.</p><p>The following sections are organized based on the analysis process of this work. In Section 2, we briefly review the over-smoothing problem in transformer and show the strength of masked autoencoder in handling this issue theoretically and experimentally. We then conduct experiments to investigate scaling masked autoencoder along depth in Section 3. Based on the consist sweet depth across scales, we suggest the Bamboo idea, using narrower and deeper configurations in masked autoencoder training. Then, we adapt the new configurations to larger scale, and conduct evaluations across different areas, vision tasks in Section 4 and NLP tasks in Section 5. Finally, we discuss the difference between this work and existing ones in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Over-smoothing in Masked Autoencoder</head><p>The over-smoothing issue is common in graph neural networks. When we stack many graph convolution networks, the node representations tend to be identical <ref type="bibr" target="#b2">[3]</ref>. Recent studies show that transformer has a similar problem <ref type="bibr" target="#b18">[19]</ref>, known as "uniform tokens" in transformer models. In deep transformer, each token representation can be seen as a node in the graph, and each attention score is an edge. Therefore, different token representations tend to be identical when we scale transformer to deeper. In this work, we mainly focus on the over-smoothing problem due to different training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Theoretical Analysis</head><p>We do theoretical analysis to compare the over-smoothing issue on sequence-level supervised learning, and masked antoencoder-based self-supervised learning, respectively. We use supervised vision transformer (ViT) <ref type="bibr" target="#b8">[9]</ref> and vision masked autoencoder (MAE) <ref type="bibr" target="#b12">[13]</ref> as two representative platforms to show our insights in both theoretical and experimental analysis.</p><p>Formally, transformer block's output (i.e., token representations) can be written as h =</p><formula xml:id="formula_0">{h 0 , h 1 , ..., h T ?1 }, where h t ? R d , d</formula><p>is the hidden dimension, T is the number of tokens at different transformer blocks. Usually, T is fixed across transformer blocks. For the token representations after l th transformer block, we denote the token representations as h l .</p><p>ViT is trained from scratch. The input is x = {x 0 , x 1 , ..., x T ?1 }, and the label is y ? R C , where C is the number of classes. We analysis the over-smoothing issue when we use either the [CLS] token or average token representations as the prediction head.</p><p>If we use [CLS] token to classify an input image, the training target is to minimize the distance between f (h L 0 ) and y, where f is a linear projection. Observe that the final classification only relays on one token representation from the [CLS] token. More importantly, when over-smoothing happens, i.e., h L 0 = h L 1 = ... = h L T ?1 , the prediction head remains meaningful for the task. If we use the average pooling prediction head, we reach the same conclusion. In this case, we are to minimize the distance between f (h L ) and y, whereh L is the average pooling vector from L th layer token representations. If h L 0 = h L 1 = ... = h L T ?1 =h L , the classification output remains the same. However, model effectiveness will be limited since less attention layers are effectively used.</p><p>Then, what would happen if we use masked autoencoder to pre-train? For masked autoencoder, the input is x = {x 0 , x 1 , ..., x T ?1 }. If x t token is masked, we denote this masked token as x m t . The set of the masked tokens is x m . The corresponding original token set before masking isx m . During masked autoencoder pre-training, we are to minimize the distance between h m andx m . We now state a key lemma to show that the masked autoencoder training target regularizes token representations. For simplicity, we treat each token representation as a scalar to analyze the variance Var(?). The following results can be extended to the vector case directly. Lemma 2.1 (Variance of final token representations is bounded in masked autoencoder.). In masked autoencoder, given the variance of masked token representation ? 2 , the variance of final token representations is bounded by the masked tokens:</p><formula xml:id="formula_1">Var(h L m ) ? ? 2 (1)</formula><p>The proof of the Lemma above is in Appendix A.1. As the transformer can also be seen as a weighted multi-view graph, given the Lemma above, following the over-smoothing issue in graph neural network, we can also infer that the intermediate token representations are also bounded:</p><formula xml:id="formula_2">Lemma 2.2 (Variance of intermediate token representation is bounded in masked autoencoder.).</formula><p>In masked autoencoder, given the variance of intermediate token representations from l th layer is bounded by the variance of the token representation from l + 1 th layer:</p><formula xml:id="formula_3">Var(h l m ) &gt; Var(h l+1 m )<label>(2)</label></formula><p>The proof of the Lemma above is in Appendix A. We do not take residual connection into consideration in the two lemmas above. In reality, the residual connection is a key module in transformer training to alleviate over-smoothing. The residual connection would keep variance increasing along depth, although over-smoothing may still be observed <ref type="bibr" target="#b7">[8]</ref>. We thus, based on the two lemmas above, state the following theorem to show the variance in masked autoencoder increases faster than the variance in training transformer by simple classification setting. Theorem 2.3 (Variance in masked autoencoder increases faster than the variance in training transformer by simple classification.). Given a converaged masked autoencoder g 1 (?) and transformer trained by simple classification g 2 (?) with residual connection, the increase in variance</p><formula xml:id="formula_4">?MAE = Var(x + g 1 (x)) ? Var(x) is larger than, ?ViT = Var(x + g 2 (x)) ? Var(x): ?MAE &gt; ?ViT<label>(3)</label></formula><p>The proof of the theorem above is in Appendix A.3.</p><p>In summary, when we train transformer on the supervised learning setting directly and treat the task as a simple classification task, the transformer does not need to consider the differences across token representations. Such behavior means there is no regularization to avoid over-smoothing. For masked autoencoder training framework, the training target is to recover the masked tokens base on the neighboring tokens. Since the different masked tokens have different semantic-related neighbors, the different unmasked tokens must cover different semantics in deeper layers. Therefore, although we are not regularizing the transformer model in a straightforward way, the masked autoencoder training framework still pushes the token representations to be different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Verification</head><p>Standard deviation. To verify our reasoning above, we conduct two sets of quantitative analysis. For the first set of quantitative analysis, we focus on the standard deviation of token representations at different transformer layers. To compare the over-smoothing issue between supervised learning and masked autoencoder, we calculate the mean standard deviation of the token representations at different transformer layers: </p><formula xml:id="formula_5">ms i = 1 T ? 1 T ?1 t=0 (h i t ?h i ) 2<label>(4)</label></formula><formula xml:id="formula_6">whereh i = 1 T ?1 T ?1 t=0</formula><p>h i t is the mean vector of token representations at i th transformer block. We use a deeper version of ViT-L and MAE-L as platforms to validate our analysis above. Note that, the ViT-L is trained on the supervised learning setting and treats image classification as a simple classification task, by average pooling prediction head. The MAE-L is trained on the masked autoencoder setting, which means the pre-training target is closed to sequence labeling task. We then fine-tune the pre-trained model for image classification. We set the width as 768 and the depth as 48. The total number of trainable parameters and computation cost is comparable with the original configuration (i.e., width is 1024, depth is 24). The results are shown in <ref type="figure" target="#fig_0">Figure 1a</ref>.</p><p>Observe that mean standard deviations of ViT-L and MAE-L both increase along depth. Such a result matches well with our theoretical analysis. The residual connection within transformer indeed alleviates over-smoothing (to some extent) <ref type="bibr" target="#b7">[8]</ref>. Nevertheless, the over-smoothing issue still exists even if we have residual connection as stated in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. We can observe that the increasing on MAE-L is much faster than that on ViT-L, which matches well with our Theorem 2.3. That means the deeper transformer blocks can learn different semantics for different token representations in MAE. In other words, the over-smoothing issue is alleviated in the transformer model pre-trained with the masked autoencoder setting.</p><p>Patch-pair cosine similarity. To further verify that the over-smoothing issue can be alleviated in masked autoencoder, following Gong, Wang, Li, Chandra, and Liu <ref type="bibr" target="#b11">[12]</ref>, we compare the patch-pair cosine similarity between ViT-L and MAE-L with deeper and narrower configuration. If there is an over-smoothing issue in the model, we should observe that the patch-pair cosine similarity increases along depth. The more serious the over-smoothing issue is, the faster cosine similarity increases. To remove the impact from input representations (residual connection), we use the zero-centered token representationsh i = h i ?h i for evaluation. The results are shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. The cosine similarity of ViT increases along depth due to over-smoothing. However, for the model pre-trained by masked autoencoder framework, cosine similarity keeps constant along depth. This comparison is interesting, as we can barely observe over-smoothing issue on the model pre-trained by the masked autoencoder, even if we are using a deeper model than usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bamboo</head><p>Our analysis above shows that the masked antoencoder helps to alleviate the over-smoothing issue, which is a main challenge in scaling transformer along depth. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder pre-training. Certainly, deeper and narrower configurations do not always improve performance, as there are a few other reasons hindering scaling transformer along depth. In this section, we devote to find a sweet point following Bamboo idea, to achieve effective masked autoencoder by experiments.</p><p>To validate our reasoning above, we conduct experiments on training transformer on ILSVRC-2012 ImageNet <ref type="bibr" target="#b5">[6]</ref> (ImageNet-1K), and report the top-1 accuracy. We use the original viT trained by supervised learning and MAE pre-trained masked image modeling as the test platform. For a fair comparison with the original transformer configurations, when we scale transformer to deeper, we also reduce the width to keep comparable number of trainable parameters and computation cost. The configurations we used are summarized in <ref type="table" target="#tab_1">Table 1</ref>.   For supervised learning models (i.e., ViT), we train base scale models for 300 epochs and large scale models for 200 epochs following He, Chen, Xie, Li, Doll?r, and Girshick <ref type="bibr" target="#b12">[13]</ref>. We use RandAugment <ref type="bibr" target="#b4">[5]</ref>, drop path <ref type="bibr" target="#b13">[14]</ref>, mixup, cutmix <ref type="bibr" target="#b31">[32]</ref>, label smoothing <ref type="bibr" target="#b19">[20]</ref> for data augmentation. Detailed hyper-parameters are summarized in Appendix B. For the masked autoencoder, we pre-train the base scale models for 1600 epochs and fine-tune for 100 epochs. For large models, we pre-train for 800 epochs and fine-tune for 50 epochs. We use the per-patch normalization following He, Chen, Xie, Li, Doll?r, and Girshick <ref type="bibr" target="#b12">[13]</ref> for better representations.</p><p>The results of scaling to deeper transformers are summarized in <ref type="figure" target="#fig_2">Figure 2</ref>. For both the base-scale and large-scale models, we report the top-1 accuracy on ImageNet-1K dataset. We can observe the models trained by supervised image classification directly (i.e., ViT-B and ViT-L) cannot improve accuracy when we use the deeper architectures. For ViT-B, the top-1 accuracy on ImageNet-1K keeps comparable when the depth is smaller than 50, but after that, there is a significant accuracy drop. For ViT-L, the accuracy decreases even faster than the ViT-B. There is a significant accuracy drop when we use the narrower and deeper models. However, for masked autoencoders, we can observe totally different patterns. Even if we keep the comparable trainable parameters and computation cost, with such a simple modification, the masked autoencoders gain significant improvements. More importantly, when we scale to 48 layers, both MAE-B and MAE-L reach sweet points. When scaling to 96 layers, we observe the training is unstable compared with shallower models. This result matches well with our observation in <ref type="figure" target="#fig_0">Figure 1b</ref>. At around 40th layer, the over-smoothing issue starts to happen slightly in masked autoencoder, which is much later than the ViT model. We suggest another reason of the unstable deep model training is the too large model updates <ref type="bibr" target="#b24">[25]</ref>. In this work, we focus on the over-smoothing issue of deep transformer training instead. The large model updates in deeper layers are out-of-scope. We leave that as our future work.</p><p>According to the experimental results above, we find that masked autoencoder can indeed scale transformer well along depth. Even if we keep comparable trainable parameters and computation cost with the original transformer, the model achieves better accuracy. Another observation is, both transformer-base and transformer-large reach their sweet points at around 50 layers. We thus recommend a new set of transformer configurations in <ref type="table" target="#tab_2">Table 2</ref> following our Bamboo idea, which are deeper and narrower than the original transformer configurations.  <ref type="table">Table 3</ref>: Top-1 accuracy on ImageNet-1K. We report two versions of ViT training from scratch. The first one is from original ViT paper <ref type="bibr" target="#b8">[9]</ref>, and the second one is from He, Chen, Xie, Li, Doll?r, and Girshick <ref type="bibr" target="#b12">[13]</ref>'s re-implementation with strong data augmentation. For MAE-B, we reproduce the results by running the official code and obtain a slightly different result (denoted by 83. We further evaluate our deeper and narrower transformer configurations on vision task. We train different models with more training epochs and compare with state-of-the-art vision models. We conduct experiments on ImageNet-1K and compare with recent supervised vision models e.g., DeepViT <ref type="bibr" target="#b32">[33]</ref> and DeiT <ref type="bibr" target="#b21">[22]</ref>, and self-supervised vision models e.g., DINO <ref type="bibr" target="#b1">[2]</ref>, MoCo v3 <ref type="bibr" target="#b3">[4]</ref>, BEiT <ref type="bibr" target="#b0">[1]</ref> and MAE <ref type="bibr" target="#b12">[13]</ref>. Compared with the MAE, the only difference is the configurations. That is, MAE uses original transformer configurations and we use our Bamboo configurations. The experiments are to verify that such a simple modification can improve model effectiveness.</p><p>We evaluate the models on three different scales, i.e., base, large, and huge. The data augmentation setting is exactly the same as MAE for a fair comparison. Again, the only difference is that we use the Bamboo configurations instead of the original transformer configurations. During fine-tuning, we use the same script with training ViT from scratch. We fine-tune base models for 100 epochs. For large and huge models, we fine-tune them for 50 epochs following existing work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Results on ImageNet-1K are reported in <ref type="table">Table 3</ref>. For training ViT from scratch, we report the original results <ref type="bibr" target="#b8">[9]</ref> and the results with strong data augmentation <ref type="bibr" target="#b12">[13]</ref>. A few recent work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref> focusing on training ViT from scratch on ImageNet-1K are also included. In general, we can find the models pre-trained by self-supervised learning (e.g., DINO, MoCO v3, MAE) perform much better than training from scratch. If we only consider the self-supervised learning approaches, masked image modeling-based methods (e.g., BEiT, MaskFeat, IBOT, MAE) outperform the contrastive learning-based methods (e.g., DINO, MoCo v3) significantly, especially on larger scale.</p><p>Since we are focusing on the scalability of training transformer with masked autoencoder, we choose MAE as our direct baseline, recent work with better scalability. We train MAE with Bamboo configurations and report the results in <ref type="table">Table 3</ref>. Observe that our Bamboo achieves the best top-1 accuracy on all scales. On the base scale, Bamboo achieves state-of-the-art performance, 84.2 top-1 accuracy, which is 0.9 (0.6) points higher than MAE-B. When we scale the model up to large scale and huge scale, Bamboo remains the best performer and achieves 86.3 and 87.1 top-1 accuracy respectively. Note that, compared to other scales, the improvement is not so significant on the huge scale. One reason is, the original huge configuration has been deep (i.e., 32 layers), which is closer to the sweet point. Similarly, this can also explain why the configurations designed under Bamboo can improve MAE-B significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation on Language Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>We further evaluate our Bamboo configurations on language tasks. We select BERT <ref type="bibr" target="#b6">[7]</ref> as the platform to evaluate our Bamboo configuration because it is widely used on many language tasks. Note that BERT is a post-layer normalization transformer model. If our configuration shows better performance on BERT, that means it has good generalization performance. We follow the BERT paper to use Wikipedia and bookscorpus to pre-train. The masked ratio is 15% as by default. During pre-training, we use LAMB optimizer and set batch size and learning rate as 4096 and 1.76e-3, respectively, following You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song, Demmel, Keutzer, and Hsieh <ref type="bibr" target="#b30">[31]</ref>.</p><p>During fine-tuning, we conduct experiments on General Language Understanding Evaluation (GLUE) benchmark. The GLUE benchmark <ref type="bibr" target="#b23">[24]</ref> is widely used in natural language understanding tasks, which include 8 tasks, i.e., CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2 and STS-B. We set the learning rate as 1e-5 or 2e-5. Batch size is fixed as 32. For small datasets, i.e., CoLA, MRPC, RTE and STS-B, we fine-tune for 10 epochs. For larger datasets, i.e., MNLI, QNLI, QQP and SST-2, we fine-tune for 3 epochs. Matthew's correlation is used as metric for CoLA. For MNLI, we report the average accuracy on MNLI-m and MNLI-mm. QNLI and RTE also adapt the accuracy as metric.</p><p>The results on MRPC and QQP are reported with the average of F1 and accuracy. We use Spearman correlation on STS-B. For all fine-tuning results, we run the code 5 times and report the median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>The results on language tasks are reported in <ref type="table" target="#tab_4">Table 4</ref>. Under the same pre-training and fine-tuning settings, models with Bamboo configurations outperform BERT by a large margin. Bamboo-L achieves the best performance in <ref type="table" target="#tab_4">Table 4</ref>. Compared with BERT-L, our Bamboo-L wins on 7 out of 8 datasets, and surpassed BERT-L by 1.1 points on average. It is also notable Bamboo configurations can outperform baselines on both large datasets (e.g., MNLI) and small datasets (e.g., CoLA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Compared with simply scaling along depth, this work keeps comparable computation cost and number of trainable parameters. When scaling along depth, we also make the deep transformer narrower. Under such setting, the deeper and narrower configurations re-designed under Bamboo idea can still outperform the baseline configurations, suggesting that we should consider narrower and deeper transformer when training by masked autoencoder. One related work is <ref type="bibr" target="#b20">[21]</ref>, an empirical study of practical scaling of transformer, which has a similar observation, deeper and narrower can improve accuracy. On this basis, our insight is, the insight in <ref type="bibr" target="#b20">[21]</ref> depends on different training objectives.</p><p>Compared with brute-force hyper-parameter tuning, we provide both theoretical analysis and experimental verification to justify our insight, i.e., masked autoencoder alleviates over-smoothing issue in transformer. Motivated by this, we propose Bamboo by experimental investigation. There is no guarantee to ensure the configurations are always the optimal ones as different tasks may require different configurations. We believe it is impossible in deep learning to know the optimal configurations before experiments. However, we justify the tendency that masked autoencoder gets more benefits from deeper configurations. Our insight may instruct future works to consider configurations according to the training objectives.</p><p>Instead of proposing a new approach or a new set of configuration, this work focuses on an existing but neglected problem. After revisiting, we find configurations should be re-designed for different training objectives. We then re-design a set of configurations for masked autoencoder, which can improve transformer on both vision and language tasks. On vision tasks, we even achieve SoTA top-1 accuracy on ImageNet. However, we highlight our main contribution is an insight instead of a SoTA model, and this insight is orthogonal to the advanced masked antoencoder modifications in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We first rethink the masked autoencoder's scalability in this work. Through theoretical analysis, we show that training transformer with masked autoencoder can alleviate the over-smoothing issue when scaling transformer along depth. We then verify this finding through experiments. The results show that, compared with supervised learning, model pre-trained by masked autoencoder can handle oversmoothing (i.e., uniform tokens) better. Under this insight, we rethink the widely used configurations in vision and language transformer, and suggest deeper transformer when training with masked autoencoder. We then explore this strategy on base and large scale vision transformer and re-design a set of vision transformer configurations for masked autoencoder empirically. To further verify the effectiveness of our configuration, we conduct experiments on larger scale vision transformer training and achieved SoTA top-1 accuracy with comparable computation cost on ImageNet-1K. To show our configurations are better than original transformer configurations, we further conduct experiments on language tasks. Compared with widely used language masked autoencoder, BERT, our model surpass it by 1.1 points on average on GLUE benchmark. In summary, our work provides an insight to explain why masked autoencoder can scale transformer along depth, and we propose Bamboo, a set of configurations to fully utilize such strength.</p><p>In the future, we plan to integrate the existing methods focusing on scaling along depth, e.g., DeepNorm <ref type="bibr" target="#b24">[25]</ref> to further scale transformer. Another future direction is to verify our Bamboo configurations on other masked autoencoder-based models (e.g., BEiT <ref type="bibr" target="#b0">[1]</ref>, RoBERTa <ref type="bibr" target="#b14">[15]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Missing Proofs</head><p>In this section we fill out the proofs of lemmas made in the main paper. For simplicity, we treat each token representation as a scalar to analysis the variance. The following results can be extent to the vector case easily.</p><p>A.1 Proof of Lemma 2.1 Lemma 2.1. In masked autoencoder, given the variance of masked token representation ? 2 , the variance of final token representations is bounded by the masked tokens:</p><formula xml:id="formula_7">Var(h L m ) ? ? 2<label>(5)</label></formula><p>Proof. In masked autoencoder, the input is x = {x 0 , x 1 , ..., x T ?1 }. During pre-training, we are to minimize the distance between last layer masked token representations h L m and corresponding original token set isx m . Given Var(x m ) = ? 2 , as we are minimizing ||h L m ?x m ||, we can assume h L m ?x m = , where ? N (0, ? 2 ). Then, the variance of h L m can be written as:</p><formula xml:id="formula_8">Var(h L m ) = Var(x m + ) = Var(x m ) + Var( ) = ? 2 + ? 2 (6) Obviously, since ? 2 ? 0, Var(h L m ) ? ? 2 .</formula><p>A.2 Proof of Lemma 2.2 Lemma 2.2. In masked autoencoder, given the the variance of intermediate token representations from l th layer is bounded by the variance of the token representation from l + 1 th layer:</p><formula xml:id="formula_9">Var(h l m ) ? Var(h l+1 m )<label>(7)</label></formula><p>Proof. Since the attention mechanism can be seen as weighted averaging, we obtain the variance of intermediate token representations from l th layer:</p><formula xml:id="formula_10">Var(h l+1 m ) = Var(g(f ( T ?1 i=0 w i h l i m ))) = T ?1 i=0 Var(w i h l i m )<label>(8)</label></formula><p>where f (?) is linear projection and g(?) is ReLU activation function. Since we initialize f (?) by Xaiver initialization in usual, it will not increase the variance in usual. After that, the ReLU would also only decrease the variance of input. Since the attention weight w has been normalized by softmax, </p><p>A.3 Proof of Theorem 2.3 Theorem 2.3. Given a converaged masked autoencoder g 1 (?) and transformer trained by simple classification g 2 (?) with residual connection, the increase in variance ?MAE = Var(x + g 1 (x)) ? Var(x) is larger than, ?ViT = Var(x + g 2 (x)) ? Var(x):</p><formula xml:id="formula_12">?MAE &gt; ?ViT<label>(10)</label></formula><p>Proof. We assume both MAE and ViT are well-trained on the same dataset until convergence, and the MAE and ViT have the same model architecture (e.g., the same hidden dimension). We use transformer with one layer to prove in this section. The results can also be extent to multiple transformer layers. First of all, we need to note the input of masked autoencoder is different from the transformer trained by simple classification. The input tokens of masked autoencoder is partly masked, we denote it by x m . Instead, the input of simple classification transformer is a sequence of tokens without masking x. Then, the variance of MAE's output can be written as:</p><p>Var(h m ) = Var(x m + g 1 (x m )) = Var(x m ) + Var(g 1 (x m )) + 2Cov(x m , g 1 (x m ))</p><p>where h m is the output of masked autoencoder. Similarly, the variance of simple classification transformer's output can be written as:</p><p>Var(h) = Var(x + g 2 (x)) = Var(x) + Var(g 2 (x)) + 2Cov(x, g 2 (x))</p><p>To simplify Eq. 11, we set x m = x ? x m , where x m is original token value if the token is masked, and x m = 0 if not the token is not masked. Then, according to Lemma 2.1, Var(h m ) can be written as:</p><p>Var </p><p>Then, ?MAE can be written as: </p><p>Simlarly, for ?ViT, we have: ?ViT = Var(h) ? Var(x) = Var(g 2 (x)) + 2Cov(g 2 (x), x)</p><p>For each linear projection without residual connection, we assume g 2 (x) is independant to x in a well-trained transformer:</p><formula xml:id="formula_18">?MAE &gt; 3Var(x) ? 2Cov(x m , x) &gt; 3Var(x) ? 2Var(x) = Var(x)<label>(16)</label></formula><p>According to Lemma 2.2, we have:</p><p>?MAE &gt; Var(x) &gt; Var(g 2 (x)) = ?ViT <ref type="bibr" target="#b16">(17)</ref> B Fine-tuning Hyper-parameters </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Over-smoothing analysis of ViT and MAE, at different transformer layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Scaling ViT and MAE with comparable number of parameters and computation cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>T ? 1 i=0 w 2 i ? 1 .</head><label>121</label><figDesc>Then, Var(h l+1 m ) can be written as: Var(h l+1 m ) = Var(h l i m )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(h m ) = Var(x m ) + Var(g 1 (x m )) + 2Cov(x m , g 1 (x m )) = Var(x m ) + Var(g 1 (x m )) + 2Cov(x m , x + ) = Var(x m ) + Var(g 1 (x m )) + 2Cov(x m , x) = Var(x m ) + Var(g 1 (x m )) + 2Cov(x ? x m , x) = 2Var(x) + Var(x m ) + Var(g 1 (x m )) ? 2Cov(x m , x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>?MAE = V ar(h m ) ? V ar(x m ) = 2Var(x) + Var(g 1 (x m )) ? 2Cov(x m , x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2. Given the theoretical analysis above, we can find both the intermediate and final token representations are bounded by the variance of the masked tokens, i.e., Var(h l m ) &gt; Var(h l+1 m ) ? ? 2 . Therefore, the over-smoothing issue is alleviated in masked autoencoder.</figDesc><table /><note>In fact, revisiting the training of ViT from scratch again, Lemma 2.2 also holds for the image classification setting. However, the difference is, Lemma 2.1 cannot work on training ViT from scratch. Without the masked token modeling target in MAE, there is no bound to push last layer's token representations to be different. Since the Lemma 2.2 still holds, the variance of token representations decreases layer-by-layer. When we scale transformer to deeper, over-smoothing happens.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The configurations we used to scale transformer along depth. For a fair comparison, we keep a comparable computation cost with the original transformer configurations (i.e., depth=12 for Base, depth=24 for Large).</figDesc><table><row><cell></cell><cell>Scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base</cell><cell></cell><cell></cell><cell>Large</cell></row><row><cell></cell><cell>Depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell><cell>24</cell><cell></cell><cell>48</cell><cell>96</cell><cell>24</cell><cell>48</cell><cell>60</cell><cell>96</cell></row><row><cell></cell><cell>Width</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">768 512</cell><cell>384 256</cell><cell>1024 768</cell><cell>640 512</cell></row><row><cell></cell><cell cols="4">#Attention Heads</cell><cell></cell><cell>12</cell><cell>8</cell><cell></cell><cell>6</cell><cell>4</cell><cell>16</cell><cell>12</cell><cell>10</cell><cell>8</cell></row><row><cell></cell><cell cols="6">Computation Cost 1?</cell><cell cols="3">0.9? 1?</cell><cell>0.9? 1?</cell><cell>1.1? 1?</cell><cell>1?</cell></row><row><cell>0</cell><cell>12</cell><cell>24</cell><cell>36</cell><cell>48 Depth</cell><cell>60</cell><cell>72</cell><cell>84</cell><cell>96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Re-designed configurations under Bamboo idea.</figDesc><table><row><cell>Scale</cell><cell>Base</cell><cell></cell><cell>Large</cell><cell></cell><cell>Huge</cell><cell></cell></row><row><cell></cell><cell cols="6">Original Bamboo Original Bamboo Original Bamboo</cell></row><row><cell>Depth</cell><cell>12</cell><cell>48</cell><cell>24</cell><cell>48</cell><cell>32</cell><cell>64</cell></row><row><cell>Width</cell><cell>768</cell><cell>384</cell><cell>1024</cell><cell>768</cell><cell>1280</cell><cell>896</cell></row><row><cell>#Attention Heads</cell><cell>12</cell><cell>6</cell><cell>16</cell><cell>12</cell><cell>16</cell><cell>14</cell></row><row><cell cols="2">Computation Cost 1?</cell><cell>1?</cell><cell>1?</cell><cell>1.1?</cell><cell>1?</cell><cell>1?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of fine-tuning on GLUE benchmark.</figDesc><table><row><cell>Method</cell><cell cols="8">CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg</cell></row><row><cell>BERT-B</cell><cell>59.6</cell><cell>83.7</cell><cell>88</cell><cell>90.4</cell><cell>88.8</cell><cell>68.6 91.5</cell><cell>89.4</cell><cell>82.5</cell></row><row><cell>BERT-L</cell><cell>60.9</cell><cell>86.2</cell><cell>89.3</cell><cell>92.3</cell><cell>89.6</cell><cell>73.1 92.5</cell><cell>90.4</cell><cell>84.3</cell></row><row><cell cols="2">Bamboo-B 60.5</cell><cell>84.3</cell><cell>88.4</cell><cell>90.5</cell><cell>89.0</cell><cell>70.0 92.2</cell><cell>89.5</cell><cell>83.1</cell></row><row><cell cols="2">Bamboo-L 62.9</cell><cell>87.1</cell><cell>89.8</cell><cell>92.4</cell><cell>89.4</cell><cell>77.3 93.8</cell><cell>90.6</cell><cell>85.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters on ImageNet fine-tuning</figDesc><table><row><cell>Parameter</cell><cell cols="3">Base Large Huge</cell></row><row><cell>Epoch</cell><cell>100</cell><cell>50</cell><cell>50</cell></row><row><cell>Warmup Epochs</cell><cell></cell><cell>5</cell><cell></cell></row><row><cell>Batch Size</cell><cell></cell><cell>1024</cell><cell></cell></row><row><cell>Learning rate</cell><cell></cell><cell>2e-3</cell><cell></cell></row><row><cell cols="2">Layer-wise learning rate decay 0.65</cell><cell>0.75</cell><cell>0.75</cell></row><row><cell>Weight Decay</cell><cell></cell><cell>0.05</cell><cell></cell></row><row><cell>DropPath</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Label smoothing</cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell>Erasing prob.</cell><cell></cell><cell>0.25</cell><cell></cell></row><row><cell>RandAug</cell><cell></cell><cell>9/0.5</cell><cell></cell></row><row><cell>Mixup prob.</cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>Cutmix prob.</cell><cell></cell><cell>1.0</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Preprint.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2793" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06905</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vision transformers with patch diversification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12753</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recent advances in deep learning based dialogue systems: A systematic survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pandelea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Adiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04387</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bert with history answer embedding for conversational question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1133" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting oversmoothing in bert from the perspective of graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scale efficiently: Insights from pre-training and fine-tuning transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10686</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training dataefficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://aclanthology.org/W18-5446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deepnet: Scaling transformers to 1,000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00555</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">One student knows all experts know: From sparse to dense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10890</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Go wider instead of deeper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11817</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An embarrassingly simple model for dialogue relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13873</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bert representations for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1556" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deepvit: Towards deeper vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ibot: Image bert pre-training with online tokenizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11304</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
