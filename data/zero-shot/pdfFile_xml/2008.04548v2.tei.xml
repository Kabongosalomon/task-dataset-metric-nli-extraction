<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DensE: An Enhanced Non-commutative Representation for Knowledge Graph Embedding with Adaptive Semantic Hierarchy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Lu</surname></persName>
							<email>luhaonan@oppo.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Hu</surname></persName>
							<email>huhailin2@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Lin</surname></persName>
							<email>lin@business.rutgers.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">OPPO Guangdong Mobile Telecommunications Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Management Science and Information Systems</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DensE: An Enhanced Non-commutative Representation for Knowledge Graph Embedding with Adaptive Semantic Hierarchy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Corresponding Author. This work was done when the author was working in Huawei Technologies. the interpretations generated by DensE also reveal how relations with distinct patterns (i.e., symmetry/anti-symmetry, inversion and composition) are modeled, which suggests several important directions of future studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Capturing the composition patterns of relations is a vital task in knowledge graph completion. It also serves as a fundamental step towards multi-hop reasoning over learned knowledge. Previously, several rotation-based translational methods have been developed to model composite relations using the product of a series of complex-valued diagonal matrices. However, these methods tend to make several oversimplified assumptions on the composite relations, e.g., forcing them to be commutative, independent from entities and lacking semantic hierarchy. To systematically tackle these problems, we have developed a novel knowledge graph embedding method, named DensE, to provide an improved modeling scheme for the complex composition patterns of relations. In particular, our method decomposes each relation into an SO(3) groupbased rotation operator and a scaling operator in the three dimensional (3-D) Euclidean space. This design principle leads to several advantages of our method: (1) For composite relations, the corresponding diagonal relation matrices can be non-commutative, reflecting a predominant scenario in real world applications;</p><p>(2) Our model preserves the natural interaction between relational operations and entity embeddings; (3) The scaling operation provides the modeling power for the intrinsic semantic hierarchical structure of entities; (4) The enhanced expressiveness of DensE is achieved with high computational efficiency in terms of both parameter size and training time; and (5) Modeling entities in Euclidean space instead of quaternion space keeps the direct geometrical interpretations of relational patterns. Experimental results on multiple benchmark knowledge graphs show that DensE is comparable to the current stateof-the-art models for missing link prediction, especially on composite relations. In addition,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are a vital component of a wide range of downstream applications, such as machine reasoning, information retrieval and knowledge-guided natural language processing <ref type="bibr">(Ji et al., 2020;</ref><ref type="bibr" target="#b30">Zhang et al., 2019b;</ref><ref type="bibr" target="#b25">Yang et al., 2019;</ref><ref type="bibr" target="#b7">Lin et al., 2019)</ref>. Especially, learning how to hop over a variety of concepts or instances stored in a knowledge graph represents a value path towards artificial general intelligence.</p><p>Knowledge graphs are defined as a collection of triplets. Each triplet, denoted by (h, r, t), indicates a relation r pointing from the head entity h to tail entity t. Currently, numerous research efforts have been devoted to developing knowledge graph embedding (KGE) methods. These methods aim to learn a set of low-dimensional representations of entities and relations <ref type="bibr">(Ji et al., 2020;</ref><ref type="bibr" target="#b11">Nguyen et al., 2017)</ref>, which is usually coupled with a score function to enable the knowledge graph completion process, i.e., predicting missing links between entities, for real-world KGs <ref type="bibr" target="#b12">(Nickel et al., 2016;</ref><ref type="bibr" target="#b6">Lacroix et al., 2018;</ref><ref type="bibr" target="#b1">Bordes et al., 2013;</ref><ref type="bibr" target="#b16">Sun et al., 2019;</ref><ref type="bibr" target="#b28">Zhang et al., 2019a)</ref>. Sometimes, neural networks can be inserted into the process <ref type="bibr" target="#b2">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b15">Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b10">Nathani et al., 2019)</ref>, though this requires additional computation costs.</p><p>In principle, the desired KGE method should be able to accommodate various relation patterns and to learn representations that are approximately able to reason over the given patterns (expressiveness property of a KGE model <ref type="bibr" target="#b16">(Sun et al., 2019;</ref><ref type="bibr" target="#b21">Wang et al., 2017)</ref>). For example, in a relation pattern such as symmetry (e.g., friend), asymmetry (e.g., uncle), inversion relations (e.g., hypernym and hyponym) and compositional relations (e.g., my father's mother is my grandmother), these patterns should be hold in the vector space. While the former three patterns are readily covered by the current methods <ref type="bibr" target="#b19">(Trouillon et al., 2016a;</ref><ref type="bibr" target="#b16">Sun et al., 2019)</ref>, it still lacks an effective modeling strategy for composite relations due to the complexity of composition patterns. In particular, we find three predominant challenges in this modeling problem. First, the composition of relations can be non-commutative (e.g., my father's mother is my grandmother, while my mother's father is my grandfather, the reasoning result can be different by changing the orders of relations in a path of knowledge graph), which is opposite to the assumption of most KGE methods <ref type="bibr" target="#b1">(Bordes et al., 2013;</ref><ref type="bibr" target="#b16">Sun et al., 2019)</ref>. Second, the expressiveness of KGE methods is often limited by the counterintuitive lack of interaction between entity and relation embeddings <ref type="bibr" target="#b26">(Yang et al., 2014)</ref>. Last but not least, while the semantic hierarchy of entities in a knowledge graph is a ubiquitous property <ref type="bibr" target="#b29">(Zhang et al., 2020</ref>) (e.g., a triplet in WordNet <ref type="bibr" target="#b9">(Miller, 1995)</ref> (palm, hypernym, tree) indicates "tree" is at a higher level than "palm" in the hierarchy), most methods do not pay attention to this and therefore fail to capture the semantic features at different semantic hierarchical levels.</p><p>In this work, to address these limitations, we develop a comprehensive solution to provide a highly expressive, efficient and interpretable modeling method for knowledge graph embedding. More specifically, we propose DensE (Distance-based Embedding with Non-commutative Rotation and Scaling in 3-D Euclidean Space), which decomposes the relation into an SO(3) group-based rotation operator and a scaling operator and in the 3-D Euclidean space. Intuitively, non-Abelian group (here we use SO(3) rotation group) is applied to introduce non-commutative nature to our model, and the scaling operation offers another important dimension to accommodate each triplet in the Euclidean space, which is barely explored in previous research. Our main contributions are summarized as the following:</p><p>(1) By integrating infinite non-Abelian groupbased relational rotation and scaling operations in the 3-D Euclidean space within a unified framework, we effectively accommodate various relation patterns including non-commutative compositions, semantic hierarchy, as well as interactions between entities and relations;</p><p>(2) Extensive experiments show that DensE achieves comparable to the current state-of-the-art models in link prediction with high computational efficiency, offering a useful tool for knowledge graph completion;</p><p>(3) We systematically consider three important scenarios of composition patterns that shall be considered by KGE methods (Section 3). Then, we show our method can provide an up-to-date most comprehensive while straightforward geometric interpretation for the modeling process of each relation type in the 3-D Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will discuss two different categories of KGE methods, especially how they evolve in terms of model expressiveness and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transnational Distance Model</head><p>Transnational distance models, represented by TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> and RotatE <ref type="bibr" target="#b16">(Sun et al., 2019)</ref> use Eucleadian distance as the score function. In particular, the embedding of relations and entity are fit so that the tail entity can be obtained by from the head entity using the operation defined by the relation. Given this intuition, these methods usually reflect some particular geometric interpretations. However, within the various relation patterns, their modeling capacity for composite relations (i.e., a relation path composed of a series of relations) tend to be insufficient because most methods assume a commutative pattern on the relation path and do not consider entity information in inferring composition patterns.</p><p>Specifically, TransE models each relation as a pure translational transformation, so it assumes a fixed addition composition pattern between relations, i.e., r 3 = r 1 + r 2 , which is commutative and irrelevant to entity embeddings. RotatE made significant progress by modeling relations as rotational operator (rotation matrix) in 2-D Euclidean space. When modeling a relation path composed of multiple relations, RotatE uses Hadamard product to combine the rotation matrices of the relations on the path, i.e., r 3 = r 1 ? r 2 . In this model, all relations in the composite relation have the same rotation axis. Thus, the compositions in RotatE are also mandatorily commutative. Also, interactions between relation and entity embeddings are precluded as the rotation axis is always perpendicular to entity embeddings.</p><p>Following the effort of RotatE, several methods have been proposed to enhance the expressiveness of rotation-based translation KGE model. For instance, <ref type="bibr" target="#b27">(Yang et al., 2020)</ref> proposes a grouptheoretic analysis for KGE methods. Their method, named NagE, represents a preliminary attempt in applying non-Abelian group in modeling relational rotations. RotatE3D <ref type="bibr" target="#b3">(Gao et al., 2020)</ref>, on the other hand, extends the rotation of RotatE into the 3-D Euclidean space. However, although these methods represent certain conceptual advances, their empirical results show limited performance advance over previous methods, probably challenged by the fitting power of pure rotation-based operations and lacking of ability to model semantic hierarchies in knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Matching Model</head><p>In constrast to translational distance model, methods in this category evaluate the matching of latent representation of relations and entities using bilinear model, e.g., RESCAL <ref type="bibr" target="#b13">(Nickel et al., 2011)</ref>, DistMult <ref type="bibr" target="#b26">(Yang et al., 2014)</ref>, and ComplEX <ref type="bibr" target="#b20">(Trouillon et al., 2016b)</ref>. Recently, as a generalization of DistMult and ComplEX, QuatE proposes a transformation on the entity representations by quaternion multiplication with the relation representation <ref type="bibr" target="#b28">(Zhang et al., 2019a)</ref>, leading to a significant advance in expressiveness. For composite relations, this method does not assume any fixed composition pattern and preserve the non-commutative nature to some extent. However, QuatE requires normalization of relation to unit quaternion, indicating it is incapable of integrating scale information. In addition, as both entities and relations are embedded in quaternion hyperplanes, QuatE cannot provide a straightforward geometric interpretation in the space, which hinders the understanding of the learned embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Methodological Advance of DensE</head><p>From the perspective of score function, DensE also belongs to translation distance model. In contrast to the previous works, our model leverages both rotation and scaling operations for relation modeling. The key idea is that we can transform any non-zero vector in the Euclidean space to another arbitrary vector through decoupled rotation and scaling transformations. In addition, our model provides a clear geometric picture to demonstrate the transformation of entity representation in various relation composition patterns.</p><p>Conceptually, some modules of our method is also related to other recent KGE methods. For instance, a recent work HAKE <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref> has explored the combination of rotation and scaling operations, which are used to model entities at same and different levels of hierarchy, respectively. In HAKE, rotation is defined following the protocol in RotatE, which leverages U(1) Abelian group and thus incapable of handling non-commutative relations. In addition, its rotation axis is vertical to the 2-D representation space of entities, again omitting the entity-relation interaction as in RotatE. On the other hand, while using the non-Abelian group in KGE model has been explored by <ref type="bibr" target="#b23">(Xu and Li, 2019)</ref>, we argue this model has lower expressiveness than us in principle since it uses finite (non-)Abelian group (using non-Abelian group is optional) in 2-D space while we consider an infinite non-Abelian group in 3-D space. Also, its rotation operation is combined with reflection, which constitutes a special case of our scaling operation (i.e., scaling with a factor of -1).</p><p>In contrast with QuatE that models both entities and relations in the quaternion space and does transformation using quaternion multiplication, DensE is based on 3-D Euclidean space rather than the space of quaternions. The continuous rotation transformation in the n-D Euclidean space (n &gt; 2) is modeled by a special orthogonal group (SO(n) group). Compared with the vanilla U(1) abelian group based KGE models (e.g. RotatE/HAKE) that perform rotation transformation in the 2-D Euclidean space, continuous rotation transformation in the 3-D space modeled by SO(3) group is the minimum non-abelian extension with geometric interpretability. Composite relations (relation paths, details can be found in Section 3) are usually modeled by the product/summation of relation matrices. The violation of the commutative law of multiplication in the non-abelian case makes modeling the complex composition patterns of relations possible (non-commutative). The quaternion system is related to the SO(3) group and corresponds to the rotation transformation in 3-D Euclidean space, it provides a mathematical way to model continuous rotation transformation in 3D space. In our model, to guarantee geometric interpretability, entities are represented by 3D vectors, relations are modeled by quaternions that perform rotation and scaling transformation in 3D space. The rigidness of the quaternion system corresponds to the mathematical properties of SO(3) group theory since the rotation transformation in 3D space must satisfy several constraints such as: non-commutative (non-abelian nature), orthogonality, invertibility, etc. QuatE also studied that increasing spatial dimensions such as to Octonion does not increase performance compared to modeling relation and entities in the space of quaternion. The reason behind this is that the octonion system is more rigid than the quaternion system, the associative law of multiplication is also violated. However, this property is even harmful to modeling patterns of relations in a KG since there's no relation pattern in a real-world KG that needs to be modeled by violating the associative law of multiplication.</p><p>Therefore, we argue that among these concurrent works, DensE is the most comprehensive solution with the geometric interpretability that accounts for all the three desiderata for modeling composite relations, i.e., covering the non-commutative relations, preserving interaction between entity and relation, and capturing the entities' semantic hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>We denote a directed knowledge graph as G(E, R, F), where E, R and F are sets of entities, relations and facts, respectively. A fact stored in a KG can be expressed as a triplet (h, r, t) ? F, where h, t ? E and r ? R. Herein, we focus on the knowledge completion task, which aims to predict missing links based on the observed facts. To fulfill this goal, a score function is used to measure the plausibility of proposed fact candidates, and the goal of model optimization is to give higher scores to true triplets (h, r, t) than the false triplets (h, r,t) or (h, r, t), wheret andh are randomly sampled tail and head entities, respectively. Mathematically, the entity and relation embeddings are usually represented by tensors, and the score function can thus be written into the form of f r (h, t).</p><p>In principle, KGE models should be designed to accommodate various relation patterns existing in real world KGs, such as symmetry, anti-symmetry, inversion and composition, which are formally defined as follows.</p><p>Let x, y, z be the entities in a given KG, and r(?, ?) maps the relation between the two entities, we have:</p><formula xml:id="formula_0">Definition 1. A relation r is symmetric if ?x, y, r(x, y) ? r(y, x).<label>(1)</label></formula><p>On the other hand, a relation is said to be antisymmetric if ?x, y, r(x, y) ? ?r(y, x).</p><p>F riend is a typical example of symmetric relation, which means if we know x is friend of y, we can infer y is also friend of x. F iliation is an example of anti-symmetric relation. Definition 2. Relation r 1 is inverse to relation r 2 if ?x, y</p><formula xml:id="formula_2">r 1 (x, y) ? r 2 (y, x)<label>(3)</label></formula><p>For instance, has_part and part_of fit into the scope of inverse relations, which means if we know x is a part of y, we can infer that y has part x. Note that both symmetric/antisymmetric and inverse relation patterns can be inferred in one hop, so they are also called atomic relation. In contrast to the above atomic relation patterns (inferable within one hop), the complex composition patterns pose a particular challenge to modeling, as discussed below.</p><p>Definition 3. Relation r 3 is composed of relation r 1 and relation r 2 if ?x, y, z</p><formula xml:id="formula_3">r 1 (x, y)?r 2 (y, z) ? r 3 (x, z)<label>(4)</label></formula><p>Here r 3 is also referred to as a composite relation and possesses certain composition pattern.</p><p>In particular, our model design takes the following properties into account: Property 1. The two relations in the composition are not always commutative.</p><p>For example, given r 1 = is_f ather_of , r 2 = is_mother_of , based on the Definition 3, we will get r 3 = is_grandmother_of . However, when we change the order, i.e., r 1 = is_mother_of , r 2 = is_f ather_of , we will get r 3 = is_grandf ather_of . Recent KGE methods usually model composite relations (relation paths) by the product (e.g., QuatE, HAKE) /summation (e.g., TransE) of relation matrices. However, it is non-trivial to model composition relation patterns since the product/summation of diagonal real-valued/complex-valued matrices is usually commutative and hence invariant with the order of relations. For instance, ComplEx (Trouillon et al., 2016a) models relation path mentioned above r 1 (x, y)?r 2 (y, z) by using the product of two complex-valued diagonal matrices: R r 2 R r 1 . However, the product of relation matrices in the diagonalized framework are commutative since that</p><formula xml:id="formula_4">R r 2 R r 1 = R r 1 R r 2 .</formula><p>Property 2. The composition patterns are not always inferable by the relations alone. For example, given that y is x s younger sister and z is y s elder brother, we can not answer whether z is elder or younger than x from the given information.</p><p>Actually, to answer this question, we need to know more about x/y/z from their own attributes and their other relationships.</p><p>Property 3. In a composition, the relations involved are not necessarily different. Given the twohop example above, besides the situation that r 1 , r 2 and r 3 are mutually different, there are also four different cases that satisfy the definition of composition, i.e., r 1 = r 2 = r 3 , r 1 = r 2 = r 3 , r 1 = r 3 = r 2 and r 1 = r 2 = r 3 <ref type="figure">(Figure 1(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we will first discuss the limitation of previous method such as RotatE, which is based on the 2-D Euclidean space. Then we will introduce each module of our method. In particular, we model a relation by a combination of an SO(3) group-based rotation (introducing the noncommutative nature) and a scaling operation (introducing the semantic hierarchy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Limitations of Modeling Relational</head><p>Rotation in the 2-D Euclidean Space</p><p>The motivation of RotatE is from Euler's identity e i? = cos ? + i sin ?, which applies rotation in the 2D complex plane by using a unitary complex number. The RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation operator that transforms the source entity to the target entity. However, as shown in the <ref type="figure" target="#fig_1">Figure 2</ref>, composite relations are assumed to be commutative. Changing the order of relational rotation of r 1 and r 2 gives the same composition r 3 . Also, the unit rotation transformation makes it difficult to model the semantic hierarchy which is a ubiquitous property in knowledge graphs. The rotation axis (perpendicular to the paper) of 2-D rotation transformation is orthogonal to entity embeddings, which hinders the method to model interactions between relational operations and entity embeddings. Continuous rotation transformation in 3D space modeled by SO(3) group is the minimum nonabelian extension with geometric interpretability. By modeling relations and entities as rotation operators and vectors in 3D space, transformation in the 3-D Euclidean space can be either noncommutative or commutative. And since the ro-tation axis of 3-D transformation is not enforced to be perpendicular to the vectors, interactions between relations and entities can also be considered. A simple example for how rotation in the 3-D Euclidean space can model non-commutative relations is shown in Supplementary Note 1. We will formally introduce the mathematical method for modeling rotations in 3D space in Section 4.2.</p><p>In addition, to model the semantic hierarchies of knowledge graphs, the rotation operation is then followed by a scaling transformation. The modulus parts of 3D vectors aims to model the entities in a KG at different levels of the semantic hierarchy. The detail of integrating the scaling transformation in our model is discussed in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Relational Rotation Using SO(3) Rotation Group</head><p>One of the ways to model a rotation operation in the 3-D space is called axis-angle representation, which parameterizes a rotation by two quan- More specifically, we can use a unit quaternion to encode the rotation using three degrees of freedom (i.e., ?, ? and ?). Actually, it can be viewed as a group structure on a 3-sphere (i.e., S3) which gives the group Spin(3). Note that this group structure is isomorphic to SU(2) group and also to the universal cover of SO(3) group. Formally, the unit quaternion q to model a rotation through an angle of ? around the aforementioned axis ? ? v can be derived using an extension of Euler's formula:</p><formula xml:id="formula_5">tities: 1) A unit vector ? ? v indicating the direction of the axis of rotation, i.e., ? ? v = (v x , v y , v z ) = (sin ? cos ?, sin ? sin ?, cos ?), where ? ? [0, ?] and ? ? [0,</formula><formula xml:id="formula_6">q = e ? 2 (vxi+vyj+vzk) = cos ? 2 +sin ? 2 * (v x i+v y j+v z k),<label>(5)</label></formula><p>where i, j, k are imaginary units of the quaternion representation, which satisfies the condition i 2 = j 2 = k 2 = ijk = ?1. Unlike real/complex numbers, the multiplication of quaternions (Hamilton product) is sensitive to the orders as we have:</p><formula xml:id="formula_7">ij = k, ji = ?k, jk = i, kj = ?i, ki = j, ik = ?j. For Q 1 = a 1 + b 1 i + c 1 j + d 1 k and Q 2 = a 2 + b 2 i + c 2 j + d 2 k, their Hamilton product is: Q 1 ? Q 2 = a 1 a 2 ? b 1 b 2 ? c 1 c 2 ? d 1 d 2 +(a 1 b 2 + b 1 a 2 + c 1 d 2 ? d 1 c 2 )i +(a 1 c 2 ? b 1 d 2 + c 1 a 2 + d 1 b 2 )j +(a 1 d 2 + b 1 c 2 ? c 1 b 2 + d 1 a 2 )k (6) A 3-D Euclidean vector ? ? w with the coordinate (x, y, z)</formula><p>can be expressed as a pure quaternion (meaning the real part of quaternion is zero), i.e., W = xi + yj + zk, giving the following theorem <ref type="bibr" target="#b5">(Jia, 2019)</ref>: Theorem 1 Given a 3-D Euclidean vector ? ? w and its counterpart in the quaternion space W, the desired rotation axis ? ? v , the magnitude of the rotation ?, the destination coordinate of the vector after the rotation, i.e.,W = x i + y j + z k, can be calculated by the Hamilton product of quaternions:</p><formula xml:id="formula_8">W = qWq ?1 (7) where q ?1 is the inverse of q, i.e., q ?1 = e ? ? 2 (vxi+vyj+vzk) = cos ? 2 ? sin ? 2 * (v x i + v y j + v z k).</formula><p>The form of Eq.7 and a factor of 1 2 for the angle ? in Eq.5 indicate that there is a 2 : 1 homomorphism from quaternions of unit norm to SO(3). Considering each 3-D Euclidean vector can also be expressed as a pure quaternion, we can now represent the rotation using a matrix R(q) by expanding Eq.7 and letting C = cos ? and S = sin ?:</p><formula xml:id="formula_9">? ? w = R(q) ? ? w = ? ? C + v 2 x (1 ? C) vxvy(1 ? C) + vzS vxvz(1 ? C) ? vyS vxvy(1 ? C) ? vzS C + v 2 y (1 ? C) vyvz(1 ? C) + vxS vxvz(1 ? C) + vyS vyvz(1 ? C) ? vxS C + v 2 z (1 ? C) ? ? ? ? x y z ? ?<label>(8)</label></formula><p>In our framework, two rotations can be combined into one equivalent rotation operation (this is also consistent with the closure property of group theory). In other words, we can define q = q 2 q 1 , where q corresponds to the rotation q 1 followed by the rotation q 2 . Therefore, a series of rotations can be composed together and then applied as a single rotation. Note that quaternion multiplication is not commutative unless q 1 and q 2 share the same rotation axes (i.e., ? ? v 1 = ? ? v 2 ), which can be seen from Eq.6. This makes it possible to model both commutative and non-commutative relation patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Integrating the Scaling Operation</head><p>In a knowledge graph, different entities may have different level of semantic hierarchy given a particular relation. For example, in WN18RR, trade is a hypernym of transaction, and man is recorded to has_part to be arm. In these cases, the head entity and tail entity show different abstraction levels or showing inclusion relationships. Intuitively, we argue that the difference of semantic hierarchy can be reflected by the scale of entity, as the entities possessing same level of abstraction tend to be achieved through rotation operations.</p><p>To define this intuition mathematically, we first obtain of norm of quaternions. Following Eq.5 and letting q to be the unit quaternion, an arbitrary quaternion with non-unit norm can be written as: Q = a + bi + cj + dk = |Q|q, with the norm given by</p><formula xml:id="formula_10">|Q| = a 2 + b 2 + c 2 + d 2 (9) where a = |Q| cos ? 2 , b = |Q| sin ? 2 sin ? cos ?, c = |Q| sin ? 2 sin ? sin ?, d = |Q| sin ? 2 cos ?.<label>(10)</label></formula><p>By multiplying a scalar |Q| in the Eq.8, we can further introduce length as another degree of freedom to better match the ground-truth tail embedding vector <ref type="figure">(Figure 1(a)</ref>,</p><p>Step 2). Formally, we have:</p><formula xml:id="formula_11">? ? w = |Q|R(q) ? ? w = O(Q) ? ? w, where Q ? H, ? ? w, ? ? w ? R 3 ,<label>(11)</label></formula><p>where O(Q) = |Q|R(q) is the combined operator of rotation and scaling transformations, H denotes the quaternion algebra, and R 3 represents the 3-D Euclidean algebra. Here we call |Q| the scaling f actor. Therefore, we now have a uniform framework with interpretable geometric meaning, i.e., (|Q|, ?, ?, ?) to describe the transformation corresponding to a specific relation type. We can also define the reverse operation O(Q ?1 ) = |Q| ?1 R(q ?1 ), which describes the reverse process: rotate a vector about the axis ? ? v with angle ?? (from another direction), and then scale the vector with a factor of |Q| ?1 . Combining the Eq.8 and Eq.11, we can always find a operator</p><formula xml:id="formula_12">O(Q 3 ) = O(Q 2 )O(Q 1 )</formula><p>, which corresponds to the application of O(Q 1 ) followed by the application of O(Q 2 ), where we have |Q 3 | = |Q 1 | * |Q 2 | and R(q 3 ) = R(q 2 )R(q 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Score Function and Optimization</head><p>A score function aims to correctly measure the plausibility of a triple of interest. Formally, as a distance-based model, our scoring function is defined as</p><formula xml:id="formula_13">f r (h, t) = ? 1 2 (|O(r)h?t|+|O(r ?1 )t?h|).<label>(12)</label></formula><p>Here, | ? | denotes the Euclidean distance and O(?) stands for the transformation conducted on each element of the entity embeddings. That is to say, for the i-th embedding unit of h, the optimization target is to minimize the Euclidean distance between t i and O(r i )h i , as well as the Euclidean distance between h i and O(r ?1</p><formula xml:id="formula_14">i )t i , where r i ? H, h i , t i ? R 3 .</formula><p>H and R 3 stand for the quaterion and 3-D Euclidean algebra, respectively. The arrow of h i and t j are omitted for clarity. To properly train the model parameters, here we use a loss function similar to the self-adversarial negative sampling loss proposed in <ref type="bibr" target="#b16">(Sun et al., 2019)</ref>:</p><formula xml:id="formula_15">L = ? log ?(? + f r (h, t)) ? n j=1 p(h j , r,t j ) log ?(?(? + f r (h (j) ,t (j) ))),<label>(13)</label></formula><p>where ? is a fixed margin, n is the number of negative sampling size, (h j , r,t j ) is the j-th negative triplet of the fact (h, r, t), and ? is the sigmoid function.h (j) andt (j) are the embeddings corresponding to the negative triplet (h j , r,t j ). p(h j , r,t j ) is the weight of the negative sample, which gives the higher scored negative samples with larger weight during training. The details about self-adversarial negative sampling technique can be found in <ref type="bibr" target="#b16">(Sun et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Settings</head><p>Datasets and evaluation metrics The experiments are conducted mainly on three commonly used benchmark datasets, including WN18RR, FB15k-237 and YAGO3-10. As pointed out by <ref type="bibr" target="#b17">(Toutanova and Chen, 2015a;</ref><ref type="bibr" target="#b2">Dettmers et al., 2018)</ref>, WN18 and FB15k suffer from the test set leakage problem. One can predict missing links and attain the state-of-the-art results even using a simple rule-based model. To avoid this issue, two much more challenging datasets (WN18RR <ref type="bibr" target="#b2">(Dettmers et al., 2018)</ref> and FB15k-237 (Toutanova and Chen,  <ref type="table" target="#tab_0">WN18RR  40943  11  86835  3034  3134  FB15k-237  14541  237  272115  17535  20466  YAGO3-10 123182  37  1079040  5000  5000   Table 2</ref>: Performance comparison on benchmark datasets. Best results are labeled in bold and the second best are underlined. The reporting scheme generally follows that in <ref type="bibr" target="#b14">(Ruffinelli et al., 2019)</ref>. First indicates the originally reported performance of each method. Enhanced records the improved performance with tuned training techniques and hyperparameters by <ref type="bibr" target="#b14">(Ruffinelli et al., 2019)</ref>. Recent shows the best results of more selected recent models. Adv+Recip reports the model performance using the same training scheme of DensE, i.e., using self-adversarial negative sampling and reciprocal learning. Ours reports the performance of DensE as well as its ablation counterparts. In addition, we also use the YAGO3-10 (Mahdisoltani et al., 2013) dataset, which consists of a large collection of triplets from multilingual Wikipedia. These three datasets aim to assess the model performance on composition patterns. The main relation patterns of them are symmetry/anti-symmetry and composition. The basic statistics of the datasets are provided in <ref type="table" target="#tab_0">Table 1</ref>. Here, we report mean reciprocal rank (MRR) and Hits at 10 (H@10) for evaluation (the higher, the better), which is consistent with <ref type="bibr" target="#b14">(Ruffinelli et al., 2019)</ref>. Other performance metrics are provided in the Supplementary Material.</p><p>Baselines We mainly compare DensE with topperforming baseline models for KG link prediction, including both translational model and semantic matching model. As the early implementation of the baseline models may lack thorough configuration tuning or advanced learning techniques, direct compassion with these performances (denoted as First) may be biased to later methods. Therefore, for early models such as RESCALL <ref type="bibr" target="#b13">(Nickel et al., 2011)</ref>, TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref>, Dist-Mult <ref type="bibr" target="#b26">(Yang et al., 2014)</ref> and ComplEx <ref type="bibr" target="#b19">(Trouillon et al., 2016a)</ref>, we also provide two improved versions, including Enhanced, which was obtained through a sophisticated hyperparameter tuning procedure by <ref type="bibr" target="#b14">(Ruffinelli et al., 2019)</ref> and Adv+Recip, which is obtained by us using the same selfadversarial negative sampling and reciprocal learning as for DensE. These two improved versions are provided to prompt the fairness of the comparison.</p><p>In addition to these methods, we also compare our model with more recently proposed KGE models (denoted as Recent), such as RotatE <ref type="bibr" target="#b16">(Sun et al., 2019)</ref>, QuatE <ref type="bibr" target="#b5">(Jia, 2019)</ref>, D4-STE <ref type="bibr" target="#b23">(Xu and Li, 2019)</ref>, TuckER <ref type="bibr" target="#b0">(Bala?evi? et al., 2019)</ref>, Rotate3D <ref type="bibr" target="#b3">(Gao et al., 2020)</ref> and HAKE <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref>. All these recent methods have included some advanced training techniques similar to <ref type="bibr" target="#b14">(Ruffinelli et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We use the Adam optimizer and tune the hyperparameters on the validation dataset. During training, we adopt a similar reciprocal learning approach as used in <ref type="bibr" target="#b6">(Lacroix et al., 2018;</ref><ref type="bibr" target="#b28">Zhang et al., 2019a)</ref>. Early stopping is applied based on the performance on the validation dataset every 1,000 steps. The ranges for hyperparameter grid search and the best hyperparameter settings are listed in Supplementary Note 2. All the parameters are randomly initialized from the interval</p><formula xml:id="formula_16">[? 1 ? 2k , 1 ? 2k ],</formula><p>where k is the embedding size.</p><p>6 Results and Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Prediction Performance</head><p>We report the link prediction results on the three benchmark datasets in <ref type="table">Table 2</ref>. On WN18RR, we show that DensE performs on par with HAKE and outperforms most other models on both the metrics, even after the baseline models are improved by hyper-parameter tuning or using advanced learning techniques. On FB15k-237, we show that the performance of DensE is superior to most of the baseline models, including RotatE and QuatE. While on this dataset we notice a particular good performance of a method called TuckER <ref type="bibr" target="#b0">(Bala?evi? et al., 2019)</ref>, this method also shows a significantly inferior performance on WN18RR, suggesting a potential drawback in generalizability. On YAGO3-10, DensE also shows a significant margin over RotatE and D4-STE (a KGE method based on 2D finite (non-)Abelian group), and also ComplEx when using the comprehensive metric MRR, which further demonstrates the superiority of DensE on various types of datasets. We also provide additional performance metrics, i.e., MR, MRR, Hits at 1 (H@1), Hits at 3 (H@3), and Hits at 10 (H@10) in Supplementary Note 3 ( <ref type="figure" target="#fig_1">Supplementary Tables 2, 3</ref>, and 4).</p><p>Then we carefully compare the performance of DensE to two recent extension of RotatE models, namely RotatE3D <ref type="bibr" target="#b3">(Gao et al., 2020)</ref> and HAKE <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref>. We find that DensE performs better than Rotate3D in most cases, validating the contribution of the scaling operation. On the other hand, HAKE and DensE generally perform comparably. After dissecting into the training details, we find that different from DensE and most other translation distance model, HAKE pays more attention to model the hierarchical nature of knowledge graphs. Firstly, unlike the score function of DensE, which directly optimize the Euclidean distance between two vectors in the 3D space, HAKE decomposes the score function into two part: 1. The modulus-distance part corresponds to the hierarchy of the knowledge graph; 2. The phase-distance part corresponds to the rotation operation in the 2D space. HAKE leverages a taskspecifically calibrated loss term by tuning the relative contributions of two terms in its score function manually to make it get better performance in the dataset with a clear hierarchical structure. Therefore, for a dataset like WN18RR with a majority of types of relations that link two entities at differ-  shows the results for WN18RR dataset. The corresponding results for FB15K-237 is shown in (c) and (d). All the results are achieved using the same setting (Adv+Recip) as described above. ent levels of the hierarchy, HAKE can get better performance than DensE. The FB15k-237 dataset has more complex relation types (237 types of relation) and fewer entities (higher average degree of vertices) than WN18RR and YAGO3-10. The advantage of tuning relative contributions of two terms in the score function manually does not exist anymore, while the information of hierarchy can be learning automatically in our DensE with adaptive semantic hierarchy. That's why we outperform HAKE in FB15k-237 dataset. Secondly, as mentioned in the HAKE, it has two versions of the score function to model the modulus-distance part. We report the results of two versions of HAKE in Table 2, labeled by HAKE 1 and HAKE 2 , respectively. Version 1 has a clear and simple mathematical form that models rotation in the 2D space. Compared with version 1, a bias and re-scaling operation on relational embedding are introduced into the model in version 2 (An additional freedom to tune embeddings of relation, thus the element of the embedding of relation is a 3D vector). For the YAGO3-10 dataset which is more complicated than WN18RR and also has a clear semantic hierarchy property, DensE outperforms HAKE 1 and get comparable result with HAKE 2 . The above two techniques proposed by HAKE do improve the ability to model the semantic hierarchy and complement its lack of expressiveness in rotation operation. We believe these techniques will also be important tricks that can be used to improve performance in future studies (just like the self-adversarial negative sampling technique).</p><formula xml:id="formula_17">(a) ? has_part + ? part_of (b) |Q has_part | * |Q part_of | (c) |Q has_part | (d) |Q part_of | (e) ?(O(r2)O(r1)) ? ?(O(r1)) (f) ?(O(r2)O(r1)) ? ?(O(r3)) (g) ?(O(r1)) ? ?(h)</formula><p>To confirm the source of performance gains, we conduct a further analysis that compares the MRR performance of DensE to RotatE on each relation type of WN18RR <ref type="table" target="#tab_2">(Table 3)</ref>. Besides the taxonomy mentioned in Section 3, relations in the WN18RR dataset can be also divided into two categories: (a) relations that link two entities in the same semantic hierarchy (e.g., "similar_to"); (b) relations that link two entities at different levels of the hierarchy (e.g., "has_part"). One can see that most of the relations that fall into category (b) are also overlap with composite relation patterns. Intriguingly, we notice a large performance increase in composite relations, as exemplified by hypernym, the most abundant composite relation in test data. We show that DensE improves MRR on this relation by as much as 3.3%. These results indicates particular advantages of DensE in modeling composition relation patterns and semantic hierarchies of knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>To examine the effectiveness of each module in our model, we perform a series of ablation experiments <ref type="table">(Table 2)</ref>. On WN18RR dataset, the most significant performance decrease occurs when we cancel the scaling operation, i.e., only model the relation as rotations. This confirms the contribution from scaling to the whole model. On FB15K-237 and YAGO3-10 datasets, we also observe a large drop in performance when removing the scaling operation. Also, self-adversarial negative sampling (adv) shows significant contribution, indicating the necessity to incorporate proper training techniques. In Supplementary Note 4, we also compared DensE and RotatE models without self-adversarial negative sampling and confirmed the superiority of DensE in this setting. Note that HAKE also uses the self adversarial technique in training. However, the ablation results are not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Computational Complexity</head><p>We show that compared with high-performance models such as RotatE and HAKE, DensE is generally more computationally efficient in terms of parameter number and training epochs. For TuckRE, although it can get better performance on FB15k237 with a relatively small model size, it needs much larger training epochs than other models on both WN18RR and FB15k237 datasets. We find that TuckRE needs roughly 500 epochs to converge to its best results. We plot the results of the first 100 epochs in <ref type="figure" target="#fig_4">Figure 3</ref>(b) and <ref type="figure" target="#fig_4">Figure 3(d)</ref> here. As is shown in <ref type="figure" target="#fig_4">Figure 3</ref>, when compared with other baseline models, DensE achieves significantly higher performance with the same parameter size or epoch number on both WN18RR and FB15K-237 dataset. When comparing the time efficiency, as different models have different training time per epoch, here we also report the training time of DensE and other baseline models. We show that although the more complex math formulation may cause longer training time, the resulting betterdesigned model can lead to a much faster convergence speed that significantly shortens the total training time under the same machine condition <ref type="table" target="#tab_3">(Table 4</ref>). On the other hand, if we let models have similar performance, e.g., only make DensE reach the final performance of RotatE, we can see it only needs 10 epochs and uses 27% of RotatE's training time. These results indicate a clear advance of DensE in computation efficiency. We reason that this is mainly achieved by introducing a decoupled scaling operation, thus lowering the embedding dimension required in rotation-only modeling.  In this section, we first discuss mathematically how DensE provides geometric interpretation of relation patterns including symmetry, antisymmetry, inversion and composition. According to the convention of axis-angle representation described in Section 4.2, all angle-related parameters (?, ?, and ?) are restricted to be in the half-closed intervals as we mentioned before. The positive direction of rotation is based on the right-handed coordinate system, the rotation angle with the minus value indicates a rotation opposite to the positive direction. To keep the values of angle to be within the above interval, we relocate angles outside intervals into the desired regions by leveraging the periodic property of the rotation system. To begin with, a relation r is symmetric in DensE if and only if each dimension of its embedding r i satisfies |r i | = 1 and the rotation angle satistifies ? r i = 0 or ?. For anti-symmetry relation pattern, the embedding r i satisfies |r i | = 1 , but the rotation angle ? r i should be neither 0 nor ?. Also, two relations r 1 and r 2 are in inverse pattern, if and only if they sat-isfy: |r 1i | * |r 2i | = 1, ? r 1i = ? r 2i , ? r 1i = ? r 2i and ? r 1i + ? r 2i = 2?, meaning the embeddings of these two relations share the same rotation axes, but rotate in two opposite directions.</p><p>As discussed in Section 4.2, the commutative and non-commutative composition patterns can be naturally modeled by the guarantee of the property of group theory, which covers Property 1 of composite relations. Also, following the intuition of Property 2, our model does not enforce a uniform mode of each element in relation representations. Instead, it learns to model the interaction between relations and entities as well as the ambiguity in composition pattern inference, leading to a disperse distribution in the relation embedding space. Last but not least, our model can smoothly deal with constraints posed by relation types in inferring composition patterns, as stated in Property 3. For instance, when modeling the pattern r 1 (x, y)?r 2 (y, z) ? r 2 (x, z), the representation from RotatE tends to degenerate to a trivial case where the rotation angle of r 1 and r 2 both set to be 0 or 2? or r 1 = 2?, r 2 = ?. In DensE, since the entity embeddings are not required to be perpendicular to the rotation axis, it can also place the embedding of entity x to be collinear with the rotation axis of r 1 . In this way, the rotation axis of r 1 and r 2 are not required to be the same, making the model to more expressive. In another example, as for the pattern r 1 (x, y)?r 1 (y, z) ? r 2 (x, z), besides capturing the relationship of the two rotation angles (i.e., ? r 2i = 2? r 1i ), the scaling transformation offers an additional degree of freedom, where our model tends to give |r 2i | = |r 1i | 2 . Again, we point out that these "rules" are not constant solutions, as the information entities will further guide the model to deviate from the statistical mode for better accommodation of each triplet. Other composition patterns presented in the Property 3 can be analyzed in a similar way (see Supplementary Note 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Case Studies</head><p>Here, we show several examples to illustrate the geometric insight given by DensE, which basically reflects the geometric intuition discussed above.</p><p>We start our analysis with inverse relations, which comes from the original WN18 dataset (We have also confirmed the good prediction performance of DensE on the WN18 dataset in Supplementary Note 6). In <ref type="figure" target="#fig_5">Figure 4(a)</ref>, we show the distribution of element-wise addition of embeddings from two inverse relations (has_part and part_of ) of ?, one representative degree of freedom in modeling relational rotation (Other degrees of freedom can be found in the Supplementary Note 5). In this way, we can visualize how the two embeddings agree with each other. As the two relations are fully inferable by each other, we do observe a clear conjugation as expected in Section 7.1. This is also reflected in the representation of scaling from these two relations, where the element-wise products tend to be one <ref type="figure" target="#fig_5">(Figure 4(b)</ref>). Interestingly, we do observe that two complementary embeddings are learned these two relations <ref type="figure" target="#fig_5">(Figure 4(c)-(d)</ref>). In particular, the relation whose head entity has a higher semantic hierarchy (i.e., has_part) tend to show a scaling norm |Q| larger than one, while the relation whose head entity has a lower semantic hierarchy (i.e., part_of ) generally shows a scaling norm |Q| smaller than one. This clearly verifies the intuition of introducing the scaling operation do capture relation-specific semantic hierarchy of entities.</p><p>For composition patterns, we slightly change the experiment protocol, with each histogram show-ing the element-wise difference between the embeddings of a composite relation and the embeddings calculated by multiplying each relation in the relation path. In a case from WN18RR, we demonstrate how DensE models a composition pattern for</p><formula xml:id="formula_18">r 1 (h, h )?r 2 (h , t) ? r 1 (h, t),</formula><p>where r 1 = derivationally_related_f orm,</p><formula xml:id="formula_19">r 2 = hypernym.</formula><p>This is a typical case where the composite relation equals the first relation in the relation path. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(e), while most embedding dimensions still agree well between the actual composite relation and the calculated relation path, the distribution tends to disperse to a large range, indicating the existence of ambiguity and interaction between entities and relations. To further explore the ambiguity issue in the above case, we perform the same analysis on another small portion of triplets that actually give</p><formula xml:id="formula_20">r 1 (h, h )?r 2 (h , t) ? r 3 (h, t) where r 3 = synset_domain_topic_of.</formula><p>Interestingly, we also observe that part of embedding dimensions of O(r2)O(r1) are aligned with O(r3) (embedding difference close to zero), demonstrating the flexibility of our model to capture potentially ambiguous relation compositions <ref type="figure" target="#fig_5">(Figure 4(f)</ref>). On the other hand, the model can also learn to put the rotation axis of r 1 collinear with the embedding of head entities h in the composition mode expressed as</p><formula xml:id="formula_21">r 1 (h, h )?r 2 (h , t) ? r 1 (h, t)</formula><p>reflecting the interaction of entities and relations <ref type="figure" target="#fig_5">(Figure 4(g)</ref>). This example clearly demonstrates the interpretability of DensE in modeling complex composition patterns. The geometric patterns for other relation patterns can be found in Supplementary Note 5. Together with the properties discussed in Section 3, here we clearly demonstrate the pros and cons of the current rotation-based translational KGE method in modeling composition relation patterns.</p><p>In this work, we propose an effective method, named DensE, for knowledge graph embedding. DensE decomposes a relation operator into an SO(3) group-based rotation as well as a scaling transformation. Extensive experiments show that DensE possesses good performance in knowledge completion with high computational efficiency. Also, DensE provides a straightforward geometric interpretation for the relations, leading to meaningful insights for the future work for modeling complex relation patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Left: A rotation about axis-z followed by a rotation about axis-x, the initial vector h is placed along axis-x. It can be seen that the final state is along axis-z, and two rotation operations are equivalent to one operation with the rotation axis to be about axis-y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The geometrical interpretation of non-commutative compositions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right:</head><p>The rotation operation sequence is reversed from the left <ref type="figure">figure.</ref> A rotation about axis-x is followed by a rotation about axis-z. The final state is then changed to be along axis-y. Since the initial vector is collinear with the first rotation axis, the two rotation operations are equal to the last rotation (rotation about axis-z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters setting</head><p>The ranges of the hyperparameters for the grid search are set as follows: Embedding size k ? {100, 200, 500, 1000} (In our model, each entity is represented with a matrix with a size of 3 ? k, and each relation with a matrix with a size of 4 ? k), batch size b ? {256, 512, 1024}, fixed margin ? ? {3.0, 6. <ref type="bibr">0, 9.0, 12.0, 15.0, 24.0, 30</ref>.0}, negative sampling size n ? {256, 512, 1024}, self-adversarial sampling temperature ? ? {0.3, 0.5, 1.0}. The initial learning rate ? is set to be 0.1, and it decays with a factor of 1/2 if the training loss does not decrease in 1000 epochs. We list the best hyperparameters setting of DensE on the benchmark datasets in <ref type="table" target="#tab_4">Supplementary Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional performance metrics</head><p>For a more complete comparison of each method, for each dataset we list MR, MRR, H@1, H@3, and H@10 in Supplementary Tables 6, 7, and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Effect of self-adversarial negative sampling on DensE and RotatE</head><p>In the ablation study, we observe a significant contribution of the self-adversarial negative sampling technique on the prediction performance of FB15k-237 and YAGO3-10. Therefore, we compare our model with RotatE in the setting where both models are trained without self-adversarial negative sampling ( <ref type="table" target="#tab_8">Supplementary Table 9</ref>). These results further confirm the superiority of our model without self-adversarial negative sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Capability of DensE in modeling relation patterns</head><p>In this Section, we provide a detailed analysis on how our method tend to model each relation pattern in an interpretable way. In our experiment, we calculate the statistical rule of each degree of freedom to reflect the effect of specific relation patterns. In addition, we also sometimes compare the embeddings of two relation types (or a relation     type and an entity) per element, i.e., we perform element-wise addition, subtraction, multiplication on each embedding dimension. Then, we use the distribution of these results to demonstrate how the two compared embeddings agree with each other. Note that below we use an addition subscript i to denote each dimension in the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Symmetry/anti-symmetry pattern</head><p>As pointed out in main text Section 7.1, for the symmetry relation pattern, the scaling factor |Q| of symmetric relation tend to be one, and the rotation angle ? should be 0 or ? in [0, 2?). For antisymmetry relation pattern, one can easily check that the scaling factor |Q| should also be one, but the rotation angle ? should be neither 0 nor ? in the range of [0, 2?). Here we show the distributions of rotation angle ? and scaling factor |Q| of four relations with symmetry pattern in WN18RR (Supplementary <ref type="figure" target="#fig_7">Figure 6</ref> (a)-(h)). We also show distributions of anti-symmetry relation pattern "mem-ber_meronym" in Supplementary Figure 6 (i)-(j).</p><p>As we can see that the scaling factor is roughly around one. For the rotation angle ?, there are just few elements fall into the bin that contains ?. It should be noted that since the embedding size k in our model for WN18RR is set to be 300, the sum of frequency in these distributions also equals to 300. sis can be done for this case. The relevant results are shown in <ref type="figure">Supplementary Figure 9(d)</ref>, (e), and (f), respectively.</p><formula xml:id="formula_22">(a) |Q derivationally_related_f orm | (b) ? derivationally_related_f orm (c) |Q also_see | (d) ? also_see (e) |Q similar_to | (f) ? similar_to (g) |Q verb_group | (h) ? verb_group (i) |Q member_meronym | (j) ? member_meronym</formula><p>Situation 2: r 1 and r 2 are the same, but not equal to r 3 . An example is given by the triangle in green with three triplets: (T rade(V B), derivationally_related_f orm, Selling), (Selling, derivationally_related_f orm, Sell) and (T rade(V B), verb_group, Sell).</p><p>As mentioned in main text Section 7.1, we expect the model to learn ? verb_group = 2? derivationally_related_f orm and |Q verb_group | = |Q derivationally_related_f orm | 2 , both of which are confirmed in this case. Besides this, we also observe that the rotation axes of the two relations are aligned on some dimensions, i.e, ? verb_group = ? derivationally_related_f orm and ? verb_group = ? derivationally_related_f orm . The corresponding distributions are shown in <ref type="figure" target="#fig_9">Supplementary Figure 10</ref>, where we have r 1 = r 2 = derivationally_related_f orm and r 3 = verb_group.</p><p>Situation 3: r 1 and r 3 are the same, but not equal to r 2 , e.g., the triangle in blue color with three triplets set: (T rade(V B),derivationally_related_f orm , T rade(N N )), (T rade(N N ), hypernym, transaction) and (T rade(V B), derivationally_related_f orm, transaction).</p><p>Here we compare the difference between the embedding of a composite relation and the embedding calculated by multiplying each relation in the relation path. We have already shown the distribution of ? in the main context. Other distributions are shown in <ref type="figure">Supplementary Figure 11</ref>, where we have r 1 = r 3 = derivationally_related_f orm and r 2 = hypernym.</p><p>Situation 4: r 2 and r 3 are the same, but not equal to r 1 , e.g., the triangle in purple color with three triplets set: (T rade(V B), hypernym, transact), (transact, derivationally_related_f orm, transaction) and (T rade(V B), derivationally_related_f orm, transaction).</p><p>Here we compare the difference between the embedding of a composite relation and the embedding calculated by multiplying each relation in the relation path.</p><p>Related distributions are shown in <ref type="figure" target="#fig_1">Supplementary Figure 12</ref>, where we have r 1 = hypernym and r 2 = r 3 = derivationally_related_f orm. In this case, we see that the composition pattern given by O(r2)O(r1) is learned to have similar scaling factor |Q| and rotation magnitude ? with r 2 , and the rotation axes (?, ?) are also partially aligned.</p><p>Situation 5: r 1 , r 2 and r 3 are mutually different relations, e.g., the triangle in red color with three triplets set: (T rade(V B), derivationally_related_f orm, Selling), (Selling, hypernym, mercantilism) and (T rade(V B), synset_domain_topic_of , mercantilism). Here we compare the difference between the embedding of a composite relation and the embedding calculated by multiplying each relation in the relation path. Related distributions are shown in <ref type="figure" target="#fig_4">Supplementary Figure 13</ref>, where we have r 1 = derivationally_related_f orm, r 2 = hypernym and r 3 = synset_domain_topic_of . The reason for relatively large dispersion here is discussed in the main text, i.e., the majority of triplets exemplify derivationally_related_f orm ? hypernym ? derivationally_related_f orm (See also the discussion above of Situation 3), while only a small portion have the pattern derivationally_related_f orm ? hypernym ? synset_domain_topic_of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Link prediction results on WN18 dataset</head><p>In the main text, we report the MRR and H@10 performance on WN18RR, FB15K237 and YAGO3-10. Here, we also report our model's performance on WN18 ( <ref type="table" target="#tab_0">Supplementary Table 10</ref>), from which inverse relations are extracted by the demonstration of geometric interpretation of DensE (main text <ref type="figure" target="#fig_4">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Variance of the prediction performance</head><p>The mean values and corresponding variance of MRR on WN18, WN18RR, FB15k-237 and YAGO3-10 datasets are shown in Supplementary     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) DensE decomposes a relation into a rotation operator and a scaling operator on the head entity h in 3-D Euclidean space. (b) Examples of composition patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>RotatE models relations as a unit rotation operator in the 2-D Euclidean space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2?); and 2) An angle ? describing the magnitude of the rotation about the rotation axis, where ? ? [0, 2?). Given an entity vector ? ? w in the 3-D space with the coordinate (x, y, z), its rotation about axis ? ? v with an angle of ? can be modeled using the SO(3) group theory (Figure 1(a), Step 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>et al., 2018) 0.430 0.490 0.241 0.419 0.340 0.540 ComplEx (Dettmers et al., 2018) 0.440 0.510 0.247 0.428 0.360 0.550 Enhanced RESCAL (Ruffinelli et al., 2019) 0.467 0.517 0.357 0Zhang et al., 2020)0.497 0.584 0.336 0.533 0.522 0.693 HAKE 2<ref type="bibr" target="#b29">(Zhang et al., 2020)</ref> 0.497 0.582 0.346 0.542 0.545 0were released. WN18RR comes from WordNet<ref type="bibr" target="#b9">(Miller, 1995)</ref>, compared with the previous version WN18, it removes inverse relations to provide a more realistic KGE method benchmark. Similarly, the FB15k-237 dataset is also extracted from the original Freebase dataset FB15K<ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> by removing inverse relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The effect of trainable parameter size and training epoch number on model performances. (a) and (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Geometric interpretation provided by DensE. Each histogram shows a distribution of each dimension of the learned embeddings. Angular parameters are in radian units. (a)-(d) A case study for inversion patterns (Other degrees of freedom can be found in the Supplementary Note 5). (e)-(f) A case study for composition patterns, reflecting the scenario of r 1 (h, h )?r 2 (h , t) ? r 1 (h, t) and r 1 (h, h )?r 2 (h , t) ? r 3 (h, t), respectively. ?(?) denotes the rotation angle about the rotation axis of a relational operator. (g) Collinearity of entity and relation embedding. ?(O(r1)) is the ? component of relation r 1 . ?(h) is the ? value of the head entity h's embedding in the spherical coordinate system of the 3-D Euclidean space. All the head entities satisfying r 1 (h, h )?r 2 (h , t) ? r 1 (h, t) are included for the analysis. The entities and relations involved can be found in the Supplementary Note 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>A simple example for how rotation in the 3-D Euclidean space can model non-commutative relations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Geometric interpretation of how DensE models symmetry patterns and anti-symmetry patterns. Each row shows the distribution of |Q| and ? for a given relation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )Figure 9 :</head><label>a9</label><figDesc>|Q has_part | (b) ? has_part (c) ?(O(r1)) ? ?(h) (has_part) (d) |Q hypernym | (e) ? hypernym (f) ?(O(r1)) ? ?(h) (hypernym)Geometric interpretation of composition patterns in Situation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>(a) Q(O(r3) ? Q 2 (O(r1)) (b) ?(O(r3)) ? 2 * ?(O(r1)) (c) ?(O(r3)) ? ?(O(r1)) (d) ?(O(r3)) ? ?(O(r1))Geometric interpretation of composition patterns in Situation 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Q(O(r2)O(r1)) ? Q(O(r1)) (b) ?(O(r2)O(r1)) ? ?(O(r1)) (c) ?(O(r2)O(r1)) ? ?(O(r1)) (d) ?(O(r2)O(r1)) ? ?(O(r1))Figure 11: Geometric interpretation of composition patterns in Situation 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>(a) Q(O(r2)O(r1)) ? Q(O(r2)) (b) ?(O(r2)O(r1)) ? ?(O(r2)) (c) ?(O(r2)O(r1)) ? ?(O(r2)) (d) ?(O(r2)O(r1)) ? ?(O(r2))Geometric interpretation of composition patterns in Situation 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Q(O(r2)O(r1)) ? Q(O(r3)) (b) ?(O(r2)O(r1)) ? ?(O(r3)) (c) ?(O(r2)O(r1)) ? ?(O(r3)) (d) ?(O(r2)O(r1)) ? ?(O(r3))Figure 13: Geometric interpretation of composition patterns in Situation 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets used in this study.</figDesc><table /><note>Dataset # Entities # Relations #Training #Validation #Test</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>MRR comparison on each relation type of WN18-RR. Performance increases are in parentheses.</figDesc><table><row><cell>Relation type</cell><cell>Relation Name</cell><cell cols="2">% in test data RotatE</cell><cell>DensE</cell></row><row><cell></cell><cell>derivationally_related_form</cell><cell>34%</cell><cell cols="2">0.947 0.955 (+0.008)</cell></row><row><cell>Atomic</cell><cell>also_see verb_group</cell><cell>1.8% 1.3%</cell><cell cols="2">0.585 0.647 (+0.062) 0.943 0.955 (+0.012)</cell></row><row><cell></cell><cell>similar_to</cell><cell>0.2%</cell><cell>1</cell><cell>1 (+0)</cell></row><row><cell></cell><cell>hypernym</cell><cell>39.5%</cell><cell cols="2">0.148 0.181 (+0.033)</cell></row><row><cell></cell><cell>instance_hypernym</cell><cell>4%</cell><cell cols="2">0.318 0.349 (+0.031)</cell></row><row><cell></cell><cell>member_meronym</cell><cell>8.1%</cell><cell cols="2">0.232 0.249 (+0.017)</cell></row><row><cell>Composite</cell><cell>synset_domain_topic_of</cell><cell>3.8%</cell><cell cols="2">0.341 0.412 (+0.071)</cell></row><row><cell></cell><cell>has_part</cell><cell>5.5%</cell><cell cols="2">0.184 0.205 (+0.021)</cell></row><row><cell></cell><cell>member_of_domain_usage</cell><cell>0.8%</cell><cell cols="2">0.318 0.326 (+0.008)</cell></row><row><cell></cell><cell>member_of_domain_region</cell><cell>1%</cell><cell>0.2</cell><cell>0.407 (+0.207)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The comparison of training time of each method on WN18RR. A lower total time results in a higher efficiency. To ensure a fair comparison, here we unify the hyperparateters of each method so that all the models have a similar parameter size around 36M.</figDesc><table><row><cell>Model</cell><cell cols="4">Training time per epoch (s) # of epochs Total training time (s) MRR</cell></row><row><cell>DensE</cell><cell>92</cell><cell>21</cell><cell>1932</cell><cell>0.492</cell></row><row><cell>RotatE</cell><cell>83</cell><cell>40</cell><cell>3320</cell><cell>0.478</cell></row><row><cell>ComplEx</cell><cell>75</cell><cell>62</cell><cell>4650</cell><cell>0.475</cell></row><row><cell>DisMult</cell><cell>52</cell><cell>68</cell><cell>3536</cell><cell>0.444</cell></row><row><cell cols="2">7 Geometric Interpretation</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">7.1 Theoretical Analysis</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters settings of DensE in this study.</figDesc><table><row><cell>Dataset</cell><cell>size k Embedding</cell><cell>size b Batch</cell><cell>? Margin</cell><cell>sample size n Negative</cell><cell>? adv temperature</cell></row><row><cell>WN18</cell><cell>200</cell><cell>512</cell><cell>12.0</cell><cell>1024</cell><cell>0.3</cell></row><row><cell>WN18RR</cell><cell>300</cell><cell>512</cell><cell>6.0</cell><cell>512</cell><cell>0.5</cell></row><row><cell>FB15k-237</cell><cell>800</cell><cell>1024</cell><cell>9.0</cell><cell>256</cell><cell>1.0</cell></row><row><cell>YAGO3-10</cell><cell>200</cell><cell>1024</cell><cell>24.0</cell><cell>512</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on WN18RR. Best results are labeled in bold and the second best are underlined. First indicates the originally reported performance of each method. Recent shows the best results of more selected recent models. Ours reports the performance of DensE. For MRR, the lower, the better; for other metrics, the higher, the better.</figDesc><table><row><cell>WN18RR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison on FB15K-237. Best results are labeled in bold and the second best are underlined. First indicates the originally reported performance of each method. Recent shows the best results of more selected recent models. Ours reports the performance of DensE. For MRR, the lower, the better; for other metrics, the higher, the better.</figDesc><table><row><cell>FB15K-237</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison on YAGO3-10. Best results are labeled in bold and the second best are underlined. First indicates the originally reported performance of each method. Recent shows the best results of more selected recent models. Ours reports the performance of DensE. For MRR, the lower, the better; for other metrics, the higher, the better.</figDesc><table><row><cell>YAGO3-10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="5">: Results of DensE and RotatE without self-adversarial negative sampling training technique, where "adv"</cell></row><row><cell>represents "self-adversarial".</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">WN18 WN18RR FB15k-237 YAGO3-10</cell></row><row><cell cols="2">DensE (w/o adv) 0.950</cell><cell>0.486</cell><cell>0.306</cell><cell>0.452</cell></row><row><cell cols="2">RotatE (w/o adv) 0.947</cell><cell>0.470</cell><cell>0.297</cell><cell>0.439</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Performance comparison on WN18. The performances of RotatE are obtained from the original paper.</figDesc><table><row><cell>Model MR MRR H@1 H@3 H@10</cell></row><row><cell>RotatE 309 0.949 0.944 0.952 0.959</cell></row><row><cell>DensE 285 0.950 0.945 0.954 0.959</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Dr. Y. Wen, Dr. W. Peng, Dr. D. Wang, Dr. W. Guo, Mr. A. Shen and Ms. X. Lin for insightful comments on the manuscript. We also thank Dr. Y. Guo and Ms. C. Jiang for helpful suggestions in the experimental settings. We also thank all the colleagues in AI Application Research Center (AARC) of Huawei Technologies for their supports. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Inversion pattern</head><p>In main text Section 7.1, we assert that if two relations r 1 and r 2 satisfy the inverse pattern, if and only if they satisfy: |r 1i | * |r 2i | = 1, ? r 1i = ? r 2i , ? r 1i = ? r 2i and ? r 1i + ? r 2i = 2?. In Supplementary <ref type="figure">Figure 7</ref>, we show a case of paired relations with inversion pattern from WN18 dataset, namely has_part, part_of . We plot the scaling factor |Q|, magnitude of the rotation ?, and (?, ?) that describe the rotation axis for each relation (first two columns), as well as their element-wise alignment results (the last column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Composition pattern E.3.1 Ambiguity in composition pattern</head><p>In a real-world KG (here we take a sub-graph from WN18RR as an example), due to the ambiguity issue mentioned in the main text (composition pattern Property 2), there exist plenty of examples where a third relation (the composite relation) cannot be inferred given the two participating relations alone ( <ref type="figure">Supplementary Figure 8(a)</ref>). For example, given r 1 = derivationally_related_f orm and r 2 = hypernym, we have the composition pattern as shown with the blue lines: (Trade(VB), deriva-tionally_related_form, Trade(NN)), (Trade(NN), hypernym, transaction) and (Trade(VB), deriva-tionally_related_form , transaction). From these cases, it seems that one can summarize the composition pattern as: r 3 = r 1 = derivationally_related_f orm, i.e., r 1 (h, h )?r 2 (h , t) ? r 1 (h, t). However, we also have the triangle with red lines, i.e., (Trade(VB), derivationally_related_form, Selling), (Selling, hypernym, mercantilism) and (Trade(VB), synset_domain_topic_of, mercantilism). In these cases, it looks like the composition pattern has the form that r 1 (h, h )?r 2 (h , t) ? r 3 (h, t), where r 3 = synset_domain_topic_of . This ambiguity means that the composition mode is not uniform but depends on specific entities and their other neighborhoods. Therefore, in order to give the model sufficient flexibility to learn this, our model does not require all the dimensions in a relation embedding to fit in one single composition mode (e.g., r 1 (h, h )?r 2 (h , t) ? r 1 (h, t) or r 1 (h, h )?r 2 (h , t) ? r 3 (h, t)). In consequence, the learned relation embedding for a composite relation are actually distributed in a disperse manner, with the majority of embedding dimensions following mode r 1 (h, h )?r 2 (h , t) ? r 1 (h, t), and some minor portions following r 1 (h, h )?r 2 (h , t) ? r 3 (h, t), which is consistent with the abundance of each mode in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.2 Case study: a two-hop composition pattern</head><p>As we mentioned in the main text, in a composition pattern, the relations involved are not necessarily different (composition pattern Property 3). Here we discuss the simplest case of a two-hop relation composition. Even under the assumption that the composite relation can be inferred from its composition alone (i.e., an unambiguous composition), there are still five possible situations, as shown in <ref type="figure">Supplementary Figure 8(b)</ref>.</p><p>Here we provide a detailed analysis on these five situations, using real cases in WN18RR as examples ( <ref type="figure">Supplementary Figure 8(a)</ref>).</p><p>Situation 1: r 1 , r 2 and r 3 are all the same, e.g., the triangle in yellow color with three triplets: (man, has_part, arm), (arm, has_part, paw) and (man, has_part, paw). To satisfy the composition relation constraint, in principle the model can adopt several approaches to fit this situation. From the perspective of relation embedding, it can learn to fit |Q has_part | = 1 or ? has_part ? {0, 2?}. Also, in terms of interaction between entities and relations, it can also learn to align the rotation axis of relation with the entity embedding (see <ref type="figure">Supplementary Figure 9(c)</ref>, showing the differences between the ? component of r 1 in the mode r 1 (h, h )?r 1 (h , t) ? r 1 (h, t) and the corresponding head entities h in spherical coordinate system). Note that when the rotation axis aligns with head entity embedding in spherical coordinate system, the composition pattern can be modeled by the scaling factor alone, i.e., |Q has_part | 2 = |Q has_part |, regardless of the rotation magnitude ?.</p><p>In this case, we see the model mainly adopts the first approach ( <ref type="figure">Supplementary Figure 9(a)</ref>), as |Q has_part | is roughly around 1. In addition, the partial alignment between relation rotation axis and entities is also observed ( <ref type="figure">Supplementary Figure 9(c)</ref>). In contrast, for ? has_part , we see very few dimensions satisfy the condition (0 or 2?), which is also consistent with the dominate role of scaling factor in this case. Noticed the fact that the relation "hypernym" dominates the composite patterns (Refer to <ref type="table">Table 3</ref> in the main text), we also find that r 1 = r 2 = r 3 = hypernym pattern exists in the WN18RR dataset. The above similar analy- <ref type="figure">Figure 7</ref>: Geometric interpretation of how DensE models inverse pattern, using an example of (has_part, part_of ) from WN18. At each row, we show the embedding from one degree of freedom of our model. The first two columns show the embeddings of each relation type, and the last column shows the alignment of the two embeddings regarding a specific degree of freedom. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09590</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rotate3d: Representing relations as rotations in three-dimensional space for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00388</idno>
		<title level="m">2020. A survey on knowledge graphs: Representation, acquisition and applications</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Bin</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quaternions. Com S</title>
		<imprint>
			<biblScope unit="volume">477</biblScope>
			<biblScope unit="page">577</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Kagnet: Knowledge-aware</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02121</idno>
		<title level="m">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth Aaai conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Icml</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">You can teach an old dog new tricks! on training knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Seventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<idno>abs/1606.06357</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On multi-relational link prediction with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1709.04808</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On evaluating embedding models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Relation embedding with dihedral group in knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00687</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<idno>FB15k-237 YAGO3-10</idno>
	</analytic>
	<monogr>
		<title level="m">The mean values and variance of MRR on WN18, WN18RR, FB15k-237 and YAGO3-10 datasets. WN18 WN18RR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhancing pre-trained language representations with rich knowledge for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1226</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2346" to="2357" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nage: Non-abelian group embedding for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3411875</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;20</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1905.07129</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
